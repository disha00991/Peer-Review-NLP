{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"name":"politeness.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"897dd6c8"},"source":["# Finding Politeness"],"id":"897dd6c8"},{"cell_type":"code","metadata":{"id":"822e886b"},"source":["import collections\n","import glob\n","import json\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","pd.set_option('display.max_colwidth', None, 'display.max_rows', None, )\n","# checking\n","dataset_dir = \"./datasets/\"\n","\n","#SUBSETS = \"train dev test\".split()\n","SUBSETS = [\"train\"]\n","\n","\n","datasets = collections.defaultdict(list)\n","\n","for subset in SUBSETS:\n","    for filename in glob.glob(dataset_dir + subset + \"/*\"):\n","        with open(filename, 'r') as f:\n","            datasets[subset].append(json.load(f))\n","            \n","all_pairs = sum(datasets.values(), [])\n","\n","def total_and_average_len(list_of_lists):\n","    big_list = sum(list_of_lists, [])\n","    return len(big_list), len(big_list)/len(list_of_lists)\n","\n","def count_dataset(pairs, subset):\n","    # TODO: Add double-annotated and adjudicated\n","    review_total, review_average = total_and_average_len([pair[\"review_sentences\"] for pair in pairs])\n","    rebuttal_total, rebuttal_average = total_and_average_len([pair[\"rebuttal_sentences\"] for pair in pairs])\n","    return {\n","        \"subset\":subset,\n","        \"pairs\": len(pairs),\n","        \"forums\": len(set(pair[\"metadata\"][\"forum_id\"] for pair in pairs)),\n","        \"adjudicated\": len([pair for pair in pairs if pair[\"metadata\"][\"annotator\"] == \"anno0\"]),\n","        \"review_sentences\": review_total,\n","        \"rebuttal_sentences\": rebuttal_total,\n","        \"review_avg_sentences\": review_average,\n","        \"rebuttal_avg_sentences\": rebuttal_average,\n","        \n","    }\n","# Distribution of examples over sets\n","df_dicts = [count_dataset(pairs, subset) for subset, pairs in datasets.items()]\n","df = pd.DataFrame.from_dict(df_dicts)"],"id":"822e886b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4c51927b","outputId":"8c38407b-fbd5-4d40-cba8-8f1a6a8102b8"},"source":["from tools.convokit_politeness import get_convokit_politeness_labels\n","\n","pair = all_pairs[1]\n","\n","all_sentences = {\n","    'metadata': {'review_id': 'xyz'},\n","    'review_sentences': [],\n","    'rebuttal_sentences': []\n","}\n","\n","for pair in all_pairs:\n","    all_sentences['review_sentences'] += pair['review_sentences']\n","    all_sentences['rebuttal_sentences'] += pair['rebuttal_sentences']\n","\n","get_convokit_politeness_labels(pair)['meta.politeness_markers'][0]\n","# pair_df = get_convokit_politeness_labels(all_sentences)\n","# pair_df.head()"],"id":"4c51927b","execution_count":null,"outputs":[{"data":{"text/plain":["{'politeness_markers_==Please==': [],\n"," 'politeness_markers_==Please_start==': [],\n"," 'politeness_markers_==HASHEDGE==': [],\n"," 'politeness_markers_==Indirect_(btw)==': [],\n"," 'politeness_markers_==Hedges==': [],\n"," 'politeness_markers_==Factuality==': [],\n"," 'politeness_markers_==Deference==': [],\n"," 'politeness_markers_==Gratitude==': [],\n"," 'politeness_markers_==Apologizing==': [],\n"," 'politeness_markers_==1st_person_pl.==': [[('we', 1, 19)]],\n"," 'politeness_markers_==1st_person==': [[('i', 1, 10)]],\n"," 'politeness_markers_==1st_person_start==': [],\n"," 'politeness_markers_==2nd_person==': [],\n"," 'politeness_markers_==2nd_person_start==': [],\n"," 'politeness_markers_==Indirect_(greeting)==': [],\n"," 'politeness_markers_==Direct_question==': [],\n"," 'politeness_markers_==Direct_start==': [],\n"," 'politeness_markers_==HASPOSITIVE==': [],\n"," 'politeness_markers_==HASNEGATIVE==': [[('problem', 1, 7)],\n","  [('problem', 1, 25)],\n","  [('problem', 1, 27)]],\n"," 'politeness_markers_==SUBJUNCTIVE==': [],\n"," 'politeness_markers_==INDICATIVE==': []}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"c330bea2","outputId":"709d7ae0-8e9f-4299-a26b-fee8b89da204"},"source":["len(all_pairs)"],"id":"c330bea2","execution_count":null,"outputs":[{"data":{"text/plain":["251"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"003e4ebd","outputId":"fc0f009d-a586-4cde-d515-90485f64ded8"},"source":["rs = pd.json_normalize(all_pairs[1], record_path=['review_sentences'])\n","rs.head()"],"id":"003e4ebd","execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_id</th>\n","      <th>sentence_index</th>\n","      <th>text</th>\n","      <th>coarse</th>\n","      <th>fine</th>\n","      <th>asp</th>\n","      <th>pol</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>B1ez1LvJcB</td>\n","      <td>0</td>\n","      <td>This paper presents a method for adapting a model that has been trained to perform one task, so that it can perform a new task (potentially without using any new training data at all—i.e., zero-shot learning).</td>\n","      <td>arg_structuring</td>\n","      <td>arg-structuring_summary</td>\n","      <td>none</td>\n","      <td>none</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>B1ez1LvJcB</td>\n","      <td>1</td>\n","      <td>In some ways the presented work is a form of meta-learning or *meta-mapping* as the authors refer to it.</td>\n","      <td>arg_structuring</td>\n","      <td>arg-structuring_summary</td>\n","      <td>none</td>\n","      <td>none</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>B1ez1LvJcB</td>\n","      <td>2</td>\n","      <td>The premise of the paper is very interesting and the overall problem is definitely of high interest and high potential impact.</td>\n","      <td>arg_evaluative</td>\n","      <td>none</td>\n","      <td>asp_motivation-impact</td>\n","      <td>pol_positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>B1ez1LvJcB</td>\n","      <td>3</td>\n","      <td>I believe that the presentation of the proposed method can be significantly improved.</td>\n","      <td>arg_request</td>\n","      <td>arg-request_edit</td>\n","      <td>asp_clarity</td>\n","      <td>pol_negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>B1ez1LvJcB</td>\n","      <td>4</td>\n","      <td>The method description was a bit confusing and unclear to me.</td>\n","      <td>arg_evaluative</td>\n","      <td>none</td>\n","      <td>asp_clarity</td>\n","      <td>pol_negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    review_id  sentence_index  \\\n","0  B1ez1LvJcB               0   \n","1  B1ez1LvJcB               1   \n","2  B1ez1LvJcB               2   \n","3  B1ez1LvJcB               3   \n","4  B1ez1LvJcB               4   \n","\n","                                                                                                                                                                                                                text  \\\n","0  This paper presents a method for adapting a model that has been trained to perform one task, so that it can perform a new task (potentially without using any new training data at all—i.e., zero-shot learning).   \n","1                                                                                                           In some ways the presented work is a form of meta-learning or *meta-mapping* as the authors refer to it.   \n","2                                                                                     The premise of the paper is very interesting and the overall problem is definitely of high interest and high potential impact.   \n","3                                                                                                                              I believe that the presentation of the proposed method can be significantly improved.   \n","4                                                                                                                                                      The method description was a bit confusing and unclear to me.   \n","\n","            coarse                     fine                    asp  \\\n","0  arg_structuring  arg-structuring_summary                   none   \n","1  arg_structuring  arg-structuring_summary                   none   \n","2   arg_evaluative                     none  asp_motivation-impact   \n","3      arg_request         arg-request_edit            asp_clarity   \n","4   arg_evaluative                     none            asp_clarity   \n","\n","            pol  \n","0          none  \n","1          none  \n","2  pol_positive  \n","3  pol_negative  \n","4  pol_negative  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"15d2efdc"},"source":["pair = all_pairs[9]\n","\n","df = get_convokit_politeness_labels(pair)\n","\n","# len(all_pairs[1]['review_sentences'])\n","df"],"id":"15d2efdc","execution_count":null,"outputs":[]}]}