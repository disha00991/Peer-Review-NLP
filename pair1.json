{"metadata": {"review_id": "xyz"}, "review_sentences": [{"review_id": "B1euHOqi37", "sentence_index": 0, "text": "The present paper proposes a fast approximation to the softmax computation when the number of classes is very large.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1euHOqi37", "sentence_index": 1, "text": "This is typically a bottleneck in deep learning architectures.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1euHOqi37", "sentence_index": 2, "text": "The approximation is a sparse two-layer mixture of experts.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1euHOqi37", "sentence_index": 3, "text": "The paper lacks rigor and the writing is of low quality, both in its clarity and its grammar.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 4, "text": "See a list of typos below.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1euHOqi37", "sentence_index": 5, "text": "An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 6, "text": "Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 7, "text": "Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 8, "text": "How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 9, "text": "The results only compare with Shim et al. Why only this method? Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 10, "text": "The column \"FLOPS\" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 11, "text": "Also, a \"1x\" label seems to be missing in for the full softmax, so that the reference is clearly specified.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 12, "text": "All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 13, "text": "A brief list of typos:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1euHOqi37", "sentence_index": 14, "text": "\"Sparse Mixture of Sparse of Sparse Experts\"", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 15, "text": "\"if we only search right answer\"", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 16, "text": "\"it might also like appear\"", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 17, "text": "\"which is to design to choose the right\"", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 18, "text": "sparsly", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 19, "text": "\"will only consists partial\"", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 20, "text": "\"with \u03b3 is a lasso threshold\"", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 21, "text": "\"an arbitrarily distance function\"", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 22, "text": "\"each 10 sub classes are belonged to one\"", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1euHOqi37", "sentence_index": 23, "text": "\"is also needed to tune to achieve\"", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1ez1LvJcB", "sentence_index": 0, "text": "This paper presents a method for adapting a model that has been trained to perform one task, so that it can perform a new task (potentially without using any new training data at all\u2014i.e., zero-shot learning).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1ez1LvJcB", "sentence_index": 1, "text": "In some ways the presented work is a form of meta-learning or *meta-mapping* as the authors refer to it.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1ez1LvJcB", "sentence_index": 2, "text": "The premise of the paper is very interesting and the overall problem is definitely of high interest and high potential impact.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "B1ez1LvJcB", "sentence_index": 3, "text": "I believe that the presentation of the proposed method can be significantly improved.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1ez1LvJcB", "sentence_index": 4, "text": "The method description was a bit confusing and unclear to me.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1ez1LvJcB", "sentence_index": 5, "text": "The experimental results presented were all done on small synthetic datasets and it\u2019s hard to evaluate whether the method is practically useful.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "B1ez1LvJcB", "sentence_index": 6, "text": "Furthermore, no comparisons were provided to any baselines/alternative methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "B1ez1LvJcB", "sentence_index": 7, "text": "For example, in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "B1ez1LvJcB", "sentence_index": 8, "text": "Also, I felt that the proposed approach in Section 5 is very similar to MAML intuitively. This makes a comparison with MAML even more desirable.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "B1ez1LvJcB", "sentence_index": 9, "text": "Without any comparisons it\u2019s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1ez1LvJcB", "sentence_index": 10, "text": "In summary, I feel the paper tackles an interesting problem with an interesting approach, but the content could be organized much better.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "B1ez1LvJcB", "sentence_index": 11, "text": "Also, this work would benefit significantly from a better experimental evaluation.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "B1ez1LvJcB", "sentence_index": 12, "text": "For these reasons I lean towards rejecting this paper for now, but would love to see it refined for a future machine learning conference.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1ez1LvJcB", "sentence_index": 13, "text": "Also, the work by Platanios, et al. on contextual parameter generation is very relevant to this work as it tackles multi-task learning using HyperNetworks.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1ez1LvJcB", "sentence_index": 14, "text": "It may be worth adding a short discussion/comparison to that work as it also considers zero-shot learning.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "B1ez1LvJcB", "sentence_index": 15, "text": "Minor comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1ez1LvJcB", "sentence_index": 16, "text": "- Capitalize: \u201csection\u201d -> \u201cSection\u201d, \u201cappendix\u201d -> \u201cAppendix\u201d, \u201cfig.\u201d -> \u201cFigure\u201d.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "B1ez1LvJcB", "sentence_index": 17, "text": "Sometimes these are capitalized, but the use is inconsistent throughout the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1ez1LvJcB", "sentence_index": 18, "text": "- \u201cHold-out\u201d vs \u201cheld-out\u201d .", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "B1ez1LvJcB", "sentence_index": 20, "text": "Be consistent and use \u201cheld-out\u201d throughout.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "B1g0bJk5h7", "sentence_index": 0, "text": "The paper shows that Bayesian neural networks, trained with Dropout MC (Gal et al.) struggle to fully capture the posterior distribution of the weights.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1g0bJk5h7", "sentence_index": 1, "text": "This leads to over-confident predictions which is problematic particularly in an active learning scenario.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1g0bJk5h7", "sentence_index": 2, "text": "To prevent this behavior, the paper proposes to combine multiple Bayesian neural networks, independently trained with Dropout MC, to an ensemble.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1g0bJk5h7", "sentence_index": 3, "text": "The proposed method achieves better uncertainty estimates than a single Bayesian neural networks model and improves upon the baseline in an active learning setting for image classification.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1g0bJk5h7", "sentence_index": 4, "text": "The paper addresses active deep learning which is certainly an interesting research direction since in practice, labeled data is notoriously scarce.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "B1g0bJk5h7", "sentence_index": 5, "text": "However, the paper contains only little novelty and does not provide sufficiently new scientific insights.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "B1g0bJk5h7", "sentence_index": 6, "text": "It is well known from the literature that combining multiply neural networks to an ensemble leads to better performance and uncertainty estimates.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1g0bJk5h7", "sentence_index": 7, "text": "For instance, Lakshminarayanan et al.[1] showed that Dropout MC can produce overconfident wrong prediction and, by simply averaging prediction over multiple models, one achieves better performance and confidence scores.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1g0bJk5h7", "sentence_index": 8, "text": "Also, Huand et al. [2] showed that by taking different snapshots of the same network at different timesteps performance improves.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1g0bJk5h7", "sentence_index": 9, "text": "It would also be great if the paper could related to other existing work that uses Bayesian neural networks in an active learning setting such as Bayesian optimization [3, 4] or Bandits[5].", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "B1g0bJk5h7", "sentence_index": 10, "text": "Another weakness of the paper is that the empirical evaluation is not sufficiently rigorous:", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "B1g0bJk5h7", "sentence_index": 11, "text": "1) Besides an comparison to the work by Lakshminarayanan et. al, I would also like to have seen a comparison to other existing Bayesian neural network approaches such as stochastic gradient Markov-Chain Monte-Carlo methods.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "B1g0bJk5h7", "sentence_index": 12, "text": "2) To provide a better understanding of the paper, it would also be interesting to see how sensitive it is with respect to the ensemble size M.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "B1g0bJk5h7", "sentence_index": 13, "text": "3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "B1g0bJk5h7", "sentence_index": 14, "text": "The same holds for the type of data, since the paper only shows results for image classification benchmarks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "B1g0bJk5h7", "sentence_index": 15, "text": "4) Figure 3: Are the results averaged over multiple independent runs? If so, how many runs did you perform and could you also report confidence intervals? Since all methods are close to each other, it is hard to estimate how significant the difference is.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "B1g0bJk5h7", "sentence_index": 16, "text": "[1] Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundel NIPS 2017", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1g0bJk5h7", "sentence_index": 19, "text": "[2] Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger Snapshot Ensembles: Train 1, get {M} for free} ICLR 2017", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1g0bJk5h7", "sentence_index": 22, "text": "[3] Bayesian Optimization with Robust Bayesian Neural Networks J. Springenberg and A. Klein and S.Falkner and F. Hutter NIPS 2016", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1g0bJk5h7", "sentence_index": 25, "text": "[4] J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and Prabhat and R. Adams Scalable Bayesian Optimization Using Deep Neural Networks ICML 2015", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1g0bJk5h7", "sentence_index": 28, "text": "[5] Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling Carlos Riquelme, George Tucker, Jasper Snoek ICLR 2018", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1g4z20Vs7", "sentence_index": 0, "text": "This paper proposes an approach for mitigating issues associated with high-frequency/amplitude control signals that may be obtained when one applies reinforcement learning algorithms to continuous control tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1g4z20Vs7", "sentence_index": 1, "text": "The approach taken by the paper is to solve a constrained optimization problem, where the constraint imposes a (potentially state-dependent) lower bound on the reward.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1g4z20Vs7", "sentence_index": 2, "text": "This is done by using a Lagrangian relaxation that learns the parameters of a control policy that satisfies the desired constraints (and also learns the Lagrange multipliers).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1g4z20Vs7", "sentence_index": 3, "text": "The presented approach is demonstrated on a cart-pole swing-up task as well as a quadruped locomotion task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1g4z20Vs7", "sentence_index": 4, "text": "Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1g4z20Vs7", "sentence_index": 5, "text": "+ The paper is generally clear and readable.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "B1g4z20Vs7", "sentence_index": 6, "text": "+ The simulation results for the Minitaur quadruped robot are performed using a realistic model of the robot.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "B1g4z20Vs7", "sentence_index": 7, "text": "Major concern:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1g4z20Vs7", "sentence_index": 8, "text": "- My biggest concern is that the technical contributions of the paper are not clear at all.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "B1g4z20Vs7", "sentence_index": 9, "text": "The motivation for the work (avoiding high amplitude/frequency control inputs) is certainly now new; this has always been a concern of control theorists and roboticists (e.g., when considering minimum-time optimal control problems, or control schemes such as sliding mode control).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "B1g4z20Vs7", "sentence_index": 10, "text": "The idea of using a constrained formulation is not novel either (constrained MDPs have been thoroughly studied since Altman (1999)).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "B1g4z20Vs7", "sentence_index": 11, "text": "The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "B1g4z20Vs7", "sentence_index": 12, "text": "Overall, the paper does not make a compelling case for the novelty of the problem or approach.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "B1g4z20Vs7", "sentence_index": 13, "text": "Other concerns:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1g4z20Vs7", "sentence_index": 14, "text": "- For the cart-pole task, the paper states that the reward is modified \"to exclude any cost objective\".", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1g4z20Vs7", "sentence_index": 15, "text": "Results are then presented for this modified reward showing that it results in high-frequency control signals (and that the proposed constrained approach avoids this).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1g4z20Vs7", "sentence_index": 16, "text": "I don't think this is really a fair comparison; I would have liked to have seen results for the unmodified reward function.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "B1g4z20Vs7", "sentence_index": 17, "text": "- The claim made in the first line of the abstract (applying RL algorithms to continuous control problems often leads to bang-bang control) is very broad and should be watered down. This is the case only when one considers a poorly-designed cost function that doesn't take into account realistic factors such as actuator limits.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "B1g4z20Vs7", "sentence_index": 20, "text": "- In the last paragraph of Section 3.3, the paper proposes making the lower-bound on the reward state-dependent.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1g4z20Vs7", "sentence_index": 21, "text": "However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1g4z20Vs7", "sentence_index": 22, "text": "Typos:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1g4z20Vs7", "sentence_index": 23, "text": "- Pg. 5, Section 3.4: \"...this is would achieve...\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "B1g4z20Vs7", "sentence_index": 24, "text": "- Pg. 6: ...thedse value of 90...\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "B1gcyfVOn7", "sentence_index": 0, "text": "This paper formulates a new inference method called DDGC for noise labels and adversarial attacks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 1, "text": "Their main idea is to induce a generative classifer on top of hidden feature spaces of the discriminative deep model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 2, "text": "To improve the robustness, their DDGC model leverages the minimum covariance determinant (MCD) estimator.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 3, "text": "Besides, the author proposes Theorem 1 to justify their MCD-based generative classifer.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 4, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 5, "text": "1. The authors find a new angle for learning with noisy labels.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "B1gcyfVOn7", "sentence_index": 6, "text": "Motivated by the fact that LDA-like generative classifer assuming the class-wise unimodal distribution might be robust, they introduce a generative classifer on top of hidden feature spaces of the discriminative deep model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 7, "text": "2. The authors perform numerical experiments to demonstrate the effectiveness of their framework in benchmark datasets. And their experimental result support their previous claims.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "B1gcyfVOn7", "sentence_index": 8, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 9, "text": "We have two questions in the following.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 10, "text": "1. Related works: In deep learning with noisy labels, there are three main directions, including small-loss trick [1-3], estimating noise transition matrix [4-6], and explicit and implicit regularization [7-9].", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 11, "text": "I would appreciate if the authors can survey and compare more baselines in their paper instead of listing some basic ones.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "B1gcyfVOn7", "sentence_index": 12, "text": "2. Experiment:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 13, "text": "2.1 Baselines: For noisy labels, the authors should add MentorNet [1] as a baseline https://github.com/google/mentornet From my own experience, this baseline is very strong.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "B1gcyfVOn7", "sentence_index": 14, "text": "At the same time, they should compare with VAT [7].", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "B1gcyfVOn7", "sentence_index": 15, "text": "For adversarial attacks, the author should compare with data type from [10], and list L-FBGS [11] as a basic baseline.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "B1gcyfVOn7", "sentence_index": 16, "text": "2.2 Datasets: For datasets, I think the author should first compare their methods on symmetric and aysmmetric noisy data.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "B1gcyfVOn7", "sentence_index": 17, "text": "Besides, the current paper only verifies on vision datasets.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 18, "text": "The authors are encouraged to conduct 1 NLP dataset.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "B1gcyfVOn7", "sentence_index": 19, "text": "References:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 20, "text": "[1] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 21, "text": "[2] M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to reweight examples for robust deep learning. In ICML, 2018.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 22, "text": "[3] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, M. Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NIPS, 2018.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 23, "text": "[4] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 24, "text": "[5] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 25, "text": "[6] S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fergus. Training convolutional networks with noisy labels. In ICLR workshop, 2015.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 26, "text": "[7] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 27, "text": "[8] A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 28, "text": "In NIPS, 2017.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 29, "text": "[9] S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In ICLR, 2017.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 30, "text": "[10] C. Nicholas and W. David. Towards evaluating the robustness of neural networks. In IEEE Symposium on SP, 2017.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1gcyfVOn7", "sentence_index": 31, "text": "[11] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In ICLR, 2013.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 0, "text": "## Summary ##", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 1, "text": "The authors apply policy gradients to combinatorial optimization problems.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 2, "text": "They suggest a surrogate reward function that mitigates the variance in the reward, and hence the update size.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 3, "text": "They demonstrate performance on a clique-finding problem.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 4, "text": "## Assessment ##", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 5, "text": "I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "B1l3zjA_h7", "sentence_index": 6, "text": "I also have concerns about the independence assumption in their sampling distribution (Section 3.2), and the fact that their experiments use the same set of (untuned) hyperparameters for each method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1l3zjA_h7", "sentence_index": 7, "text": "They both approximate the reward CDF from K samples and use this to construct a surrogate reward.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 8, "text": "The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 9, "text": "## Specific Comments and Questions ##", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 10, "text": "1. Cakewalk is *very* closely related to the cross-entropy method.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 11, "text": "The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "B1l3zjA_h7", "sentence_index": 12, "text": "Both Cakewalk and CE approximate the reward CDF from K samples and use this to construct a surrogate reward.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 13, "text": "The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 14, "text": "2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1l3zjA_h7", "sentence_index": 15, "text": "Consider $x$ a binary vector and reward equal to the parity $S(x) = \\sum{x_j} % 2$.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 16, "text": "3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1l3zjA_h7", "sentence_index": 17, "text": "Is there any explanation for this?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1l3zjA_h7", "sentence_index": 18, "text": "4. How were the hyperparameters (learning rate, AdaGrad $\\delta$, Adam $\\beta_1, \\beta_2$) chosen?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "B1l3zjA_h7", "sentence_index": 19, "text": "It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1l3zjA_h7", "sentence_index": 20, "text": "I would suggest tuning these values for each method independently.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "B1l3zjA_h7", "sentence_index": 21, "text": "5. It would be nice to see experimental results on more than one problem.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "B1l3zjA_h7", "sentence_index": 22, "text": "The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "B1l3zjA_h7", "sentence_index": 23, "text": "6. In Table 3, the figure in bold is not the lowest (best) in the table.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "B1l3zjA_h7", "sentence_index": 24, "text": "The reason for this is only given in a single sentence at the end of Section 6, so it is a little confusing.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1l3zjA_h7", "sentence_index": 25, "text": "I would replace these values with N/A or something similar.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "B1lK2M8jnX", "sentence_index": 0, "text": "This work extends Schlichtkrull et al. (2018) by adding attention in two distinct ways: attention between pairs of nodes per relation, and attention between pairs of nodes averaged over all relations.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "B1lK2M8jnX", "sentence_index": 1, "text": "The paper is well written and the equations easy to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "B1lK2M8jnX", "sentence_index": 2, "text": "The results are not strong. And, unfortunately, the model contribution currently is too modest.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "B1lK2M8jnX", "sentence_index": 3, "text": "Inductive task results: Wu et al. (2018) reports that for Tox21 (Duvenauld et al. 2015) is the best-performing approach.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "B1lK2M8jnX", "sentence_index": 4, "text": "We should see the performance on other datasets  (e.g., some of the other datasets in Wu et al. (2018)).", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "B1lK2M8jnX", "sentence_index": 5, "text": "My introduction suggestion: do not talk about Convolutional neural networks (CNNs). There is a *lot* of work on graph convolutional networks (GCNs).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "B1lK2M8jnX", "sentence_index": 6, "text": "Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1lK2M8jnX", "sentence_index": 7, "text": "--- After rebuttal ---", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1lK2M8jnX", "sentence_index": 8, "text": "Still not convinced of the value of the work to the community. Will keep my score the same.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "B1lrh6UpYS", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 1, "text": "The authors propose using non-Euclidean spaces for GCNs.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 2, "text": "This is inspired by the recent work into non-Euclidean, and especially hyperbolic, embeddings.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 3, "text": "A few papers have recently tried to go past embeddings into building non-Euclidean models, requiring the lifting of standard operations in Euclidean space to non-Euclidean settings.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 4, "text": "This has been done in particular in hyperbolic space, but some datasets benefit from more complex spaces.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 5, "text": "The authors combine the mixed-curvature product formalism that uses products of Euclidean, hyperbolic, and spherical spaces for embeddings, but use these for GCN operations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 6, "text": "Doing this requires, in particular, developing a reasonable way to perform these operations in spherical space (since Euclidean is trivial and hyperbolic has been recently worked on).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 7, "text": "The authors do a nice lifting via complex operations, and both the hyperbolic and spherical spaces can devolve into the flat Euclidean space when their curvature goes to 0.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 8, "text": "The authors implement these GCNs, train the curvatures, and demonstrate performance improvements over Euclidean only versions on node classification on benchmark datasets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 9, "text": "They also give a fairly nice introduction to all of these ideas in an extended appendix.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 10, "text": "Strengths, Weaknesses, Recommendation:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 11, "text": "This paper is reasonably interesting---it joins an effort to produce non-Euclidean models in a tractable way, which is fairly challenging, but could have a good impact.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "B1lrh6UpYS", "sentence_index": 12, "text": "On the plus side, it's great that the authors added the nice development for the spherical operations, since that will come in handy for many models.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "B1lrh6UpYS", "sentence_index": 13, "text": "The experiments are also good.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "B1lrh6UpYS", "sentence_index": 14, "text": "On the downside, everything here is an extension of existing work, and the body of the paper is hard to read (though this may be inevitable, there's a lot of background to go over here).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1lrh6UpYS", "sentence_index": 15, "text": "Overall I recommend accepting it; I think it's a solid contribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "B1lrh6UpYS", "sentence_index": 16, "text": "Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 17, "text": "- I don't understand why the authors say that their space \"interpolates smoothly\" just because the limit in the curvature is the same from the left and right side.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1lrh6UpYS", "sentence_index": 18, "text": "For example, the absolute value function has the same limit from the left and the right at 0, but it's not differentiable there. Is it actually true that if we take the derivatives of the piecewise hyperbolic/spherical distance function that it's differentiable at c=0?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "B1lrh6UpYS", "sentence_index": 19, "text": "- There are a couple of recent papers that also consider hyperbolic GCNs, and in fact use  similar ideas for the aggregation and update steps (i.e., same lift to hyperbolic space).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 20, "text": "However, these were recently NeurIPS papers, and the text is not yet out, so I don't think this should affect the authors' independent work (and also the product part is new).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 21, "text": "I do recommend that the authors compare against those results in a future update of this work.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "B1lrh6UpYS", "sentence_index": 22, "text": "The papers are \"Hyperbolic Graph Convolutional Neural Networks\" by Chami et al and Hyperbolic Graph Neural Networks by Liu et al.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 23, "text": "- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1lrh6UpYS", "sentence_index": 24, "text": "For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1lrh6UpYS", "sentence_index": 25, "text": "As an example, consider S^2 and the mean of two antipodal points on it---there's many choices for the midpoint.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 26, "text": "You probably have to limit the operation to a half-sphere (there's some ideas for this in Gu et al).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1lrh6UpYS", "sentence_index": 27, "text": "- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1lrh6UpYS", "sentence_index": 28, "text": "- Are the curvatures the same for each layer for the GCNs?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "B1lrh6UpYS", "sentence_index": 29, "text": "This is an interesting point to discuss (some of the NeurIPS papers I mentioned train the curvature for each layer).", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "B1lrh6UpYS", "sentence_index": 30, "text": "Also, how do you select the number of factors of each type?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "B1lrh6UpYS", "sentence_index": 31, "text": "- Minor, but some of these citations can be updated.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 32, "text": "The \"De Sa\" et al 2018 arxiv citation is really Sala et al and is an ICML '18 paper.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "B1lrh6UpYS", "sentence_index": 33, "text": "Similarly, Gulcehre et al is a 2019 ICLR paper, and so on. It's always good to get these right.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "B1lrh6UpYS", "sentence_index": 34, "text": "- Is there any actual empirical importance from recovering the Euclidean case exactly for 0 curvature?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "B1lrh6UpYS", "sentence_index": 35, "text": "The reason I ask is that my experience is that the hyperboloid is typically easier to work with .", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 38, "text": "- One useful thing to point out in B.3.3 is that in general, it need not be a diffeomorphism for all of M for any manifold, which leads to non-uniqueness.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "B1lrh6UpYS", "sentence_index": 39, "text": "In differential geometry, the \"cut locus\" is the region beyond which there is this non-uniqueness.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1lrh6UpYS", "sentence_index": 40, "text": "- In the appendix, the statement \"Sarkar (2011) show that a similar statement as in Theorem 2 holds for a very general class of trees\" is confusing to me. The \"general class\", as far as I know, is actually *all* trees, weighted or unweighted.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1x81eSE2m", "sentence_index": 0, "text": "The main contributions of this work are essentially on the theoretical aspects.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1x81eSE2m", "sentence_index": 1, "text": "It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "B1x81eSE2m", "sentence_index": 2, "text": "The authors need to describe in detail the algorithmic novelty of their work.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "B1x81eSE2m", "sentence_index": 3, "text": "The definition of \u201crecovering true factor exactly\u201d need to be given.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1x81eSE2m", "sentence_index": 4, "text": "The proposed algorithm involves several tuning parameters, when alternating between two updating rules, an IHT-based update for coefficients and a gradient descent-based update for the dictionary.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1x81eSE2m", "sentence_index": 5, "text": "Therefore, an appropriate choice of their values need to be given.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "B1x81eSE2m", "sentence_index": 6, "text": "In the algorithm, the authors need to define the HT function in (3) and (4).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1x81eSE2m", "sentence_index": 7, "text": "In the experiments, the authors compare the proposed method to only the one proposed by Arora et al. 2015.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "B1x81eSE2m", "sentence_index": 8, "text": "We think that this is not enough, and more extensive experimental results would provide a better paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "B1x81eSE2m", "sentence_index": 9, "text": "There are some typos that can be easily found, such as \u201cof the out algorithm\u201d.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "B1xOut6DpQ", "sentence_index": 0, "text": "This paper proposes an efficient zero-confidence attack algorithm, MARGINATTACK, which uses the modified Rosen's algorithm to optimize the same objective as CW attack.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1xOut6DpQ", "sentence_index": 1, "text": "Under a set of conditions, the authors proved convergence of the proposed attack algorithm.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1xOut6DpQ", "sentence_index": 2, "text": "My main concern about this paper is why this algorithm has a better performance than CW attack?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "B1xOut6DpQ", "sentence_index": 3, "text": "I would suggest comparing with CW attack under different sets of hyper-parameters.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "B1xOut6DpQ", "sentence_index": 4, "text": "Minor comment:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "B1xOut6DpQ", "sentence_index": 5, "text": "The theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "B1xtSy6t2Q", "sentence_index": 0, "text": "The paper deals with the problem of recovering an exact solution for both the dictionary and the activation coefficients.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1xtSy6t2Q", "sentence_index": 1, "text": "As other works, the solution is based on a proper initialization of the dictionary.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1xtSy6t2Q", "sentence_index": 2, "text": "The authors suggest using Aurora 2015 as a possible initialization.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1xtSy6t2Q", "sentence_index": 3, "text": "The contribution improves Arora 2015 in that it converges linearly and recovers both the dictionary and the coefficients with no bias.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1xtSy6t2Q", "sentence_index": 4, "text": "The main contribution is the use of a IHT-based strategy to update the coefficients, with a gradient-based update for the dictionary (NOODL algorithm).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1xtSy6t2Q", "sentence_index": 5, "text": "The authors show that, combined with a proper initialization, this has exact recovery guaranties.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1xtSy6t2Q", "sentence_index": 6, "text": "Interestingly, their experiments show that NOODL converges linearly in number of iterations, while Arora gets stuck after some iterations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1xtSy6t2Q", "sentence_index": 7, "text": "I think the paper is relevant and proposes an interesting contribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "B1xtSy6t2Q", "sentence_index": 8, "text": "The paper is well written and the key elements are in the body.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "B1xtSy6t2Q", "sentence_index": 9, "text": "However, there is a lot of important material in the Appendix, which I think may be relevant to the readers.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "B1xtSy6t2Q", "sentence_index": 10, "text": "It would be nice to have some more intuitive explanations at least of Theorem 1.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "B1xtSy6t2Q", "sentence_index": 11, "text": "Also, it is clear in the experiments the superiority with respect to Arora in terms of iterations (and error), but what about computational time?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "B1xw3F5KhQ", "sentence_index": 0, "text": "This paper proposes using spectral normalization (SN) as a regularization for adversarial training, which is based on [Miyato et. al., ICLR 2018], where the original paper used SN for GAN training.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1xw3F5KhQ", "sentence_index": 1, "text": "The paper also uses the results from [Neyshabur et. al., ICLR 2018], where the original paper provided generalization bounds that depends on spectral norm of each layer.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1xw3F5KhQ", "sentence_index": 2, "text": "The paper is well written in general, the experiments are extensive.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "B1xw3F5KhQ", "sentence_index": 3, "text": "The idea of studying based on the combination of the results from two previous papers is quite natural, since one uses spectral normalization in practice for GAN training, and the other provides generalization bound that depends on spectral norm.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_neutral"}, {"review_id": "B1xw3F5KhQ", "sentence_index": 4, "text": "The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "B1xw3F5KhQ", "sentence_index": 5, "text": "The experimental result itself is quite comprehensive.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "B1xw3F5KhQ", "sentence_index": 6, "text": "On the other hand, this paper provides specific generalization bounds under three adversarial attack methods, which explains the power of SN under those settings.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "B1xw3F5KhQ", "sentence_index": 7, "text": "However, it is not clear to me that these are some novel results that can better help adversarial training.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "B1xXOITI9H", "sentence_index": 0, "text": "Though rather dense in its exposition, this paper is an interesting contribution to the area of self-supervised learning  based on discrete representations.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "B1xXOITI9H", "sentence_index": 1, "text": "What would make it stronger imo is to address the issue of how much is gained from a discrete vs. continuous representation.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "B1xXOITI9H", "sentence_index": 2, "text": "The authors take it as a given that discrete is good because it allows us to leverage work in NLP.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "B1xXOITI9H", "sentence_index": 3, "text": "That makes sense -- but at what cost?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "B1xXOITI9H", "sentence_index": 4, "text": "\"Table 4 shows that our first results are promising, even though they are not as good as the state of the art.\" The state of the art on LibriSpeech is not Mohamed at al. 2019. See e.g. Irie et al. Interspeech 2019 for better result", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "B1xXOITI9H", "sentence_index": 5, "text": "The Conclusion is very sparse. \"In future work, we are planning to apply other algorithms requiring discrete inputs to audio data\":", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "B1xXOITI9H", "sentence_index": 6, "text": "can  you elaborate?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 0, "text": "The authors propose a method for image restoration, where the restored image is the MAP estimate.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJe2djKE3X", "sentence_index": 1, "text": "A pretrained GAN is utilized to approximate the prior distribution of the noise-free images.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJe2djKE3X", "sentence_index": 2, "text": "Then, the likelihood induces a constraint which is based on the degradation function.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJe2djKE3X", "sentence_index": 3, "text": "In particular, the method tries to find the latent point for which the GAN generates the image, which if gets degraded will match the given degraded image.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJe2djKE3X", "sentence_index": 4, "text": "Also, an optimization algorithm is presented that solves the proposed constrained optimization problem.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "BJe2djKE3X", "sentence_index": 5, "text": "I find the paper very well written and easy to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BJe2djKE3X", "sentence_index": 6, "text": "Also, the idea is pretty clean, and the derivations are simple and clear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BJe2djKE3X", "sentence_index": 7, "text": "Additionally, the Figures 2,3 are very intuitive and nicely explain the theory.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BJe2djKE3X", "sentence_index": 8, "text": "However, I think that there are some weaknesses (see comments):", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJe2djKE3X", "sentence_index": 9, "text": "Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJe2djKE3X", "sentence_index": 10, "text": "#1) I do not understand exactly what the \"general method\" means. Does it mean that you propose a method, where you can just change the F, such that to solve a different degradation problem? So you provide the general framework where somebody has to specify only the F?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 11, "text": "#2) Clearly, the efficiency of the method is highly based on the ability of the GAN to approximate well the prior distribution of the noise-free images.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 12, "text": "#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJe2djKE3X", "sentence_index": 13, "text": "For instance, Eq. 2,3 can be easily combined using the proportional symbol, Eq. 8,9,10,11 show actually the same thing.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 14, "text": "#4) I think that the function F has to be differentiable, and this should be mentioned in the text.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 15, "text": "Also, I believe that some actual (analytic) examples of F should be provided, at least  in the experiments.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 16, "text": "The same holds for the p(Omega).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 17, "text": "This parameter Omega is estimated individually for each degraded image?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 18, "text": "#5) Before Eq. 8 the matrix V is a function of z and should be presented as such in the equations.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 19, "text": "#6) I believe that it would be nice to include a magnified image of Fig. 3, where the gradient steps are shown.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 20, "text": "Also, my understanding is that the optimization goal is to find first a feasible solution, and then find the point that maximizes f. I think that this can be clarified in the text.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 21, "text": "#7) The optimization steps seem to be intuitive, however, there is not any actual proof of converge.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 22, "text": "Of course, the example in the Figure 3 is very nice and intuitive, but it is also rather simple.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 23, "text": "I would suggest, at least, to include some empirical evidences in the experiments that show convergence.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 24, "text": "#8) In the experiments I think that at least one example of F and p(Omega) should be presented.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 25, "text": "Also, what the numbers in Table 4 show? Which is the best value that can be achieved?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 26, "text": "These numbers correspond to several images, or to a unique image?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 27, "text": "#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJe2djKE3X", "sentence_index": 28, "text": "I believe that a more challenging experiment should be conducted e.g. using celebA dataset.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJe2djKE3X", "sentence_index": 29, "text": "Minor comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJe2djKE3X", "sentence_index": 30, "text": "#1) In the paragraph after Eq. 4 the equality p_r(x)=p_G(x) is very strong assumption.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 31, "text": "I would suggest to use the \\simeq symbol instead.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 32, "text": "#2) After Eq. 6 the \"nonnegative\" should be \"nonzero\".", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 33, "text": "#3) Additional density estimation models can be used e.g. VAEs, GMM.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 34, "text": "Especially, I believe that the VAE will provide a way to approximate the prior easier than the GAN.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 35, "text": "#4) In Section 2 paragraph 2, the sentence \"However, they only ... and directly\" is not clear what means.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 36, "text": "In general, I find both the proposed model and optimization algorithm interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJe2djKE3X", "sentence_index": 37, "text": "Additionally, the idea is nicely presented in the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BJe2djKE3X", "sentence_index": 38, "text": "Most of my comments are improvements which can be easily included.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "BJe2djKE3X", "sentence_index": 39, "text": "The two things that make me more skeptical, is the convergence of the proposed algorithm and the experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJe2djKE3X", "sentence_index": 40, "text": "The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJe2djKE3X", "sentence_index": 41, "text": "Also, I think that additional methods to compute the image prior should be included in the experiments.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe3wuvchQ", "sentence_index": 0, "text": "This is an interesting paper with a new approach to learn a sparse, positive (and hence interpretable) semantic space that maximizes human similarity judgements, by training to specifically maximize the prediction of human similarity judgements.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJe3wuvchQ", "sentence_index": 1, "text": "The authors have collected the dataset themselves and have rating of sets of 3 objects from 1854 unique objects.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJe3wuvchQ", "sentence_index": 2, "text": "They end up with a space (SPoSE) with relatively low dimensionality with respect to usual word embeddings (49 dimension) but perhaps not surprising when considering the small size of the words to embed.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJe3wuvchQ", "sentence_index": 3, "text": "The authors run a set of experiment to show the usefulness of SPoSE.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJe3wuvchQ", "sentence_index": 4, "text": "The most interesting one is the prediction of its dimensions by the CSLB features, which reveals a nice clustering in the different SPoSE dimensions.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJe3wuvchQ", "sentence_index": 5, "text": "Perhaps the results would be a little more convincing if additional common word embeddings were also tested.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJe3wuvchQ", "sentence_index": 6, "text": "Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJe3wuvchQ", "sentence_index": 7, "text": "A good extension of this work would be to combine a text-derived embedding  or the synsets to interpolate the SPoSE dimensions for missing words in the original set.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe3wuvchQ", "sentence_index": 8, "text": "Or perhaps the object similarity ratings could be used in a semi-supervised setting to inform the learning of a co-occurence word embedding.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe3wuvchQ", "sentence_index": 9, "text": "This will allow the model to better describe a larger set of words.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJe3wuvchQ", "sentence_index": 10, "text": "Another possible extension is to test this larger set of words on a non-behavioral NLP task to show possible improvements that the behavioral data and the interpretable space give.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJeDySijKr", "sentence_index": 0, "text": "The paper proposes pre-training strategies (PT) for graph neural networks (GNN) from both node and graph levels.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJeDySijKr", "sentence_index": 1, "text": "Two new large-scale pre-training datasets are created and extensive experiments are conducted to demonstrate the benefits of PT upon different GNN architectures.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJeDySijKr", "sentence_index": 2, "text": "I am relative positive for this work.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJeDySijKr", "sentence_index": 3, "text": "Detail review of different aspects and questions are as follows.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJeDySijKr", "sentence_index": 4, "text": "Novelty: As far as I know, this work is among the earliest works to think about GNN pre-training.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "BJeDySijKr", "sentence_index": 5, "text": "The most similar paper at the same period is [Z Hu, arXiv:1905.13728] .", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJeDySijKr", "sentence_index": 7, "text": "I read both papers and found they have similar idea about PT although they have different designs.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJeDySijKr", "sentence_index": 8, "text": "This paper leverages graph structure (e.g., context neighbors) and supervised labels/attributes (e.g., node attributes, graph labels) for PT.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJeDySijKr", "sentence_index": 9, "text": "These strategies are not surprising for me and the novelty is incremental.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "BJeDySijKr", "sentence_index": 10, "text": "Experiment: The experiments are overall good.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJeDySijKr", "sentence_index": 11, "text": "The authors created two new large scale pre-training graph datasets.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJeDySijKr", "sentence_index": 12, "text": "Experimental results of different GNN architectures w/o different PT for different tasks are provided.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJeDySijKr", "sentence_index": 13, "text": "Comparing to non-pretraining GNN, the improvements are significant for most cases.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJeDySijKr", "sentence_index": 14, "text": "Writing: The writing is good and easy to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BJeDySijKr", "sentence_index": 15, "text": "Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJeDySijKr", "sentence_index": 16, "text": "Comparing to the other work, what are strengths of this work? In addition, have the authors compared the performances of their work and [Z Hu, arXiv:1905.13728] using the same data?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJeYeRM0jm", "sentence_index": 0, "text": "The paper builds upon Deep Image Prior (DIP) - work which shows that one can optimize a neural generator to fit a single image without learning on any dataset, and the output of the generator (which approximates the image) can be used for denoising / super resolution / etc.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJeYeRM0jm", "sentence_index": 1, "text": "The paper proposes a new architecture for the DIP method which has much less parameters, but works on par with DIP.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJeYeRM0jm", "sentence_index": 2, "text": "Another contribution of the paper is theoretical treatment of (a simplified version of) the proposed architecture showing that it can\u2019t fit random noise (and thus maybe better suited for denoising).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJeYeRM0jm", "sentence_index": 3, "text": "The paper is clearly written, and the proposed architecture has too cool properties: it\u2019s compact enough to be used for image compression; and it doesn\u2019t overfit thus making early stopping notnesesary (which was crucial for the original DIP model).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "BJeYeRM0jm", "sentence_index": 4, "text": "I have two main concerns about this paper.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJeYeRM0jm", "sentence_index": 5, "text": "First, it is somewhat misleading about its contributions: it's not obvious from abstract/introduction that the whole model is the same as DIP except for the proposed architecture.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJeYeRM0jm", "sentence_index": 6, "text": "Specifically, the first contribution listed in the introduction makes it look like this paper introduces the idea of not learning the decoder on the dataset (the one that starts with \u201cThe network is not learned and itself incorporates all assumptions on the data.\u201d).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJeYeRM0jm", "sentence_index": 7, "text": "My second concern is about the theoretical contribution.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJeYeRM0jm", "sentence_index": 8, "text": "On the one hand, I enjoyed the angle the authors tackled proving that the network architecture is underparameterized enough to be a good model for denoising.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJeYeRM0jm", "sentence_index": 9, "text": "On the other hand, the obtained results are very weak: only one layered version of the paper is analysed and the theorem applies only to networks with less than some threshold of parameters.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJeYeRM0jm", "sentence_index": 10, "text": "Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJeYeRM0jm", "sentence_index": 11, "text": "This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJeYeRM0jm", "sentence_index": 12, "text": "Also, the theorem only applies to the iid noise, while most natural noise patterns have structure (e.g. JPEG artifacts, broken pixels, etc) and thus can probably be better approximated with deep models.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJeYeRM0jm", "sentence_index": 13, "text": "Since the paper manages to use very few parameters (BTW, how many parameters in total do you have? Can you please add this number to the text?), it would be cool to see if second order methods like LBFGS can be applied here.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJeYeRM0jm", "sentence_index": 14, "text": "Some less important points:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJeYeRM0jm", "sentence_index": 15, "text": "Fig 4 is very confusing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJeYeRM0jm", "sentence_index": 16, "text": "First, it doesn\u2019t label the X axis.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJeYeRM0jm", "sentence_index": 17, "text": "Second, the caption mentions that early stopping is beneficial for the proposed method, but I can\u2019t see it from the figure.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJeYeRM0jm", "sentence_index": 18, "text": "Third, I don\u2019t get what is plotted on different subplots.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJeYeRM0jm", "sentence_index": 19, "text": "The text mentions that (a) is fitting the noisy image, (b) is fitting the noiseless image, and (c) is fitting noise. Is it all done independently with three different models?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJeYeRM0jm", "sentence_index": 20, "text": "Then why does the figure says test and train loss? And why DIP loss goes up, it should be able to fit anything, right? If not and it\u2019s a single model that gets fitted on the noisy image and tested on the noiseless image, then how can you estimate the level of noise fitting? ||G(C) - eta|| should be high if G(C) ~= x.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJeYeRM0jm", "sentence_index": 21, "text": "Also, in this quote \u201cIn Fig. 4(a) we plot the Mean Squared Error (MSE) over the number of iterations of the optimizer for fitting the noisy astronaut image x + \u03b7 (i.e., FORMULA ...\u201d the formula doesn\u2019t correspond to the text.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJeYeRM0jm", "sentence_index": 22, "text": "And finally, the discussion of this figure makes claims about the behaviour of the model that seems to be too strong to be based on a single image experiment.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJeYeRM0jm", "sentence_index": 23, "text": "I don\u2019t get the details of the batch normalization used: with respect to which axis the mean and variance are computed?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJeYeRM0jm", "sentence_index": 24, "text": "The authors claim that the model is not convolutional.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJeYeRM0jm", "sentence_index": 25, "text": "But first, it\u2019s not obvious why this would be a good thing (or a bad thing for that matter) .", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJeYeRM0jm", "sentence_index": 27, "text": "Second, it\u2019s not exactly correct (as noted in the paper itself): the architecture uses 1x1 convolutions and upsampling, which combined give a weak and underparametrized analog of convolutions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJeYeRM0jm", "sentence_index": 28, "text": "> The deep decoder is a deep image model G: R N \u2192 R n, where N is the number of parameters of the model, and n is the output dimension, which is typically much larger than the number of parameters (N << n).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJeYeRM0jm", "sentence_index": 29, "text": "I think it should be vice versa, N >> n", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "BJeYeRM0jm", "sentence_index": 30, "text": "The following footnote", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJeYeRM0jm", "sentence_index": 31, "text": "> Specifically, we took a deep decoder G with d = 6 layers and output dimension 512\u00d7512\u00d73, and choose k = 64 and k = 128 for the respective compression ratios.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJeYeRM0jm", "sentence_index": 32, "text": "Uses unintroduced (at that point) notation and is very confusing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJeYeRM0jm", "sentence_index": 33, "text": "It would be nice to have a version of Figure 6 with k = 6, so that one can see all feature maps (in contrast to a subset of them).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJeYeRM0jm", "sentence_index": 34, "text": "I\u2019m also wondering, is it harder to optimize the proposed architecture compared to DIP?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJeYeRM0jm", "sentence_index": 35, "text": "The literature on distillation indicates that overparameterization can be beneficial for convergence and final performance.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJgF7g7jFr", "sentence_index": 0, "text": "= Summary", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJgF7g7jFr", "sentence_index": 1, "text": "A method to predict likely type of program variables in TypeScript is presented.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJgF7g7jFr", "sentence_index": 2, "text": "It consists of a translation of a program's type constraints and defined objects into a (hyper)graph, and a specialised neural message passing architecture to learn from the generated graphs.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJgF7g7jFr", "sentence_index": 3, "text": "Experiments show that the method substantially outperforms sound typing in the TypeScript compiler, as well as a recent method based on deep neural networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJgF7g7jFr", "sentence_index": 4, "text": "= Strong/Weak Points", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJgF7g7jFr", "sentence_index": 5, "text": "+ The graph representation of the problem is novel, and draws both on core ideas from Hindley-Milner typing (in the subtyping/assignment graph bits) as well as neural ideas (in name similiarity)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "BJgF7g7jFr", "sentence_index": 6, "text": "+ The neural message passing architecture is adapted to the problem, handling features not present in the standard GNN literature (hyperedges, ...)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "BJgF7g7jFr", "sentence_index": 7, "text": "+ Experiments compare with relevant baselines and consider interesting ablations, studying the effect of the GNN extensions in detail.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "BJgF7g7jFr", "sentence_index": 8, "text": "- The hyperparameter selection regime (and the experiments used to find them) is not described", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "BJgF7g7jFr", "sentence_index": 9, "text": "= Recommendation", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJgF7g7jFr", "sentence_index": 10, "text": "This is an application-driven paper with nice practical results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJgF7g7jFr", "sentence_index": 11, "text": "The fact that standard neural architectures are extended and adapted to the task, and the way domain knowledge is used to design the graph representation makes this interesting even to people outside the task-specific audience, and hence I strongly recommend acceptance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJgF7g7jFr", "sentence_index": 12, "text": "= Minor Comments", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJgF7g7jFr", "sentence_index": 13, "text": "- page 2: \"network's type to be class\" -> \"to be a class\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJgF7g7jFr", "sentence_index": 14, "text": "- Evaluation Datasets: Did you take duplication in the crawled datasets into account? (Lopes et al. 2017 (D\u00e9j\u00e0Vu: a map of code duplicates on GitHub) suggests that this is particularly problematic for JavaScript/TypeScript)", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJgICDN92m", "sentence_index": 0, "text": "The paper considers adaptive regularization, which has been popular in neural network learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJgICDN92m", "sentence_index": 1, "text": "Rather than adapting diagonal elements of the adaptivity matrix, the paper proposes to consider a low-rank approximation to the Gram/correlation matrix.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJgICDN92m", "sentence_index": 2, "text": "When you say that full-matrix computation \"requires taking the inverse square root\", I assume you know that is not really correct?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJgICDN92m", "sentence_index": 3, "text": "As a matter of good implementation, one never takes the inverse of anything.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJgICDN92m", "sentence_index": 4, "text": "Instead, on solves a linear system, via other means.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJgICDN92m", "sentence_index": 5, "text": "Of course, approximate linear system solvers then permit a wide tradeoff space to speed things up.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJgICDN92m", "sentence_index": 6, "text": "There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJgICDN92m", "sentence_index": 7, "text": "The latter may be important in practice, but it is orthogonal to the full matrix theory.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJgICDN92m", "sentence_index": 8, "text": "There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJgICDN92m", "sentence_index": 9, "text": "Instead, it is a low-rank approximation to the full matrix.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJgICDN92m", "sentence_index": 10, "text": "If there were theory to be had here, then I would guess that the low-rank approximation may work even when full matrix did not, e.g., since the full matrix case would involve too may parameters.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJgICDN92m", "sentence_index": 11, "text": "The discussion of convergence to first order critical points is straightforward.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "BJgICDN92m", "sentence_index": 12, "text": "Adaptivity ratio is mentioned in the intro but not defined there.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJgICDN92m", "sentence_index": 13, "text": "Why mention it here, if it's not being defined.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJgICDN92m", "sentence_index": 14, "text": "You say that second order methods are outside the scope, but you say that your method is particularly relevant for ill-conditioned problems.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "BJgICDN92m", "sentence_index": 15, "text": "It would help to clarify the connection between the Gram/correlation matrix of gradients and the Hessian and what is being done to ill-conditioning, since second order methods are basically designed for ill-conditioned problems..", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJgICDN92m", "sentence_index": 16, "text": "It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJgICDN92m", "sentence_index": 17, "text": "The results shown in Figure 4 are much more interesting than the usual training curves which are shown in the other figures.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJgICDN92m", "sentence_index": 18, "text": "If this method is to be useful, understanding how these spectral properties change during training for different types of networks is essential.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "BJgICDN92m", "sentence_index": 19, "text": "More papers should present this, and those that do should do it more systematically.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJgICDN92m", "sentence_index": 20, "text": "You say that you \"informally state the main theorem.\"  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJgmhEfTcH", "sentence_index": 0, "text": "This paper is an empirical contribution regarding SGD arguing that it presents two different behaviors which the authors name a noise dominated regimen, and a curvature dominated regime.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJgmhEfTcH", "sentence_index": 1, "text": "They observe that the behaviors seem to arise in different batch sizes", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJgmhEfTcH", "sentence_index": 2, "text": "The authors derive empirical conclusions and perform experiments in different settings.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJgmhEfTcH", "sentence_index": 3, "text": "The paper is well-written and the experimental setup seems to be carefully carried out.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJgmhEfTcH", "sentence_index": 4, "text": "I find the observations interesting, but the contribution is empirical and not entirely new. It would be nice if there were some theoretical results to back up the observations.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "BJgzh7Schm", "sentence_index": 0, "text": "The authors propose to use the combination of model ensemble and MC dropout in Bayesian deep active learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJgzh7Schm", "sentence_index": 1, "text": "They empirically show that there exists the mode collapse problem due to the MC dropout which can be regarded as a variational approximation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJgzh7Schm", "sentence_index": 2, "text": "The authors introduce an ensemble of MC-Dropout models with different initialization to remedy this mode collapse problem.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJgzh7Schm", "sentence_index": 3, "text": "The paper is clearly written and easy to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BJgzh7Schm", "sentence_index": 4, "text": "It is interesting to empirically show that the mode collapse problem of MC-Dropout is important in active learning.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BJgzh7Schm", "sentence_index": 5, "text": "The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJgzh7Schm", "sentence_index": 6, "text": "Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJgzh7Schm", "sentence_index": 7, "text": "Therefore, it is a little misleading to still call it Bayesian active learning.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJgzh7Schm", "sentence_index": 8, "text": "Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJgzh7Schm", "sentence_index": 9, "text": "The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJgzh7Schm", "sentence_index": 10, "text": "So it seems not a reasonable solution for the mode collapse problem of MC-Dropout.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJgzh7Schm", "sentence_index": 11, "text": "It is not clear to me why we need to add MC-Dropout to the ensemble.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "BJgzh7Schm", "sentence_index": 12, "text": "What is the benefit of DEBAL over an ensemble method if both of them do not have Bayesian theoretic support?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "BJgzh7Schm", "sentence_index": 13, "text": "In terms of the empirical results, the better performance of DEBAL compared to a single MC-Dropout model is not supervising as Beluch et al. (2018) already demonstrated that an ensemble is better than a single MC-Dropout.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJgzh7Schm", "sentence_index": 14, "text": "While the improvement of DEBAL compared to an ensemble is marginal but is reasonable.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJgzh7Schm", "sentence_index": 15, "text": "The labels of figures are hard to read.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJgZjv4c37", "sentence_index": 0, "text": "The authors propose in this paper an approach for learning models with tractable approximate posterior inference.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJgZjv4c37", "sentence_index": 1, "text": "The paper is well motivated (fast and accurate posterior inference) and the construction of the solutions (invertible architecture, appending vectors to input and output, choice of cost function) well described.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BJgZjv4c37", "sentence_index": 2, "text": "From my understanding, it seems this method is also to be compatible with other methods of approximate Bayesian Computation (ABC).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJgZjv4c37", "sentence_index": 3, "text": "Concerning the experimental section:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJgZjv4c37", "sentence_index": 4, "text": "- The Mixture of Gaussians experiment is a good illustration of how the choice of cost functions influences the solution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJgZjv4c37", "sentence_index": 5, "text": "However, I do not understand how are the *discrete* output y is handled.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "BJgZjv4c37", "sentence_index": 6, "text": "Is it indeed a discrete output (problem with lack of differentiability)? Softwax probability? Other modelling choice?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJgZjv4c37", "sentence_index": 7, "text": "- The inverse kinematics is an interesting illustration of the potential advantage of this method over conditional VAE and how close it is to ABC which can be reasonably computed for this problem.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJgZjv4c37", "sentence_index": 8, "text": "- For the medical application, INN outperforms other methods (except sometimes for ABC, which is far more expensive, or direct predictor, which doesn\u2019t provide uncertainty estimates) over some metrics such as the error on parameters recovery (Table 1), calibration error, and does indeed have a approximate posterior which seems to correspond to the ABC solution better.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJgZjv4c37", "sentence_index": 9, "text": "I\u2019m not sure I understand what we are supposed to learn from the astrophysics experiments.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJgZjv4c37", "sentence_index": 10, "text": "The method proposed and the general problem it aims at tackling seem interesting enough, the toy experiments demonstrates well the advantage of the method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BJgZjv4c37", "sentence_index": 11, "text": "However, the real-world experiments are not necessarily the easiest to read.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJgZjv4c37", "sentence_index": 12, "text": "EDIT: the concerns were mostly addressed in the revision.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJl-jwU6tB", "sentence_index": 0, "text": "This paper studies non-additive utility aggregation for sets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJl-jwU6tB", "sentence_index": 1, "text": "The problem is very interesting.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJl-jwU6tB", "sentence_index": 2, "text": "Choquet Integral is used to deal with set input.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJl-jwU6tB", "sentence_index": 3, "text": "The authors propose two architectures.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJl-jwU6tB", "sentence_index": 4, "text": "The two architectures, though not novel enough, are towards representing \u201cnon-additive utility\u201d.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJl-jwU6tB", "sentence_index": 5, "text": "However, the experimental comparison is not fair, the description of the model (e.g. how Choquet is integrated into the model and help to learn \u201cintermediate meaningful results\u201d) is not clear, some claims are not true.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJl-jwU6tB", "sentence_index": 6, "text": "First, the authors claim that they are the first to combine Choquet integral with deep learning.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJl-jwU6tB", "sentence_index": 7, "text": "However, there are a few, though not many, works in the literature trying to combine Choquet integral with deep learning.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "BJl-jwU6tB", "sentence_index": 8, "text": "For example, \u201cFuzzy Choquet Integration of Deep Convolutional Neural Networks for Remote Sensing\u201d by Derek T. Anderson et al.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "BJl-jwU6tB", "sentence_index": 9, "text": "Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "BJl-jwU6tB", "sentence_index": 10, "text": "How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "BJl-jwU6tB", "sentence_index": 11, "text": "These need to be further clarified.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "BJl-jwU6tB", "sentence_index": 12, "text": "Third, the comparison to baseline and \u201cDeepSet\u201d is not fair.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BJl-jwU6tB", "sentence_index": 13, "text": "According to the illustration, it seems that you first obtain \u201cfeatures/representations\u201d.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJl-jwU6tB", "sentence_index": 14, "text": "Then the representations are fed to the four architectures you listed in figure one.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJl-jwU6tB", "sentence_index": 15, "text": "RNN-based approaches are with better \u201ccomplexity\u201d comparing to your sum baseline and \u201cDeepset\u201d approach.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BJl-jwU6tB", "sentence_index": 16, "text": "So, I have some doubts about the experimental results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BJl5fZS6YS", "sentence_index": 0, "text": "This paper studies backdoor attacks under federated learning setting.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 1, "text": "To inject a certain backdoor pattern, existing work generate poisoning samples by blending the same pattern with different input samples.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 2, "text": "Even for federated learning where the adversary can control multiple parties, such as [1], all parties still use the same global backdoor pattern to generate poisoning samples locally.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 3, "text": "On the contrary, in this work, they decompose the global pattern into several small local patterns, and each adversarial party only uses a local pattern to generate poisoning samples.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 4, "text": "In their evaluation, they show that the backdoor attacks generated in this way are more effective, resilient to benign model parameter updates, and also survive better against existing defense algorithms against attacks in federated learning settings.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 5, "text": "I think the topic studied in this paper is very important and meaningful, and I am convinced that by decomposing a global pattern into several smaller local pieces, the model parameter updates computed by each party should be more similar to benign updates and thus can better bypass the defense algorithms.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BJl5fZS6YS", "sentence_index": 6, "text": "Meanwhile, the evaluation is pretty comprehensive and it is good to see that the conducted backdoor attacks are effective.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJl5fZS6YS", "sentence_index": 7, "text": "However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJl5fZS6YS", "sentence_index": 8, "text": "Thus, I would like to see more possible explanation on it.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJl5fZS6YS", "sentence_index": 9, "text": "Specifically, I have the following questions for clarification:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 10, "text": "1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "BJl5fZS6YS", "sentence_index": 11, "text": "2. To evaluate A-S setting, I understand that it may be tricky to enable a fair comparison between the centralized attack and DBA.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 12, "text": "However, one explanation of why DBA is more persistent in this case is because the adversarial parameter updates happen 4x times compared to the centralized attack.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 13, "text": "Therefore, another baseline to check is to conduct centralized attacks with the same number of times as DBA, but each update includes 1/4 number of poisoning samples, so that the total number of poisoning samples included to compute the gradient update still stays the same.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 14, "text": "3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJl5fZS6YS", "sentence_index": 15, "text": "For backdoor attacks, a line of work studies physical triggers, e.g., glasses in [2].", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 16, "text": "It is not natural to decompose such kind of patterns into several smaller pieces, unless the performance is significantly boosted.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJl5fZS6YS", "sentence_index": 17, "text": "4. Can the authors show concrete examples on how the attacks are generated?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "BJl5fZS6YS", "sentence_index": 18, "text": "The details are especially unclear on LOAN.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "BJl5fZS6YS", "sentence_index": 19, "text": "Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "BJl5fZS6YS", "sentence_index": 20, "text": "[1]  Bagdasaryan et al., How to backdoor federated learning.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 21, "text": "[2] Chen et al., Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 22, "text": "------------", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 23, "text": "Post-rebuttal comments", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJl5fZS6YS", "sentence_index": 24, "text": "I appreciate the authors' great effort to address my concerns! I think the evaluation in the current version of the paper is pretty comprehensive and provides a valuable study, and I am happy to raise my score accordingly.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJl5fZS6YS", "sentence_index": 25, "text": "-------------", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJloMuTnFr", "sentence_index": 0, "text": "The paper talks about calculating various statistics over data streams.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJloMuTnFr", "sentence_index": 1, "text": "This is a very popular topic and is very relevant in big data analysis.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BJloMuTnFr", "sentence_index": 2, "text": "A lot of work has been done in this general area and on the problems that are discussed in the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "BJloMuTnFr", "sentence_index": 3, "text": "The new idea in the paper is better streaming algorithms under the assumption that there is a \u201cheavy hitters\u201d oracle that returns data items that have a lot of representation in the stream.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "BJloMuTnFr", "sentence_index": 4, "text": "The authors give provably better algorithms for the distinct elements problem, F_p moment problem (p > 2), and some more problems.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "BJloMuTnFr", "sentence_index": 5, "text": "These are important problems in streaming data analysis.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BJloMuTnFr", "sentence_index": 6, "text": "They improve the space bounds and interestingly in some cases the bounds are better than what is possible without the oracle assumption.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJloMuTnFr", "sentence_index": 7, "text": "This also shows the power of such an oracle.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJloMuTnFr", "sentence_index": 8, "text": "There are experimental results to demonstrate the efficiency of the algorithms.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BJloMuTnFr", "sentence_index": 9, "text": "At a high level the work seems good and interesting for a large audience interested in streaming data analysis.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "BJloMuTnFr", "sentence_index": 10, "text": "I have not gone over the proofs in detail (much of which is in the appendix).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJloMuTnFr", "sentence_index": 11, "text": "- Even though oracle results are interesting, to make it practical it may make sense to talk about a more realistic, weaker oracle where some of the queries may be incorrect.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJloMuTnFr", "sentence_index": 12, "text": "- It may even make sense to minimise the number of oracle calls which can be thought of as a resource and discuss the relationship between number of oracle calls and other resources such as space.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJlOnG0gTX", "sentence_index": 0, "text": "The paper proposes an approach to learn nonlinear causal relationship from time series data that is based on empirical risk minimization regularized by mutual information.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJlOnG0gTX", "sentence_index": 1, "text": "The mutual information at the minimizer of the objective function  is used as causal measure.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlOnG0gTX", "sentence_index": 2, "text": "The paper is well written and the proposed method well motivate and intuitive.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BJlOnG0gTX", "sentence_index": 3, "text": "However I am concerned by the assumption that the lagged variables X_{t-1}^{(j)} follow a diagonal gaussian distribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJlOnG0gTX", "sentence_index": 4, "text": "This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJlOnG0gTX", "sentence_index": 5, "text": "Another key concern concerns scalability.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJlOnG0gTX", "sentence_index": 6, "text": "The authors mention gene regulatory networks , neuroscience etc as key applications.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlOnG0gTX", "sentence_index": 7, "text": "Yet the experiments considered in the paper are limited to very few time series.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJlOnG0gTX", "sentence_index": 8, "text": "For instance the simulation experiments use  N=30,", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlOnG0gTX", "sentence_index": 9, "text": "which is much smaller than the number of time series usually involved say in gene regulatory network data .", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJlOnG0gTX", "sentence_index": 11, "text": "The real data experiments use N= 6 or N=2.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlOnG0gTX", "sentence_index": 12, "text": "This is way to small.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJlOnG0gTX", "sentence_index": 13, "text": "The real data experiments (sections 4.2 and 4.3) are not very convincing, not only because of the very small size of N, but also because there is no comparison with the other approaches.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BJlOnG0gTX", "sentence_index": 14, "text": "How do these compare? Does the proposed approach offer  insights on these datasets which are not captured by the comparison methods?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BJlqUqW3tH", "sentence_index": 0, "text": "Summary: the paper purposes a dataset of abductive language inference and generation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJlqUqW3tH", "sentence_index": 1, "text": "The dataset is generated by human, while the testing set is adversarially selected using BERT.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJlqUqW3tH", "sentence_index": 2, "text": "The paper experiments the popular deep learning models on the dataset and observe shortcoming of deep learning on this task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJlqUqW3tH", "sentence_index": 3, "text": "Comments: overall, the problem on abductive inference and abductive generation in language in very interesting and important.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BJlqUqW3tH", "sentence_index": 4, "text": "This dataset seems valuable. And the paper is simple and well-written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BJlqUqW3tH", "sentence_index": 5, "text": "Concerns: I find the claim on deep networks kind of irresponsible.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlqUqW3tH", "sentence_index": 6, "text": "1. The dataset is adversarially filtered using BERT and GPT, which gives deep learning model a huge disadvantage. After all, the paper says BERT scores 88% before the dataset is attacked.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlqUqW3tH", "sentence_index": 7, "text": "2. The human score of 91.4% is based on majority vote, which should be compared with an ensemble of deep learning prediction.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BJlqUqW3tH", "sentence_index": 8, "text": "To compare the author should use the average score of human.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BJlqUqW3tH", "sentence_index": 9, "text": "3. The ground truth is selected by human.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlqUqW3tH", "sentence_index": 10, "text": "On a high level, the main difficulty of abduction is to search in the exponentially large space of hypothesis.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlqUqW3tH", "sentence_index": 11, "text": "Formulating the abduction task as a (binary) classification problem is less interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlqUqW3tH", "sentence_index": 12, "text": "The generative task is a better option.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlqUqW3tH", "sentence_index": 13, "text": "Decision: despite the seeming unfair comparison, this task is novel. I vote for weak accept.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "BJlU6IBK9S", "sentence_index": 0, "text": "The paper proposed variational selective autoencoders (VSAE) to learn from partially-observed multimodal data.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 1, "text": "Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 2, "text": "See below for detailed comments.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 3, "text": "[Pros]", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 4, "text": "1. The main idea of the paper is to propose a generative model that can handle partially-observed multimodal data during training.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 5, "text": "Specifically, prior work considered non-missing data during training, while we can't always guarantee that all the modalities are available.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 6, "text": "Especially in the field of multimodal learning, we often face the issue of imperfect sensors.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 7, "text": "This line of work should be encouraged.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 8, "text": "2. In my opinion, the idea is elegant.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BJlU6IBK9S", "sentence_index": 9, "text": "The way the author handles the missingness is by introducing an auxiliary binary random variable (the mask) for it.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 10, "text": "Nevertheless, its presentation and Figure 1 makes this elegant idea seems over-complicated.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 11, "text": "[Cons]", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 12, "text": "1. [The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 13, "text": "Specifically, the author mentioned Tsai et al. assumed factorized latent variables from the multimodal data, while Tsai et al. actually assumed the generation of multimodal data consists of disentangled modality-specific and multimodal factors.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 14, "text": "It seems to me; the author assumed data from one modality is generated by all the latent factors (see Eq. (11)), then what is the point for assuming the prior of the latent factor is factorized (see Eq. (4) and (5))?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 15, "text": "One possible explanation is because we want to handle the partially-observable issues from multimodal data, and it would be easier to make the latent factors factorized (see Eq. (6)).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 16, "text": "The author should comment on this.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 17, "text": "2. [Phrasing.] There are too many unconcise or informal phrases in the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 18, "text": "For example, I don't understand what does it mean in \"However, if training data is complete, ..... handle during missing data during test.\" Another example would be the last few paragraphs on page 4; they are very unclear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 19, "text": "Also, the author should avoid using the word \"simply\" too often (see the last few paragraphs on page 5).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 20, "text": "3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 21, "text": "I list some instances here.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 22, "text": "a. In Eq. (3), it surprises me to see the symbol \\epsilon without any explanation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 23, "text": "b. In Eq. (6), it also surprises me to see no description of \\phi and \\psi.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 24, "text": "The author should also add more explanation here, since Eq. (6)  stands a crucial role in the author's method.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJlU6IBK9S", "sentence_index": 26, "text": "c. Figure 1 is over-complicated.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 27, "text": "d. What is the metric in Table 1 and 2?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BJlU6IBK9S", "sentence_index": 28, "text": "The author never explains. E.g., link to NRMSE and PFC to the Table.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 29, "text": "e. What are the two modalities in Table 2? The author should explain.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 30, "text": "f. The author completely moved the results of MNIST-SVHN to Supplementary.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 31, "text": "It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 32, "text": "g. The author mentioned, in Table , the last two rows serve the upper bound for other methods.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 33, "text": "While some results are even better than the last two rows.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 34, "text": "The author should explain this.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "BJlU6IBK9S", "sentence_index": 35, "text": "h. Generally speaking, the paper does require a significant effort to polish Section 3 and 4.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 36, "text": "4. [Experiments.] The author presented a multimodal representation learning framework for partially-observable multimodal data, while the experiments cannot corraborrate the claim.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 37, "text": "First, I consider the tabular features as multi-feature data and less to be the multimodal data.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 38, "text": "Second, the synthetic image pairs are not multimodal in nature.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 39, "text": "These synthetic setting can be used for sanity check, but cannot be the main part of the experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 40, "text": "The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 41, "text": "Also, since the synthetic image pairs are not multimodal in nature, it is unclear to me for what the messages are conveyed in Figure 3 and 4.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJlU6IBK9S", "sentence_index": 42, "text": "I do expect the paper be a strong submission after a significant effort in presentation and experimental designs.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJlU6IBK9S", "sentence_index": 43, "text": "Therefore, I vote for weak rejection at this moment.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJxHwvW23X", "sentence_index": 0, "text": "The authors argue that not knowing the distribution of rewards observed in the policy gradient algorithm hinders learning (and the tuning process).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJxHwvW23X", "sentence_index": 1, "text": "They propose to replace the reward term in the policy gradient algorithm with its centered empirical cumulative distribution, which has a fixed and known U[-1, 1] distribution.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJxHwvW23X", "sentence_index": 2, "text": "They test their methods on a toy task that consists in finding inclusion maximal cliques (which tests for local optimality) against REINFORCE (including their variants: centering the rewards with a mean baseline or normalizing them), the cross-entropy method and Exp3.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJxHwvW23X", "sentence_index": 3, "text": "I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJxHwvW23X", "sentence_index": 4, "text": "The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BJxHwvW23X", "sentence_index": 5, "text": "The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJxHwvW23X", "sentence_index": 6, "text": "I would like the method to be applied on other domains such as continuous non-convex optimization and reinforcement learning.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "BJxHwvW23X", "sentence_index": 7, "text": "Additionally, I find the motivation for caring about local optimality unconvincing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJxHwvW23X", "sentence_index": 8, "text": "I take exception that people care more about local optimality than the actual objective.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJxHwvW23X", "sentence_index": 9, "text": "From a practical point of view, local optimality is a mean (that can be achieved via heuristic algorithms) to an end (the objective itself).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJxHwvW23X", "sentence_index": 10, "text": "This also holds for k-means, which is usually run multiple times with different starting conditions.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJxHwvW23X", "sentence_index": 11, "text": "Some comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJxHwvW23X", "sentence_index": 12, "text": "- Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJxHwvW23X", "sentence_index": 13, "text": "e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJxHwvW23X", "sentence_index": 14, "text": "- It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJxHwvW23X", "sentence_index": 15, "text": "- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BJxHwvW23X", "sentence_index": 16, "text": "- I would also encourage the authors to come up with a more descriptive name for the approach.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "BJxndjHwnm", "sentence_index": 0, "text": "This paper provides a new dynamic perspective on deep neural network.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJxndjHwnm", "sentence_index": 1, "text": "Based on Gaussian weights and biases, the paper investigates the evolution of the covariance matrix along with the layers.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJxndjHwnm", "sentence_index": 2, "text": "Eventually the matrices achieve a stationary point, i.e., fixed point of the dynamic system.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJxndjHwnm", "sentence_index": 3, "text": "Local performance around the fixed point is explored.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJxndjHwnm", "sentence_index": 4, "text": "Extensions are provided to include the batch normalization.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BJxndjHwnm", "sentence_index": 5, "text": "I believe this paper may stimulate some interesting ideas for other researchers.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BJxndjHwnm", "sentence_index": 6, "text": "Two technical questions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BJxndjHwnm", "sentence_index": 7, "text": "1. When the layers tends to infinity, the covariance matrix reaches stationary (fixed) point.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BJxndjHwnm", "sentence_index": 8, "text": "How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BJxndjHwnm", "sentence_index": 9, "text": "This somewhat conflicts the commonsense of \"the deeper the better?\"", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BJxndjHwnm", "sentence_index": 10, "text": "2. Typos: the weight matrix in the end of page 2 should be N_l times N_{l-1}. Also, the x_i's in the first line of page 3 should be bold.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BkeBNXOJT7", "sentence_index": 0, "text": "This paper presents a new method to measure the importance of hidden neurons in deep neural networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 1, "text": "The method integrates notions of activation value, input influence to a neuron and neuron influence to the network's output.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 2, "text": "They provide results confirming that this measure is able to identify neurons that are important for specific tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 3, "text": "Quality", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 4, "text": "The experiments are well designed to verify their hypothesis, although there could be more to make sure those results are not particular to the few selected problems.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "BkeBNXOJT7", "sentence_index": 5, "text": "Nevertheless, the results are consistent across those experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "BkeBNXOJT7", "sentence_index": 6, "text": "Clarity", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 7, "text": "The text is well written in general, but the structure could be improved.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 8, "text": "The introduction contains too much related work, which should be divided in another section.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkeBNXOJT7", "sentence_index": 9, "text": "Section 2 contains mostly high level explanations of the work, which should be integrated in the introduction, and thus before the related work section, to improve readability.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 10, "text": "See minor comments for more specific suggestions.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 11, "text": "It is difficult to understand the goal of Section 4.2.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkeBNXOJT7", "sentence_index": 12, "text": "Section 2 states that section 4.2 proves that a \"path method\" must be used in order to satisfy the axioms, but why such axioms are important is not stressed enough.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 13, "text": "Also, it is not clear why those are called axioms since they are not use to build anything else.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkeBNXOJT7", "sentence_index": 14, "text": "It seems to me that those are rather \"desirable properties\" than axioms.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 15, "text": "Originality", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 16, "text": "A important number of related works are cited and compared with the current work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "BkeBNXOJT7", "sentence_index": 17, "text": "Although the proposed measure is close to what is proposed by Datta et al., this paper makes the distinction clear and benchmarks its results properly against it.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "BkeBNXOJT7", "sentence_index": 18, "text": "Significance", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 19, "text": "There is an increasing need to interpretability of deep neural networks as they get more and more applied to real-world problems.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 20, "text": "Measures as the one proposed in this paper are a very important building block towards this.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BkeBNXOJT7", "sentence_index": 21, "text": "Conclusion", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 22, "text": "For its original importance measure and the proper experiment benchmarks, I believe this paper should be accepted.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "BkeBNXOJT7", "sentence_index": 23, "text": "There is however many minor issues that should be fixed for the camera-ready version.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 24, "text": "Although the recommended length is 8 pages, the strict limit is 10, so I would recommended to use a bit of the remaining extra space to conclude the paper properly with a discussion on the results and their consequences, as well as a conclusion to wrap up the paper.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 25, "text": "***", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 26, "text": "Minor Comments", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 27, "text": "Introduction:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 28, "text": "- The term flow is never defined precisely, we need to infer it based on the definition of conductance and attribution.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 30, "text": "- First paragraph would be more clear with simple word explanation rather than maths.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 31, "text": "Also, second sentence is not a complete sentence", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkeBNXOJT7", "sentence_index": 32, "text": "- Work on image indicators of importance could be compared better with current work.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 33, "text": "Indicators can be seen as a measure of importance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 34, "text": "- This sentence is not clear: \"[...]; the nature of correlations in the two models may differ\".", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 35, "text": "Section 2:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 36, "text": "- Last paragraph of section 2 can be true for any well-performing importance measure.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 37, "text": "The statements should be put in perspective with others.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 38, "text": "Section 3:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 39, "text": "- Section 3 should be introduced by explaining the goal of the section otherwise it breaks the flow of reading.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 40, "text": "- The role of the baseline x' should be better explained when it is presented (first paragraph of section 3).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 41, "text": "- The interchangeable use of the term \"conductance of neuron j\" for equations 2 and 3 is confusing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkeBNXOJT7", "sentence_index": 42, "text": "Different terms should be use, even if the context makes it possible to infer which one is being referred to.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 43, "text": "- Remark 1 seems trivial, but the selection of baseline x' seems less trivial.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 44, "text": "Some explanations should be devoted to it.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 45, "text": "- Second paragraph of remark 1 is not clear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 46, "text": "Why couldn't we take another layer's neuron as the neuron of interest, bounding the conductance measure on one layer as the input and the output of the model? If we make the input to be a neuron y rather than the true input x, we could take another neuron z in a subsequent layer to be the neuron of interest, resulting in conductance measure Cond^z_i(y).", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 47, "text": "Section 4:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 48, "text": "- List of importance measure at beginning of Section 4 should probably have citations.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 49, "text": "- Backward reference to section 3 seems to be a mistake, should it be subsection 4.2?", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 50, "text": "- Each of the justifications to get around the issue of distinguishing strange model behavior from bad feature importance technique should be explained briefly in paragraph before section 4.1.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 51, "text": "Subsection 4.1:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 52, "text": "- I do not understand the problem explained in fourth paragraph of section 4.1.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 53, "text": "g(f(1 - epsilon)) = 0, why would it be 1- epsilon?", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 54, "text": "- Problem explained in fifth Paragraph of section is not clear unless what the influence of the unit is clearly stated. Is it simply dg/df?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 55, "text": "- A short explanation of what is tested in section 6 should be given at last paragraph of section 4.1.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 56, "text": "Although the results are favorable to the conductance metric, it is not clear how they precisely confirm the problem of incorrect signs presented in the caricature examples.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 57, "text": "Subsection 4.2:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 58, "text": "- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only \"path methods\" can satisfy.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkeBNXOJT7", "sentence_index": 59, "text": "- Footnote 2 on page 5 it difficult to read.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkeBNXOJT7", "sentence_index": 60, "text": "- Although the proof does not seem to use the axioms as a building block, which is fortunate since it would make it a circular argument otherwise, the text suggests so: \"Given these three axioms, we can show that:\".", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 61, "text": "- The importance of section 4.2 should be clarified.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 62, "text": "More emphasis on the importance of the axioms (desirable properties) should be made.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 63, "text": "Section 5:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 64, "text": "- Choices for experiments should be explained. Why choosing layers mixed** rather than others? Why choosing filters?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 65, "text": "- Figures 1-4 are difficult to interpreted on a printed version.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkeBNXOJT7", "sentence_index": 66, "text": "Since this is qualitative, I suggest to change the saturation of the images to make them easier to interpret.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 67, "text": "The absolute values are not important for a qualitative interpretation", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 68, "text": "- Figure 4 could be more interesting if compared with other classes, like other animal faces.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 69, "text": "Anyhow, I understand that those were chosen based on the subset of classes used for the experiments.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 70, "text": "- Space should be added between figures to better divide the captions", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 71, "text": "Section 6:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 72, "text": "- The difference between experiments of Figure 5 and 6 should be made more clear.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeBNXOJT7", "sentence_index": 73, "text": "Section 7-8:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeBNXOJT7", "sentence_index": 74, "text": "- Where are they? No discussion? No conclusion?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "BkemLDvAn7", "sentence_index": 0, "text": "This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkemLDvAn7", "sentence_index": 3, "text": "The basic idea is to combine the graph attention networks (Velic\u030ckovic\u0301 et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkemLDvAn7", "sentence_index": 8, "text": "This paper is generally easy to follow and written clearly.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BkemLDvAn7", "sentence_index": 9, "text": "Several experiments are conducted to demonstrate the performance of the proposed model.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BkemLDvAn7", "sentence_index": 11, "text": "Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "BkemLDvAn7", "sentence_index": 14, "text": "i) The proposed architecture is mainly adopted from the graph attention networks (Velic\u030ckovic\u0301 et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkemLDvAn7", "sentence_index": 18, "text": "Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "BkemLDvAn7", "sentence_index": 21, "text": "ii) In table 2, I don\u2019t really see any promising results compared to baselines. There are", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkemLDvAn7", "sentence_index": 22, "text": "little improvements over the baselines or even significantly worse. More importantly,", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkemLDvAn7", "sentence_index": 23, "text": "compared two schemes of this work, the ones with attentions are \u201calmost\u201d identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkemLDvAn7", "sentence_index": 25, "text": "in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkemLDvAn7", "sentence_index": 27, "text": "iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkemLDvAn7", "sentence_index": 29, "text": "MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkemLDvAn7", "sentence_index": 31, "text": "MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkemLDvAn7", "sentence_index": 33, "text": "Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkemLDvAn7", "sentence_index": 35, "text": "Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "BkeNja15nm", "sentence_index": 0, "text": "Brief summary:", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeNja15nm", "sentence_index": 1, "text": "This paper presents a deep decoder model which given a target natural image and a random noise tensor learns to decode the noise tensor into the target image by a series of 1x1 convolutions, RELUs, layer wise normalizations and upsampling.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeNja15nm", "sentence_index": 2, "text": "The parameter of the convolution are fitted to each target image, where the source noise tensor is fixed.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeNja15nm", "sentence_index": 3, "text": "The method is shown to serve as a good model for natural image for a variety of image processing tasks such as denoising and compression.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeNja15nm", "sentence_index": 4, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeNja15nm", "sentence_index": 5, "text": "* an interesting model which is quite intriguing in its simplicity.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "BkeNja15nm", "sentence_index": 6, "text": "* good results and good analysis of the model", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "BkeNja15nm", "sentence_index": 7, "text": "* mostly clear writing and presentation (few typos etc. nothing too serious).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BkeNja15nm", "sentence_index": 8, "text": "Cons and comments:", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeNja15nm", "sentence_index": 9, "text": "* The author say explicitly that this is not a convolutional model because of the use of 1x1 convolutions.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeNja15nm", "sentence_index": 10, "text": "I disagree and I actually think this is important for two reasons.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BkeNja15nm", "sentence_index": 11, "text": "First, though these are 1x1 convolutions, because of the up-sampling operation and the layer wise normalizations the influence of each operation goes beyond the 1x1 support.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeNja15nm", "sentence_index": 12, "text": "Furthermore, and more importantly is the weight sharing scheme induced by this - using convolutions is a very natural choice for natural images (no pun intended) due to the translation invariant statistics of natural images.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeNja15nm", "sentence_index": 13, "text": "I doubt this would have worked so well hadn't it been modeled this way (not to mention this allows a small number of parameters).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BkeNja15nm", "sentence_index": 14, "text": "* The upsampling analysis is interesting but it is only done on synthetic data - will the result hold for natural images as well? should be easy to try and will allow a better understanding of this choice.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BkeNja15nm", "sentence_index": 15, "text": "Natural images are only approximately piece-wise smooth after all.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeNja15nm", "sentence_index": 16, "text": "* The use of the name \"batch-norm\" for the layer wise normalization is both wrong and misleading.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkeNja15nm", "sentence_index": 17, "text": "This is just channel-wise normalization with some extra parameters - no need to call it this way (even if it's implemented with the same function) as there is no \"batch\".", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeNja15nm", "sentence_index": 18, "text": "* I would have loved to see actual analysis of the method's performance as a function of the noise standard deviation.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BkeNja15nm", "sentence_index": 19, "text": "Specifically, for a fixed k, how would performance increase or decrease, and vice versa - for a given noise level, how would k affect performance.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BkeNja15nm", "sentence_index": 20, "text": "* The actual standard deviation of the noise is not mentioned in any of the experiments (as far as I could tell)", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeNja15nm", "sentence_index": 21, "text": "* What does the decoder produce when taking a trained C on a given image and changing the source noise tensor?", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BkeNja15nm", "sentence_index": 22, "text": "I think that would shed light on what structures are learned and how they propagated in the image, possibly more than Figure 6 (which should really have something to compare to because it's not very informative out of context).", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkeWgLUpnX", "sentence_index": 0, "text": "This paper aims to estimate time-delayed, nonlinear causal influences from time series under the causal sufficiency assumption.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeWgLUpnX", "sentence_index": 1, "text": "It is easy to follow and contains a lot of empirical results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BkeWgLUpnX", "sentence_index": 2, "text": "Thanks for the results, but I have several questions.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeWgLUpnX", "sentence_index": 3, "text": "First, In Theorem 2, which seems to be a main result of the paper, the authors were concerned with the condition when W_{ji} >0, but there is not conclusion if W_{ji} =0.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkeWgLUpnX", "sentence_index": 4, "text": "In order to correctly estimate causal relations from data, both cases must be considered.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeWgLUpnX", "sentence_index": 5, "text": "Second, the conclusion of Theorem 2 seems to be flawed.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BkeWgLUpnX", "sentence_index": 6, "text": "Let me try to make it clear with the following example.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeWgLUpnX", "sentence_index": 7, "text": "Suppose x^1_{t-2} directly causes x^2_{t-1} and that x^2_{t-1} directly causes x^3_{t}, without a direct influence from x^1_{t-2}  to x^3_{t}. Then when minimizing (2), we have the following results step by step:", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeWgLUpnX", "sentence_index": 8, "text": "1) The noise standard deviation in x^2_{t-1}, denoted by \\eta_2, may be non-zero.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeWgLUpnX", "sentence_index": 9, "text": "This is because we minimize a tradeoff of the prediction error (the first term in (2)) and a function of the reciprocal of the noise standard deviation \\eta_2 (the second term in (2)), not only the prediction error.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeWgLUpnX", "sentence_index": 10, "text": "2) If \\eta_2 is non-zero, then x^1_{t-2} will be useful for the purpose of predicting x^3_{t}. (Note that if \\eta_2 is zero, then x^1_{t-2} is not useful for predicting x^3_{t).) From the d-separation perspective, this is because x^1_{t-2} and x^3_{t} are not d-separated by x^2_{t-1} + \\eta_2 \\cdot \\epsilon_2, although they are d-separated by x^2_{t-1}. Then the causal Markov condition tells use that x^1_{t-2} and x^3_{t} are not independent conditional on x^2_{t-1} + \\eta_2 \\cdot \\epsilon_2, which means that x^1_{t-2} is useful for predicting x^3_{t}.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeWgLUpnX", "sentence_index": 11, "text": "3) Given that x^1_{t-2} is useful for predicting x^3_{t}, when (2) is minimized, \\eta_1 will not go to infinity, resulting in a non-zero W_{13), which *mistakenly* tells us that X^{1}_{t-1} directly structurally causes x^{(3)}_t.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeWgLUpnX", "sentence_index": 12, "text": "This illustrates that the conclusion of Theorem 2 may be wrong.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BkeWgLUpnX", "sentence_index": 13, "text": "I believe this is because the proof of Theorem 2 is flawed in lines 5-6 on Page 16.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeWgLUpnX", "sentence_index": 14, "text": "It does not seem sensible to drop X^{j}_{t-1} + \\eta_X \\cdot \\epsilon_X and attain a smaller value of the cost function at the same time.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkeWgLUpnX", "sentence_index": 15, "text": "Please carefully check it, especially the argument given in lines 10-13.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeWgLUpnX", "sentence_index": 16, "text": "Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkeWgLUpnX", "sentence_index": 17, "text": "Such methods are directly applicable to time-delayed causal relations by further considering the constraint that effects temporally follow the causes.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeWgLUpnX", "sentence_index": 18, "text": "Fourth, please make it clear that the proposed method aims to estimate \"causality-in-mean\" because of the formulation in terms of regression.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkeWgLUpnX", "sentence_index": 19, "text": "For instance, if x^j_{t-1} influences only the variance of x^i_{t}, but not its mean, then the proposed method may not detect such a causal influence, although the constraint-based methods can.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeWgLUpnX", "sentence_index": 20, "text": "Any response would be highly appreciated.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkeXRoYqhm", "sentence_index": 0, "text": "The paper proposed a hierarchical framework for problem embedding and intended to apply it to adaptive tutoring.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeXRoYqhm", "sentence_index": 1, "text": "The system first used a rule-based method to extract the concepts for problems and then learned the concept embeddings and used them for problem representation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeXRoYqhm", "sentence_index": 2, "text": "In addition, the paper further proposed negative pre-training for training with imbalanced data sets to decrease false negatives and positives.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeXRoYqhm", "sentence_index": 3, "text": "The methods are compared with some other word-embedding based methods and showed 100% accuracy in a similarity detection test on a very small dataset.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeXRoYqhm", "sentence_index": 4, "text": "In sum, the paper has a very good application but not good enough as a research paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "BkeXRoYqhm", "sentence_index": 5, "text": "Some of the problems are listed as follows:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeXRoYqhm", "sentence_index": 6, "text": "1.\tLack of technical novelty.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "BkeXRoYqhm", "sentence_index": 7, "text": "It seems to me just a combination of several mature techniques.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "BkeXRoYqhm", "sentence_index": 8, "text": "I do not see much insight into the problem.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "BkeXRoYqhm", "sentence_index": 9, "text": "For example, if the rule-based concept extractor can already extract concepts very well, the \u201cproblem retrieval\u201d should be solved by searching with the concepts as queries.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BkeXRoYqhm", "sentence_index": 10, "text": "Why should we use embedding to compare the similarity?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BkeXRoYqhm", "sentence_index": 11, "text": "Also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BkeXRoYqhm", "sentence_index": 12, "text": "2.\tData size is too small, and the baselines are not state-of-the-art. There are some unsupervised sentence embedding methods other than the word-embedding based models.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkeXRoYqhm", "sentence_index": 14, "text": "Some clarity issues.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeXRoYqhm", "sentence_index": 15, "text": "For example, Page 6. \u201cis pre-trained on a pure set of negative samples\u201d\u2014 what is the objective function? How to train on only negative samples?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkgIGYrAKS", "sentence_index": 0, "text": "*Summary*", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 1, "text": "This paper study the convergence of Hamiltonian gradient descent (HGD) on minmax games.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 2, "text": "The paper show that under some assumption on the cost function of the min max that are (in some sense) weaker than strong convex-concavity.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 3, "text": "More precisely, they use the \u2018bilinearity\u2019 of the objective (due to the interaction between the players) to prove that the squared norm of the vector field of the game follows some Polyak Lojasiewicz condition.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 4, "text": "Thus the proof is concluded by the linear (resp. sublinear) convergence of gradient descent (resp. stochastic GD) under PL assumption.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 5, "text": "*Decision*", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 6, "text": "I think that is work is clearly very interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BkgIGYrAKS", "sentence_index": 7, "text": "The fact to prove linear convergence rate without strong-convex-concavity is quite surprising. And this paper brings nice tools to analyse HGD.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BkgIGYrAKS", "sentence_index": 8, "text": "Also the result on Stochastic HGD is very interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BkgIGYrAKS", "sentence_index": 9, "text": "However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkgIGYrAKS", "sentence_index": 10, "text": "One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkgIGYrAKS", "sentence_index": 11, "text": "Regarding the practical limitation of this work:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 12, "text": "- the sufficient bilinearity condition are hard to meet in practice. (even for convex-concave problems)", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 13, "text": "- In a non-convex-concave setting, Hamiltonian gradient descent is attracted the any stationary point, even \u201clocal maxima\u201d (or the equivalent in the minmax setting).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 14, "text": "Making this algorithm not very practical. (However, CO is)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkgIGYrAKS", "sentence_index": 16, "text": "However, I really think that the community is currently lacking of understanding on minmax optimization and that we need better training method in many practical emergent frameworks that are minmax (such as GANs or multi agent learning). That is why, I would vote for a weak accept.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BkgIGYrAKS", "sentence_index": 17, "text": "*Questions*", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 18, "text": "- What are the practical implication of your work ? for instance does it say anything on how to tune $\\gamma$ for CO ?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "BkgIGYrAKS", "sentence_index": 19, "text": "*Remarks*", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 20, "text": "- It is claimed that Theorem 3.4 gives the first linear convergence rate for minmax that does not require strong-convex or linearity.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 21, "text": "Note that, recently [1] seem to propose a result on extragradient in the same vein (i.e. without strong convexity or linearity).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 22, "text": "- (Minor) $\\alpha$ not alway have the same unit: Thm 3.2 it is proportional to a strong convexity and in Lemma 4.7 it is proportional to a strong convexity squared (actually the PL of the squared norm of the gradient).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "BkgIGYrAKS", "sentence_index": 23, "text": "For clarity it might be interesting to use the notation $\\alpha^2$ in Lemma 4.7.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkgIGYrAKS", "sentence_index": 24, "text": "The same way for unit consistency I would use $L_H^2$ instead of $L_H$", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkgIGYrAKS", "sentence_index": 25, "text": "[1] Azizian, Wa\u00efss, et al. \"A Tight and Unified Analysis of Extragradient for a Whole Spectrum of Differentiable Games.\" arXiv preprint arXiv:1906.05945 (2019).", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 26, "text": "=== After rebuttal = ==", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 28, "text": "I've read the authors's response.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 29, "text": "The concern raised by reviewer 3 is very important.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "BkgIGYrAKS", "sentence_index": 30, "text": "The descent lemma used by the author is not valid for the stochastic result.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BkgIGYrAKS", "sentence_index": 31, "text": "The authors should address that in their revision.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BkgIGYrAKS", "sentence_index": 32, "text": "I however maintain my grade.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 0, "text": "The paper studies self-supervised learning from very few unlabeled images, down to the extreme case where only a single image is used for training.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 1, "text": "From the few/single image(s) available for training, a data set of the same size as some unmodified reference data set (ImageNet, Cifar-10/100) is generated through heavy data augmentation (cropping, scaling, rotation, contrast changes, adding noise).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 2, "text": "Three popular self-supervised learning algorithms are then trained on this data sets, namely (Bi)GAN, RotNet, and DeepCluster, and the linear probing accuracy on different blocks is compared to that obtained by training the same methods on the reference data sets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 3, "text": "The linear probing accuracy from the first few conv layers of the network trained on the single/few image data set is found to be comparable to or better than that of the same model trained on the full reference data set.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 4, "text": "I enjoyed the paper; it addresses the interesting setting of an extremely small data set which complements the large number of studies on scaling up self-supervised learning algorithms.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BkgPU07CFS", "sentence_index": 5, "text": "I think it is not extremely surprising that using the proposed strategy allows to learn low level features as captured by the first few layers, but I think it is worth studying and quantifying.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BkgPU07CFS", "sentence_index": 6, "text": "The experiments are carefully described and presented, and the paper is well-written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BkgPU07CFS", "sentence_index": 7, "text": "Here are a few questions and concerns:", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 8, "text": "- How much does the image matter for the single-image data set?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkgPU07CFS", "sentence_index": 9, "text": "The selected images A and B are of very high entropy and show a lot of different objects (image A) and animals (image B). How do the results change if e.g. a landscape image or an abstract architecture photo is used?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkgPU07CFS", "sentence_index": 10, "text": "- How general is the proposed approach? How likely is it to generalize to other approaches such as Jigsaw (Doersch et al., 2015) and Exemplar (Dosovitskiy et al., 2016)? It would be good to comment on this.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkgPU07CFS", "sentence_index": 11, "text": "- [1] found that the network architecture for self-supervised learning can matter a lot, and that by using a ResNet architecture, performance of SSL methods can be significantly improved.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 12, "text": "In particular, the linear probing accuracy appears to be often monotonic as a function of the depth of the layer it is computed from.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 13, "text": "This is in contrast to what is observed for AlexNet in Tables 2 and 3, where the conv5 accuracy is lower than the conv4.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 14, "text": "It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkgPU07CFS", "sentence_index": 15, "text": "- Does the MonoGAN exhibit stable training dynamics comparable to training WGAN on CIFAR-10, or do the training dynamics change on the single-image data set?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkgPU07CFS", "sentence_index": 16, "text": "Overall, I\u2019m leaning towards accepting the paper, but it would be important to see how well the experiments generalize to i) ResNet and ii) other (lower entropy) input images.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BkgPU07CFS", "sentence_index": 17, "text": "[1] Kolesnikov, A., Zhai, X. and Beyer, L., 2019. Revisiting self-supervised visual representation learning. arXiv preprint arXiv:1901.09005.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 19, "text": "---", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 20, "text": "Update after rebuttal:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 21, "text": "I thank the authors for their detailed response.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 22, "text": "I appreciate the efforts of the authors into investigating the issues raised, the described experiments sound promising.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 23, "text": "Unfortunately, the new results are not presented in the revision.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgPU07CFS", "sentence_index": 24, "text": "I will therefore keep my rating.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgYfE5o3Q", "sentence_index": 0, "text": "This paper proposes a justification to one observation on VAE: \"restricting the family of variational approximations can, in fact, have a positive regularizing effect, leading to better generalization\".", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkgYfE5o3Q", "sentence_index": 1, "text": "The explanation given in this work is based on Gaussian mean-field approximation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkgYfE5o3Q", "sentence_index": 2, "text": "I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkgYfE5o3Q", "sentence_index": 3, "text": "- the sentence under eq. (2)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkgYfE5o3Q", "sentence_index": 4, "text": "- the sentence \"Bacause the identity of the datapoint can never be learned by ...\" What is the identity of a dat point?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkgYfE5o3Q", "sentence_index": 5, "text": "It looks like section 2.1 wants to show the connections between eq. (2) and other popularly used inference methods.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgYfE5o3Q", "sentence_index": 6, "text": "Somehow, those connections are not clear to me.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkgYfE5o3Q", "sentence_index": 7, "text": "Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BkgYfE5o3Q", "sentence_index": 8, "text": "As in (Cover and Thomas, 2012), which is also cited in this paper, DPI is defined on a Markov chain X -> Y -> Z and we have I(X,Y) >= I(X,Z).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkgYfE5o3Q", "sentence_index": 9, "text": "However, based on the definition of \\theta and \\tilde{\\theta} given in the first sentence of section 2.3, the relation between \\theta, \\tilde{\\theta} and D should be: D <- \\theta -> \\tilde{\\theta} (if it is a generative model) or D -> \\theta -> \\tilde{\\theta} (if a discriminative model).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BkgYfE5o3Q", "sentence_index": 11, "text": "Either case, I don't think we can have the inequality in eq. (5).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BklhkXfqn7", "sentence_index": 0, "text": "The paper tackles the problem of semi-supervised classification using GAN-based models.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BklhkXfqn7", "sentence_index": 1, "text": "They proposed a manifold regularization by approximating the Laplacian norm using the stochastic finite difference.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BklhkXfqn7", "sentence_index": 2, "text": "The motivation is that making the classifier invariant to perturbations along the manifold is more reasonable than random perturbations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BklhkXfqn7", "sentence_index": 3, "text": "The idea is to use GAN to learn the manifold.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BklhkXfqn7", "sentence_index": 4, "text": "The difficulty is that (the gradient of) Laplacian norm is impractical to compute for DNNs.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BklhkXfqn7", "sentence_index": 5, "text": "They stated that another approximation of the manifold gradient, i.e. adding Gaussian noise \\delta to z directly (||f(z) - f(g(z+\\delta))||_F) has some drawbacks when the magnitude of noise is too large or too small.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BklhkXfqn7", "sentence_index": 6, "text": "The authors proposed another improved gradient approximation by first computing the normalized manifold gradient \\bar r(z) and then adding a tunable magnitude of \\bar r(z) to g(z), i.e., ||f(z) - f(g(z) +\\epsilon \\bar r(z) )||_F. Since several previous works Kumar et al. (2017) and Qi et al. (2018) also applied the idea of manifold regularization into GAN, the authors pointed out several advantages of their new regularization.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BklhkXfqn7", "sentence_index": 7, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BklhkXfqn7", "sentence_index": 8, "text": "- The paper is clearly written and easy to follow. It gives some intuitive explanations of why their method works.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BklhkXfqn7", "sentence_index": 9, "text": "- The idea is simple and easy to implement based on a standard GAN.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "BklhkXfqn7", "sentence_index": 10, "text": "- The authors conduct various experiments to show the interaction of the regularization and the generator.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BklhkXfqn7", "sentence_index": 11, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BklhkXfqn7", "sentence_index": 12, "text": "- For semi-supervised classification, the paper did not report the best results in other baselines.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BklhkXfqn7", "sentence_index": 13, "text": "E.g., in Table 1 and 2,  the best result of VAT (Miyato et al., 2017) is VAT+Ent, 13.15 for CIFAR-10 (4000 labels) and 4.28 for SVHN (1000 labels).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BklhkXfqn7", "sentence_index": 14, "text": "The performance of the proposed method is worse than the previous work but they claimed \"state-of-the-art\" results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BklhkXfqn7", "sentence_index": 15, "text": "The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2].", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BklhkXfqn7", "sentence_index": 16, "text": "The experimental results are not very convincing because many importance baselines are neglected.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BklhkXfqn7", "sentence_index": 17, "text": "- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "BklhkXfqn7", "sentence_index": 18, "text": "I'm wondering whether other smoothness regularizations can achieve the same effect when applied to semi-supervised learning, e.g. spectral normalization[3].", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "BklhkXfqn7", "sentence_index": 19, "text": "It would be better to compare with them.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "BklhkXfqn7", "sentence_index": 20, "text": "References:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BklhkXfqn7", "sentence_index": 21, "text": "[1] Adversarial Dropout for Supervised and Semi-Supervised Learning, AAAI 2018", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklhkXfqn7", "sentence_index": 22, "text": "[2] Smooth Neighbors on Teacher Graphs for Semi-supervised Learning, CVPR 2018", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklhkXfqn7", "sentence_index": 23, "text": "[3] Spectral Normalization for Generative Adversarial Networks, ICLR 2018", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 0, "text": "Second update:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 1, "text": "We thank the authors for updating their paper.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 2, "text": "The work is now improving, and is on the right track for publication at a future conference.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 3, "text": "There are a few comments on the new results, and suggestions for further improvement:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 4, "text": "* The issue of possible false positives due to sample dependence in fMRI data has again been ignored in the rebuttal. Without careful treatment of this effect, these results are vacuous.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BklLqYip9B", "sentence_index": 6, "text": "* There is still no comparison with competing nonparametric tests on the fMRI data.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklLqYip9B", "sentence_index": 7, "text": "* the results linking the COCO and MINE estimators are interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "BklLqYip9B", "sentence_index": 8, "text": "Some statements don't make sense, however, eg. \"HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BklLqYip9B", "sentence_index": 9, "text": "\" First, f and g are functions, not kernels.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BklLqYip9B", "sentence_index": 10, "text": "Second, testing with HSIC or COCO does not require generalisation to unseen data points: this is why testing is an easier problem than regression. For HSIC testing, I am surprised to read in the footnote that the Gamma approximation report significantly more than 5% errors, especially given the Table 4 results that show the correct level. In any case, I very strongly suggest using a permutation approach to obtain the test threshold for HSIC, which is by far the most robust and reliable method.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 13, "text": "The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklLqYip9B", "sentence_index": 14, "text": "Permutation gives a guarantee of the correct level. See the NeurIPS paper for details. Once you have verified the correct false positive rate for the permutation threshold, then you can compute the p-value on the alternative.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 17, "text": "While the comparison with HSIC is a helpful one, it is also required to compare with the competing method closest to yours, i.e. the Berrett  and Samworth test. In addition, HSIC is a non-adaptive test, but your test is adaptive, so a fairer comparison would be to a modern adaptive test such as \"An Adaptive Test of Independence with Analytic Kernel Embeddings.\" *", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BklLqYip9B", "sentence_index": 21, "text": "The false positive rate in the sanity check is far below the design level of 0.05.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 22, "text": "This is as I expected, given the use of the Hoeffding bound.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 23, "text": "This should be stated clearly in the main text, and not disclosed in the final sentence of the final page of the appendix.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BklLqYip9B", "sentence_index": 24, "text": "====================", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 25, "text": "Update: thank you for your rebuttal.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 26, "text": "\"the necessary and sufficient condition of dependency, is more general and is complementary to other techniques that make stronger assumptions about the data. \"", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 27, "text": "The alternative tests listed are nonparametric.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 28, "text": "That is to say, unlike Pearson correlation, they do not make specific parametric assumptions on the nature of the dependence.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 29, "text": "Rather they make generic smoothness assumptions (your test also makes such assumptions by the choice of neural network architecture).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 30, "text": "Thus, comparison with the prior work in Statistics and Machine Learning is relevant, since these tests have the same aims and scope as your tests.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BklLqYip9B", "sentence_index": 31, "text": "The cited Berrett and Samworth MI test uses a permutation approach to obtaining the test threshold, not an asymptotic approach  (see the results of Section 4 of that paper).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BklLqYip9B", "sentence_index": 32, "text": "Several of the other cited tests also use a permutation approach for the test threhsold.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 33, "text": "These tests are therefore relevant prior work.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 34, "text": "\" If things change very little from one second to the next, the signals could be very similar and may not really be, intuitively, independent samples and may bias result of the study.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 35, "text": "However, which independence assumptions to use is not in scope for our paper, because our fMRI study is trying to show that dependency testing works \"", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 37, "text": "In the work cited by Chwialkowski et al, failure to account for the dependence between samples results in excessive false positives.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 38, "text": "This is because, for dependent data, the effective sample size is reduced, and the tests must be made more conservative to correct for this effect.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 39, "text": "It is therefore the case that the fMRI results may be false positives.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 40, "text": "Re level:\"This proof could be experimentally verified ...\"  This should be verified. In particular, Hoeffding can be very loose in practice, which is likely to be observed in experiments.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 41, "text": "======", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 42, "text": "The authors propose a procedure for improving neural mutual information estimates, via a combination of data augmentation and cross validation. They then use these estimates in hypothesis testing, on low dimensional toy datasets and on high dimensional real-world fMRI data.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 44, "text": "The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved. In hypothesis testing, it is important to verify that the test has the correct level (false positive rate). This is all the more essential when the estimate has required optimisation over parameters.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklLqYip9B", "sentence_index": 47, "text": "It is not clear from the presentation that this has been confirmed.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BklLqYip9B", "sentence_index": 48, "text": "There are a number of prior approaches to testing for multivariate statistical dependence in the machine learning and statistics literature (including a 2017 paper which uses mutual information).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 49, "text": "A small selection is given below, although a literature search will reveal many more papers.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 50, "text": "In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklLqYip9B", "sentence_index": 51, "text": "In statistics: ---------------", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 53, "text": "https://arxiv.org/abs/1711.06642 Nonparametric independence testing via mutual information", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 55, "text": "Thomas B. Berrett, Richard J. Samworth 2017 Measuring and testing dependence by correlation of distances", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 58, "text": "G\u00e1bor J. Sz\u00e9kely, Maria L. Rizzo, and Nail K. Bakirov Ann. Statist. Volume 35, Number 6 (2007), 2769-2794. Large-scale kernel methods for independence testing", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 62, "text": "Qinyi ZhangEmail Sarah Filippi, Arthur Gretton, Dino Sejdinovic Statistics and Computing January 2018, Volume 28, Issue 1, pp 113\u2013130| Cite as", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 65, "text": "In machine learning: ---------------------", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 67, "text": "Multivariate tests of association based on univariate tests Heller, Ruth and Heller, Yair", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 69, "text": "Advances in Neural Information Processing Systems 29 2016", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 71, "text": "A Kernel Statistical Test of Independence Gretton, Arthur and Fukumizu, Kenji and Choon H. Teo and Song, Le and Sch\\\"{o}lkopf, Bernhard and Alex J. Smola", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 73, "text": "Advances in Neural Information Processing Systems 20 2008 http://papers.nips.cc/paper/3201-a-kernel-statistical-test-of-independence.pdf http://proceedings.mlr.press/v70/jitkrittum17a/jitkrittum17a.pdf", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 77, "text": "An Adaptive Test of Independence with Analytic Kernel Embeddings Wittawat Jitkrittum, Zolt\u00e1n Szab\u00f3, Arthur Gretton ; ICML 2017, PMLR 70:1742-1751", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 79, "text": "Time dependence ----------------", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 81, "text": "It is also the case that if the variables have time dependence, then appropriate corrections must be made for the test threshold, to avoid excessive false positives.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklLqYip9B", "sentence_index": 82, "text": "Does the fMRI data exhibit time dependence? For the case of multivariate statistical dependence testing, such corrections are described e.g. in: https://papers.nips.cc/paper/5452-a-wild-bootstrap-for-degenerate-kernel-tests.pdf A Wild Bootstrap for Degenerate Kernel Tests Kacper Chwialkowski, Dino Sejdinovic, Arthur Gretton NeurIPS 2014", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BklyWIdlcS", "sentence_index": 0, "text": "This paper targets at studying the mutual exclusive bias which existed in children learning, to help understand whether there exists similar bias in deep networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BklyWIdlcS", "sentence_index": 1, "text": "In general, the whole paper tries to tell a very interesting, and good story.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BklyWIdlcS", "sentence_index": 2, "text": "The paper is very well organized and written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BklyWIdlcS", "sentence_index": 3, "text": "However, I have the following concerns.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BklyWIdlcS", "sentence_index": 4, "text": "1, the ME problem is quite similar to the concept ontology, e.g. , a \u201cDalmatian,\u201d a \u201cdog\u201d, or a \u201cmammal\u201d.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BklyWIdlcS", "sentence_index": 5, "text": "So what\u2019s the key difference? Hierarchical learners can avoid this problem.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BklyWIdlcS", "sentence_index": 6, "text": "2, I would say, the SOTA neural networks fundamentally are just representation learned, i.e., feature representation learning.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklyWIdlcS", "sentence_index": 7, "text": "The learned features could in principle be employed to construct advanced learners, .e.g., hierarchical Bayesian.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklyWIdlcS", "sentence_index": 8, "text": "It\u2019s probably a bit unfair or misleading to claim neural networks suffering from ME bias.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklyWIdlcS", "sentence_index": 9, "text": "3, In Sec. 3, the first and section paragraph, I can not quite understand how the ME bias theory guide the following synthetic experiments.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BklyWIdlcS", "sentence_index": 10, "text": "Please give more explanations.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklyWIdlcS", "sentence_index": 11, "text": "4, in Sec. 3.1, considering the small number of training instances, whether it is large enough to train the NN/ not overfitting issues?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BklyWIdlcS", "sentence_index": 12, "text": "5. Whether the ME bias mostly attributed to the one-hot representation? If one uses word2vec as the representation in NN , the ME bias will be solved.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BklyWIdlcS", "sentence_index": 14, "text": "Actually, this is the standard practice in some learning tasks, e.g., zero-shot learning.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklyWIdlcS", "sentence_index": 15, "text": "6, the experimental design of Sec. 4.2 is also a bit unfair.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklyWIdlcS", "sentence_index": 16, "text": "It seems to me that the tasks are organized as unbalanced instances of each classes, and asking the common NNs to learn this  task. Of course, this common NNs can not address it.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BklyWIdlcS", "sentence_index": 17, "text": "----", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklyWIdlcS", "sentence_index": 18, "text": "I read the rebuttal. the authors clarified and answered the questions. I would like to raise the score.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_positive"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 0, "text": "The paper proposes a method to do math reasoning purely using formula embeddings.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 1, "text": "The proposed method employs a graph neural network to embed math formulas to a latent space.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 2, "text": "The formula embeddings are then combined with theorem embeddings (also formulas, computed in the same way as formula embeddings) to predict whether one can do one step of math reasoning using the corresponding theorem, and also to predict the embeddings of the resulting formula.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 3, "text": "Empirically the authors demonstrate that the method can be chained end-to-end to do multiple steps of reasoning purely in the latent space.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 4, "text": "I tend to accept this paper, (but also OK if it gets rejected), for the following reasons: (1) the idea is novel and interesting; (2) the writing of the paper is below conference standard and very hard to read, especially the method and the experiment sections.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 5, "text": "===========================================================================", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 6, "text": "Novelty and significance", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 7, "text": "I really like the idea of doing math reasoning in latent space.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 8, "text": "The idea is definitely novel and interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 9, "text": "It is related to existing works such as neural logic induction[1] and planning in latent space[2].", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 10, "text": "It is amazing that one can do multiple steps of math reasoning after only training the model using data from one single step.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 11, "text": "It would be interesting to see how it can improve existing learning-based theorem provers.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 12, "text": "My question is if we want to integrate the proposed method into theorem provers, after multiple steps of math reasoning, how would us know the goal has been proved? Is it possible that we can train a decoder that maps back from the latent space to the formula space?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 13, "text": "Also can it work with theorems that decompose the current goal into several sub-goals?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 14, "text": "I know these are not the concerns of this paper, but I would be really grateful if you could provide some intuitive answers!", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 15, "text": "===========================================================================", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 16, "text": "Writing", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 17, "text": "The paper is not well-organized and not written in a consistent way. For the method and the experiment sections, I need to jump back and forth several times in order to understand what the authors are trying to say.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 18, "text": "1. Typo: Third paragraph in section 1, \"...which is makes use of ...\".", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 19, "text": "2. It's very confusing when the authors introduce \\sigma and \\omega in the beginning of section 4: why would you need two networks predict the same thing?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 20, "text": "3. Mentioning \"merging \\sigma and \\omega, is left for future work\" is confusing before formally introducing \\sigma and \\omega.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 21, "text": "4. Even when the authors formally introduce \\sigma and \\omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 22, "text": "5. In fact, I don't know why \\omega needs to output p. It's never mentioned in the experiment section.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 23, "text": "6. The rationale of the two tower design (why not combine two) is not clearly explained.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 24, "text": "7. Typo: Page 5 last paragraph, \"... negative instances for for each ...\".", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 25, "text": "8. The itemized part in 5.3, \"...carefully selected baselines: 1.xxx, 2.xxx, 3. xxx, 4. xxx\". However, both 3 and 4 are not baselines!", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 26, "text": "9. It is not clear that baseline 1 and 2 correspond to which baselines in later experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 27, "text": "10. Reading the baselines before the experiments is very confusing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 28, "text": "For example, for baseline 1, it is very hard to understand why would we want to use such an unusual baseline, and why it is called a \"random baseline\".", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 29, "text": "11. Baseline 2 is actually referred to as \"usage baseline\" but this name is not introduced in the itemized part.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 30, "text": "[1] Rockt\u00e4schel, Tim, and Sebastian Riedel. \"End-to-end differentiable proving.\" Advances in Neural Information Processing Systems. 2017.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Bkx-GQrhtS", "sentence_index": 31, "text": "[2] Srinivas, Aravind, et al. \"Universal planning networks.\" arXiv preprint arXiv:1804.00645 (2018).", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkxH95gchQ", "sentence_index": 0, "text": "The idea is nice.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "BkxH95gchQ", "sentence_index": 1, "text": "It is well aligned with tools that are needed to understand neural networks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BkxH95gchQ", "sentence_index": 2, "text": "However, the experiments feel like they are missing motivation as to why this method is being used.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkxH95gchQ", "sentence_index": 3, "text": "The paper does not provide very significant evidence that this method is useful.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BkxH95gchQ", "sentence_index": 4, "text": "The negation example is nice but this doesn't seem to display the potential power of the method to understand a neural network.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BkxH95gchQ", "sentence_index": 5, "text": "More motivation for experimental section is needed.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "BkxH95gchQ", "sentence_index": 6, "text": "If the authors don't discuss a motivation then how will a reader know how to apply the tool?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "BkxH95gchQ", "sentence_index": 7, "text": "It seems there is no conclusion to take away from the experiments in section 5 (convolutions).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkxH95gchQ", "sentence_index": 8, "text": "The authors should rethink the structure of the experimental section from the standpoint of convincing someone to use this method.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BkxH95gchQ", "sentence_index": 9, "text": "In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkxH95gchQ", "sentence_index": 10, "text": "The paper needs more discussion and experiments to explain how and why to use this approach.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BkxH95gchQ", "sentence_index": 11, "text": "While the authors say \"attributing a deep network\u2019s prediction to its input is well-studied\" they don't compare directly against these methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BkxH95gchQ", "sentence_index": 12, "text": "There are many typos and grammar errors", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BkxH95gchQ", "sentence_index": 13, "text": "While I think the paper could be much more impactful if the experimental section was greatly reworked; I believe the first 5 pages of the paper are a very good contribution and it should be accepted.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_positive"}, {"review_id": "BkxPCGTX9B", "sentence_index": 0, "text": "The authors introduce strategies for pre-training graph neural networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 1, "text": "Pre-training is done at the node level as well as at the graph level.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 2, "text": "They evaluate their approaches on two domains, biology and chemistry on a number of downstream tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 3, "text": "They find that not all pre-training strategies work well and can in fact lead to negative transfer.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 4, "text": "However, they find that pre-training in general helps over non pre-training.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 5, "text": "Overall, this paper was well written with useful illustrations and clear motivations.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BkxPCGTX9B", "sentence_index": 6, "text": "The authors evaluate their models over a number of datasets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BkxPCGTX9B", "sentence_index": 7, "text": "Experimental construction and analysis also seems sound.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BkxPCGTX9B", "sentence_index": 8, "text": "I would have liked to see a bit more analysis as to why some pre-training strategies work over others.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkxPCGTX9B", "sentence_index": 9, "text": "However, the authors mention that this is in their planned future work.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 10, "text": "Also, in figure 4, the authors mention that their pre-trained models tend to converge faster.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 11, "text": "However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 0, "text": "This paper proposes a self-auxiliary-training method that aims to improve the generalization performance of simple supervised learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 1, "text": "The basic idea is to train the classification network to predict fine-level auxiliary labels in addition to the ground-truth coarse label, where the auxiliary labels used in training is generated by a generator network.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 2, "text": "During training, the classification network and the generator network are alternatively updated, and the update of the latter aims to maximize the improvement of the former after using the generated auxiliary label for training.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 3, "text": "The method requires a class hierarchy in advance to define the binary mask applied to the output layer for auxiliary class prediction.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 4, "text": "A KL divergence term is attached to the optimization objective to avoid generating trivial and collapsing auxiliary classes.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 5, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 6, "text": "1) The main idea is simple and easy to understand.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 7, "text": "2) It discusses the class collapsing problem in generating pseudo (auxiliary) labels and provides a reasonable solution, i.e., using KL divergence as regularization.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 8, "text": "3) Uses several visualizations to show experimental results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 9, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 10, "text": "1) The problem it aims to solve is neither multi-task learning nor meta-learning: it tries to solve a supervised classification problem defined on principle classes, with the help of simultaneously predicting/generating auxiliary class labels.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 11, "text": "Although the concept of \"task\" is not explicitly defined in this paper, the authors seem to associate each task with a specific class. This is not correct: in meta-learning, each task is a subset of classes drawn from a ground set of classes, and different tasks are independently sampled.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 13, "text": "In addition, the classification models for different tasks are independent, though their training might be related by a meta-learner.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 14, "text": "Hence, the claims in multiple places of this paper and the names for the two networks are misleading.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 15, "text": "2) At the end of Page 4, the authors show that the update of the generator only depends on the improvement of the classifier after using the auxiliary label for training.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 16, "text": "In fact, the optimal auxiliary labels minimizing the objective is the ground truth label for principle classes.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 17, "text": "This results in the class collapsing problem observed by the authors.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 18, "text": "The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness. In other words, the auxiliary labels for a specific principle class are very possible to be multiple noisy copies of the principal label with random perturbations. So it is not convincing to me that the auxiliary labels generated by the generator can be really helpful.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 21, "text": "My conjecture is that the observed improvements are mainly due to the softness of the auxiliary labels, which has been proved by model compression/knowledge distillation and recent \"born-again neural networks\".", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 22, "text": "To verify this, the authors might need to compare the results with those methods (which use the generated soft probability of ground truth classes for training), and the \"random-noisy copies of soft principle label\" mentioned above.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 23, "text": "3) The experiments lack comparisons to several important baselines from self-supervised learning community, and methods using soft labels for training (as mentioned in 2) above).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 24, "text": "A successful idea of self-supervised learning is to use the output feature map of the trained classification network to generate auxiliary training signals, since it provides extra information about the learned distance beyond the ground-truth labels.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 25, "text": "The authors might want to compare to \"Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 26, "text": "Deep Clustering for Unsupervised Learning of Visual Features. ECCV 2018.\" and \"Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. ICCV 2017.\" Moreover, since the method is not a meta-learning approach for few-shot learning, it is not fair and also not appropriate to compare with Prototypical Network.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 27, "text": "4) Although the paper claims that the ground truth fine labels are not required, it requires a class hierarchy, which in the experiments are provided by the dataset and defined between true coarse and fine classes.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 28, "text": "In practice, such hierarchy might be much harder to achieve than the primary (coarse) labels, and might be as costly to obtain as the true fine-class labels. This weakens the feasibility of the proposed method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 30, "text": "5) The experiments only test the proposed method on CIFAR100 and CIFAR10, which has at most 100 fine classes.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 31, "text": "It is necessary to test it on datasets with much more fine classes and much-complicated hierarchy, e.g., ImageNet, MS COCO or their subsets, which have ideal class hierarchy structures.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 32, "text": "Minor comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Bkxpr4aq3m", "sentence_index": 33, "text": "Some important equations in the paper should be numbered.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "ByezUKSq2Q", "sentence_index": 0, "text": "Summary of the paper:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 1, "text": "The paper has two intertwined goals.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 2, "text": "These goals are to illuminate the generalization/memorization properties of large and deep ConvNets in tandem with trying to develop procedures related to identifying whether an input to a trained ConvNet has actually been used to train the network.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 7, "text": "The latter task is generalized to detecting if a particular dataset has been used to train a ConvNet. These goal are tackled empirically with multiple sets of experiments on largescale datasets such as ImageNet22k and modern deep ConvNets architectures such as VGG and ResNet.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 13, "text": "Paper's positive points", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 14, "text": "+ The paper has a very comprehensive set of references in the areas it touches upon.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ByezUKSq2Q", "sentence_index": 16, "text": "+ Some of the experimental results presented are quite interesting. They show that regularization data-augmentation helps prevent a network from explicit memorization and could be used as a way to help make training data more anonymous.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ByezUKSq2Q", "sentence_index": 20, "text": "+ Large scale experiments are reported on modern architectures.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_positive"}, {"review_id": "ByezUKSq2Q", "sentence_index": 21, "text": "Paper's negative points", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 22, "text": "- The paper makes use of a result from the David MacKay textbook which defines the capacity of a single layer network to memorize the labelling of $n$ inputs in $d$-dimensional space. If I'm not mistaken, from this result the authors extrapolate that the capacity of a (deep) neural network is proportional to the number of parameters in the network.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 30, "text": "This is true, but there are a couple of caveats.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 32, "text": "The first is that the coefficient of proportionality must depend very much on the number of layers in the network.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 35, "text": "Increasing the network's depth increases the efficiency of the representation (i.e. fewer total parameters needed to have the same representational power as a shallow network). And as MacKay also says in his book (chapter 44 quoting findings from Radford Neal) that for MLPs what determines the complexity of the typical function (once the network has a large enough width) represented by the MLP is the \"characteristic magnitude of the weights\". So the regularization technique applied is very significant in the controlling the effective capacity of a network.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 44, "text": "This paper experimentally shows that is the case multiple times as it is shown that with increasing degrees of regularization (figure 1, figure 2) it becomes harder and harder to memorize the positive training", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 48, "text": "images. It would be great if the paper also made some attempt to consider these connections. Or at least comment on how these factors could be incorporated into a more sophisticated analysis of the capacity of a network.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 52, "text": "- There is a slight oxymoron in the premise of the first set of experiments. The network is forced to memorize a set of positive examples relative to the negative set it sees during training. What is memorized I presume depends a lot on the negative set used for training (its diversity, closeness to the positive set and how frequently each negative example is seen during training).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ByezUKSq2Q", "sentence_index": 60, "text": "This issue is not really commented upon in the paper. Is there a training task which would allow one to more explicitly memorize the image (some sort of reconstruction task) as opposed to an in/out classification task?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 64, "text": "- This paper is a slightly difficult read - not because of the language or the presentation of the material but more because there is not one main coherent argument or goal for the paper. This is reflected in the \"Related work\" section where 4 different issues/tasks are referred to.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 70, "text": "Each one of these topics is worthy of a paper in itself, but this paper dips into each one and then swiftly moves onto the next one. For example in section 3 the paper explores if a network can be forced to explicitly memorize a set of images and how the size of this set is affected by the number of parameters in the network and data augmentation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 77, "text": "High-level conclusions are made: more parameters in the network implies more images can be memorized and data-augmentation makes explicit memorization more difficult. Then it is off to considering pre-trained networks and determining whether by analyzing the statistics of the responses at different layers one can decide if a set of images was used for training or not (or similar tasks). Yes the different sections are related but it is does not feel like they build upon each other to help form a clearer picture of memorization within neural networks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 89, "text": "- The conclusions focus on the importance of section 3 and the results of the experiments performed. Do the conclusions accurately reflect the opinions of the author? If yes, would it better to re-organize the paper and devote more of it to the material presented in section 3 and filling this out with more analysis and experiments to perhaps explore the issue of the capacity of a network in more", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 96, "text": "Queries/ points that need some clarification", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 97, "text": "- I'm a little unclear when data-augmentation is included in the training phase whether the goal is to be able to also recognise perturbed versions of the input images at test time. In section 3 is a perturbed positive image considered a positive training image? And in the testing phase are only unperturbed versions of the positive images given to the ConvNet as input?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "ByezUKSq2Q", "sentence_index": 104, "text": "- Last paragraph page 4: \"when the accuracy gets over 60\\% and again at 90\\%\". Is this training or validation accuracy?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "ByezUKSq2Q", "sentence_index": 106, "text": "Typos possible errors spotted along the way:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 107, "text": "* First paragraph page 5: \"more shallow\" --> \"shallower\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "ByezUKSq2Q", "sentence_index": 108, "text": "* Page 7, first paragraph of section 5.: \"is ran\" --> \"is run\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "ByezUKSq2Q", "sentence_index": 109, "text": "* Using \"scenarii\" for the plural of \"scenario\" I would say is pretty non-standard and most people would use \"scenarios\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BygGS54qnQ", "sentence_index": 0, "text": "This paper proposes to combine unsupervised adversarial domain adaptation with prototypical networks and finds that the proposed model performs well on few-shot learning task with domain shift, much better than other few-shot learning baselines that do not consider.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 1, "text": "Specifically it tests on Omniglot with natural image background and cliparts to real images.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 2, "text": "It is true that current meta-learning approaches do not address the problem of domain shift, and as a result, the testing domain has to be the same with the training domain.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 3, "text": "However, this paper rather than proposing solution address the meta-learning problem, albeit the title \u201cmeta domain adaptation\u201d, only brings few-shot learning to domain adaptation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BygGS54qnQ", "sentence_index": 4, "text": "Here\u2019s why:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 5, "text": "In order for a meta-learning model to be called \u201cmeta domain adaptation,\u201d the type of adaptation cannot be seen during training, and the goal is to test on adaptation that the model has not seen before.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 6, "text": "Indeed, each task in meta domain adaptation should be seen as a pair of source task and target task.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 7, "text": "The problem with the current model is that during training, it is trained to target at one specific type of test domain--the generator network G aims to generated images that align with the unsupervised  test domain X_test.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BygGS54qnQ", "sentence_index": 8, "text": "Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BygGS54qnQ", "sentence_index": 9, "text": "In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BygGS54qnQ", "sentence_index": 10, "text": "Therefore, the paper cannot be qualified for ``meta domain adaptation\u2019\u2019 and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "BygGS54qnQ", "sentence_index": 11, "text": "For the rest of my review, I will treat the paper as \u201cfew-shot learning with domain adaptation\u201d for more appropriate analysis.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 12, "text": "For the experiments, there seems to have a great win of the proposed algorithm against the baselines.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BygGS54qnQ", "sentence_index": 13, "text": "However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BygGS54qnQ", "sentence_index": 14, "text": "Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BygGS54qnQ", "sentence_index": 15, "text": "Then use the same network to extract the features and then using the nearest neighbor to retrieve the classes.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 16, "text": "Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BygGS54qnQ", "sentence_index": 17, "text": "Another concern is that the evaluation of domain adaptation does not have much varieties.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BygGS54qnQ", "sentence_index": 18, "text": "Only two domains shifts are evaluated in the paper, specifically Omniglot + BSD500 and Office-Home.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 19, "text": "BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BygGS54qnQ", "sentence_index": 20, "text": "Other domain transfer settings such as synthetic rendered vs. real (e.g. visDA challenge) could have been considered.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BygGS54qnQ", "sentence_index": 21, "text": "In conclusion, the paper presents a interesting combination of ProtoNet + Adversarial DA + Cycle consistency.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 22, "text": "However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BygGS54qnQ", "sentence_index": 23, "text": "Therefore, I recommend reject.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 24, "text": "---", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 25, "text": "Note: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being \"meta domain adaptation\".", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BygGS54qnQ", "sentence_index": 26, "text": "===", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 27, "text": "After rebuttal:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 28, "text": "I would like to thank the authors for the response and updating the draft.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 29, "text": "They have addressed 1) the title issue and 2) adding domain adaptation baselines.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BygGS54qnQ", "sentence_index": 30, "text": "Considering these improvements, I would like to raise the score to 5, since the setting of combining few-shot learning and domain adaptation is interesting and the proposed model outperforms the baselines.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BygGS54qnQ", "sentence_index": 31, "text": "However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "BygGS54qnQ", "sentence_index": 32, "text": "The experiments use fairly small datasets, where the performance can be largely influenced by how good the feature extractor backbone is (e.g. training on more data and using deeper architecture would warrant better performance, and thus may change the conclusion).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Bygh4ga53m", "sentence_index": 0, "text": "This paper makes concerted efforts to examine the existing beliefs about the significance of various statistical characteristics of hidden layer activations (or representations) in a DNN.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bygh4ga53m", "sentence_index": 1, "text": "In the past, many works have argued for encouraging the certain statistical behavior of these representations (e.g., sparsity, low correlation etc) in order to have better classification accuracy.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bygh4ga53m", "sentence_index": 2, "text": "However, this paper tries to argue that such efforts are not very useful as these statistical characteristics don't provide any systematic explanation for the performance of DNNs across different settings.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bygh4ga53m", "sentence_index": 3, "text": "First, the paper argues that given a DNN, it's possible to construct either an identical output network or a comparable network that can have very different behavior for some of the statistical characteristics.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bygh4ga53m", "sentence_index": 4, "text": "This casts doubt on the usefulness of these characteristics in explaining the performance of the network.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bygh4ga53m", "sentence_index": 5, "text": "The paper conducts experiments with different regularizers associated with some of the standard statistical characteristics using the MNIST, CIFAR-10, and CIFAR-100 datasets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bygh4ga53m", "sentence_index": 6, "text": "The paper claims that for each dataset the best performing network cannot be attributed to any single regularizer.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bygh4ga53m", "sentence_index": 7, "text": "For the same set of regularizers and the MNIST dataset, the paper then explores the mutual information between the inputs and the hidden layer activations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bygh4ga53m", "sentence_index": 8, "text": "The paper observes that the best performing regularizer is the one which minimizes this mutual information.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bygh4ga53m", "sentence_index": 9, "text": "Therefore, it is plausible that the mutual information regularization can consistently explain the performance of an NN.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Bygh4ga53m", "sentence_index": 10, "text": "The paper addresses an interesting problem and makes some good contributions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "Bygh4ga53m", "sentence_index": 11, "text": "However, the reviewer feels that the brief treatment of mutual information regularizer leaves something to be desired.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Bygh4ga53m", "sentence_index": 12, "text": "Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Bygh4ga53m", "sentence_index": 13, "text": "In these tables, how do the authors decide which hidden layer representations should be explored for their statistical characteristics?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Bygh4ga53m", "sentence_index": 14, "text": "The reviewer feels that for CIFAR-10 and 100, some regularizers do consistently give best or close to best networks. Could the authors comment on this?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Byl04qxA2X", "sentence_index": 0, "text": "This paper proposes a Self-Modulation framework for the generator network in GANs, where middle layers are directly modulated as a function of the generator input z.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl04qxA2X", "sentence_index": 1, "text": "Specifically, the method is derived via batch normalization (BN), i.e. the learnable scale and shift parameters in BN are assumed to depend on z, through a small one-hidden layer MLP.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl04qxA2X", "sentence_index": 2, "text": "This idea is something new, although quite straight-forward.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "Byl04qxA2X", "sentence_index": 3, "text": "Extensive experiments with varying losses, architectures, hyperparameter settings are conducted to show self-modulation improves baseline GAN performance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Byl04qxA2X", "sentence_index": 4, "text": "The paper is mainly empirical, although the authors compute two diagnostic statistics to show the effect of the self-modulation method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Byl04qxA2X", "sentence_index": 5, "text": "It is still not clear why self-modulation stabilizes the generator towards small conditioning values.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Byl04qxA2X", "sentence_index": 6, "text": "The paper presents two loss functions at the beginning of section 3.1 - the non-saturating loss and the hinge loss.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl04qxA2X", "sentence_index": 7, "text": "It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Byl04qxA2X", "sentence_index": 8, "text": "It seems that the authors are not aware of this difference.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Byl04qxA2X", "sentence_index": 9, "text": "In addition to report the median scores, standard deviations should be reported.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Byl04qxA2X", "sentence_index": 10, "text": "===========", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Byl04qxA2X", "sentence_index": 11, "text": "comments after reading response ===========", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Byl04qxA2X", "sentence_index": 12, "text": "I do not see in the updated paper that this typo (in differentiating D in hinge loss and non-saturating loss) is corrected.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Byl04qxA2X", "sentence_index": 13, "text": "Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Byl0ceB1cH", "sentence_index": 0, "text": "This manuscript studies mutual-information estimation, in particular variational lower bounds, and focuses on reducing their sample complexity.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 1, "text": "The first contribution is based on adapting the MINE energy-based MI estimator family to out-of-sample testing.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 2, "text": "MINE involves fitting a very flexible parametric form of the distribution, such as a neural network, to the data to derive a mutual information lower bound.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 3, "text": "The present work separates the data fitting from the mutual information evaluation to decrease sample complexity, the argument being that the function class is no longer a limiting factor to sample complexity of the mutual information estimation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 4, "text": "The second contribution uses meta learning to decrease the sample complexity required to fit the neural network, creating a family of tasks derived from the data with data transformation that do not modify the mutual information.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 5, "text": "The approaches are demonstrated on synthetic data as well as fMRI data, to detect significant inter-subject dependencies in time-series of neural responses.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 6, "text": "There are some very interesting and strong contributions of this manuscript.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Byl0ceB1cH", "sentence_index": 7, "text": "However, I worry that one of the central theoretical arguments does not seem correct to me.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Byl0ceB1cH", "sentence_index": 8, "text": "Indeed, the manuscript introduces sample complexity results to justify the benefits of the out-of-sample procedure (th 1), but it seems to me that these give an incomplete picture.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Byl0ceB1cH", "sentence_index": 9, "text": "Th 1 gives the number of validation samples required to bound error between the mutual information estimate at finite samples and asymptotically for a function T parametrized by \\tilda{theta}. This control is of a very different nature from the control established by MIME which controls the error to the actual best possible variational bound.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 10, "text": "In other terms, the control of th 1 does not control the estimation error of T. This is the reason why it is independent from the function class.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 11, "text": "The total error in estimating the mutual information must take this error in account, and not only the validation error.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Byl0ceB1cH", "sentence_index": 12, "text": "Hence the theoretical sample complexities contributed are not comparable to those of MIME.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "Byl0ceB1cH", "sentence_index": 13, "text": "The meta-learning estimator seems to involve a significant implementation complexity, for instance heuristic switchs between estimation approaches.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 14, "text": "The danger is that could hard to make reliable in a wide set of applications.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 15, "text": "It would be more convincing to see more experiments.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Byl0ceB1cH", "sentence_index": 16, "text": "Along this direction, it is problematic that, in the synthetic examples, the relative  performance of methods changes significantly from experiment to experiment and there does not seem to be a simple way to control that.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Byl0ceB1cH", "sentence_index": 17, "text": "On the other hand, the MIME-f-ES tends to have a reasonably good failure mode.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 18, "text": "I would have been interested in \"false detection\" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Byl0ceB1cH", "sentence_index": 19, "text": "This is particularly important when the application is to test for independence, as in the fMRI experiments.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 20, "text": "For hyper-parameter search (using hyperopt), the manuscript should make it explicit what metric is optimized.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 21, "text": "Is it the data fit of the neural networks? With what specific measure?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Byl0ceB1cH", "sentence_index": 22, "text": "With regards to the fMRI experiments, good baselines are missing: DEMINE is compared to Pearson correlation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "Byl0ceB1cH", "sentence_index": 23, "text": "Additionally, CNNs are not a particularly good architecture for fMRI, as fMRI is not locally translation invariant (see Ayd\u00f6re ICML 2019 for instance).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 24, "text": "Finally, it seems that the goal here is to test for independence.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 25, "text": "In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Byl0ceB1cH", "sentence_index": 26, "text": "Minor comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 27, "text": "Alg 1 seems a fairly generic neural-network fitting algorithm.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl0ceB1cH", "sentence_index": 28, "text": "In its current state, I am not sure that it adds a lot to the manuscript.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Byl0ceB1cH", "sentence_index": 29, "text": "There are many acronyms that are never defined: MINE, TCPC", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 0, "text": "Review for MANIFOLD REGULARIZATION WITH GANS FOR SEMISUPERVISED LEARNING", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 1, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 2, "text": "The paper proposed to incorporate a manifold regularization penalty to the GAN to adapt to semi-supervised learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 3, "text": "They approximate this penalty empirically by calculating stochastic finite differences of the generator\u2019s latent variables.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 4, "text": "The paper does a good job of motivating the additional regularization penalty and their approximation to it with a series of experiments and intuitive explanations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 5, "text": "The experiment results are very through and overall promising.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 6, "text": "The paper is presented in a clear manner with only minor issues.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 7, "text": "Novelty/Significance:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 8, "text": "The authors\u2019 add a manifold regularization penalty to GAN discriminator\u2019s loss function.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 9, "text": "While this is a simple and seemingly obvious approach, it had to be done by someone.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 10, "text": "Thus while I don\u2019t think their algorithm is super novel, it is significant and thus novel enough.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 11, "text": "Additionally, the authors\u2019 use of gradients of the generator as an approximation for the manifold penalty is a clever.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 12, "text": "Questions/Clarity:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 13, "text": "It would be helpful to note in the description of Table 3 what is better (higher/lower).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 14, "text": "Also Table 3 seems to have standard deviations missing in Supervised DCGANs and Improved GAN for 4000 labels. And is there an explanation on why there isn\u2019t an improvement in the FID score of SVHN for 1000 labels?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 15, "text": "What is the first line of Table 4? Is it supposed to be combined with the second? If not, then it is missing results. And is the Pi model missing results or can it not be run on too few labels? If it can\u2019t be run, it would be helpful to state this.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 16, "text": "On page 11, \u201cin Figure A2\u201d the first word needs to be capitalized.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 17, "text": "In Figure A1, why is there a dark point at one point in the inner circle? What makes the gradient super high there?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Byl2kX8c2Q", "sentence_index": 18, "text": "What are the differences of the 6 pictures in Figure A7? Iterations?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BylNfGini7", "sentence_index": 0, "text": "In the manuscript entitled \"Neural Causal Discovery with Learnable Input Noise\" the authors describe a method for automated causal inference under the scenario of a stream of temporally structured random variables (with no missingness and a look-back window of given size).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BylNfGini7", "sentence_index": 1, "text": "The proposed approach combines a novel measure of the importance of fidelty in each variable to predictive accuracy of the future system state (\"learnable noise risk\") with a flexible functional approximation (neural network).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BylNfGini7", "sentence_index": 2, "text": "Although the setting (informative temporal data) is relatively restricted with respect to the general problem of causal inference, this is not unreasonable given the proposed direction of application to automated reasoning in machine learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BylNfGini7", "sentence_index": 3, "text": "The simulation and real data experiments are interesting and seem well applied.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BylNfGini7", "sentence_index": 4, "text": "A concern I have is that the manuscript as it stands is positioned somewhere between two distinct fields (sparse learning/feature selection, and causal inference for counterfactual estimation/decision making), but doesn't entirely illustrate its relationship to either.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "BylNfGini7", "sentence_index": 5, "text": "In particular, the derived criterion is comparable to other sparsity-inducing penalities on variable inclusion in machine learning models; although it has motivation in causality it is not exclusively derived from this position, so one might wonder how alternative sparsity penalities might perform on the same challenge.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "BylNfGini7", "sentence_index": 6, "text": "Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BylNfGini7", "sentence_index": 7, "text": "In the ordinary feature selection regime one is concerned simply with improving the predictive capacity of models: e.g. a non-linear model might be fit using just the causal variables that might out-perform both a linear model and a non-linear model fit using all variables.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "BylNfGini7", "sentence_index": 8, "text": "Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BylSI7T6qr", "sentence_index": 0, "text": "This paper introduces a simple measure of tunability that allows to compare optimizers under varying resource constraints.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BylSI7T6qr", "sentence_index": 1, "text": "The tunability of the optimizer is a weighted sum of best performance at a given budget.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BylSI7T6qr", "sentence_index": 2, "text": "The authors found that in a setting with low budget for hyperparameter tuning, tuning only Adam optimizer\u2019s learning rate is likely to be a very good choice; it doesn\u2019t guarantee the best possible performance, but it is evidently the easiest to find well-performing hyperparameter configurations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BylSI7T6qr", "sentence_index": 3, "text": "Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BylSI7T6qr", "sentence_index": 4, "text": "The paper is easy to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BylSI7T6qr", "sentence_index": 5, "text": "The motivation of defining tunability of optimizer is a very interesting question, however, the study seems to preliminary and the conclusion is not quite convencing due to several reasons: In section 3.2, to characterize its difficulties of finding best hyperparameters or tunability, the authors seem to try to connect the concept of \u201csharpness\u201d of a minima in loss surface to the tunability of an optimizer, which is similar to comparing the loss landscape of minimums. However, while the authors made intuitive explanation about the tunability in section 2.2, I did not see the actual plot of the true hyperpaparameter loss surface of each optimizer to verify these intuitions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BylSI7T6qr", "sentence_index": 8, "text": "Can the author be more specific about the x-axis in the illustration 1.a and 1.b? If I understand correctly, they are not the number of trails. In addition, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations, but the proposed definition is a weighted sum of the incumbents in terms of the HPO budgets.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BylSI7T6qr", "sentence_index": 10, "text": "The definition of the tuning budgets is not clear, is it the number of trials or the time/computation budgets?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BylSI7T6qr", "sentence_index": 11, "text": "The authors seems interchangeably using \u201cruns\u201d and \u201citerations\u201d, which makes the concept more confusable.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BylSI7T6qr", "sentence_index": 12, "text": "The authors further proposed three weighting schemes to emphasize the tunability of different stage of HPO.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BylSI7T6qr", "sentence_index": 13, "text": "My concern is that is highly dependent  on the order of hyperparameter searched, which could impact the tunability significantly. For instance, in case of grid search HPO and 0.1 is the best learning rate, different search order such as [10, 1, 0.01, 0.1] and [0.1, 0.01, 1, 10] could results in dramatic different CPE and CPL. My major concern is the hyperparameter distributions for each optimizer highly requires prior knowledge.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BylSI7T6qr", "sentence_index": 16, "text": "A good prior of one optimizer could significantly affect the HPO cost or increase the tunability, i.e., the better understanding the optimizer, the less tuning cost.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BylSI7T6qr", "sentence_index": 17, "text": "My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true. Actually hyperparameters are highly correlated, such as momentum, batch size and learning rate are correlated in terms of effective learning rate [1,2], so as weight decay and learning rate are [3], which means using non-zero momentum is equivalent to using large learning rate as long as the effective learning rate is the same. This could significantly increase the tunability of SGDM.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BylSI7T6qr", "sentence_index": 20, "text": "Another concurrent submission [4] verified this equivalence and showed one can also just tune learning rate for SGDM. The assumption of independent hyperparameters might be fine for black box optimization or with the assumption that practitioners have no knowledge of the importance of each hyperparameter, then the tunability of the optimizer could be different based on the prior knowledge of hyperparameter and their correlations. But it is not rigorous enough to make the conclusion that Adam is easier to tune than SGD.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BylSI7T6qr", "sentence_index": 22, "text": "The author states their method to determine the priors by training each task specified in the DEEPOBS with a large number of hyperparameter samplings and retain the hyperparameters which resulted in performance within 20% of the best performance obtained.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BylSI7T6qr", "sentence_index": 23, "text": "Could the authors be more specific on the hyperparameters searched? Is this process counted in the tunability measurement?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BylSI7T6qr", "sentence_index": 24, "text": "[1] Smith and Le, A Bayesian Perspective on Generalization and Stochastic Gradient Descent, https://arxiv.org/abs/1710.06451", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BylSI7T6qr", "sentence_index": 25, "text": "[2] Smith et al, Don't Decay the Learning Rate, Increase the Batch Size, https://arxiv.org/abs/1711.00489", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BylSI7T6qr", "sentence_index": 26, "text": "[3] van Laarhoven et al, L2 Regularization versus Batch and Weight Normalization, https://arxiv.org/abs/1706.05350", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BylSI7T6qr", "sentence_index": 27, "text": "[4] Rethinking the Hyperparameters for Fine-tuning", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BylSI7T6qr", "sentence_index": 28, "text": "https://openreview.net/forum?id=B1g8VkHFPH", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "ByltNYc0tr", "sentence_index": 0, "text": "This paper proposes a new task/dataset for language-based abductive reasoning in narrative texts.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ByltNYc0tr", "sentence_index": 1, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ByltNYc0tr", "sentence_index": 2, "text": "-\tThe proposed task is interesting and well motivated.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "ByltNYc0tr", "sentence_index": 3, "text": "The paper contributes a dataset (20,000 commonsense narratives and 200,000 explanatory hypotheses).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "ByltNYc0tr", "sentence_index": 4, "text": "The construction of the dataset was performed carefully (e.g., avoiding annotation artifacts).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "ByltNYc0tr", "sentence_index": 5, "text": "-\tThe paper established many reasonable baselines.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ByltNYc0tr", "sentence_index": 6, "text": "-\tThe paper conducted detailed analysis, which invites more research on this task: despite the strong performance of many existing systems on NLI/RTE, there are larger gaps between the performance of these models and human performance on the proposed task.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ByltNYc0tr", "sentence_index": 7, "text": "The experiments well support the conclusions made in the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "ByltNYc0tr", "sentence_index": 8, "text": "-\tThe paper is well structured and easy to follow. It is well written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "ByltNYc0tr", "sentence_index": 9, "text": "Cons/comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ByltNYc0tr", "sentence_index": 10, "text": "-\tWhile this is a new and interesting task, the contribution (as discussed above in \u201cpros\u201d above) is somewhat limited.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "ByltNYc0tr", "sentence_index": 11, "text": "I also suggest the paper discusses e-SNLI a bit more.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "ByltNYc0tr", "sentence_index": 12, "text": "-\tThe paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "ByltNYc0tr", "sentence_index": 13, "text": "-\tShould the title of the paper specify the paper is about \u201clanguage-based\u201d abductive reasoning.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "ByltNYc0tr", "sentence_index": 14, "text": "- A minor one: \u201cTable 7 reports results on the \u03b1NLI task.\u201d Should it be \u201cTable 2\u201d?", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "ByxBaTGy67", "sentence_index": 0, "text": "The paper presents a maximally expressive parameter-sharing scheme for hypergraphs, and in general when modeling the high order interactions between elements of a set.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ByxBaTGy67", "sentence_index": 1, "text": "This setting is further generalized to multiple sets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ByxBaTGy67", "sentence_index": 2, "text": "The paper shows that the number of free parameters in invariant and equivariant layers corresponds to the different partitioning of the index-set of input and output tensors.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ByxBaTGy67", "sentence_index": 3, "text": "Experimental results suggest that the proposed layer can outperform existing methods in supervised learning with graphs.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ByxBaTGy67", "sentence_index": 4, "text": "The paper presents a comprehensive generalization of a recently proposed model for interaction across sets, to the setting where some of these sets are identical.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ByxBaTGy67", "sentence_index": 5, "text": "This is particularly useful and important due to its applications to graphs and hyper-graphs, as demonstrated in experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ByxBaTGy67", "sentence_index": 6, "text": "Overall, I enjoyed reading the paper. My only concern is the experiments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ByxBaTGy67", "sentence_index": 7, "text": "1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al\u201918 and references in there) are missing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "ByxBaTGy67", "sentence_index": 8, "text": "2) Applying the model of Hartford et al\u201918 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation. (In both cases the equivariance group of data is a strict subgroup of the equivariance of the layer.)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "ByxBaTGy67", "sentence_index": 10, "text": "Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Byxe5udgcS", "sentence_index": 0, "text": "This paper presents  a controllable model from a video of a person performing a certain", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byxe5udgcS", "sentence_index": 1, "text": "activity. It generates novel image sequences of that person, according", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byxe5udgcS", "sentence_index": 2, "text": "to user-defined control signals, typically marking the displacement of the moving", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byxe5udgcS", "sentence_index": 3, "text": "body. The generated video can have an arbitrary background, and effectively", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byxe5udgcS", "sentence_index": 4, "text": "capture both the dynamics and appearance of the person.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byxe5udgcS", "sentence_index": 5, "text": "It has two networks, Pose2Pose, and Pose2Frame.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Byxe5udgcS", "sentence_index": 6, "text": "The overall pipeline makes sense; and the paper is well written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Byxe5udgcS", "sentence_index": 7, "text": "The main problems come from the experiments, which I would ask for more things.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Byxe5udgcS", "sentence_index": 8, "text": "It has two components, i.e., Pose2Pose and Pose2Frame.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Byxe5udgcS", "sentence_index": 9, "text": "So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Byxe5udgcS", "sentence_index": 10, "text": "How about combining only Pose2Pose/ Pose2Frame  with pix2pixHD? Whether the performance can get improved?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1e8c5rL3Q", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1e8c5rL3Q", "sentence_index": 1, "text": "This paper introduces a generative model for 3D point clouds.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1e8c5rL3Q", "sentence_index": 2, "text": "Authors aim at theoretically showing the difficulties of using existing generative models to learn distributions of point clouds, and propose a variant that supposedly solves the issues.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1e8c5rL3Q", "sentence_index": 3, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1e8c5rL3Q", "sentence_index": 4, "text": "+ The problem of designing generative models for 3D data is important.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1e8c5rL3Q", "sentence_index": 5, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1e8c5rL3Q", "sentence_index": 6, "text": "- Paper is often hard to follow, and contains a significant number of typos.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1e8c5rL3Q", "sentence_index": 7, "text": "- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1e8c5rL3Q", "sentence_index": 8, "text": "As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1e8c5rL3Q", "sentence_index": 9, "text": "- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "H1e8c5rL3Q", "sentence_index": 10, "text": "- It is not clear why authors did not follow the evaluation protocol of [Achlioptas\u201917] or [Wu\u201916] more closely.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1e8c5rL3Q", "sentence_index": 11, "text": "In particular, evaluation for the classification task should be compatible with the proposed model, which would give a much better picture of the learned representations.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 0, "text": "The paper proposes a novel training method for variational autoencoders that allows using partially-observed data with multiple modalities.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 1, "text": "A modality can be a whole block of features (e.g., a MNIST image) or just a single scalar feature.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 2, "text": "The probabilistic model contains a latent vector per modality.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 3, "text": "The key idea is to use two types of encoder networks: a unimodal encoder for every modality which is used when the modality is observed, and a shared multimodal encoder that is provided all the observed modalities and produces the latent vectors for the unobserved modalities.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 4, "text": "The whole latent vector is passed through a decoder that predicts the mask of observed modalities, and another decoder that predicts the actual values of all modalities.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 5, "text": "The \u201cground truth\u201d values for the unobserved modalities are provided by sampling from the corresponding latent variables from the prior distribution once at some point of training.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 6, "text": "While I like the premise of the paper, I feel that it needs more work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 7, "text": "My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic \u201cground truth\u201d for these modalities, which in turn means that the model would produce underconfident predictions for them.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 8, "text": "The samples from MNIST in Figure 3 are indeed very blurry, supporting this.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 9, "text": "Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 10, "text": "I believe that the paper should currently be rejected, but I encourage the authors to revise the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 11, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 12, "text": "* Generative modelling of partially observed data is a very important topic that would benefit from fresh ideas and new approaches", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1eOXTuD5B", "sentence_index": 13, "text": "* I really like the idea of explicitly modelling the mask/missingness vector. I agree with the authors that this should help a lot with non completely random missingness.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1eOXTuD5B", "sentence_index": 14, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 15, "text": "* The text is quite hard to read.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 16, "text": "There are many typos (see below).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 17, "text": "The text is over the 8 page limit, but I don\u2019t think this is justified.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 18, "text": "For example, the paragraph around Eqn. (11) just says that the decoder takes in a concatenated latent vector.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 19, "text": "The MNIST+SVHN dataset setup is described in detail, yet there is no summary of the experimental results, which are presented in the appendix.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 20, "text": "* The approach taken to train on partially-observed data is described in three sentences after the Eqn. (10).", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 21, "text": "The non-observed dimensions are imputed by reconstructions from the prior from a partially trained model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 22, "text": "I think that this is the crux of the paper that should be significantly expanded and experimentally validated.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 23, "text": "It is possible that due to this design choice the method would not produce sharper reconstructions than the original samples from the prior.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 24, "text": "Figures 3, 5 and 6 indeed show very blurry samples from the model.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 25, "text": "Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 26, "text": "* The paper states multiple times that VAEAC [Ivanov et al., 2019] cannot handle partially missing data, but I don\u2019t think this is true, since their missing features imputation experiment uses the setup of 50% truly missing features.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 27, "text": "The trick they use is adding \u201csynthetic\u201d missing features in addition to the real ones and only train on those.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 28, "text": "See Section 4.3.3 of that paper for more details.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 29, "text": "* The paper states that \u201cit can model the joint distribution of the data and the mask together and avoid limiting assumptions such as MCAR\u201d.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 30, "text": "However, all experiments only show results in the MCAR setting, so the claim is not experimentally validated.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 31, "text": "* The baselines in the experiments could be improved.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 32, "text": "First of all, the setup for the AE and VAE is not specified.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 33, "text": "Secondly, it would be good to include a GAN-based baseline such as GAIN, as well as some more classic feature imputation method, e.g. MICE or MissForest.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1eOXTuD5B", "sentence_index": 34, "text": "* The experiments do not demonstrate that the model learns a meaningful *conditional* distribution for the missing modalities, since the provided figures show just one sample per conditioning image.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 35, "text": "Questions to the authors:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 36, "text": "1. Could you comment on the differences in your setup in Section 4.1 compared to the VAEAC paper? I\u2019ve noticed that the results you report for this method significantly differ from the original paper, e.g. for VAEAC on Phishing dataset you report PFC of 0.24, whereas the original paper reports 0.394; for Mushroom it\u2019s 0.403 vs. 0.244. I\u2019ve compared the experimental details yet couldn\u2019t find any differences, for example the missing rate is 0.5 in both papers.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 37, "text": "2. How do you explain that all methods have NRMSE > 1 on the Glass dataset (Table 1), meaning that they all most likely perform worse than a constant baseline?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1eOXTuD5B", "sentence_index": 38, "text": "Typos and minor comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 39, "text": "* Contributions (1) and (2) should be merged together.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1eOXTuD5B", "sentence_index": 40, "text": "* Page 2: to literature -> to the literature", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1eOXTuD5B", "sentence_index": 42, "text": "* Page 2: \u201cThis algorithm needs complete data during training cannot learn from partially-observed data only.\u201d", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1eOXTuD5B", "sentence_index": 43, "text": "* Equations (1, 2): z and \\phi are not consistently boldfaced", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1eOXTuD5B", "sentence_index": 44, "text": "* Equations (4, 5): you can save some space by only specifying the factorization (left column) and merging the two equations on one row", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1eOXTuD5B", "sentence_index": 46, "text": "* Page 4, bottom: use Bernoulli distribution -> use factorized/independent Bernoulli distribution", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1eOXTuD5B", "sentence_index": 47, "text": "* Page 5, bottom: the word \u201csimply\u201d is used twice", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "none"}, {"review_id": "H1eOXTuD5B", "sentence_index": 48, "text": "* Page 9: learn to useful -> learn useful", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1eOXTuD5B", "sentence_index": 49, "text": "* Page 9: term is included -> term included", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1eOXTuD5B", "sentence_index": 50, "text": "* Page 9: variable follows Bernoulli -> variable following Bernoulli", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1eOXTuD5B", "sentence_index": 51, "text": "* Page 9: conditions on -> conditioning on", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1eqf5OA3m", "sentence_index": 0, "text": "The idea of having a separate class for out-distribution is a very interesting idea but unfortunately previously explored.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1eqf5OA3m", "sentence_index": 1, "text": "In fact, in machine learning and NLP there is the OOV class which sometimes people in computer vision also use.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1eqf5OA3m", "sentence_index": 2, "text": "Some of the claims in the paper can be further substantiated or explored.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1eqf5OA3m", "sentence_index": 3, "text": "For example in abstract there is a simple claim that is presented too strong: We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1eqf5OA3m", "sentence_index": 4, "text": "This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eqf5OA3m", "sentence_index": 5, "text": "Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eqf5OA3m", "sentence_index": 6, "text": "In general it is very unlikely that you will be able to choose every variation of out-distribution cases.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eqf5OA3m", "sentence_index": 7, "text": "Much easier if you just try to solve the problem using a set of n Sigmoids (n total number of classes) and consider each output a probability distribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eqf5OA3m", "sentence_index": 8, "text": "However, the studies in this paper are still valuable and I strongly recommend continuing on the same direction.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1eQwEeRYS", "sentence_index": 0, "text": "In this paper, the authors address representation learning in non-Euclidean spaces.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eQwEeRYS", "sentence_index": 1, "text": "The authors are motivated by constant curvature geometries, that can  provide a useful trade-off between Euclidean representations and Riemannian manifolds, i.e. arriving at more suitable representations than possible in the Euclidean space, while not sacrifising closed-form formulae for estimating distances, gradients and so on.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eQwEeRYS", "sentence_index": 2, "text": "The authors point out that an extension of the gyrovector space formalization to spaces of constant positive curvature (spherical) is required, and with the corresponding formalization for hyperbolic spaces, one can arrive at a unified formalism that can interpolate smoothly between all geometries of constant curvature.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eQwEeRYS", "sentence_index": 3, "text": "The authors propose to do so by replacing  the curvature while flipping the sign in the standard Poincare model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eQwEeRYS", "sentence_index": 4, "text": "This is a strong point reagarding this work, as it seems that no such unification has been attempted in the past (although simply replacing the curvature in the Poincare model seems a bit too straightforward to not have been attempted, it seems to be the case).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eQwEeRYS", "sentence_index": 5, "text": "The authors also provide extensive supplementary material (around 20 pages) with detailed derivations and descriptions of experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1eQwEeRYS", "sentence_index": 6, "text": "This also makes me wonder if this paper is more suitable for a journal - both in terms of the extensive supplementary material (e.g., curvature sampling algorithm and other details can be found only in supplementary), as well as the more rigorous review process that a journal paper goes through.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1eQwEeRYS", "sentence_index": 7, "text": "In the main paper, only proof of concept experiments are provided (one experiment), that nevertheless show competitive performance under varying settings.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eQwEeRYS", "sentence_index": 8, "text": "However, it seems to me that such contributions in the rising field of geometric deep learning, where several challenges are yet to be overcome, can be beneficial for future research.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1eQwEeRYS", "sentence_index": 9, "text": "Since in the supplementary experiments also, it seems that curvature does have small variance in the results, how would the authors assess the robustness of the curvature sampling method with respect to the results?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1euiaMah7", "sentence_index": 0, "text": "Overall:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1euiaMah7", "sentence_index": 1, "text": "This paper introduces the Scratchpad Encoder, a novel addition to the sequence to sequence (seq2seq) framework and explore its effectiveness in generating natural language questions from a given logical form.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1euiaMah7", "sentence_index": 2, "text": "The proposed model enables the decoder at each time step to modify all the encoder outputs, thus using the encoder as a \u201cscratchpad\u201d memory to keep track of what has been generated so far and to guide future generation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1euiaMah7", "sentence_index": 3, "text": "Quality and Clarity:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1euiaMah7", "sentence_index": 4, "text": "-- The paper is well-written and easy to read.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "H1euiaMah7", "sentence_index": 5, "text": "-- Consider using a standard fonts for the equations.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "H1euiaMah7", "sentence_index": 6, "text": "Originality :", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1euiaMah7", "sentence_index": 7, "text": "The idea of question generation: using logical form to generate meaningful questions for argumenting data of QA tasks is really interesting and useful.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1euiaMah7", "sentence_index": 8, "text": "Compared to several baselines with a fixed encoder, the proposed model allows the decoder to attentively write \u201cdecoding information\u201d to the \u201cencoder\u201d output.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "H1euiaMah7", "sentence_index": 9, "text": "The overall idea and motivation looks very similar to the coverage-enhanced models where the decoder also actively \u201cwrites\u201d a message (\u201ccoverage\u201d) to the encoder's hidden states.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "H1euiaMah7", "sentence_index": 10, "text": "In the original coverage paper (Tu et.al, 2016), they also proposed a \u201cneural network based coverage model\u201d where they used a general neural network output to encode attention history, although this paper works differently where it directly updates the encoder hidden states with an update vector from the decoder.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1euiaMah7", "sentence_index": 11, "text": "However, the modification is slightly marginal but seems quite effective.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1euiaMah7", "sentence_index": 12, "text": "It is better to explain the major difference and the motivation of updating the hidden states. -------------------", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1euiaMah7", "sentence_index": 14, "text": "Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1euiaMah7", "sentence_index": 15, "text": "-- In Equation (13), is there an activation function between W1 and W2?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1euiaMah7", "sentence_index": 16, "text": "-- Based on Table 1, why did not evaluate the proposed model with beam-search?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1evEoR6YH", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 1, "text": "The paper, considers methods for solving smooth unconstrained min-max optimization problems.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 2, "text": "In particular, the authors prove that the Hamiltonian Gradient Descent (HGD) algorithm converges with linear convergence rate to the min-max solution.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 3, "text": "One of the main contributions of this work is that the proposed analysis is focusing on last iterate convergence guarantees for the HGD.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 4, "text": "This result, as the authors claim can be particularly useful in the future for analyzing more general settings (nonconvex-nonconcave min-max problems).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 5, "text": "In addition, two preliminary convergence theorems were provided for two extensions of HGD: (i) a stochastic variant of HGD and (ii)  Consensus Optimization Algorithm (CO) (by establishing connections of CO and HGD).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 6, "text": "Main Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 7, "text": "The paper is well written and the main contributions are clear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "H1evEoR6YH", "sentence_index": 8, "text": "I believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding  the presentation and the combination of different assumptions used in the theory.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "H1evEoR6YH", "sentence_index": 9, "text": "1) I think definition 2.5 of Higher order Lipschitz is very strong assumption to have. What exactly means? Essentially the authors upper bounded any difficult term appear in the theorems. Is it possible to avoid having something so strong? Please elaborate.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1evEoR6YH", "sentence_index": 10, "text": "2) In assumption 3.1 is not clear what $L_H$ is.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1evEoR6YH", "sentence_index": 11, "text": "This quantity never mentioned before.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1evEoR6YH", "sentence_index": 12, "text": "Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1evEoR6YH", "sentence_index": 13, "text": "3) What is the main difference on the combination of assumptions on Theorems 3.2, 3.2 and 3.4. Which one is stronger. Is there a reason for the existence of Theorem 3.3?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1evEoR6YH", "sentence_index": 14, "text": "4) All the results heavily depend on the PL condition.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 15, "text": "I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1evEoR6YH", "sentence_index": 16, "text": "In particular, one can propose several combinations of assumptions in order for the function H to satisfy the PL condition.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 17, "text": "Can we avoid having the PL condition?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1evEoR6YH", "sentence_index": 18, "text": "The authors need to elaborate more on this.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1evEoR6YH", "sentence_index": 19, "text": "5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1evEoR6YH", "sentence_index": 20, "text": "Minor Suggestions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 21, "text": "In first paragraph of page 5 where the authors divide the existing literature into the three particular cases, I am suggesting to add the refereed papers inside each one of this cases (which papers assumed function g bilinear , which papers strongly convex-concave etc.)", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "H1evEoR6YH", "sentence_index": 22, "text": "I understand that the main contribution of the work is the theoretical analysis of the proposed method but would like to see some numerical evaluation in the main paper.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1evEoR6YH", "sentence_index": 23, "text": "There are some preliminary results in the appendix but it will be useful for the reader if there are are some plots showing the benefit of the method in comparison with existing methods that guarantee convergence (which method is faster?).", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1evEoR6YH", "sentence_index": 24, "text": "In the current experiments there is a comparison only with CO algorithm and SGDA.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1evEoR6YH", "sentence_index": 25, "text": "In general i find the paper interesting, with nice ideas and I believe that will be appreciated from researchers that are interested on smooth games and their connections to machine learning applications.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1evEoR6YH", "sentence_index": 26, "text": "I suggest weak accept but I am open to reconsider in case that my above concerns are answered.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 27, "text": "**********after rebuttal ********", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 29, "text": "I would like to thank the authors for their reply and for the further clarification.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 30, "text": "I will keep my score the same but I highly encourage the authors to add some clarification related to my last comment on the globally bounded gradient.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1evEoR6YH", "sentence_index": 31, "text": "In their response they mentioned that the analysis only requires that  H is smooth and that $\\|\\xi(x^{(0)})\\|$ is sufficient bound.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 32, "text": "This needs to be clear in the paper (add clear arguments and related references).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1evEoR6YH", "sentence_index": 33, "text": "In addition, in their response they highlight the non-increasing nature of function H over the course of the algorithm which is important for their argument.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 34, "text": "Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1evEoR6YH", "sentence_index": 35, "text": "In SGD,  function H does not necessarily decrease over the course of the algorithm.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1evEoR6YH", "sentence_index": 36, "text": "The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eZXehVnX", "sentence_index": 0, "text": "Authors provide a variant of WGAN, called PC-GAN, to generate 3D point clouds.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 1, "text": "The drawback of a vanilla GAN with a DeepSet classifier is analyzed.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 2, "text": "The rationality that decoupling the point generator with the object generator is also discussed.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 3, "text": "A sandwiching objective function is proposed to achieve a better estimation of Wasserstein distance.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 4, "text": "Compared with AAE and the simplified variants of the proposed PC-GAN, the proposed PC-GAN achieves incremental results on point cloud generation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 5, "text": "Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 6, "text": "1. Authors calculate W_U in a primal form via solving an assignment programming problem.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 7, "text": "Have authors ever tried Sinkhorn iteration?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1eZXehVnX", "sentence_index": 8, "text": "To my knowledge, sinkhorn iteration is a very popular method to solve OT problem effectively.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 9, "text": "It would be nice if authors can provide some reasons and comparisons for their choice on the optimizer of W_U.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "H1eZXehVnX", "sentence_index": 10, "text": "2.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 11, "text": "Authors proved that the sandwiching object W_s is closer to the real Wasserstein distance, but it increases the variance of the loss function.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 12, "text": "Specifically, the dynamics of W_U, and W_L, according to lemma1, is (epsilon2-epsilon1)*w(P, G) while the dynamics of W_s is 2*epsilon1 * w(P, G), and 2epsilon1 > epsilon2 - epsilon1 (according to the assumption in lemma 1).", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 13, "text": "Does it mean that the W_s is not as stable as W_L or W_U during training?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1eZXehVnX", "sentence_index": 14, "text": "Additionally, authors combined W_U with W_L with a mixture 20:1, i.e., the s in Eqs(6, 13, 14) is smaller than 0.05.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 15, "text": "In such a situation, both the value and the dynamics of W_s will be very close to that of W_U. Does it mean that W_L is not so important as W_U?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1eZXehVnX", "sentence_index": 16, "text": "Authors should analyze the stability of their method in details.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eZXehVnX", "sentence_index": 17, "text": "Essentially, the proposed method is a variant of WGAN, which estimates Wasserstein distance with lower bias but may suffer from worse stability.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 18, "text": "In the experiments, both the setting and the experimental results show that the proposed W_s will be very close to W_U. As a result, the improvement caused by the proposed method is incremental compared with its variants.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1eZXehVnX", "sentence_index": 19, "text": "Typos:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1eZXehVnX", "sentence_index": 20, "text": "- The end of the 2nd line of lemma 1: P, G should be \\mathbb{P}, \\mathbb{G}", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1eZXehVnX", "sentence_index": 21, "text": "- The 3rd line of lemma 1: epsilon1 -> epsilon_1", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1eZXehVnX", "sentence_index": 22, "text": "- Page 14, Eq(14), \\lambda should be s", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1eZXehVnX", "sentence_index": 23, "text": "- Page 14, Eqs(13, 14), w(\\mathbb{P}, \\mathbb{G}) should appear on the right.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1g7-dMz3m", "sentence_index": 0, "text": "The authors propose three improvements to the DNC model: masked attention, erasion of de-allocated elements, and sharpened temporal links --- and show that this allows the model to solve synthetic memory tasks faster and with better precision.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1g7-dMz3m", "sentence_index": 1, "text": "They also show the model performs better on average on bAbI than the original DNC.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1g7-dMz3m", "sentence_index": 2, "text": "The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1g7-dMz3m", "sentence_index": 3, "text": "I think the paper is particularly clearly written, and I would vote for it being accepted as it has implications beyond the DNC.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1g7-dMz3m", "sentence_index": 4, "text": "The fact that masked attention works so much better than the standard cosine-weighted content-based attention is pretty interesting in itself.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1g7-dMz3m", "sentence_index": 5, "text": "The insights (e.g. Figure 5) are interesting and show the study is not just trying to be a benchmark paper for some top-level results, but actually cares about understanding a problem and fixing it.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1g7-dMz3m", "sentence_index": 6, "text": "Although most recent memory architectures do not seem to have incorporated the DNC's slightly complex memory de-allocation scheme, any resurgent work in this area would benefit from this study.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1gcRli3YB", "sentence_index": 0, "text": "This paper proposes a method to address the interesting task, i.e. controllable human activity synthesis, by conditioning on the previous frames and the input control signal.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gcRli3YB", "sentence_index": 1, "text": "To synthesis the next frame, a Pose2Pose network is proposed to first transfer the input information into the next frame body structure and object.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gcRli3YB", "sentence_index": 2, "text": "Then, a Pose2Frame network is applied to generate the final result.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gcRli3YB", "sentence_index": 3, "text": "The results on several video sequences look nice with more natural boundaries, object, and backgrounds compared to previous methods.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gcRli3YB", "sentence_index": 5, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1gcRli3YB", "sentence_index": 6, "text": "1. The proposed Pose2Pose successfully transfer the pose conditioned on the past pose and the input control signal.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1gcRli3YB", "sentence_index": 7, "text": "The proposed conditioned residual block, occlusion augmentation and stopping criteria seem to help the Pose2Pose network work well.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1gcRli3YB", "sentence_index": 8, "text": "Besides, the object is also considered in this network, which makes the method generalized well to the videos where human holds some rigid object.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1gcRli3YB", "sentence_index": 9, "text": "2. The Pose2Frame network is similar to previous works but learns to predict the soft mask to incorporate the complex background and to produce shallow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1gcRli3YB", "sentence_index": 10, "text": "The mask term in Eq. (7) seems to work well for the foreground (body+object) and the shallow regions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1gcRli3YB", "sentence_index": 11, "text": "3. The paper is easy to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "H1gcRli3YB", "sentence_index": 12, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1gcRli3YB", "sentence_index": 13, "text": "1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1gcRli3YB", "sentence_index": 14, "text": "Results on more scenes will make the performance more convincing.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1gcRli3YB", "sentence_index": 15, "text": "I also wonder if the video data will be released, which could be important for the following comparisons.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "H1gcRli3YB", "sentence_index": 16, "text": "2. As to the results of the Pose2Pose network, I wonder if there are some artifacts that will affect the performance of the Pose2Frame network.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1gcRli3YB", "sentence_index": 17, "text": "Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "H1gcRli3YB", "sentence_index": 18, "text": "I assume the authors first train the Pose2Pose network, then use the output to train the Pose2Frame network.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1gcRli3YB", "sentence_index": 19, "text": "Otherwise, the artifacts from Pose2Pose will affect the testing performance of the Pose2Frame network.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1gcRli3YB", "sentence_index": 20, "text": "3. The mask term seems to work well for the shallow part.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1gcRli3YB", "sentence_index": 21, "text": "I wonder how the straightforward regression term plus the smooth term will perform for the mask.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1gcRli3YB", "sentence_index": 22, "text": "Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1gMR8h6h7", "sentence_index": 0, "text": "The paper propose to incorporate an additional class for adversarial and out-distribution samples in CNNs.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gMR8h6h7", "sentence_index": 1, "text": "The paper propose to incorporate natural out-distribution images and interpolated images to the additional class, but the problem of selecting the out-distribution images is itself an important problem.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gMR8h6h7", "sentence_index": 2, "text": "The paper presents a very simple approaches for selecting the out-distribution images that relies on many hidden assumptions on the images source or the base classier, and the interpolation mechanism is also too simple and there is the implicit assumption of low complexity images.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gMR8h6h7", "sentence_index": 3, "text": "There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1gMR8h6h7", "sentence_index": 4, "text": "In summary, the quality of the paper is poor and the originality of the work is low.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1gMR8h6h7", "sentence_index": 5, "text": "The paper is easily readable.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "H1goRfkKnm", "sentence_index": 0, "text": "Summary: This paper is about models for solving basic math problems.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 1, "text": "The main contribution is a synthetically generated dataset that includes a variety of types and difficulties of math problems; it is both larger and more varied than previous datasets of this type.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 2, "text": "The dataset is then used to evaluate a number of recurrent models (LSTM, LSTM+attention, transformer); these are very powerful models for general sequence-sequence tasks, but they are not explicitly tailored to math problems.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 3, "text": "The results are then analyzed and insights are derived explaining where neural models seemingly cope well with math tasks, and where they fall down.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 4, "text": "Strengths: I am happy to see the proposal of a very large dataset with a lot of different axes for measuring and examining the performance of models.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1goRfkKnm", "sentence_index": 5, "text": "There are challenging desiderata involved in building the training+tests sets, and the authors have an interesting and involved methodology to accomplish these.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1goRfkKnm", "sentence_index": 6, "text": "The paper is very clearly written. I'm not aware of a comparable work, so the novelty here seems good.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "H1goRfkKnm", "sentence_index": 7, "text": "Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1goRfkKnm", "sentence_index": 8, "text": "It would have been useful to compare the general models here with some specific math problem-focused ones as well.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "H1goRfkKnm", "sentence_index": 9, "text": "Some details weren't clear to me.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1goRfkKnm", "sentence_index": 10, "text": "More in the comments below.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 11, "text": "Verdict: I thought this was generally an interesting paper that has some very nice benefits, but also has some weaknesses that could be resolved. I view it as borderline, but I'm willing to change my mind based on the discussion.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 12, "text": "Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 13, "text": "- One area that could stand to be improved is prior work.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "H1goRfkKnm", "sentence_index": 14, "text": "I'd like to see more of a discussion of *prior data sets* rather than papers proposing models for problems.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "H1goRfkKnm", "sentence_index": 15, "text": "Since this is the core contribution, this should also be the main comparison.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 16, "text": "For example, EMLNP 2017 paper \"Deep Neural Solver for Math Word Problems\" mentions a size 60K problem dataset.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 17, "text": "A more extensive discussion will help convince the readers that the proposed dataset is indeed the largest and most diverse.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 18, "text": "- The authors note that previous datasets are often specific to one type of problem (i.e., single variable equation solving). Why not then combine multiple types of extant problem sets?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1goRfkKnm", "sentence_index": 19, "text": "- The authors divide dataset construction into crowdsourcing and synthetic.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 20, "text": "This seems incomplete to me: there are tens of thousands (probably more) of exercises and problems available in workbooks for elementary, middle, and high school students.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1goRfkKnm", "sentence_index": 21, "text": "These are solved, and only require very limited validation.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 22, "text": "They are also categorized by difficulty and area.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 23, "text": "Presumably the cost here would be to physically scan some of these workbooks, but this seems like a very limited investment. Why not build datasets based on workbooks, problem solving books, etc?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1goRfkKnm", "sentence_index": 24, "text": "- How do are the difficulty levels synthetically determined?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1goRfkKnm", "sentence_index": 25, "text": "- When generating the questions, the authors \"first sample the answer\". What's the distribution you use on the answer? This seems like it dramatically affects the resulting questions, so I'm curious how it's selected.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1goRfkKnm", "sentence_index": 26, "text": "- The general methodology of generating questions and ensuring that no question is too rare or too frequent and the test set is sufficiently different---these are important questions and I commend the authors for providing a strong methodology.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1goRfkKnm", "sentence_index": 27, "text": "- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1goRfkKnm", "sentence_index": 28, "text": "This is certainly a scientific decision, i.e., the authors are determining which models to use in order to determine the possible insights they will derive.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 29, "text": "But it's not clear to me why testing more sophisticated models that are tailored for math questions would *not* be useful.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1goRfkKnm", "sentence_index": 30, "text": "In fact, assuming that such methods outperform general-purpose models, we could investigate why and where this is the case (in fact the proposed dataset is very useful for this).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 31, "text": "On the other hand, if these specialized approaches largely fail to outperform general-purpose models, we would have the opposite insights---that these models' benefits are dataset-specific and thus limited.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 32, "text": "- Really would be good to do real-world tests in a more extensive way.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1goRfkKnm", "sentence_index": 33, "text": "A 40-question exam for 16 year olds is probably far too challenging for the current state of general recurrent models. Can you add some additional grades here, and more questions?", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1goRfkKnm", "sentence_index": 34, "text": "- For the number of thinkings steps, how does it scale up as you increase it from 0 to 16? Is there a clear relationships here?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1goRfkKnm", "sentence_index": 35, "text": "- The 1+1+...+1 example is pretty intriguing, and could be a nice \"default\" question!", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1goRfkKnm", "sentence_index": 36, "text": "- Minor typo: in the abstract: \"test spits\" should be \"test splits\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1gOvMYT37", "sentence_index": 0, "text": "* Summary", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 1, "text": "This paper addresses machine reading tasks involving tracking the states of entities over text.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 2, "text": "To this end, it proposes constructing a knowledge graph using recurrent updates over the sentences of the text, and using the graph representation to condition a reading comprehension module.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 3, "text": "The paper reports positive evaluations on three different tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 4, "text": "* Review", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 5, "text": "This is an interesting paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1gOvMYT37", "sentence_index": 6, "text": "The key technical component in the proposed approach is the idea that keeping track of entity states requires (soft) coreference between newly read entities and locations and the ones existing in the knowledge graph constructed so far.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 7, "text": "The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1gOvMYT37", "sentence_index": 8, "text": "This is especially the case in a few places involving coreference:", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 9, "text": "1. The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 10, "text": "2. The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 11, "text": "While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1gOvMYT37", "sentence_index": 12, "text": "Why does the graph update require coreference pooling again?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1gOvMYT37", "sentence_index": 13, "text": "Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1gOvMYT37", "sentence_index": 14, "text": "Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1gOvMYT37", "sentence_index": 15, "text": "That the model implicitly learns constraints from data is interesting!", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "H1gOvMYT37", "sentence_index": 16, "text": "Bottomline: The paper presents interesting ideas and good results, but would be better if the modeling choices were better explored/motivated.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 0, "text": "[Summary]", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 1, "text": "This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 2, "text": "The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 3, "text": "The final prediction uses all the states and question to infer the final answers.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 4, "text": "The authors verify the effectiveness of the proposed method on the performance of different tasks and modules.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 5, "text": "Experiment on VQA shows the proposed model benefits from utilizing different modules.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 6, "text": "The authors also qualitatively show the model's reasoning process and human study on judging answering quality.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 7, "text": "[Strength]", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 8, "text": "1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 9, "text": "This is different from most existing work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 10, "text": "2: By examing different modules, the proposed method is more interpretable compare to canonical methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 11, "text": "3: The experiment results are good, especially for the counting problem.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 12, "text": "[Weakness]", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 13, "text": "1. The title of the paper is \"visual reasoning by progressive module networks.\" The title may be a little overstated since the major task is focused on visual question answering (VQA).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 14, "text": "2. Annotation is not clear in this paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 15, "text": "For example, on page 3, Query transmitter and receiver, \"the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k).", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 16, "text": "\" There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 17, "text": "On page 4, State update function, what is the meaning of variable \"Epsilon\" in the equation?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 18, "text": "From the supplementary, it seems Epsilon means the environment?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 19, "text": "3. On the object counting task, the query transmitter needs to produce a query for a relationship module.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 20, "text": "The authors mentioned that this is softly calculated by softmax on the importance score.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 21, "text": "Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 22, "text": "4. The cider score of image captioning is 109 compared to the baseline 108.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 23, "text": "The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 24, "text": "Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 25, "text": "My assumption is the visual feature already contains the label information for image captioning.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 26, "text": "5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 27, "text": "6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 28, "text": "7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1gUmqkh3Q", "sentence_index": 29, "text": "Is the number right?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1l3mdg7qS", "sentence_index": 0, "text": "This paper studies multi-source domain adaptation problem.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1l3mdg7qS", "sentence_index": 1, "text": "First this paper proposes a new theory for this domain that extends generalized discrepancy theory to multi-source setting.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1l3mdg7qS", "sentence_index": 2, "text": "After derive a new generalization bound, this paper also proposes a new method based on the theory.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1l3mdg7qS", "sentence_index": 3, "text": "Evaluation on real world datasets are proposed to show the efficiency of the proposed method.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1l3mdg7qS", "sentence_index": 4, "text": "+ The theory in this paper improve bounds for multi-source DA in previous paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "H1l3mdg7qS", "sentence_index": 5, "text": "The new bound provides new insight and helps the design of algorithm.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "H1l3mdg7qS", "sentence_index": 6, "text": "+ This paper proposes elegant method to tackle new terms in the loss function and gives its complexity analysis.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "H1l3mdg7qS", "sentence_index": 7, "text": "+ The evaluation results show that the algorithm is efficient.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1l3mdg7qS", "sentence_index": 8, "text": "Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1l3mdg7qS", "sentence_index": 9, "text": "- The main contribution of the proposed theory is the alpha term.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1l3mdg7qS", "sentence_index": 10, "text": "Is \\eta_{\\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \\eta_{\\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1l3mdg7qS", "sentence_index": 11, "text": "- The evaluation of the proposed method is not complete.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1l3mdg7qS", "sentence_index": 12, "text": "Some baseline DA methods [A, B] and datasets [C, D] are not considered.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1l3mdg7qS", "sentence_index": 13, "text": "[A] S. Sankaranarayanan, Y. Balaji, C. D. Castillo, and R. Chellappa. Generate to adapt: Aligning domains using generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1l3mdg7qS", "sentence_index": 14, "text": "[B] Saito, Kuniaki, et al. \"Maximum classifier discrepancy for unsupervised domain adaptation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1l3mdg7qS", "sentence_index": 15, "text": "[C] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In European Conference on Computer Vision (ECCV), 2010.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1l3mdg7qS", "sentence_index": 16, "text": "[D]H.Venkateswara, J.Eusebio, S.Chakraborty, and S.Panchanathan. Deep hashing network for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1l4PVq4KH", "sentence_index": 0, "text": "The paper devises a pipeline that aims to address catastrophic forgetting in continual learning (CL) by the well-known generative replay (GR) technique.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1l4PVq4KH", "sentence_index": 1, "text": "The key ingredient of the pipeline is a modern variational auto-encoder (VAE) that is trained with class labels with respect to a mutual information maximization criterion.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1l4PVq4KH", "sentence_index": 2, "text": "The paper does not follow a smooth story line, where an open research question is presented and a solution to this problem is developed in steps.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1l4PVq4KH", "sentence_index": 3, "text": "The flowchart in Fig 1 is rather a system design consisting of many components, the functionality of which is not clearly described and existence of which is not justified.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1l4PVq4KH", "sentence_index": 4, "text": "This complex flowchart does not even describe the complete task.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1l4PVq4KH", "sentence_index": 5, "text": "It is in the end plugged into a continual learning algorithm which also performs domain transformation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1l4PVq4KH", "sentence_index": 6, "text": "All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1l4PVq4KH", "sentence_index": 7, "text": "Hence, I kindly do not think the outcome is truly a research result.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1l4PVq4KH", "sentence_index": 8, "text": "It is more system engineering than science.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1l4PVq4KH", "sentence_index": 9, "text": "The next submission of the paper could choose one or few of these pieces as target research problems and develop a thoroughly analyzed novel technical solution for them.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1l4PVq4KH", "sentence_index": 10, "text": "If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1l4PVq4KH", "sentence_index": 11, "text": "Minor: The abstract could be improved by providing more clear pointers to the presented novelty.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1l6suNjhX", "sentence_index": 0, "text": "Summary", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 1, "text": "The paper proposes to modify the \"Dual Learning\" approach to supervised (and unsupervised) translation problems by making use of additional pretrained mappings for both directions (i.e. primal and dual).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 2, "text": "These pre-trained mappings (\"agents\") generate targets from the primal to the dual domain, which need to be mapped back to the original input.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 3, "text": "It is shown that having >=1 additional agents improves training of the BLEU score in standard MT and unsupervised MT tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 4, "text": "The method is also applied to unsupervised image-to-image \"translation\" tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 5, "text": "Positives and Negatives", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 6, "text": "+1 Simple and straightforward method with pretty good results on language translation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "H1l6suNjhX", "sentence_index": 7, "text": "+2 Does not require additional computation during inference, unlike ensembling.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 8, "text": "-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1l6suNjhX", "sentence_index": 9, "text": "-2 Diversity of additional \"agents\" not analyzed (more below).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1l6suNjhX", "sentence_index": 10, "text": "-3 For image-to-image translation experiments, no quantitative analysis whatsoever is offered so the reader can't really conclude anything about the effect of the proposed method in this domain.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1l6suNjhX", "sentence_index": 11, "text": "-4 Talking about \"agents\" and \"Multi-Agent\" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just \"mapping\" or \"network\"?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1l6suNjhX", "sentence_index": 12, "text": "-1: Potential Issues with the Maths.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 13, "text": "The maths is not clear, in particular the gradient derivation in equation (8).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1l6suNjhX", "sentence_index": 14, "text": "Let's just consider the distortion objective on x (of course it also applies to y without loss of generality).", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 15, "text": "At the very least we need another \"partial\" sign in front of the \"\\delta\" function in the numerator.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1l6suNjhX", "sentence_index": 16, "text": "But again, it's not super clear how the paper estimates this derivative.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1l6suNjhX", "sentence_index": 17, "text": "Intuitively the objective wants f_0 to generate samples which, when mapped back to the X domain, have high log-probability under G, but its samples cannot be differentiated in the case of discrete data.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 18, "text": "So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1l6suNjhX", "sentence_index": 19, "text": "In the case of continuous data x, is the reparameterization trick used?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1l6suNjhX", "sentence_index": 20, "text": "This should at the very least be explained more clearly.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1l6suNjhX", "sentence_index": 21, "text": "Note that the importance sampling does not affect this issue.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 22, "text": "-2: Diversity of Agents.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 23, "text": "As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1l6suNjhX", "sentence_index": 24, "text": "The paper proposes to use different random seeds and iterate over the dataset in a different order for distinct pretrained f_i.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 25, "text": "The paper should quantify that this leads to diverse \"agents\".", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1l6suNjhX", "sentence_index": 26, "text": "I suppose the proof is in the pudding; as we have argued, multiple agents can only improve performance if they are distinct, and Figure 1 shows some improvement as the number of agents are increase (no error bars though).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 27, "text": "The biggest jump seems to come from N=1 -> N=2 (although N=4 -> N=5 does see a jump as well).", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 28, "text": "Presumably if you get a more diverse pool of agents, that should improve things.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1l6suNjhX", "sentence_index": 29, "text": "Have you considered training different agents on different subsets of the data, or trying different learning algorithms/architectures to learn them?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1l6suNjhX", "sentence_index": 30, "text": "More experiments on the diversity would help make the paper more convincing.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1lALLCRhX", "sentence_index": 0, "text": "The paper investigates the Frank-Wolfe (FW) algorithm for constructing adversarial examples both in a white-box and black-box setting.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 1, "text": "The authors provide both a theoretical analysis (convergence to a stationary point) and experiments for an InceptionV3 network on ImageNet.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 2, "text": "The main claim is that the proposed algorithm can construct adversarial examples faster than various baselines (PGD, I-FGSM, CW, etc.), and from fewer queries in a black-box setting.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 3, "text": "The FW algorithm is a classical method in optimization, but (to the best of my knowledge) has not yet been evaluated yet for constructing adversarial examples.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 4, "text": "Hence it is a natural question to understand whether FW performs significantly better than current algorithms in this context.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 5, "text": "Indeed, the authors find that FW is 6x - 20x faster for constructing white-box adversarial examples than a range of relevant baseline, which is a significant speed-up.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 6, "text": "However, there are several points about the experiments that are unclear to me:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 7, "text": "- It is well known that the running times of optimization algorithms are highly dependent on various hyperparameters such as the step size.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 8, "text": "But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1lALLCRhX", "sentence_index": 9, "text": "Hence it is unclear how large the running time improvement is compared to a well-tuned baseline.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1lALLCRhX", "sentence_index": 10, "text": "- Other algorithms in the comparison achieve a better distortion (smaller perturbation).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 11, "text": "Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1lALLCRhX", "sentence_index": 12, "text": "Instead of reporting a single time-vs-distortion data point, the authors could show the full trade-off curve.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1lALLCRhX", "sentence_index": 13, "text": "- The authors only provide running times, not the number of iterations.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1lALLCRhX", "sentence_index": 14, "text": "In principle all the algorithms should have a similar bottleneck in each iteration (computing a gradient for the input image), but it would be good to verify this with an iteration count vs success rate (or distortion) plot.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1lALLCRhX", "sentence_index": 15, "text": "This would also allow the authors to compare their theoretical iteration bound with experimental data.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1lALLCRhX", "sentence_index": 16, "text": "In addition to these three main points, the authors could strengthen their results by providing experiments on another dataset (e.g., CIFAR-10) or model architecture (e.g., a ResNet), and by averaging over a larger number of test data points (currently 200).", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1lALLCRhX", "sentence_index": 17, "text": "Overall, I find the paper a promising contribution. But until the authors provide a more thorough experimental evaluation, I hesitate to recommend acceptance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1lALLCRhX", "sentence_index": 18, "text": "Additional comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 19, "text": "The introduction contains a few statements that may paint an incomplete or confusing picture of the current literature in adversarial attacks on neural networks:", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1lALLCRhX", "sentence_index": 20, "text": "* The abstract claims that the poor time complexity of adversarial attacks limits their practical usefulness.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 21, "text": "However, the running time of attacks is typically measured in seconds and should not be the limiting element in real-world attacks on deep learning systems.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 22, "text": "I am not aware of a setting where the running time of an attack is the main computational bottleneck (outside adversarial training).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 23, "text": "* The introduction distinguishes between \"gradient-based methods\" and \"optimization-based methods\".", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 24, "text": "This distinction is potentially confusing to a reader since the gradient-based methods can be seen as optimization algorithms, and the optimization-based methods rely on gradients.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 25, "text": "* The introduction claims that black-box attacks need to estimate gradients coordinate-wise.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 26, "text": "However, this is not the case already in some of the prior work that uses random directions for estimating gradients (e.g., the cited paper by Ilyas et al.)", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lALLCRhX", "sentence_index": 27, "text": "I encourage the authors to clarify these points in an updated version of their paper.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1ly2z2RFS", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1ly2z2RFS", "sentence_index": 1, "text": "This paper proposes to impute multimodal data when certain modalities are present.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1ly2z2RFS", "sentence_index": 2, "text": "The authors present a variational selective autoencoder model that learns only from partially-observed data.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1ly2z2RFS", "sentence_index": 3, "text": "VSAE is capable of learning the joint distribution of observed and unobserved modalities as well as the imputation mask, resulting in a model that is suitable for various down-stream tasks including data generation and imputation .", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1ly2z2RFS", "sentence_index": 6, "text": "The authors evaluate on both synthetic high-dimensional and challenging low-dimensional multimodal datasets and show improvement over the state-of-the-art data imputation models.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1ly2z2RFS", "sentence_index": 7, "text": "Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1ly2z2RFS", "sentence_index": 8, "text": "- This is an interesting paper that is well written and motivated.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1ly2z2RFS", "sentence_index": 9, "text": "- The authors show good results on several multimodal datasets, improving upon several recent works in learning from missing multimodal data.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1ly2z2RFS", "sentence_index": 10, "text": "Weaknesses:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1ly2z2RFS", "sentence_index": 11, "text": "- How multimodal are the datasets provided by UCI? It seems like they consist of different tabular datasets with numerical or categorical variables, but it was not clear what the modalities are (each variable is a modality?) and how correlated the modalities are. If they are not correlated at all and share no joint information I'm not sure how these experiments can represent multimodal data.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1ly2z2RFS", "sentence_index": 12, "text": "- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1ly2z2RFS", "sentence_index": 13, "text": "They should consider larger-scale datasets including image and text-based like VQA/VCR, or video-based like the datasets in (Tsai et al., ICLR 2019).", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1ly2z2RFS", "sentence_index": 14, "text": "- In terms of prediction performance, the authors should also compare to [1] and [2] which either predict the other modalities completely during training or use tensor-based methods to learn from noisy or missing time-series data.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "H1ly2z2RFS", "sentence_index": 15, "text": "- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1ly2z2RFS", "sentence_index": 16, "text": "[1] Pham et al. Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities, AAAI 2019", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1ly2z2RFS", "sentence_index": 17, "text": "[2] Liang et al. Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization, ACL 2019", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1ly2z2RFS", "sentence_index": 18, "text": "### Post rebuttal # ##", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1ly2z2RFS", "sentence_index": 21, "text": "Thank you for your detailed answers to my questions.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lZP6Jchm", "sentence_index": 0, "text": "This paper proposed Compound Density Networks (CDNs), a neural network architecture that parametrises conditional distributions as infinite mixtures, thus generalising the traditional finite mixture density networks (MDNs).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1lZP6Jchm", "sentence_index": 1, "text": "The authors realise CDNs by treating the weights of each neural network layer probabilistically, and letting them be matrix variate Gaussians (MVGs) with their parameters given as a function of the layer input via a hypernetwork.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1lZP6Jchm", "sentence_index": 2, "text": "CDNs can then be straightforwardly optimised with SGD for a particular task by using the reparametrization trick.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1lZP6Jchm", "sentence_index": 3, "text": "The authors further argue that in case that overfitting is present at CDNs, then an extra KL-divergence term can be employed such that the input dependent MVG distribution is close to a simple prior that is input agnostic.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1lZP6Jchm", "sentence_index": 4, "text": "They then proceed to evaluate the predictive uncertainty that CDNs offer on three tasks: a toy regression problem, out-of-distribution example detection on MNIST/notMNIST and adversarial example detection on MNIST and CIFAR 10.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1lZP6Jchm", "sentence_index": 5, "text": "The objective of this work is to provide a method for better uncertainty estimates from deep learning models.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1lZP6Jchm", "sentence_index": 6, "text": "This is an important research area and relevant for ICLR.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1lZP6Jchm", "sentence_index": 7, "text": "The paper is generally well written with a clear presentation of the proposed model.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "H1lZP6Jchm", "sentence_index": 8, "text": "The generalisation from the finite MDN to the continuous CDN seems straightforward, the model is relatively easy to implement and it is evaluated extensively against several modern baselines.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1lZP6Jchm", "sentence_index": 9, "text": "Nevertheless, I believe that it still has to address some points in order to be better suited for publication:", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "H1lZP6Jchm", "sentence_index": 10, "text": "- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1lZP6Jchm", "sentence_index": 11, "text": "Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1lZP6Jchm", "sentence_index": 12, "text": "- How many samples did you use from p(theta|x) during training?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1lZP6Jchm", "sentence_index": 13, "text": "It seems that with a single sample the method becomes an instance of VIB [1], only considering the weights of the network as latent variables rather than the hidden units.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1lZP6Jchm", "sentence_index": 14, "text": "- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1lZP6Jchm", "sentence_index": 15, "text": "- Have you checked / visualised what type of weight distributions do CDNs capture? It would be interesting to see if e.g. the marginal (across the dataset) weight distribution at each layer has any multimodality as that could hint that the network learns to properly specialise to individual data points.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1lZP6Jchm", "sentence_index": 16, "text": "- The authors mention that in order to avoid overfitting they add an extra (weighted) KL-divergence term to the log-likelihood of the dataset, that encourages the weight distributions for specific points to be close to simple priors.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lZP6Jchm", "sentence_index": 17, "text": "How influential is that extra term to the uncertainty quality that you obtain in the end?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1lZP6Jchm", "sentence_index": 18, "text": "How does this term affect the learned distributions in case of CDNs?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1lZP6Jchm", "sentence_index": 19, "text": "Furthermore, the way that CDNs are constructed seems to be more appropriate at capturing input specific uncertainty (i.e. aleatoric) rather than global uncertainty about the data (i.e. epistemic).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lZP6Jchm", "sentence_index": 20, "text": "I believe that for the specific uncertainty evaluation tasks this paper considers the latter is more appropriate.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lZP6Jchm", "sentence_index": 21, "text": "More discussion on both of these aspects can help in improving this paper.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1lZP6Jchm", "sentence_index": 22, "text": "-", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1lZP6Jchm", "sentence_index": 23, "text": "As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1lZP6Jchm", "sentence_index": 24, "text": "For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1lZP6Jchm", "sentence_index": 25, "text": "For KFLA a hyper parameter \u201ctau\u201d was tuned; this hyperparameter instead corresponds to the precision of the Gaussian prior on the parameters.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lZP6Jchm", "sentence_index": 26, "text": "In this case, KFLA always optimises a \u201ccorrect\u201d Bayesian model for every value of the hyperparameter whereas MNF and noisy K-FAC do not.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1lZP6Jchm", "sentence_index": 27, "text": "Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1lZP6Jchm", "sentence_index": 28, "text": "[1] Deep Variational Information Bottleneck", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1x3aUom2X", "sentence_index": 0, "text": "This paper is well set-up to target the interesting problem of degraded generalisation after adversarial training.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1x3aUom2X", "sentence_index": 1, "text": "The proposal of applying spectral normalisation (SN) is well motivated, and is supported by margin-based bounds.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1x3aUom2X", "sentence_index": 2, "text": "However, the experimental results are weak in justifying the paper's claims.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1x3aUom2X", "sentence_index": 3, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1x3aUom2X", "sentence_index": 4, "text": "* The problem is interesting and well explained", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "H1x3aUom2X", "sentence_index": 5, "text": "* The proposed method is clearly motivated", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1x3aUom2X", "sentence_index": 6, "text": "* The proposal looks theoretically solid", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "H1x3aUom2X", "sentence_index": 7, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1x3aUom2X", "sentence_index": 8, "text": "* It is unclear to me whether the \"efficient method for SN in convolutional nets\" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1x3aUom2X", "sentence_index": 9, "text": "There is no direct comparison of performance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "H1x3aUom2X", "sentence_index": 10, "text": "* Fig. 3 needs more explanation.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1x3aUom2X", "sentence_index": 11, "text": "The horizontal axes are unlabelled, and \"margin normalization\" is confusing when shown together with SN without an explanation.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1x3aUom2X", "sentence_index": 12, "text": "Perhaps it's helpful to briefly introduce it in addition to citing Bartlett et al. 2017.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "H1x3aUom2X", "sentence_index": 13, "text": "* The epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1x3aUom2X", "sentence_index": 14, "text": "* Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1x3aUom2X", "sentence_index": 15, "text": "However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1x3aUom2X", "sentence_index": 16, "text": "Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "H1x3aUom2X", "sentence_index": 17, "text": "It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1x3aUom2X", "sentence_index": 18, "text": "A typo in page 6, last line: wth -> with", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "H1xdKxiDp7", "sentence_index": 0, "text": "* Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1xdKxiDp7", "sentence_index": 1, "text": "- The paper gives theoretical insight into why Batch Normalization is useful in making neural network training more robust and is therefore an important contribution to the literature.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1xdKxiDp7", "sentence_index": 2, "text": "- While the actual arguments are somewhat technical as is expected from such a paper, the motivation and general strategy is very easy to follow and insightful.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1xdKxiDp7", "sentence_index": 3, "text": "* Weaknesses:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1xdKxiDp7", "sentence_index": 4, "text": "- The bounds do not immediately apply in the batch normalization setting as used by neural network practitioners, however there are practical ways to link the two settings as pointed out in section 2.4", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1xdKxiDp7", "sentence_index": 5, "text": "- As the authors point out, the idea of using a batch-normalization like strategy to set an adaptive learning rate has already been explored in the WNGrad paper.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1xdKxiDp7", "sentence_index": 6, "text": "However it is valuable to have a similar analysis closer to the batch normalization setting used by most practitioners.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "H1xdKxiDp7", "sentence_index": 7, "text": "- Currently there is no experimental evaluation of the claims, which would be valuable given that the setting doesn't immediately apply in the normal batch normalization setting.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1xdKxiDp7", "sentence_index": 8, "text": "I would like to see evidence that the main benefit from batch normalization indeed comes from picking a good adaptive learning rate.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1xdKxiDp7", "sentence_index": 9, "text": "Overall I recommend publishing the paper as it is a well-written and insightful discussion of batch normalization. Be aware that I read the paper and wrote this review on short notice, so I didn't have time to go through all the arguments in detail.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "H1xSQUW2tS", "sentence_index": 0, "text": "This paper introduced a latent space model for reinforcement learning in vision-based control tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1xSQUW2tS", "sentence_index": 1, "text": "It first learns a latent dynamics model, in which the transition model and the reward model can be learned on the latent state representations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1xSQUW2tS", "sentence_index": 2, "text": "Using the learned latent state representations, it used an actor-critic model to learn a reactive policy to optimize the agent's behaviors in long-horizon continuous control tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1xSQUW2tS", "sentence_index": 3, "text": "The method is applied to vision-based continuous control in 20 tasks in the Deepmind control suite.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1xSQUW2tS", "sentence_index": 4, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1xSQUW2tS", "sentence_index": 5, "text": "1. The method used a latent dynamics model, which avoids reconstruction of the future images during inference.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "H1xSQUW2tS", "sentence_index": 6, "text": "2. The learned actor-critic model replaced online planning, where actions can be evaluated in a more efficient manner.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "H1xSQUW2tS", "sentence_index": 7, "text": "3. The model achieved better performances in challenging control tasks compared to previous latent space planning methods, such as PlaNet.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "H1xSQUW2tS", "sentence_index": 8, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1xSQUW2tS", "sentence_index": 9, "text": "1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1xSQUW2tS", "sentence_index": 10, "text": "In contrast to PlaNet, the difference is that this work learns an actor-critic model in place of online planning with the cross entropy method.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1xSQUW2tS", "sentence_index": 11, "text": "However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1xSQUW2tS", "sentence_index": 12, "text": "2. Since the actor-critic model is the novel component in this model (propagating gradients through the learned dynamics), I would like to see additional analysis and baseline comparisons of this method to previous actor-critic policy learning methods, such as DDPG and SAC training on the (fixed) latent state representations, and recent work of MVE or STEVE that use the learned dynamics to accelerate policy learning with multi-step updates.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1xSQUW2tS", "sentence_index": 13, "text": "3. The world model is fixed while learning the action and value models, meaning that reinforcement learning of the actor-critic model cannot be used to improve the latent state model.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1xSQUW2tS", "sentence_index": 14, "text": "It'd be interesting to see how optimization of the actions would lead to better state representations by propagating gradients from the actor-critic model to the world model.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "H1xSQUW2tS", "sentence_index": 15, "text": "Typos:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1xSQUW2tS", "sentence_index": 16, "text": "Reward prediction along --> Reward prediction alone", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "H1xSQUW2tS", "sentence_index": 17, "text": "this limitation in latenby?", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "HJe1O6BchX", "sentence_index": 0, "text": "Quality: A simple approach accompanied with a theoretical justification and large number of experimental results.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJe1O6BchX", "sentence_index": 1, "text": "The theoretical justification is spread out in the main body and appendices.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJe1O6BchX", "sentence_index": 2, "text": "The proof given in the appendix is overly short and not detailed enough.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJe1O6BchX", "sentence_index": 3, "text": "The large number of experiment although welcoming needs to be properly discussed and related to the state of the art numbers, including any work that the authors are referring themselves in this submission.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "HJe1O6BchX", "sentence_index": 4, "text": "The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJe1O6BchX", "sentence_index": 5, "text": "Clarity: The simple approach is clearly described.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "HJe1O6BchX", "sentence_index": 6, "text": "However, the theoretical justification and experimental results are not.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJe1O6BchX", "sentence_index": 7, "text": "Originality: The work is moderately original.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "HJe1O6BchX", "sentence_index": 8, "text": "Significance: It is hard to assess given the current submission.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "HJeleGmwnQ", "sentence_index": 0, "text": "This paper studies the characteristics of representations and their roles in neural network expressiveness.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJeleGmwnQ", "sentence_index": 1, "text": "The results  are overall not very impressive.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "HJeleGmwnQ", "sentence_index": 2, "text": "1. There are many characteristics of representations such as scaling, permutation, covariance, correlation, sparsity, dead units, rank.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJeleGmwnQ", "sentence_index": 3, "text": "The papers discusses some (not surprising) theoretical properties relating to scaling, permutation, covariance, correlation, while making less efforts on the more interesting characteristics  sparsity, dead units, rank, mutual information.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJeleGmwnQ", "sentence_index": 4, "text": "Only some heuristic results are obtained for them without rigorous theory.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJeleGmwnQ", "sentence_index": 5, "text": "It would be better if these heuristic arguments can be formed as theorems as well.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "HJeleGmwnQ", "sentence_index": 6, "text": "2. Probably the most interesting experimental finding of this paper is that the mutual information between z and output is constant, while the one between z and input strongly depends on the regularizers.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJeleGmwnQ", "sentence_index": 7, "text": "That is, the dependence between z and y does not vary with regularizers but the one between z and x does.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJeleGmwnQ", "sentence_index": 8, "text": "Is this a coincidence or a general phenomenon? Is there a theoretical explanation?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "HJeleGmwnQ", "sentence_index": 9, "text": "3.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 1, "text": "In this paper, the authors propose a framework for continual learning based on explanations for performed classifications of previously learned tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 2, "text": "In this framework, an average saliency map is computed for all images in the test set of a previous task to identify image regions, which are important for that task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 3, "text": "When learning the next task, this average saliency map is used in an attention mechanism to help learning the new task and to prevent catastrophic forgetting of previously learned tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 4, "text": "Furthermore, the authors propose a new metric for the goodness of a saliency map by taking into account the number of pixels in the map, the average distance between pixels in the map, as well as the prediction probability given only the salient pixels.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 5, "text": "The authors report that their approach achieves the best average classification accuracy for 3 out of 4 benchmark datasets compared to other state-of-the-art approaches.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 6, "text": "Relevance:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 7, "text": "This work is relevant to researchers in the field of continual/life-long learning, since it proposes a framework, which should be possible to integrate into different approaches in this field.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 8, "text": "Significance:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 9, "text": "The proposed work is significant, since it explores a new direction of using learner generated, interpretable explanations of the currently learned task as help for learning new tasks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "HJeXDu9h2X", "sentence_index": 10, "text": "Furthermore, it proposes a new metric for the goodness of saliency maps.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "HJeXDu9h2X", "sentence_index": 11, "text": "Soundness:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 12, "text": "In general, the proposed approach of using the average saliency map as attention mask for learning appears to be reasonable.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HJeXDu9h2X", "sentence_index": 13, "text": "However, the following implicit assumptions/limitations of the approach should be made more clear:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 14, "text": "- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJeXDu9h2X", "sentence_index": 15, "text": "- the locations for important features should be comparatively stable (for example, one would expect the average saliency map to become fairly meaningless if important features, such as the face of a dog, can appear anywhere in the image.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJeXDu9h2X", "sentence_index": 16, "text": "Therefore, an interesting baseline for the evaluation of the ICL approach would be a predefined, fixed attention map consisting of concentric circles with the image center as their center, to show that the proposed approach does more than just deemphasizing the corners of the image)", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJeXDu9h2X", "sentence_index": 17, "text": "Furthermore, the authors appear to imply that increased FSM values for an old task after training on a new task indicate catastrophic forgetting.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 18, "text": "While this is a reasonable assumption, it does not necessarily seem to be the case that a larger, more disconnected saliency map indicates worse classification performance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJeXDu9h2X", "sentence_index": 19, "text": "Comparatively small changes in FSM may not affect the classification performance at all, while larger changes may not necessarily lead to worse classifications either.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJeXDu9h2X", "sentence_index": 20, "text": "For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJeXDu9h2X", "sentence_index": 21, "text": "Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJeXDu9h2X", "sentence_index": 22, "text": "Evaluation:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 23, "text": "The evaluation of the proposed approach on the four used datasets appears to be reasonable and well done.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJeXDu9h2X", "sentence_index": 24, "text": "However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJeXDu9h2X", "sentence_index": 25, "text": "Furthermore, it may be informative to show the saliency maps in Figure 5 not only for cases in which the learner classified the image correctly in both time steps, but also cases in which the learner classified the image correctly the first time and incorrectly the second time.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJeXDu9h2X", "sentence_index": 26, "text": "Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJeXDu9h2X", "sentence_index": 27, "text": "Clarity:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 28, "text": "The paper is clearly written and easy to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "HJeXDu9h2X", "sentence_index": 29, "text": "One minor issue is that the first sentence of the third paragraph in Section 4 is not a full sentence and therefore difficult to understand.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJeXDu9h2X", "sentence_index": 30, "text": "Furthermore, on page 6, it is stated that the surrounding square $\\hat{x}_i$ is 15 x 15 pixels, while the size of the square $x_i$ is 10 x 10.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJeXDu9h2X", "sentence_index": 31, "text": "This appears strange, since it would mean that $x_i$ cannot be in the center of $\\hat{x}_i$.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJg1qgpZTm", "sentence_index": 0, "text": "This paper discusses the optimization of robot structures, combined with their controllers.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJg1qgpZTm", "sentence_index": 1, "text": "The authors propose a scheme based on a graph representation of the robot structure, and a graph-neural-network as controllers.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJg1qgpZTm", "sentence_index": 3, "text": "The experiments show that the proposed scheme is able to produce walking and swimming robots in simulation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJg1qgpZTm", "sentence_index": 5, "text": "The results in this paper are impressive, and the paper seems free of technical errors.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HJg1qgpZTm", "sentence_index": 6, "text": "The main criticism I have is that I found the paper harder to read.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJg1qgpZTm", "sentence_index": 7, "text": "In particular, the exact difference between the proposed method and the ES baseline is not as clear as it could be.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJg1qgpZTm", "sentence_index": 8, "text": "This makes the contribution of this paper in terms of the method hard to judge. Please include further description of the ES cost function and algorithm in the main body of the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJg1qgpZTm", "sentence_index": 10, "text": "The second point is that the proposed approach seems to modify a few things from the ES baseline.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJg1qgpZTm", "sentence_index": 11, "text": "The efficacy of the separate modifications should be tested.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HJg1qgpZTm", "sentence_index": 12, "text": "Therefore I would like to see experiments with the ES cost function, but with inclusion of the pruning step, and experiments with the AF-function but without the pruning step.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HJgjezT1Tm", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJgjezT1Tm", "sentence_index": 1, "text": "The manuscript proposes a modification of generators in GANs which improves performance under two popular metrics for multiple architectures, loss, benchmarks, regularizers, and hyperparameter settings.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJgjezT1Tm", "sentence_index": 2, "text": "Using the conditional batch normalization mechanism, the input noise vector is allowed to modulate layers of the generator.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJgjezT1Tm", "sentence_index": 3, "text": "As this modulation only depends on the noise vector, this technique does not require additional annotations.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJgjezT1Tm", "sentence_index": 4, "text": "In addition to the extensive experimentation on different settings showing performance improvements, the authors also present an ablation study, that shows the impact of the method when applied to different layers.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJgjezT1Tm", "sentence_index": 5, "text": "Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJgjezT1Tm", "sentence_index": 6, "text": "- The idea is simple.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJgjezT1Tm", "sentence_index": 7, "text": "The experimentation is extensive and results are convincing in that they show a clear improvement in performance using the method in a large majority of settings.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJgjezT1Tm", "sentence_index": 8, "text": "- I also like the ablation study showing the impact of the method applied at different layers.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJgjezT1Tm", "sentence_index": 9, "text": "Requests for clarification/additional information:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJgjezT1Tm", "sentence_index": 10, "text": "- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJgjezT1Tm", "sentence_index": 11, "text": "- The ablation study shows that the impact is highest when modulation is applied to the last layer (if only one layer is modulated).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJgjezT1Tm", "sentence_index": 12, "text": "It seems modulation on layer 4 comes in as a close second.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJgjezT1Tm", "sentence_index": 13, "text": "I am curious about why that might be.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJgjezT1Tm", "sentence_index": 14, "text": "- I would like to see some more interpretation on why this method works.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJgjezT1Tm", "sentence_index": 15, "text": "- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJgjezT1Tm", "sentence_index": 16, "text": "Overall, the idea is simple, the explanation is clear and experimentation is extensive. I would like to see more commentary on why this method might have long-term impact (or not).", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "HJgQO5qAnQ", "sentence_index": 0, "text": "The problem that the paper tackles is very important and the approach to tackle it id appealing.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJgQO5qAnQ", "sentence_index": 1, "text": "The idea of regarding the history as a tree looks very promising.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJgQO5qAnQ", "sentence_index": 2, "text": "However, it\u2019s noteworthy that embedding to a vector could be useful too if the embedding espace is representative of the entire history and the timing of the events.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "HJgQO5qAnQ", "sentence_index": 3, "text": "Using neural network if an interesting choice for capturing the influence probability and its timing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "HJgQO5qAnQ", "sentence_index": 4, "text": "The authors need to be clear about their contribution. Is the paper only about replacing the traditional parametric functions of influence and probability with  deep neural networks?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "HJgQO5qAnQ", "sentence_index": 5, "text": "The experimental sections look rather mechanical. I would have put some results on the learned embedding. Or some demonstration of the embedded history or probability to intuitively convey the idea and how it works.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "HJgQO5qAnQ", "sentence_index": 6, "text": "This could have made the paper much stronger.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "HJgQO5qAnQ", "sentence_index": 7, "text": "It was nice that the paper iterated and reviewed the possible inference and learning ways.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HJgQO5qAnQ", "sentence_index": 8, "text": "There is one more way.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJgQO5qAnQ", "sentence_index": 9, "text": "Similar to [1] one can use MCMC with importance sampling on auxiliary variables to infer the hidden diffusion given the observed cascades in continuous-time independent cascade model.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJgQO5qAnQ", "sentence_index": 10, "text": "The paper can benefit from a proofreading.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJgQO5qAnQ", "sentence_index": 11, "text": "There are a few typos throughout the paper such as: Reference is missing in section 2.1 Page 2 paragraph 1: \u201can neural attention mechanism\u201d [1] Back to the Past: Source Identification in Diffusion Networks from Partially Observed Cascades, AISTATS 2015", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJl5cVvoYS", "sentence_index": 0, "text": "= Summary", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJl5cVvoYS", "sentence_index": 1, "text": "Embeddings of mathematical theorems and rewrite rules are presented.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJl5cVvoYS", "sentence_index": 2, "text": "An in-depth analysis of the resulting embeddings is presented, showing that a network can learn to \"apply\" embedded rewrite rules to embedded theorems, yielding results that are similar to the embedding of the rewritten theorem.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJl5cVvoYS", "sentence_index": 3, "text": "[i.e., app'(emb(thm), emb(rule)) is near to emb(app(thm, rule))] This is an interesting property for the application of deep learning to automated theorem proving, though not directly a breakthrough result.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJl5cVvoYS", "sentence_index": 4, "text": "= Strong/Weak Points", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJl5cVvoYS", "sentence_index": 5, "text": "+ Simply a cute result, showing that proof search can remain in embedding space for a limited time horizon without having to switch back into the theorem prover environment.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "HJl5cVvoYS", "sentence_index": 6, "text": "+ Nicely designed experiments testing this (somewhat surprising) property empirically", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJl5cVvoYS", "sentence_index": 7, "text": "- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJl5cVvoYS", "sentence_index": 8, "text": "- Writing sometimes a bit overcomplicated (e.g., Sect. 4.5 could just be a figure of a commuting diagram and two sentences...)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJl5cVvoYS", "sentence_index": 9, "text": "- Architecture choice unclear: Why are $\\sigma$ and $\\omega$ separate networks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HJl5cVvoYS", "sentence_index": 10, "text": "This is discussed on p4, but it's unclear to me how keeping $\\sigma$ separate is benefitial for the analysis, and this is not picked up again explicitly again?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJl5cVvoYS", "sentence_index": 11, "text": "= Recommendation", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJl5cVvoYS", "sentence_index": 12, "text": "Overall, this is a nice, somewhat surprising result.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJl5cVvoYS", "sentence_index": 13, "text": "The writing and experiments could use some improvement, but I believe that the majority of the ICLR audience would enjoy seeing this result (even though it would have no impact on most people's research)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJl5cVvoYS", "sentence_index": 14, "text": "= Detailed Comments", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJl5cVvoYS", "sentence_index": 15, "text": "- page 4, Sect. 4.4: Architecture of $\\alpha$ would be nice (more than a linear layer?)", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HJl5cVvoYS", "sentence_index": 16, "text": "- page 5, paragraph 3: \"we from some\" -> \"we start from some\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJl5cVvoYS", "sentence_index": 17, "text": "- p6par1: \"much cheaper then computing\" -> than", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJl5cVvoYS", "sentence_index": 18, "text": "- p6par6: \"on formulas that with\" -> no that", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJl5cVvoYS", "sentence_index": 19, "text": "- p6par7: \"measure how rate\" -> \"measure the rate\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJl5cVvoYS", "sentence_index": 20, "text": "- p8par1: \"approximate embedding $\\alpha(e(\\gamma'(...)))$ - $e$ is undefined and should probably be $e'$ (this is also the case in the caption of Fig. 5), and $c'$ should probably be included as well.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJl5cVvoYS", "sentence_index": 21, "text": "However, I don't understand the use of $\\alpha$ here.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJl5cVvoYS", "sentence_index": 22, "text": "If Fig. 4 is following Fig. 3 in considering $p(c(\\gamma(T), \\pi(P)))$, then Fig. 4 should plot the performance of, e.g., $p(c(e'(c'(\\gamma'(T_{i-1}), \\pi'(P_{i-1}))), \\pi(P_i)))$ (i.e., $p$ applied to approximate embedding of $T_i$ and (\"true\") embedding of $P_i$).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJl5cVvoYS", "sentence_index": 23, "text": "I believe that's what \"Pred (One Step)\" expresses, but it would maybe be generally helpful to be more precise about the notation in Sect. 6.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 0, "text": "The paper proposes two additional steps to improve the compression of weights in deep neural networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlAUngO2X", "sentence_index": 1, "text": "The first is to quantize the weights after pruning, and the second is to further encode the quantized weights.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlAUngO2X", "sentence_index": 2, "text": "There are several weaknesses in this paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 3, "text": "The first one is clarity.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 4, "text": "The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 5, "text": "The paper can be made more mathematically precise.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 6, "text": "The input and output types of each block in Figure 1. should be clearly stated.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 7, "text": "For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 8, "text": "Since the paper relies so heavily on Lee et al., the authors should make an effort to summarize the approach in a mathematically precise way.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 9, "text": "The figures are almost useless, because the captions contain very little information.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 10, "text": "For example, the authors should at least say that the \"D\" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 11, "text": "Many more can be said in all the figures.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 12, "text": "The second weakness is experimental design.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 13, "text": "There are two conflicting qualities that need to be optimized--performance and compression rate.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJlAUngO2X", "sentence_index": 14, "text": "When optimizing the compression rate, it is important not to look at the test set error.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 15, "text": "If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 16, "text": "The test set is typically small compared to the training set, so it is no surprise that the compression rate can be as high as 90%.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJlAUngO2X", "sentence_index": 17, "text": "Optimizing compression rates should be done on the training set with a separate development set.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 18, "text": "The test set should not used before the best compression scheme is selected.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 19, "text": "Both the results on the development set and on the test set should be reported for the validity of the experiments.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 20, "text": "I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJlAUngO2X", "sentence_index": 21, "text": "Lee et al. seem to make similar mistakes, and it is likely that their experimental design is also flawed.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJlO-N9psQ", "sentence_index": 0, "text": "I read the other reviewers' comments as well as the rebuttal.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 1, "text": "I think that the other reviewers make a number of valid points, especially with regards to the theoretical analysis of the paper.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 2, "text": "Therefore, I do not feel confident in championing this paper.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 3, "text": "PS: I am downgrading my confidence in my evaluation.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 4, "text": "---", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 5, "text": "Paper 93 proposes an empirical evaluation of the memorization properties of convnets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 6, "text": "More specifically, it evaluates three aspects:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 7, "text": "-", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 8, "text": "First it evaluates whether convnets can learn to distinguish images from two different sets by training a binary classifier.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 9, "text": "The conclusion is that, indeed, deep convnets can learn to make such a decision. As could be guessed from intuition, the larger the capacity of the network and the smaller the size of the sets, the higher the accuracy.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 10, "text": "-", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 11, "text": "Second, it evaluates whether we can detect that a group of samples of a dataset was used to train a model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 12, "text": "For this purpose, it is proposed to compute the distribution of maximal activation scores of the output softmax layer and to make use of the Kolmogorov-Smirov distance between the cumulative distributions.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 13, "text": "It is shown experimentally that one can detect (even partial) leakage with such a technique.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 14, "text": "-", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 15, "text": "Third, it evaluates whether we can detect that a single images was used to train a convnet.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 16, "text": "Two simple techniques are proposed.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 17, "text": "The first one considers that a sample is part of the training set if it correctly classified.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 18, "text": "The second one considers that a sample is part of the training set if its loss is below a threshold.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 19, "text": "It is shown experimentally that one can make such a decision with moderate accuracy.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 20, "text": "On the positive side:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 21, "text": "-\tThis is a topic that should be of broad interest to the ICLR community.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "HJlO-N9psQ", "sentence_index": 22, "text": "-\tThe paper is generally well-written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "HJlO-N9psQ", "sentence_index": 23, "text": "-\tThe experiments are reported on large-scale datasets on high-capacity networks which is more realistic than small-scale settings.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJlO-N9psQ", "sentence_index": 24, "text": "On the negative side:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 25, "text": "-", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJlO-N9psQ", "sentence_index": 26, "text": "It is unclear whether the data augmentation techniques is applied only at training time or also at test time.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HJlO-N9psQ", "sentence_index": 27, "text": "In other words: at test time, do you present the original images only or transformed images too?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HJlO-N9psQ", "sentence_index": 28, "text": "-\tIn section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJlO-N9psQ", "sentence_index": 29, "text": "-\tSection 5 is somewhat less clear than the previous sections.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJlO-N9psQ", "sentence_index": 30, "text": "The authors should more clearly define what the private, public and evaluation sets are, right from the beginning.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "none", "pol": "pol_negative"}, {"review_id": "HJlO-N9psQ", "sentence_index": 31, "text": "The purpose of the public set is explained only in section 5.2.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJlO-N9psQ", "sentence_index": 32, "text": "-\tThe experimental results of section 5.2 are somewhat disappointing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJlO-N9psQ", "sentence_index": 33, "text": "Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJlO-N9psQ", "sentence_index": 34, "text": "Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJlO-N9psQ", "sentence_index": 35, "text": "This seems to be too low to be of practical use.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJlO-N9psQ", "sentence_index": 36, "text": "This might be because the Bayes and MAT attacks are too simplistic.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJlO-N9psQ", "sentence_index": 37, "text": "Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJlWUOjV37", "sentence_index": 0, "text": "The paper studies the problem of question generation from sparql queries.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlWUOjV37", "sentence_index": 1, "text": "The motivation is to generate more training data for knowledge base question answering systems to be trained on.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJlWUOjV37", "sentence_index": 2, "text": "However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them: - Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young, EMNLP 2015: https://arxiv.org/abs/1508.01745 - Globally Coherent Text Generation with Neural Checklist Models Chloe Kiddon Luke Zettlemoyer Yejin Choi: https://aclweb.org/anthology/D16-1032", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "HJlWUOjV37", "sentence_index": 7, "text": "Thus the main novelty claim of the paper needs to be hedged appropriately.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "HJlWUOjV37", "sentence_index": 8, "text": "Also, to demonstrate the superiority of the proposed method an appropriate comparison against previous work is needed.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "HJlWUOjV37", "sentence_index": 9, "text": "Some other points:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJlWUOjV37", "sentence_index": 10, "text": "- How is the linearization of the inout done? It  typically matters", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HJlWUOjV37", "sentence_index": 11, "text": "- Given the small size of the dataset, I would propose experimenting with non-neural approaches as well, which are also quite common in NLG.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HJlWUOjV37", "sentence_index": 12, "text": "- On the human evaluation: showing the gold standard reference to the judges introduces bias to the evaluation which is inappropriate as in language generation tasks there are multiple correct answers. See this paper for discussion in the context of machine translation: http://www.aclweb.org/anthology/P16-2013", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJlWUOjV37", "sentence_index": 14, "text": "- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJlWUOjV37", "sentence_index": 15, "text": "Also, this would allow to compare the references against each other (filling in the missing number in Table 4) and this would allow an evaluation of the evaluation itself: while perfect scores are unlikely, the human references should be much better than the systems.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "HJlWUOjV37", "sentence_index": 16, "text": "- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect. E.g. \"what job did jefferson have\" is semntically related to his role in the declaration of independence but rather different. SImilarly, being married to someone is not the same as having a baby with someone.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJlWUOjV37", "sentence_index": 19, "text": "While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJlWUOjV37", "sentence_index": 20, "text": "What were the guidelines used?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "HJly3bK2h7", "sentence_index": 0, "text": "The paper proposed to use a prior distribution to constraint the network embedding.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJly3bK2h7", "sentence_index": 1, "text": "The paper used very restricted Gaussian distributions for the formulation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HJly3bK2h7", "sentence_index": 2, "text": "The proposed approach should compared to other stronger methods such as graph convolution neural network/message passing neural networks/structure2vec.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "HJx6UQbfhX", "sentence_index": 0, "text": "The paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJx6UQbfhX", "sentence_index": 1, "text": "There is nothing strikingly novel in this work, using unlabeled test samples in a transductive way seem to help slightly.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "HJx6UQbfhX", "sentence_index": 2, "text": "However, the paper does cover a setup that I am not aware that was studied before.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "HJx6UQbfhX", "sentence_index": 3, "text": "The paper is written clearly, and the experiments seem solid.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "HJx6UQbfhX", "sentence_index": 4, "text": "Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJx6UQbfhX", "sentence_index": 5, "text": "-- What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "HJx6UQbfhX", "sentence_index": 6, "text": "-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HJx6UQbfhX", "sentence_index": 7, "text": "-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJxHV7JPjB", "sentence_index": 0, "text": "This paper proposes a new method to compare existing classifiers, which does not use fixed test set and adaptively sample it from an arbitrarily large corpus of unlabeled images.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJxHV7JPjB", "sentence_index": 1, "text": "The main idea seems similar to adopting active learning for the test set selection.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJxHV7JPjB", "sentence_index": 2, "text": "One of the main advantage is that it can select a sample set from an arbitrarily large unlabeled images.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HJxHV7JPjB", "sentence_index": 3, "text": "However, to compare different classifiers, the proposed algorithm still needs humans to annotate the selected dataset, which is very expensive compared with traditional methods.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJxHV7JPjB", "sentence_index": 4, "text": "Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HJxHV7JPjB", "sentence_index": 5, "text": "The authors invite five volunteer graduate students to annotate the selected example.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJxHV7JPjB", "sentence_index": 6, "text": "However, for many categories, it\u2019s nor easy for normal people to distinguish.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJxHV7JPjB", "sentence_index": 7, "text": "So the experiments in this paper is also not convincing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJxl4O3AYB", "sentence_index": 0, "text": "The paper proposes to use the triplet loss as a convex relaxation of the ordinal embedding problem.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJxl4O3AYB", "sentence_index": 1, "text": "The loss is solved using feed-forward neural network with the input to the network being the ids of the items encoded in binary codes.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJxl4O3AYB", "sentence_index": 2, "text": "The benefit of using a deep network is to exploit its optimization capability and the parallelism on GPUs.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJxl4O3AYB", "sentence_index": 3, "text": "The experiments presented in the paper include a set of simulation experiments and a real-world task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJxl4O3AYB", "sentence_index": 4, "text": "I am giving a score of 3.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJxl4O3AYB", "sentence_index": 5, "text": "This work is an interesting application of deep learning, but it gives little insight as to why deep networks are able to solve the problem and how to solve ordinal embedding itself.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJxl4O3AYB", "sentence_index": 6, "text": "To elaborate, the problem is known to be NP-hard in the worst case, while the data sets used in the paper seem to have certain nice properties.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJxl4O3AYB", "sentence_index": 7, "text": "It would be interesting to see how deep networks do for the hard cases.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HJxl4O3AYB", "sentence_index": 8, "text": "It would also be interesting to see if additional assumptions, such as the existence of clusters or separation between clusters, make ordinal embedding simpler and thus tractable.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HJxl4O3AYB", "sentence_index": 9, "text": "Another approach is to assume the solution to have low surrogate loss (4), and any convex solver with sufficiently large number of points is able to find such a solution.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJxl4O3AYB", "sentence_index": 10, "text": "Then the question becomes how deep networks solve the particular convex optimization problem.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJxl4O3AYB", "sentence_index": 11, "text": "Thinking along these directions would bring more insight and impact to both the ordinal embedding problem and optimization in deep networks.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HJxl4O3AYB", "sentence_index": 12, "text": "one quick question:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJxl4O3AYB", "sentence_index": 13, "text": "equations (3) and (4)", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HJxl4O3AYB", "sentence_index": 14, "text": "--> isn't this the same as using the hinge loss to bound the zero-one loss?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HJxwToa927", "sentence_index": 0, "text": "The authors propose a notion of conductance to attribute the deep neural network\u2019s prediction to its hidden units.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJxwToa927", "sentence_index": 1, "text": "The conductance is the flow of attribution via the hidden unit(s) in consideration.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJxwToa927", "sentence_index": 2, "text": "The paper proposes using conductance to not only evaluate importance of hidden unit to the prediction for a specific input but also over a set of inputs.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJxwToa927", "sentence_index": 3, "text": "The strongest part of the analysis of conductance is that conductance naturally couples  the path at the base features with that of the hidden layer.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJxwToa927", "sentence_index": 4, "text": "The authors position their work well within the existing approaches in the community and generalizes the efficient use of measuring hidden activation wrt to specific input or set of inputs.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HJxwToa927", "sentence_index": 5, "text": "The analysis makes efficient use of mean value theorem in the context of  parametrization of the loss function.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HJxwToa927", "sentence_index": 6, "text": "Conductance seems to satisfy the completeness of hidden features.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HJxwToa927", "sentence_index": 7, "text": "Further, it also satisfies the layer-wise conservation principle with the outputs completely redistributed  to the inputs.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJxwToa927", "sentence_index": 8, "text": "It would be good to see more analysis on the axioms 1 through to 4 for the sake of completeness in the light of partial axiomatization of conductance.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HJxwToa927", "sentence_index": 9, "text": "The authors provide empirical evaluation of conductance over a variety of tasks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HJxwToa927", "sentence_index": 10, "text": "It would be good to see some more insight in order to relate to interpretability of the importance of neurons, although there has been no claims made on it as its hard to measure importance without interpretability.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HJxwv99GqS", "sentence_index": 0, "text": "Algorithms for Streaming data using a machine learning oracle is analyzed theoretically and empirically.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJxwv99GqS", "sentence_index": 1, "text": "The idea is to build on some recent work (Hsu 19) which used RNNs to predict heavy hitters in streaming data.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HJxwv99GqS", "sentence_index": 2, "text": "The purpose of this paper is to analyze whether such an oracle can help streaming algorithms to obtain improved bounds.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "HJxwv99GqS", "sentence_index": 3, "text": "I am not very familiar with this line of research so my comments will be more general in this case.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HJxwv99GqS", "sentence_index": 4, "text": "The idea of improved bounds for streaming algorithms using machine learning oracle seems to be very appealing to me.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "HJxwv99GqS", "sentence_index": 5, "text": "The authors present novel theoretical results supporting this.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "HJxwv99GqS", "sentence_index": 6, "text": "Experiments are performed on real as well as synthetic datasets using Hsu et al.\u2019s method as an oracle.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJxwv99GqS", "sentence_index": 7, "text": "Two real-world problems are selected, i.e., distinct packets in a network flow, Number of occurrences of each type of search query, and it is shown that using a oracle improves performance as compared to methods that do not use the oracle.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HJxwv99GqS", "sentence_index": 8, "text": "Overall, I think the paper seems to be  an interesting direction which has both formal guarantees and experiments validating them in real-world datasets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "HJxwv99GqS", "sentence_index": 9, "text": "One issue is perhaps, very little in terms of related work.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "HJxwv99GqS", "sentence_index": 10, "text": "I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 1, "text": "This paper is built on the top of DNC model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 2, "text": "Authors observe a list of issues with the DNC model: issues with deallocation scheme, issues with the blurring of forward and backward addressing, and issues in content-based addressing.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 3, "text": "Authors propose changes in the network architecture to solve all these three issues.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 4, "text": "With toy experiments, authors demonstrate the usefulness of the proposed modifications to DNC.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 5, "text": "The improvements are also seen in more realistic bAbI tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 6, "text": "Major Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 7, "text": "The paper is well written and easy to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 8, "text": "The proposed improvements seem to result in very clear improvements.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 9, "text": "The proposed improvements also improve the convergence of the model.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 10, "text": "I do not have any major concerns about the paper.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 11, "text": "I think that contributions of the paper are good enough to accept the paper.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 12, "text": "I also appreciate that the authors have submitted the code to reproduce the results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_positive"}, {"review_id": "Hkg0R50bpQ", "sentence_index": 13, "text": "I am curious to know if authors observe similar convergence gains in bAbI tasks as well. Can you please provide the mean learning curve for bAbI task for DNC vs proposed modifications?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Hkgcm01WqB", "sentence_index": 0, "text": "The paper paper proposes a mutual information maximization objective for discovering unsupervised robotic manipulation skills.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hkgcm01WqB", "sentence_index": 1, "text": "The paper assumes that the state space can be divided into two parts - the state of the robot (\u201ccontext states\u201d) which is controllable via actions and the state of an object (\u201cstates of interest\u201d) which must be manipulated by the robot.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hkgcm01WqB", "sentence_index": 2, "text": "Given these two categories of states, the proposed algorithm maximizes a lower bound on the mutual information between the two categories of states such that a policy is learnt that is able to manipulate the object with the robot meaningfully.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hkgcm01WqB", "sentence_index": 3, "text": "I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an \u201cobject\u201d or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hkgcm01WqB", "sentence_index": 4, "text": "My main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "Hkgcm01WqB", "sentence_index": 5, "text": "The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hkgcm01WqB", "sentence_index": 6, "text": "It doesn\u2019t seem like a surprising discovery that maximizing the mutual information between the robot state and object state will lead to skills that actually make the robot move the object.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hkgcm01WqB", "sentence_index": 7, "text": "Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "Hkgcm01WqB", "sentence_index": 8, "text": "Can the authors elaborate on why this choice should intuitively be better than the proposed method alone?", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "Hkgcm01WqB", "sentence_index": 9, "text": "The paper does not talk about how these skills can be used as primitive actions by a higher level controller (in a hierarchical RL setup), which would help in demonstrating the usefulness of these skills - e.g.: are these skills sequentially composable such that they can solve a complex task?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HkgKMIsTFr", "sentence_index": 0, "text": "The paper proposed a novel image classifier comparison approach that went beyond one fixed testing set for all.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkgKMIsTFr", "sentence_index": 1, "text": "Instead, for a pair of classifiers to be compared, it advocated to sample their \"most disagreed\" test set from a large corpus of unlabeled images.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkgKMIsTFr", "sentence_index": 2, "text": "The level of disagreement was measured by a semantic-aware distance derived from WordNet ontology.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkgKMIsTFr", "sentence_index": 3, "text": "Because of the efficacy of such \"worst-case\" comparison, the needed set size is very small and thus minimizes the human annotation workload.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkgKMIsTFr", "sentence_index": 4, "text": "The proposed MAD competition distinguishes classifiers by finding their respective counterexamples.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkgKMIsTFr", "sentence_index": 5, "text": "It is therefore an \"error spotting\" mechanism, rather than a drop-in replacement of standard test accuracy.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkgKMIsTFr", "sentence_index": 6, "text": "I feel the approach to implicitly assume that the classifiers to be compared are already \"reasonably accurate\"; since if not, both classifiers might be easily falsified by certain trivial examples, making the \"disagreed examples\" not as meaningful.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HkgKMIsTFr", "sentence_index": 7, "text": "If that is true, I would suggest the authors to make this hidden assumption clearer in the paper", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "HkgKMIsTFr", "sentence_index": 8, "text": "The idea shows clear liaison to the \"differential testing\" concept in software engineering besides the cited work of perceptual quality assessment.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HkgKMIsTFr", "sentence_index": 9, "text": "The idea has a cross-disciplinary nature and is fairly interesting to me.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "HkgKMIsTFr", "sentence_index": 10, "text": "I can see the paper to be of interest to a quite broad audience and can motivate many subsequent works.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "HkgKMIsTFr", "sentence_index": 11, "text": "One minor comment: for images in \"Case III\", the authors considered them \"contribute little to performance comparison between the two classifiers\" and therefore did not source labels for them.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "HkgKMIsTFr", "sentence_index": 12, "text": "However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HkgmPcrZpX", "sentence_index": 0, "text": "This paper develops a mean field theory for batch normalization (BN) in fully-connected networks with randomly initialized weights.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkgmPcrZpX", "sentence_index": 1, "text": "There are a number of interesting predictions made in this paper on the basis of this analysis.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkgmPcrZpX", "sentence_index": 2, "text": "The main technical results of the paper are Theorems 5-8 which compute the statistics of the covariance of the activations and the gradients.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkgmPcrZpX", "sentence_index": 3, "text": "Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HkgmPcrZpX", "sentence_index": 4, "text": "1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HkgmPcrZpX", "sentence_index": 5, "text": "2. In a similar vein, there a number of highly technical results in the paper and it would be great if the authors provide an intuitive explanation of their theorems.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "HkgmPcrZpX", "sentence_index": 6, "text": "3. Can the statistics of activations be controlled using activation functions or operations which break the symmetry?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "HkgmPcrZpX", "sentence_index": 7, "text": "For instance, are BSB1 fixed points good for training neural networks?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "HkgmPcrZpX", "sentence_index": 8, "text": "4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HkgmPcrZpX", "sentence_index": 9, "text": "For instance, when the authors observe that the structure of the fixed point is such that activations are of identical norm equally spread apart in terms of angle, this is quite far from practice.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HkgmPcrZpX", "sentence_index": 10, "text": "It would be good to mention this in the introduction or the conclusions.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "HkljIvR3tr", "sentence_index": 0, "text": "This paper shows that Hamiltonian gradient descent (HGD), which is gradient descent on the norm of the squared norm of the vector field, achieves linear convergence for a broader range of problems than bilinear and convex-strongly concave formulations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkljIvR3tr", "sentence_index": 1, "text": "In particular, the authors show the result for convex-concave problems satisfying a \u201csufficiently bilinear\u201d condition that is related to the PL conditions.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkljIvR3tr", "sentence_index": 2, "text": "Finally, the authors argue that consensus optimization (CO) can be viewed as a perturbation of HGD when the parameter choice is big enough.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkljIvR3tr", "sentence_index": 3, "text": "From this viewpoint they derive convergence rates for CO on the broader set of problems.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkljIvR3tr", "sentence_index": 4, "text": "This provides some further theoretical justification of the success of CO on large scale GAN problems.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkljIvR3tr", "sentence_index": 5, "text": "The paper is presented in a clear manner, with the objectives and analysis techniques delineated in the main paper.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkljIvR3tr", "sentence_index": 6, "text": "This was helpful to get a sense of the main points before going through the appendix.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkljIvR3tr", "sentence_index": 7, "text": "The objective of the paper is to extend the problem settings for which there is last iterate min-max convergence rates, which now exist for bilinear, strongly convex-strongly concave, and convex-strongly concave problems.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkljIvR3tr", "sentence_index": 8, "text": "The authors achieve this by analyzing HGD and giving convergence rates for when a \u201csufficiently bilinear condition is satisfied\u201d.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkljIvR3tr", "sentence_index": 9, "text": "The primary idea behind the proof techniques is to show that the objective (Hamiltonian) satisfies the PL condition.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkljIvR3tr", "sentence_index": 10, "text": "I found this to be an interesting approach.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkljIvR3tr", "sentence_index": 11, "text": "As a result, the main question in evaluating this paper is on the significance of the result and the generality of the \u201csufficiently bilinear\u201d condition.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkljIvR3tr", "sentence_index": 12, "text": "I tend to lean toward the result carrying some significance since it does extend the class of problems for which the convergence rates exists.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HkljIvR3tr", "sentence_index": 13, "text": "However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HkljIvR3tr", "sentence_index": 14, "text": "I do acknowledge that the authors did a reasonable job of trying to clear this up in section 3.2 and section G of the appendix.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "HkljIvR3tr", "sentence_index": 15, "text": "It did still leave me wanting more with respect to the practical significance though.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HkljIvR3tr", "sentence_index": 16, "text": "Finally, I found the connection to CO valuable.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HkljIvR3tr", "sentence_index": 17, "text": "In particular, since this paper does not show large-scale experiments, the connection serves to provide some more theoretical evidence for they CO performs well in practice.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HkljIvR3tr", "sentence_index": 18, "text": "Post Author Response: Thanks for the response. I agree with your perspective and think this paper should be accepted.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 1, "text": "This paper proposes a suite of tricks for training large-scale GANs, and obtaining state-of-the-art results for high-resolution images.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 2, "text": "The paper starts from a self-attention GAN baseline (Zhang 2018), and proposes:", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 3, "text": "-\tIncreasing batch size (8x) and model size (2x)", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 4, "text": "-\tSplitting noise z in multiple chunks, and injecting it in multiple layers of the generator", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 5, "text": "- Sampling from truncated normal distribution, where samples with norms that exceed a specific threshold are resampled.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 7, "text": "This seems to be used only at test-time and is used to control variety-fidelity tradeoff.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 8, "text": "The generator is encouraged to be smooth using an orthogonal regularization term.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 9, "text": "In addition, the paper proposes practical recipes for characterizing collapse in GANs.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 10, "text": "In the generator, the exploding of the top 3 singular values of each weight matrix seem to indicate collapse.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 11, "text": "In the discriminator, the sudden increase of the ratio of first/second singular value of weight matrices indicate collapse in GANs.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 12, "text": "Interestingly, the paper suggests that various regularization methods which can improve stability in GAN training, do not necessarily correspond to improvement in performance.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 13, "text": "Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 14, "text": "-\tProposed techniques are intuitive and very well motivated", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "HklmZ1xqhm", "sentence_index": 15, "text": "-\tOne of the big pluses of this work is that authors try to \"quantify\" each proposed technique with training speed and/or performance improvement. This is really a good practice.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HklmZ1xqhm", "sentence_index": 16, "text": "-\tDetailed analysis for detecting collapse and improving stability in large-scale GAN", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HklmZ1xqhm", "sentence_index": 17, "text": "-\tProbably no need to mention that, but results are quite impressive", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HklmZ1xqhm", "sentence_index": 18, "text": "Weaknesses:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 19, "text": "-\tComputational budget required is massive.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HklmZ1xqhm", "sentence_index": 20, "text": "The paper mentions model use from 128-256 TPUs, which severely limits reproducibility of results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HklmZ1xqhm", "sentence_index": 21, "text": "Comments/Questions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 22, "text": "-\tCan you elaborate more on why BatchNorm statistics are computed across all devices as opposed to per-device? Was this crucial for best performance?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "HklmZ1xqhm", "sentence_index": 23, "text": "-\tIt is not clear if provided analysis for large-scale GANs apply for small-medium sized GANs.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "HklmZ1xqhm", "sentence_index": 24, "text": "Providing such analysis would be also helpful for the community.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "HklmZ1xqhm", "sentence_index": 25, "text": "-\tHow do you see the impact of the suggested techniques on tackling harder data-modalities for GANs, e.g. text or sequential data in general?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "HklmZ1xqhm", "sentence_index": 26, "text": "Overall recommendation:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HklmZ1xqhm", "sentence_index": 27, "text": "The paper is well written, ideas are well motivated/justified and results are very compelling.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "HklmZ1xqhm", "sentence_index": 28, "text": "This is a good paper and I higly recommend acceptance.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_positive"}, {"review_id": "HklXxIdqn7", "sentence_index": 0, "text": "The author's present a dual learning framework that, instead of using a single mapping for each mapping task between two respective domains, the authors learn multiple diverse mappings.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklXxIdqn7", "sentence_index": 1, "text": "These diverse mappings are learned before the two main mappings are trained and are kept constant during the training of the two main mappings.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklXxIdqn7", "sentence_index": 2, "text": "Though I am not familiar with BLEU scores and though I didn't grasp some of the details in 3.1, the algorithm yielded consistent improvement over the given baselines.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklXxIdqn7", "sentence_index": 3, "text": "The author's included many different experiments to show this.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklXxIdqn7", "sentence_index": 4, "text": "The idea that multiple mappings will produce better results than a single mapping is reasonable given previous results on ensemble methods.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklXxIdqn7", "sentence_index": 5, "text": "For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HklXxIdqn7", "sentence_index": 6, "text": "Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HklXxIdqn7", "sentence_index": 7, "text": "Minor Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HklXxIdqn7", "sentence_index": 8, "text": "Dual-1 and Dual-5 are introduced without explanation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HklXxIdqn7", "sentence_index": 9, "text": "Perhaps I missed it, but I believe Dan Ciresan's paper \"Multi-Column Deep Neural Networks for Image Classification\" should be cited.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "HklXxIdqn7", "sentence_index": 10, "text": "### After reading author feedback", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HklXxIdqn7", "sentence_index": 12, "text": "Thank you for the feedback.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HklXxIdqn7", "sentence_index": 13, "text": "After reading the updated paper I still believe that 6 is the right score for this paper.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HklXxIdqn7", "sentence_index": 14, "text": "The method produces better results using ensemble learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HklXxIdqn7", "sentence_index": 15, "text": "While the results seem impressive, the method to obtain them is not very novel; nonetheless, I would not have a problem with it being accepted, but I don't think it would be a loss if it were not accepted.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HkxrqOb6nm", "sentence_index": 0, "text": "The authors propose a new on-policy exploration strategy by using a policy with a hierarchy of stochasticity.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkxrqOb6nm", "sentence_index": 1, "text": "The authors use a two-level hierarchical distribution as a policy, where the global variable is used for dropout.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkxrqOb6nm", "sentence_index": 2, "text": "This work is interesting since the authors use dropout for policy learning and exploration.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkxrqOb6nm", "sentence_index": 3, "text": "The authors show that parameter noise exploration is a particular case of the proposed policy.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkxrqOb6nm", "sentence_index": 4, "text": "The main concern is the gap between the problem formulation and the actual optimization problem in Eq 12.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HkxrqOb6nm", "sentence_index": 5, "text": "I am very happy to give a higher rating if the authors address the following points.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HkxrqOb6nm", "sentence_index": 6, "text": "Detailed Comments", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HkxrqOb6nm", "sentence_index": 7, "text": "(1) The authors give the derivation for Eq 10.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HkxrqOb6nm", "sentence_index": 8, "text": "However, it is not obvious that how to move from line 3 to line 4 at Eq 15.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HkxrqOb6nm", "sentence_index": 9, "text": "Minor:  Since the action is denoted by \"a\",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of \"\\alpha\" at Eq 10 and 15.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HkxrqOb6nm", "sentence_index": 10, "text": "(2) Due to the use of the likelihood ratio trick, the authors use the mean policy as an approximation at Eq 12.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HkxrqOb6nm", "sentence_index": 11, "text": "Does such approximation guarantee the policy improvement?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HkxrqOb6nm", "sentence_index": 12, "text": "Any justification?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HkxrqOb6nm", "sentence_index": 13, "text": "(3) Instead of using the mean policy approximation in Eq 12, the authors should consider existing Monte Carlo techniques to reduce the variance of the gradient estimation.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HkxrqOb6nm", "sentence_index": 14, "text": "For example, [1] could be used to reduce the variance of gradient w.r.t. \\phi.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HkxrqOb6nm", "sentence_index": 15, "text": "Note that the gradient is biased if the mean policy approximation is used.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HkxrqOb6nm", "sentence_index": 16, "text": "(4) Are \\theta and \\phi jointly and simultaneously optimized at Eq 12?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HkxrqOb6nm", "sentence_index": 17, "text": "The authors should clarify this point.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HkxrqOb6nm", "sentence_index": 18, "text": "(5) Due to the mean policy approximation, does the mean policy depend on \\phi?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HkxrqOb6nm", "sentence_index": 19, "text": "The authors should clearly explain how to update \\phi when optimizing Eq 12.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HkxrqOb6nm", "sentence_index": 20, "text": "(6) If the authors jointly and simultaneously optimize \\theta and \\phi, why a regularization term about q_{\\phi}(z)  is missing in Eq 12 while a regularization term about \\pi_{\\theta|z} does appear in Eq 12?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HkxrqOb6nm", "sentence_index": 21, "text": "(7) The authors give the derivations about \\theta such as the gradient and the regularization term about \\theta (see, Eq 18-19).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HkxrqOb6nm", "sentence_index": 22, "text": "However, the derivations about \\phi are missing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HkxrqOb6nm", "sentence_index": 23, "text": "For example, how to compute the gradient w.r.t. \\phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \\phi.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HkxrqOb6nm", "sentence_index": 24, "text": "Minor, 1/2 is missing in the last line of Eq 19.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HkxrqOb6nm", "sentence_index": 25, "text": "Reference:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HkxrqOb6nm", "sentence_index": 26, "text": "[1] AUEB, Michalis Titsias RC, and Miguel L\u00e1zaro-Gredilla. \"Local expectation gradients for black box variational inference.\" In Advances in neural information processing systems, pp. 2638-2646. 2015.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HyecCk_TFH", "sentence_index": 0, "text": "[Summary]", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HyecCk_TFH", "sentence_index": 1, "text": "The paper presents a video classification framework that employs 4D convolution to capture longer term temporal structure than the popular 3D convolution schemes.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyecCk_TFH", "sentence_index": 2, "text": "This is achieved by treating the compositional space of local 3D video snippets as an individual dimension where an individual convolution is applied.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyecCk_TFH", "sentence_index": 3, "text": "The 4D convolution is integrated in resnet blocks and implemented via first applying 3D convolution to regular spatio-temporal video volumes and then the compositional space convolution, to leverage existing 3D operators.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyecCk_TFH", "sentence_index": 4, "text": "Empirical evaluation on three benchmarks against other baselines suggested the advantage of the proposed method.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyecCk_TFH", "sentence_index": 5, "text": "[Decision]", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HyecCk_TFH", "sentence_index": 6, "text": "Overall, the paper addresses an important problem in computer vision (video action recognition) with an interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "HyecCk_TFH", "sentence_index": 7, "text": "I found the motivation and solution are reasonable (despite some questions pending more elaboration), and results also look promising, thus give it a weak accept (conditional on the answers though).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HyecCk_TFH", "sentence_index": 8, "text": "[Comments]", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HyecCk_TFH", "sentence_index": 9, "text": "At the conceptual level, the idea of jointly modeling local video events is not novel, and can date back to at least ten years ago in the paper \u201cLearning realistic human actions from movies\u201d, where the temporal pyramid matching was combined with the bag-of-visual-words framework to capture long-term temporal structure.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "HyecCk_TFH", "sentence_index": 10, "text": "The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied. It seems to me that the proposed framework also falls in this category, with a treatment from deep learning. It is definitely worth some discussion on this path.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HyecCk_TFH", "sentence_index": 13, "text": "That said, I would like to see more analysis on the behavior of the proposed method under various interesting cases not tested yet.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HyecCk_TFH", "sentence_index": 14, "text": "Despite the claim that the proposed method can capture long-term video patterns, the static compositional nature seems to work best for activities with well-defined local events and clear temporal boundaries.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HyecCk_TFH", "sentence_index": 15, "text": "These assumptions hold mostly true for the three datasets used in the experiment, and also are suggested by results in table 2(e), where 3 parts are necessary to achieve optimal results.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HyecCk_TFH", "sentence_index": 16, "text": "How does the proposed method perform in more complicated tasks such as - action detection or localization (e.g., in benchmarks JHMDB or UCF101-24). - complex video event modeling (e.g., recognizing activities in extended video of TRECVID).", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HyecCk_TFH", "sentence_index": 19, "text": "Will it still be more favorable than other concerning baselines?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "HyecCk_TFH", "sentence_index": 20, "text": "Besides, on the computation side, it would be complexity, an explicit comparison of complexity makes it easier to evaluate the performance when compared to other state-of-the-art methods.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HyecCk_TFH", "sentence_index": 21, "text": "[Area to improve]", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HyecCk_TFH", "sentence_index": 22, "text": "Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HyecCk_TFH", "sentence_index": 23, "text": "Proof reading", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HyecCk_TFH", "sentence_index": 24, "text": "- The word in the title should be \u201cConvolutional\u201d, right?", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 0, "text": "This paper describes the method for performing self-training where the unlabeled datapoints are iteratively added to the training set only if their predictions by the classifier are confident enough.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyeHzlJ537", "sentence_index": 1, "text": "The contributions of this paper are to add datapoints based on the prediction of the confidence level by a separate selection network and a number of heuristics applied for better selection.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HyeHzlJ537", "sentence_index": 2, "text": "On the experimental side, the contribution is to test the scenario where datapoints from irrelevant classes are included in the unlabeled dataset.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HyeHzlJ537", "sentence_index": 3, "text": "The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HyeHzlJ537", "sentence_index": 4, "text": "Also, the writing can be improved by making the writing more concise and formal (examples of informal: \"spoil the network\", \"model is spoiled\", \"problem of increased classes\", \"many recent researches have been conducted\", \"lots of things to consider for training\", \"supervised learning was trained\" etc.).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HyeHzlJ537", "sentence_index": 5, "text": "The contributions of the method could also be underlined more clearly in the abstract and introduction.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HyeHzlJ537", "sentence_index": 6, "text": "The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HyeHzlJ537", "sentence_index": 7, "text": "The idea of selective sampling for self-training is promising and the investigated questions are interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "HyeHzlJ537", "sentence_index": 8, "text": "As far as I understand, the main contribution of this paper is the use of separate \"selection network\" to estimate the confidence of predictions by \"classification network\".", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 9, "text": "However, as the \"selection network\" uses exactly the same input as \"classification network\", it is hard to imagine how it can learn additional information.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "HyeHzlJ537", "sentence_index": 10, "text": "For example, imagine the case of binary classification.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HyeHzlJ537", "sentence_index": 11, "text": "If the selection network predicts 0 in come cases, it can be used to improve the result of \"classification network\" by flipping the corresponding label.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HyeHzlJ537", "sentence_index": 12, "text": "How can you interpret such a thought experiment?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 13, "text": "One could understand the use of \"selection network\" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of \"selection network\" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HyeHzlJ537", "sentence_index": 14, "text": "Could you elaborate more on why the selection network is needed? How would it compare to a simple strategy of only including the datapoints whose top-1 prediction of \"classification network\" is greater than some threshold?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 15, "text": "Finally, could you show a plot of top-1 prediction of \"classification network\" vs score of \"selection network\" and elaborate on that?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 16, "text": "Then, in sections 3.2 and 3.3 the authors introduce a few additional tricks for self-training: exclude datapoints whose predictions are changing and balance the classes.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HyeHzlJ537", "sentence_index": 17, "text": "Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including \"selection network\" with threshold) is not very principled.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HyeHzlJ537", "sentence_index": 18, "text": "Ablation study shows that the use of the \"selection network\" strategy does not improve the results without these heuristics.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HyeHzlJ537", "sentence_index": 19, "text": "It would be interesting to see how these heuristics would do without \"selection network\", for example, either by doing simple self-training with thresholding on the score of the classifier or by applying only these heuristics in combination with TempEns+SNTG.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 20, "text": "In the current form of evaluation, it is hard to say if there is any benefit of using the \"selection network\" that is the main novelty of the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "HyeHzlJ537", "sentence_index": 21, "text": "It is very valuable that the experimental results include many recently proposed methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "HyeHzlJ537", "sentence_index": 22, "text": "Besides, the settings are described in details that could help for the reproducibility of the results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_positive"}, {"review_id": "HyeHzlJ537", "sentence_index": 23, "text": "However, I have a few concerns about the results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HyeHzlJ537", "sentence_index": 24, "text": "First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "HyeHzlJ537", "sentence_index": 25, "text": "Besides, as the base classifier is different for various baselines, it is hard to compare the methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "HyeHzlJ537", "sentence_index": 26, "text": "Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HyeHzlJ537", "sentence_index": 27, "text": "How did you chose the current values? How sensitive is it? Why various datasets need different settings? How the threshold value can be set in practice?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 28, "text": "Another important parameters is the number of iterations of the algorithm.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 29, "text": "How was it chosen?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 30, "text": "Concerning the experiments of section 4.2, how would the baseline methods of section 4.1 do in this case? Why did you select to study animal vs non-animals sets of classes?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 31, "text": "What would happen if you use random class splits or split animal classes (like in a more realistic scenario)?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 32, "text": "To conclude, while I find the studied problem quite interesting and intuitions behind the method very reasonable, the current methodology is not very principled and the experiment evaluation did not convince me that such an elaborate strategy is needed.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyeHzlJ537", "sentence_index": 33, "text": "Some questions and comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HyeHzlJ537", "sentence_index": 34, "text": "- The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 35, "text": "- In the training procedure of \"selection network\" of Sections 3.1, do you use the same datapoints to train a \"classification network\" and \"selection network\"? If it is the case, how do you insure that the \"classification network\" does not learn to fit the data perfectly and thus all labels s_i are 1?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 36, "text": "- In the last sentences of the first paragraph on p.2 you make a contrast between using softmax and sigmoid functions, however, normally the difference between them is their use in binary or multiclass classification. Is there anything special that you want to show in you case?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 37, "text": "- What do you mean in section 3.3 by \"if one class dominates the dataset, the model tends to overfit\"?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 38, "text": "- I think parameters of training the networks from the beginning of section 4 could be moved to the supplementary materials.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 39, "text": "- Figure 3: wouldn't the plot of accuracy vs amount of data be more suitable here?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 40, "text": "- Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 41, "text": "- Can you explain the sentence \"To prevent data being added suddenly, no data was added until 5 iterations\"?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeHzlJ537", "sentence_index": 42, "text": "- How was it possible to improve the performance in experiment of section 4.2 with 100% of irrelevant classes?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Hyen_JS9nX", "sentence_index": 0, "text": "This paper proposes to extend VAE-GAN from the static image generation setting to the video generation setting.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyen_JS9nX", "sentence_index": 1, "text": "It\u2019s a well-written, simple paper that capitalizes on the trade-off between model realism and diversity, and the fact that VAEs and GANs (at least empirically) tend to lie on different sides of this spectrum.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Hyen_JS9nX", "sentence_index": 2, "text": "The idea to extend the use of VAE-GANs to the video prediction setting is a pretty natural one and not especially novel.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "Hyen_JS9nX", "sentence_index": 3, "text": "However, the effort to implement it successfully is commendable and will, I think, serve as a good reference for future work on video prediction.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "Hyen_JS9nX", "sentence_index": 4, "text": "There are also several interesting design choices that I think are worth of further exposition.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "Hyen_JS9nX", "sentence_index": 5, "text": "Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyen_JS9nX", "sentence_index": 6, "text": "Please provide a response to these questions.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyen_JS9nX", "sentence_index": 7, "text": "If the authors have any ablation studies to back up their design choices, that would also be much appreciated, and will make this a more valuable paper for readers.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Hyen_JS9nX", "sentence_index": 8, "text": "I think Figure 5 is the most interesting figure in the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Hyen_JS9nX", "sentence_index": 9, "text": "I would imagine that playing with the hyperparameters would allow one to traverse the trade-off between realism and diversity.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyen_JS9nX", "sentence_index": 10, "text": "I think having such a curve will help sell the paper as giving the practitioner the freedom to select their own preferred trade-off.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyen_JS9nX", "sentence_index": 11, "text": "I don\u2019t understand the claim that \u201cGANs prioritize matching joint distributions of pixels over per-pixel reconstruction\u201d and its implication that VAEs do not prioritize joint distribution matching.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Hyen_JS9nX", "sentence_index": 12, "text": "VAEs prioritize matching joint distributions of pixels and latent space: min KL(q(z, x) || p(z, x)) and is a variational approximation of the problem min KL(q(x) || p(x)), where q(x) is the data distribution.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyen_JS9nX", "sentence_index": 13, "text": "The explanation provided by the authors is thus not sufficiently precise and I recommend the retraction of this claim.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Hyen_JS9nX", "sentence_index": 14, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hyen_JS9nX", "sentence_index": 15, "text": "+ Well-written", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Hyen_JS9nX", "sentence_index": 16, "text": "+ Natural extension of VAE-GANs to video prediction setting", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Hyen_JS9nX", "sentence_index": 17, "text": "+ Establishes a good baseline for future video prediction work", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "Hyen_JS9nX", "sentence_index": 18, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hyen_JS9nX", "sentence_index": 19, "text": "- Limited novelty", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "Hyen_JS9nX", "sentence_index": 20, "text": "- Limited analysis of model/architecture design choices", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "Hyere82c2m", "sentence_index": 0, "text": "Summary: The paper focuses on comparing the impact of explicit modularity and structure on systematic generalization by studying neural modular networks and \u201cgeneric\u201d models.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 1, "text": "The paper studies one instantiation of this systematic generalization for the setting of binary \u201cyes\u201d or \u201cno\u201d visual question answering task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 2, "text": "They introduce a new dataset called in which model has to answer questions that require spatial reasoning about pairs of randomly scattered letters and digits in the image.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 3, "text": "While the models are evaluated on all possible object pairs, they are trained on a smaller subset.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 4, "text": "They observe that NMNs generalize better than other neural models when an appropriate choice of layout and parametrization is made.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 5, "text": "They also show that current end-to-end approaches for inducing model layout or learning model parametrization fail to generalize better than generic models.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 6, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 7, "text": "- The conclusions of the paper regarding the generalization ability of neural modular networks is timely given the widespread interest in these class of algorithms.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "Hyere82c2m", "sentence_index": 8, "text": "- Additionally, they present interesting observations regarding how sensitive NMNs are to the layout of models.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Hyere82c2m", "sentence_index": 9, "text": "Experimental evidence (albeit on specific type of question) of this behaviour will be helpful for the community and hopefully motivate them to incorporate regularizers or priors that steer the learning towards better layouts.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 10, "text": "- The authors provide a nice summary of all the models analyzed in Section 3.1 and Section 3.2.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Hyere82c2m", "sentence_index": 11, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 12, "text": "- While the results on SQOOP dataset are interesting, it would have been very exciting to see results on other synthetic datasets.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "Hyere82c2m", "sentence_index": 13, "text": "Specifically, there are two datasets which are more complex and uses templated language to generate synthetic datasets similar to this paper:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 14, "text": "- CLEVR environment or a modification of that dataset to reflect the form of systematic the authors are studying in the paper.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 15, "text": "- Abstract Scenes VQA dataset introduced in\u201cYin and Yang: Balancing and Answering Binary Visual Questions\u201d by Zhang and Goyal et al. They provide a balanced dataset in which there are a pairs of scenes for every question, such that the answer to the question is \u201cyes\u201d for one scene, and \u201cno\u201d for the other for the exact same question.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 16, "text": "- Perhaps because the authors study a very specific kind of question, they limit their analysis to only three modules and two structures (tree & chain).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 17, "text": "However, in the most general setting NMN will form a DAG and it would have been interesting to see what form of DAGs generalize better than other.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Hyere82c2m", "sentence_index": 18, "text": "- It is not clear to me how the analysis done in this paper will generalize to other more complex datasets where the network layout NMN might be more complex, the number of modules and type of modules might also be more.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Hyere82c2m", "sentence_index": 19, "text": "Because, the results are only shown on one dataset, it is harder to see how one might extend this work to other form of questions on slightly harder datasets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "Hyere82c2m", "sentence_index": 20, "text": "Other Questions / Remarks:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hyere82c2m", "sentence_index": 21, "text": "- Given that the accuracy drop is very significant moving from NMN-Tree to NMN-Chain, is there an explanation for this drop?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Hyere82c2m", "sentence_index": 22, "text": "- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hyere82c2m", "sentence_index": 23, "text": "- Small typo in the last line of section 4.3 on page 7. It should say: This is in stark contrast with \u201cNMN-Tree\u201d \u2026 ..", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Hyere82c2m", "sentence_index": 25, "text": "- Small typo in the \u201cLayout induction\u201d paragraph, line 6 on Page 7:", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Hyere82c2m", "sentence_index": 26, "text": "\u2026 and for $p_0(tree) = 0.1$ and when we use the Find module", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "HyeuvtzF2X", "sentence_index": 0, "text": "This paper proposed to use dropout to randomly choose only a subset of neural network as a potential way to perform exploration.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyeuvtzF2X", "sentence_index": 1, "text": "The dropout happens at the beginning of each episode, and thus leads to a temporally consistent exploration.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyeuvtzF2X", "sentence_index": 2, "text": "The paper shows that with small amount of Gaussian multiplicative dropout, the algorithm can achieve the state-of-the-art results on benchmark environments.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyeuvtzF2X", "sentence_index": 3, "text": "And it can significantly outperform vanilla PPO for environments with sparse rewards.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyeuvtzF2X", "sentence_index": 4, "text": "The paper is clearly written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "HyeuvtzF2X", "sentence_index": 5, "text": "The introduced technique is interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "HyeuvtzF2X", "sentence_index": 6, "text": "I wonder except for the difference of memory consumption, how different it is compared to parameter space exploration.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeuvtzF2X", "sentence_index": 7, "text": "I feel that it is a straightforward extension/generalization of the parameter space exploration. But the stochastic alignment and policy space constraint seem novel and important.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "HyeuvtzF2X", "sentence_index": 8, "text": "The motivation of this paper is mostly about learning with sparse reward.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HyeuvtzF2X", "sentence_index": 9, "text": "I am curious whether the paper has other good side effects.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyeuvtzF2X", "sentence_index": 10, "text": "For example, will the dropout cause the policy to be more robust?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "HyeuvtzF2X", "sentence_index": 11, "text": "Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HyeuvtzF2X", "sentence_index": 12, "text": "In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HyeuvtzF2X", "sentence_index": 13, "text": "Overall, I like this paper. It is well written.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_positive"}, {"review_id": "HyeuvtzF2X", "sentence_index": 14, "text": "The method seems technically sound and achieves good results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HyeuvtzF2X", "sentence_index": 15, "text": "For this reason, I would recommend accepting this paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_positive"}, {"review_id": "HygHWpHH2m", "sentence_index": 0, "text": "This paper proposed a general method for image restoration based on GAN.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HygHWpHH2m", "sentence_index": 1, "text": "In particular, the latent variable z is optimized based on the MAP framework.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HygHWpHH2m", "sentence_index": 2, "text": "And the results are obtained by G(z).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HygHWpHH2m", "sentence_index": 3, "text": "This method looks reasonable to achieve good results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HygHWpHH2m", "sentence_index": 4, "text": "However, the idea is very related to Yeh et al.\u2019s work which has already published but not mentioned at all.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "HygHWpHH2m", "sentence_index": 5, "text": "Yeh, Raymond A., et al. \"Image Restoration with Deep Generative Models.\" 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "HygHWpHH2m", "sentence_index": 6, "text": "Both the proposed method and Yeh et al.\u2019s method optimize the latent variable z of the generator using MAP, although the loss functions are slightly different.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_neutral"}, {"review_id": "HygHWpHH2m", "sentence_index": 7, "text": "In addition, the applications are very similar: image inpainting, denoising, super-resolution etc. Yeh et al.\u2019s method should be the right baseline instead of the nearest neighbor algorithm.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "HygHWpHH2m", "sentence_index": 8, "text": "In addition, the results seem very weak.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HygHWpHH2m", "sentence_index": 9, "text": "There are tons of algorithms for image inpainting, denoising, and super-resolution, but the proposed method was not compared with them.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "HygHWpHH2m", "sentence_index": 10, "text": "The paper claims that only the nearest neighbor algorithm can handle different degradations.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HygHWpHH2m", "sentence_index": 11, "text": "This is not true.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HygHWpHH2m", "sentence_index": 12, "text": "For example, total variation regularization can do all these tasks.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HygHWpHH2m", "sentence_index": 13, "text": "Some other comments: what are the parameters of the degradation in the applications? For example, in image inpainting, does the proposed method learn the mask as well? So it is blind inpainting?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HygLj-cG9B", "sentence_index": 0, "text": "The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HygLj-cG9B", "sentence_index": 1, "text": "The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to \"directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems.\" Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HygLj-cG9B", "sentence_index": 2, "text": "As such the paper is not convincing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "HygLj-cG9B", "sentence_index": 3, "text": "On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "none", "pol": "none"}, {"review_id": "HygLj-cG9B", "sentence_index": 4, "text": "The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HygLj-cG9B", "sentence_index": 5, "text": "The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "Hyguk1hhKB", "sentence_index": 0, "text": "1. I had hard time to understand latent canonicalization.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hyguk1hhKB", "sentence_index": 1, "text": "Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hyguk1hhKB", "sentence_index": 2, "text": "More explanation of canonicalization is needed.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hyguk1hhKB", "sentence_index": 3, "text": "Perhaps an example in linear algebra is needed.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hyguk1hhKB", "sentence_index": 4, "text": "2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hyguk1hhKB", "sentence_index": 5, "text": "3. How can the proposed method be generalized to non-image data?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hyguk1hhKB", "sentence_index": 6, "text": "The experiments were only done on simple image datasets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hyguk1hhKB", "sentence_index": 7, "text": "I am wondering this method can be applied to other complex datasets whose latent factors are unknown.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hyguk1hhKB", "sentence_index": 8, "text": "4. I do not understand this: \"to fit well the method overfitting rate\" in Section 3.3.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Hyguk1hhKB", "sentence_index": 9, "text": "Minors:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hyguk1hhKB", "sentence_index": 10, "text": "(1) than -> that", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Hyguk1hhKB", "sentence_index": 11, "text": "(2) Eq. (3): is there a superscription \"(j)\" on z_canon in decoder?", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 0, "text": "* Summary", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 1, "text": "The paper proposes an improved method for computing derivatives of the expectation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 2, "text": "Such problems arises with many probabilistic models with noises or latent variables.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 3, "text": "The paper proposes a new gradient estimator of low variance applicable in certain scenarios, in particular it allows training of generative models in which observations and/or latent variables are discrete.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 4, "text": "The submission clearly improves the state-of-the-art, experimentally demonstrates the method on several problems comparing with the alternative techniques.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HygWY_1c2X", "sentence_index": 5, "text": "In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 6, "text": "The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 7, "text": "In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 8, "text": "It also contains lots of additional technical details and experiments in the appendix, which I unfortunately did not review.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "HygWY_1c2X", "sentence_index": 9, "text": "*", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 10, "text": "Clarity", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 11, "text": "In the abstract the paper promises more than it delivers.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 12, "text": "Many problems can be cast as optimizing an expectation-based objective.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 13, "text": "The result does not at all apply to all of them.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 14, "text": "The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 15, "text": "Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional \u201ccontinuous variables\u201d (to which the reparameterization trick is applicable).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 16, "text": "This very much limits the utility of the method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 17, "text": "In particular it is not applicable to learning e.g. sigmoid belief networks [Neal, 92] (with conditional Bernoulli units) and many other problems.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 18, "text": "\u201creparametrizable distributions\u201d", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 19, "text": "A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 20, "text": "Because of the above many discussions about discrete vs. continuous variables are missleading.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 21, "text": "Section 2.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 22, "text": "The notation of the true distribution as \u201cq\u201d the model as p and the approximate posterior of the model as \u201cq\u201d again is inconsistent.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 23, "text": "I find the background on ELBO and GANs unnecessary occluding the clarity at this point.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 24, "text": "For the purpose of introduction, it might be better to give examples of expectation objectives such as:", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "HygWY_1c2X", "sentence_index": 25, "text": "- dropout: q is the distribution of NN outputs given the input image and integrating out latent dropout noises, gamma are parameters of this NN.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "HygWY_1c2X", "sentence_index": 26, "text": "- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 27, "text": "- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 28, "text": "Then to state to which of these cases the results of the paper are applicable, allow for an improvement of the variance and at what additional computational cost (considering the cost of evaluating the discrete derivatives).", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 29, "text": "Section 3.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 30, "text": "Contrary to the discussion, there are examples of non-negative distributions to which the reparameterization trick can be applied, including log-Normal and Gamma distributions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "HygWY_1c2X", "sentence_index": 31, "text": "Method:", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 32, "text": "In the case when Rep trick is applicable, is it identical to GO?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HygWY_1c2X", "sentence_index": 33, "text": "The difference seems to be only in that the mapping tau may be different from Q^-1.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "HygWY_1c2X", "sentence_index": 34, "text": "However, this only affects the method of drawing the samples from a fixed known distribution and should have no more effect on the results than say a choice of a pseudo-random number generator.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 35, "text": "Yet, in Fig.1 some difference is observed between the methods, why is that so?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 36, "text": "Sec 7.1", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 37, "text": "\u201cWe adopt the sticking approach hereafter\u201d. Does it mean it is applied with all experiments with GO?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HygWY_1c2X", "sentence_index": 38, "text": "* Related Work", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 39, "text": "The state of the art allows combining differentiable and non-differentiable pieces of computation:", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 40, "text": "[Schulman, J., Heess, N., Weber, T., Abbeel, P.: Gradient estimation using stochastic computation graphs.]", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 41, "text": "I believe it should be discussed in related work.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HygWY_1c2X", "sentence_index": 42, "text": "Limitations / where the proposed method brings an improvement should be highlighted.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "HygWY_1c2X", "sentence_index": 43, "text": "* Technical Correctness", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HygWY_1c2X", "sentence_index": 44, "text": "Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HygWY_1c2X", "sentence_index": 45, "text": "Equation (7) (integration by parts) holds only with some additional requires on f.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "HygWY_1c2X", "sentence_index": 46, "text": "Theorem 1 does not take account for the above conditions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyl73mFK9H", "sentence_index": 0, "text": "This paper researches the pooling operation, which is an important component in convolutional neural networks (CNN) for image classification.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "Hyl73mFK9H", "sentence_index": 1, "text": "Taking the perspective from signal processing, this paper proposes a pooling operation called frequency pooling (F-pooling).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_neutral"}, {"review_id": "Hyl73mFK9H", "sentence_index": 2, "text": "The key motivation is to make the pooling operation shift-equivalent and anti-aliasing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "Hyl73mFK9H", "sentence_index": 3, "text": "This paper gives an improved definition on shift-equivalent functions and shows that the proposed F-pooling is optimal in the sense of reconstructing the orignal signal.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_neutral"}, {"review_id": "Hyl73mFK9H", "sentence_index": 4, "text": "The F-pooling is then implemented with matrix multiplications and tested with recent convolutional neural networks for image classifiation on CIFAR-100 and a subset of ImageNet dataset.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Hyl73mFK9H", "sentence_index": 5, "text": "It is interesting to take the perspective from signal processing to give pooling operation in CNN a formal treatment.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "Hyl73mFK9H", "sentence_index": 6, "text": "As indicated in the recent literature, enforcing shift-invariance does help to improve the performance of a CNN on classification accuracy and the robustness with respect to image shift.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Hyl73mFK9H", "sentence_index": 7, "text": "At the same time, this work can be further enhanced at the following aspects:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hyl73mFK9H", "sentence_index": 8, "text": "1. This work can make it clearer in principle how anti-aliasing contributes to improving the classification performance and robustness.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Hyl73mFK9H", "sentence_index": 9, "text": "This will help to make this paper more self-contained.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyl73mFK9H", "sentence_index": 10, "text": "2. When showing the optimality of F-pooling in Section 2.3, the criterion is to reconstruct the original signal x. Considering that the ultimate goal is classification, the information to be maximally preserved through each operation through the layers shall be the information that relates to the class label y. In light of this, some justification and explanation shall be provided for using this criterion for optimality.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Hyl73mFK9H", "sentence_index": 12, "text": "3. The experimental study is weak.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyl73mFK9H", "sentence_index": 13, "text": "Experiments could be conducted on more benchmark datasets with more CNN architectures to convincingly show the effectiveness of the proposed F-pooling.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Hyl73mFK9H", "sentence_index": 14, "text": "Also, from the three Tables in the experimental part, the improvement of F-pooling over AA-pooling (developed by the main reference of this work) does not seem to be significant or consistent.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyl73mFK9H", "sentence_index": 15, "text": "For example, in Table 2, the F-pooling only wins at either accuracy (marginally) or consistency, but not both.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyl73mFK9H", "sentence_index": 16, "text": "In Table 3, the F-pooling consistently shows inferior classification performance, although obtaining slightly higher consistency.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyl73mFK9H", "sentence_index": 17, "text": "This makes the advantage of F-pooling over the existing AA-pooling unclear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HylcynXA2X", "sentence_index": 0, "text": "PROS:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HylcynXA2X", "sentence_index": 1, "text": "* Original idea of using separate \"discriminator\" paths for unknown classes", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "HylcynXA2X", "sentence_index": 2, "text": "* Thorough theoretical explanation *", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HylcynXA2X", "sentence_index": 4, "text": "A variety of experiments", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HylcynXA2X", "sentence_index": 5, "text": "* Very well-written, and clear paper", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "HylcynXA2X", "sentence_index": 6, "text": "CONS:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HylcynXA2X", "sentence_index": 7, "text": "* The biggest problem for me was the unconvincing results. MNIST-to-MNIST-M has better baselines  (PixelDA performed better on this task for example), Office is not suitable for domain adaptation experiments anymore unless one wants to be in a few-datasample regime or work with data with noisy labels(the dataset is plagued with label pollution, and there are too few examples per class per domain for NN-based domain adaptation); the results on CELL were not convincing, I don't know the dataset but it seems that baseline NN does better than DA most of the times.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HylcynXA2X", "sentence_index": 9, "text": "* Comparison with other methods did not take into account a variety of hyperparameters.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "HylcynXA2X", "sentence_index": 10, "text": "Although I do understand the problem of evaluation in unsupervised DA, this should have at least been done in the semi-supervised case, and some analysis/discussion should be included for the unsupervised one.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "HylcynXA2X", "sentence_index": 11, "text": "What if the proposed method performs that much better than baselines but they hyperparameters are not set correctly?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Hylj7vZsh7", "sentence_index": 0, "text": "This paper analyzes the AlphaGo Zero algorithm by showing that the optimal policy corresponds to a Nash equilibrium.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hylj7vZsh7", "sentence_index": 1, "text": "The authors then show that the equilibrium corresponds to a KL-minimization.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hylj7vZsh7", "sentence_index": 2, "text": "Finally, the show on a classical scheduling task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hylj7vZsh7", "sentence_index": 3, "text": "On the positive side, the paper is well written and structured.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Hylj7vZsh7", "sentence_index": 4, "text": "The results presented are very interesting, specially showing that stochastic approximation of a KL-divergence minimization.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Hylj7vZsh7", "sentence_index": 5, "text": "The case-study is also interesting, although does not improve current state-of-the-art.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Hylj7vZsh7", "sentence_index": 6, "text": "On the negative side, I think the relevance and novelty of the results should be explained better.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Hylj7vZsh7", "sentence_index": 7, "text": "For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Hylj7vZsh7", "sentence_index": 8, "text": "The MDP formalization is rather straightforward.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hylj7vZsh7", "sentence_index": 9, "text": "Also, MCTS has been used extensively to find Nash equilibria in both perfect and imperfect games, e.g., \"Online monte carlo counterfactual regret minimization for search in imperfect information games\".", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hylj7vZsh7", "sentence_index": 10, "text": "Maybe the authors can elaborate more on the significance/relevance of this contribution.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Hylj7vZsh7", "sentence_index": 11, "text": "Besides, the power of AlphaGo Zero resides in the combination of the MCTS together with the compact representation learning of the value functions.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hylj7vZsh7", "sentence_index": 12, "text": "The presented analysis seems to neglect the error term corresponding to the value function.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hylj7vZsh7", "sentence_index": 13, "text": "There are other minor details:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hylj7vZsh7", "sentence_index": 14, "text": "- Eq(2) . notation: \\forall s is missing", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Hylj7vZsh7", "sentence_index": 17, "text": "- Theorem 2 should be Theorem 1", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Hylj7vZsh7", "sentence_index": 18, "text": "- \"there are constraints per which state can transition\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Hylj7vZsh7", "sentence_index": 19, "text": "- \"P1 is agent\" -> \"P1 is the agent\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Hylj7vZsh7", "sentence_index": 20, "text": "- \"Pinker\" -> \"Pinsker\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Hylj7vZsh7", "sentence_index": 21, "text": "- C_R in Eq(5) is not introduced.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "HyljDMze6m", "sentence_index": 0, "text": "This paper seeks to understand the AlphaGo Zero (AGZ) algorithm and extend the algorithm to regular sequential decision-making problems.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyljDMze6m", "sentence_index": 1, "text": "Specifically, the paper answers three questions regarding AGZ: (i) What is the optimal policy that AGZ is trying to learn?", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyljDMze6m", "sentence_index": 2, "text": "(ii) Why is cross-entropy the right objective?", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyljDMze6m", "sentence_index": 3, "text": "(iii) How does AGZ extend to generic sequential decision-making problems?", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyljDMze6m", "sentence_index": 4, "text": "This paper shows that AGZ\u2019s optimal policy is a Nash equilibrium, the KL divergence bounds distance to optimal reward, and the two-player zero-sum game could be applied to sequential decision making by introducing the concept of robust MDP.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyljDMze6m", "sentence_index": 5, "text": "Overall the paper is well written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "HyljDMze6m", "sentence_index": 6, "text": "However, there are several concerns about this paper.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HyljDMze6m", "sentence_index": 7, "text": "In fact, the key results obtained in this paper is that minimizing the KL-divergence between the parametric policy and the optimal policy (Nash equilibrium) (using SGD) will converge to the optimal policy.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyljDMze6m", "sentence_index": 8, "text": "It is based on a bound (2), which states that when the KL-divergence between a policy and the optimal policy goes to zero then the return for the policy will approach that of the optimal policy.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyljDMze6m", "sentence_index": 9, "text": "This bound is not so surprising because as long as certain regularity condition holds, the policies being close should lead to the returns being close.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "HyljDMze6m", "sentence_index": 10, "text": "Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ\u2019s core learning algorithm.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HyljDMze6m", "sentence_index": 11, "text": "As mentioned earlier, the actual conclusion in Section 4.2 is that minimizing the KL-divergence between the parametric policy and the optimal policy by SGD will converge to the optimal policy, which is straightforward and is not what AlphaGo Zero does.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HyljDMze6m", "sentence_index": 12, "text": "This is because there is an important gap: the MCTS policy is not the same as the optimal policy.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HyljDMze6m", "sentence_index": 13, "text": "The effect of the imperfection in the target policy is not taken into account in the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HyljDMze6m", "sentence_index": 14, "text": "A more interesting question to study is how this gap affect the iterative algorithm, and whether/how the error in the imperfect target policy accumulates/diminishes so that iteratively minimizing KL-divergence with imperfect \\pi* (by MCTS) could still lead to optimal policy (Nash equilibrium).", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "HyljDMze6m", "sentence_index": 15, "text": "Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HyljDMze6m", "sentence_index": 16, "text": "It is more or less like a reformulation of the AGZ setting in the MDP problem. And it is commonly known that two-player zero-sum game is closely related to minimax robust control. Therefore, it cannot be called as \u201cgeneralizing AlphaGo Zero\u201d as stated in the title of the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HylTsRJxqS", "sentence_index": 0, "text": "This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HylTsRJxqS", "sentence_index": 1, "text": "These transformations are called canonicalizations in the paper.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HylTsRJxqS", "sentence_index": 2, "text": "The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HylTsRJxqS", "sentence_index": 3, "text": "The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HylTsRJxqS", "sentence_index": 4, "text": "It also proposes a loss function and sampling scheme for training the model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HylTsRJxqS", "sentence_index": 5, "text": "The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HylTsRJxqS", "sentence_index": 6, "text": "The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HylTsRJxqS", "sentence_index": 7, "text": "I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "HylTsRJxqS", "sentence_index": 8, "text": "A couple things I thought were missing in the paper:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HylTsRJxqS", "sentence_index": 9, "text": "Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "HylTsRJxqS", "sentence_index": 10, "text": "For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HylTsRJxqS", "sentence_index": 11, "text": "Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HylTsRJxqS", "sentence_index": 12, "text": "There might be other constructions that are more efficient and less restrictive.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "HylTsRJxqS", "sentence_index": 13, "text": "I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "HylTsRJxqS", "sentence_index": 14, "text": "Minor comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HylTsRJxqS", "sentence_index": 15, "text": "* \"data  tripets\" on page 2", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HylTsRJxqS", "sentence_index": 16, "text": "* Figure 5 should appear after Figure 4.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HylyhLLF3X", "sentence_index": 0, "text": "In this paper, the authors presented a large experimental study of curiosity-driven reinforcement learning on various tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HylyhLLF3X", "sentence_index": 1, "text": "In the experimental studies, the authors also compared several feature space embedding methods, including identical mapping (pixels), random embedding, variational autoencoders and inverse dynamics features.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HylyhLLF3X", "sentence_index": 2, "text": "The authors found that in many of the tasks, learning based on intrinsic rewards could generate good performance on extrinsic rewards, when the intrinsic rewards and extrinsic rewards are correlated.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HylyhLLF3X", "sentence_index": 3, "text": "The authors also found that random features embedding, somewhat surprisingly, performs well in the tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HylyhLLF3X", "sentence_index": 4, "text": "Overall, the paper is well written with clarity.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "HylyhLLF3X", "sentence_index": 5, "text": "Experimental setup is easy to understand.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "HylyhLLF3X", "sentence_index": 6, "text": "The authors provided code, which could help other researchers reproduce their result.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_positive"}, {"review_id": "HylyhLLF3X", "sentence_index": 7, "text": "Weaknesses:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HylyhLLF3X", "sentence_index": 8, "text": "1) as an experimental study, it would be valuable to compare the performance of curiosity-based learning versus learning based on well-defined extrinsic rewards.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "HylyhLLF3X", "sentence_index": 9, "text": "The author is correct that in many tasks, well-behaved extrinsic rewards are hard to find.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HylyhLLF3X", "sentence_index": 10, "text": "But for problems with well-defined extrinsic rewards, such a comparison could help readers understand the relative performance of curiosity-based learning and/or how much headroom there exists to improve the current methods.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "HylyhLLF3X", "sentence_index": 11, "text": "2) it is surprising that random features perform so well in the experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HylyhLLF3X", "sentence_index": 12, "text": "The authors did provide literature in classification that had similar findings, but it would be beneficial for the authors to explore reasons that random features perform well in reinforcement learning.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "HylYyr91T7", "sentence_index": 0, "text": "# Weaknesses", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HylYyr91T7", "sentence_index": 1, "text": "Applications are a bit unclear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "HylYyr91T7", "sentence_index": 2, "text": "It would be nice to see a better case made for spherical convolutions within the experimental section.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HylYyr91T7", "sentence_index": 3, "text": "The experiments on SHREC17 show all three spherical methods under-performing other approaches.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HylYyr91T7", "sentence_index": 4, "text": "It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "HylYyr91T7", "sentence_index": 5, "text": "Is there a task that this representation significantly outperforms other spherical methods and non-spherical methods?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HylYyr91T7", "sentence_index": 6, "text": "Or a specific useful application where spherical methods in general outperform other approaches?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HylYyr91T7", "sentence_index": 7, "text": "# Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HylYyr91T7", "sentence_index": 8, "text": "The method is well developed and explained.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "HylYyr91T7", "sentence_index": 9, "text": "Ability to implement in a straight-forward manner on GPU.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "HyxflLtAYB", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HyxflLtAYB", "sentence_index": 1, "text": "This paper describes a contextual encoding scheme for reconstruction of 3D pointclouds from 2D images.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyxflLtAYB", "sentence_index": 2, "text": "An encoder outputs the parameters of a hierarchy of reconstruction networks that can be applied in succession to map random samples on a unit sphere to the surface of the reconstructed shape.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyxflLtAYB", "sentence_index": 3, "text": "Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HyxflLtAYB", "sentence_index": 4, "text": "The author's model was quite novel in my opinion.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "HyxflLtAYB", "sentence_index": 5, "text": "Deep 2D->3D is becoming a crowded space and there are many other models that encode image inputs, and many others that perform recursive or composition-based decoding.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HyxflLtAYB", "sentence_index": 6, "text": "However, the particular link here was interesting, and I appreciate the small number of parameters resulting in solid reconstruction performance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "HyxflLtAYB", "sentence_index": 7, "text": "While most related work was covered well, I believe the authors could have a more up-to-date list of recent work that reconstructs triangle-mesh representations from images [A-C] (especially since several of these methods has an architecture that involves encoding and subsequent compositional refinement).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "HyxflLtAYB", "sentence_index": 8, "text": "Some of the reconstructions shown in this paper are quite impressive, and the quantitative results show outperforming 2 recent methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HyxflLtAYB", "sentence_index": 9, "text": "I did appreciate also the novel path-based evaluation of shape accuracy in the Appendix, although it would have been helpful to see more discussion of this in the main paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "HyxflLtAYB", "sentence_index": 10, "text": "Areas for improvement:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HyxflLtAYB", "sentence_index": 11, "text": "I found that the core technical description was quite brief and would have benefited from simply more detail and space.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyxflLtAYB", "sentence_index": 12, "text": "You have argued that your method is sensible to try (cog. sci motivations), and shown that one instance works, but what can we expect in a more mathematical or general sense? Can any sizes of encoder and mapping network fit together? How does the number of mapping layers effect performance? Won't we eventually expect vanishing/exploding gradients with particular activation and can one address this in some way?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "HyxflLtAYB", "sentence_index": 13, "text": "I note that recent papers in this field tend to perform significantly more extensive experimental evaluation, typically selecting a wider range of competitors and using a number of more standardized metrics including IOU, F1 score and CD and typically repeating these at a variety of resolutions or on additional datasets or category splits etc.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "HyxflLtAYB", "sentence_index": 14, "text": "Decision:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HyxflLtAYB", "sentence_index": 15, "text": "Weak reject because the idea is quite interesting, but I believe a more thorough explanation and expanded experimental comparison would be of great help to ensure the community can appreciate this work.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "HyxflLtAYB", "sentence_index": 16, "text": "Additional citations suggested:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "HyxflLtAYB", "sentence_index": 17, "text": "[A] Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. Wang, Zhang, Li, Fu, Liu and Jiang. ECCV 2018.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "HyxflLtAYB", "sentence_index": 18, "text": "[B] MeshCNN: A Network with an Edge. Hanocka, Hertz, Fish, Giryes, Fleishman and Cohen-Or. SIGGRAPH 2019.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "HyxflLtAYB", "sentence_index": 19, "text": "[C] GEOMetrics: Exploiting Structure for Graph-Encoded Objects. Smith, Fujimoto, Romero and Meger. ICML 2019.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 0, "text": "###Summary###", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 1, "text": "This paper tackles the multi-source domain adaptation by aggregate multiple source domains dynamically during the training phase.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 2, "text": "The observation is that in many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target datasets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 3, "text": "Firstly, the paper derives a multiple-source domain adaptation upper-bound from single-to-single domain adaptation generalization bound, based on the theoretical work from Cortes et al (2019).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 4, "text": "The idea is similar to Zhao et al (2019), which introduces a weighted parameter \\alpha to combine the source domains together.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 5, "text": "Secondly, based on the theoretical result, the paper proposes an algorithm to minimize the upper bound of the theoretical result.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 6, "text": "The upper bound can be simplified as the quartic form (Eq. 4) and can be optimized with the Lagrangian form.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 7, "text": "Since no closed-form expression for the optimal v can be derived, the authors propose to use binary search to find it.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 8, "text": "Based on the theoretical results and the algorithm, the paper introduces Domain AggRegation Network (DARN), which contains a base network for feature extraction, h_y to minimize the task loss and h_d to evaluate the discrepancy between each source domain and target domain.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 9, "text": "The loss is aggregation with the parameter \\alpha.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 10, "text": "Finally, the paper conduct experiments on sentimental analysis benchmark, Amazon Review and digit datasets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 11, "text": "The paper selects MDAN, DANN, MDMN as the baselines.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 12, "text": "On the amazon review dataset, the performance of the proposed DARN model is comparable with the MDMN baseline.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 13, "text": "On the digit dataset, the model can outperform the baselines.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 14, "text": "### Novelty ## #", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 16, "text": "The theoretical results in this paper are extended from Cortes et al (2019) and Zhao et al (2018).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 17, "text": "Thus, the theoretical contribution of this paper is limited.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 18, "text": "The algorithm proposed in this paper is interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 19, "text": "However, the motivation of the proposed method is to minimize the upper bound, not the loss itself, i.e. L_T(h, f_T).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 20, "text": "Intuitively, when the upper bound of the loss is minimized, it will be beneficial to minimize the loss itself.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 21, "text": "But it's not guaranteed as the upper bound contains other variables, such as the number of training samples and model complexity.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 22, "text": "If the training samples and model complexity (think about the parameters in the deep models) are significantly large, the upper bound of the loss might be also very large.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 23, "text": "As for the experimental results, the paper only provides results on the sentimental analysis results and digit datasets, which are small benchmarks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 24, "text": "The selected baselines are not sufficient.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 25, "text": "The improvement from the baselines is also limited.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 26, "text": "###Clarity## #", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 28, "text": "Overall, the paper is well organized and logically clear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 29, "text": "The images are well-presented and well-explained by the captions and the text.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 30, "text": "The derivation of the algorithm in Sec 3.2 is logically clear and easy to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 31, "text": "###Pros# ##", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 33, "text": "1) The paper proposes a new theoretical upper-bound based on the prior works, the upper-bound and its derivation are interesting and heuristic to the domain adaptation research community.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 34, "text": "2) The paper is applicable to many practical scenarios since the data from the real-world application is typically collected from multiple sources.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 35, "text": "3) The paper is overall well-organized and well-written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 36, "text": "The claims of the paper are verified by the experimental results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 37, "text": "## #Cons# ##", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 40, "text": "1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 41, "text": "The idea is intuitive when the upper bound is small.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 42, "text": "However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 43, "text": "It's an intuitive idea to weight different source domains in multi-source domain adaptation.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 44, "text": "The paper derives the weight by the Lagrangian form to minimize the upper bound.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 45, "text": "While another trivial trick is to evaluate \\alpha by the domain closeness between each source domain with the target domain.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 46, "text": "2) The experimental results provided in this paper are weak.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 47, "text": "In the abstract and introduction ,  the paper motivates the multi-source domain adaptation (MSDA) problem by arguing that the MSDA has a lot of real applications.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 49, "text": "But the paper only provides empirical results on sentimental analysis and digit recognition.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 50, "text": "Besides, the results on the sentimental analysis are comparable with the compared baselines.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 51, "text": "It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 52, "text": "DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 53, "text": "http://ai.bu.edu/DomainNet/", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 54, "text": "Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 55, "text": "http://hemanthdv.org/OfficeHome-Dataset/", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 56, "text": "3) The novelty of this paper is incremental as the theoretical results are extended from Cortes et al (2019) and Zhao et al (2018).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 57, "text": "Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 58, "text": "To improve the rating, the author should explain the following questions:", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 59, "text": "1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \\alpha by the closeness of the source domain with the target domain?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 60, "text": "2) .", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 62, "text": "In the introduction, the paper motivates the multi-source domain adaptation (MSDA) problem by arguing that the MSDA has a lot of real applications.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxp9P_RuH", "sentence_index": 63, "text": "While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 0, "text": "The paper investigates the possibility of learning a model to predict the training behaviour of deep learning architectures from hyperparameter information and a history of training observations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 1, "text": "The model can then be used by researchers or a reinforcement learning agent to make better hyperparameter choices.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 2, "text": "The paper first adapts the Transformer model to be suitable to this prediction task by introducing a discretization scheme that prevents the transformer decoder's predictions from collapsing to a single curve.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 3, "text": "Next, the problem is formalized as a partially-observable MDP with a discrete action set, and PPO and SimPLe are introduced.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 4, "text": "The proposed model-based method is compared against a human and a model-free baseline training a Wide ResNet on CIFAR-10.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 5, "text": "The model-based method achieves better validation error than the other baselines that use actual data.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 6, "text": "Next, the method is compared against a human and a model-free baseline training Transformer models on the Penn Treebank dataset.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 7, "text": "While the human achieves the best performance at the end of the run, the proposed method appears to learn more quickly than the others and finishes with performance comparable to the model-free baseline.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 8, "text": "Currently I lean towards accepting this paper for publication, despite a few issues.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 9, "text": "It asks an interesting question: can we learn a model of the training dynamics to avoid actually having to do the training?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 10, "text": "This could potentially prevent a lot of unnecessary computation and also lead to better-performing models.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 11, "text": "It then shows some experimental evidence suggesting that this is possible.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 12, "text": "Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 13, "text": "1. In the PTB experiment, it looks like the human only adapts the learning rate and leaves the rest of the hyperparameters alone.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 14, "text": "Why was this policy used as the baseline? It seems extremely basic and unlikely to truly lead to optimal performance.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 15, "text": "2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 16, "text": "3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 17, "text": "4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Hyxs5tB0FS", "sentence_index": 18, "text": "5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "HyxsH-ORFH", "sentence_index": 0, "text": "The paper proposes a way to pre-train quantized representations for speech.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyxsH-ORFH", "sentence_index": 1, "text": "The approach proposed is a two-stage process: 1. train a quantized version of wav2vec [my understanding is that wav2vec is the same thing as CPC for Audio except for using a binary cross-entropy loss instead of InfoNCE softmax-cross entropy loss].", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyxsH-ORFH", "sentence_index": 2, "text": "the authors propose to use gumbel softmax / VQ codebook for the vector quantization.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyxsH-ORFH", "sentence_index": 3, "text": "2. once you have a discrete representation, you could train BERT (as if it were a seq of language tokens).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyxsH-ORFH", "sentence_index": 4, "text": "this makes a lot of sense especially given that CPC / wav2vec recovers phonemes and quantizing the phonemes will recover a language-like version of the raw audio. And running BERT across those tokens will allow you to capture the dependencies at the phoneme level.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyxsH-ORFH", "sentence_index": 5, "text": "After pre-training, the authors use the learned representations for speech recognition.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyxsH-ORFH", "sentence_index": 6, "text": "They compare this to using log-mel filterbanks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyxsH-ORFH", "sentence_index": 7, "text": "The results (WER / LER) is lower for the proposed pipeline compared to using dense wav2vec representation for n-gram and character LM.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyxsH-ORFH", "sentence_index": 8, "text": "It also makes sense that BERT helps for the k-means (vq) setting since the number of codes is large.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "HyxsH-ORFH", "sentence_index": 9, "text": "The authors also cleverly adopt/adapt span-BERT which is more suited to this setting.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "HyxsH-ORFH", "sentence_index": 10, "text": "I think this paper presents a useful contribution as far as improving speech / phoneme recognition using self-supervised learning goes, and also has useful engineering aspects in terms of combining CPC and BERT. I would like to see this paper accepted.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 0, "text": "Overview:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 1, "text": "This paper considers unsupervised (or self-supervised) discrete representation learning of speech using a combination of a recent vector quantized neural network discritization method and future time step prediction.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 2, "text": "Discrete representations are fine-tuned by using these as input to a BERT model; the resulting representations are then used instead of conventional speech features as the input to speech recognition models.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 3, "text": "New state-of-the-art results are achieved on two datasets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 4, "text": "Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 5, "text": "The core strength of this paper is in the results that are achieved on standard speech recognition benchmarks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 6, "text": "The results indicate that, while discritization in itself does not give improvements, coupling this with the BERT-objective results in speech features which are better in downstream speech recognition than standard features. I think the main technical novelty is in combining discritization with future time step prediction (but see the weakness below).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 7, "text": "Weaknesses:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 8, "text": "The main weakness of the paper is that it does not situate itself within existing literature in this area.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 9, "text": "Over the last few years, researchers in the speech community have invested significant effort in learning better speech representations, and this is not discussed.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 10, "text": "See e.g. [1].", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 11, "text": "Even more importantly, very recently there has been a number of papers investigating discrete representations of speech; see the review [2].", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 12, "text": "Some of these papers specifically use VQ-VAEs [3].", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 13, "text": "[4] actually compares VQ-VAE and the Gumbel-Softmax approach.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 14, "text": "These studies should be mentioned.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 15, "text": "This paper is different in that it incorporates future time step prediction.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 16, "text": "But context prediction has also been considered before, also for speech [5, 6, 7].", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 17, "text": "This paper can be situated as a new contribution combining these two strands of research.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 18, "text": "In the longer run it would be extremely beneficial to the community if this approach is applied to the standard benchmarks as set out in [2].", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 19, "text": "As a minor weakness, some parts of the paper is not described in enough detail and the motivation is weak or not exactly clear (see detailed comments below).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 20, "text": "Overall assessment:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 21, "text": "I think the results as well as the new combination of existing approaches in the paper warrants publication. But it should be amended significantly to situate itself within the existing literature. I therefore award a \"weak accept\".", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 22, "text": "Detailed questions and suggestions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 23, "text": "- Section 1: As motivation for this work, it is stated that \"we aim to make well performing NLP algorithms more widely applicable\".", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 24, "text": "As noted above, some NLP-like ideas (such as prediction of future speech segments, stemming from text-based language modelling) have already been considered within the speech community.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 25, "text": "Rather than motivating the work in this way, it might be helpful to focus the contribution as a combination of future time step prediction and discretization (both of which have been considered in previous work, but not in combination).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 26, "text": "- Section 4: Would it be possible to train the vq-wav2vec model jointly with BERT, i.e. as one model? I suspect it would be difficult since, for the masking objective, the discrete units are already required, but maybe there is a scheme where this could work.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 27, "text": "- Section 2.2: Similarly to the above question, would there be a way to incorporate the BERT principles directly into an end-to-end model, e.g. by randomly masking some of the continuous input speech?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 28, "text": "- Section 3.3: What exactly does \"mode collapse\" refer to in this context? Would this be using only one codebook entry, for instance?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 29, "text": "- Section 6: It seems that in all cases to obtain improvements from discritization, BERT is required on top of the vq-wav2vec discrete symbols.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 30, "text": "Is it possible that the output acoustic model is simply better-matched to continuous rather than discrete input (direct vq-wav2vec gives discrete while BERT gives continuous)? Would it make sense to train the wav2vec acoustic model on top of the vqvae codebook entries (e) instead of directly on the symbols?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 31, "text": "Typos, grammar and style:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 32, "text": "- \"gumbel\" -> \"Gumbel\" (throughout; or just be consistent in capitalization)", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 33, "text": "- \"which can be mitigated my workarounds\" -> \"which can be mitigated *by* workarounds\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 35, "text": "- \"work around\" -> \"workaround\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 36, "text": "Missing references:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 37, "text": "1. Versteegh, M., Anguera, X., Jansen, A. & Dupoux, E. (2016). The Zero Resource Speech Challenge 2015: Proposed Approaches and Results. In SLTU-2016 Procedia Computer Science, 81, (pp 67-72).", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 38, "text": "2. https://arxiv.org/abs/1904.11469", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 39, "text": "3. https://arxiv.org/abs/1905.11449", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 40, "text": "4. https://arxiv.org/abs/1904.07556", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 41, "text": "5. https://arxiv.org/abs/1904.03240", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 42, "text": "6. https://arxiv.org/abs/1807.03748 (this paper is cited)", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 43, "text": "7. https://arxiv.org/abs/1803.08976", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1e9Ipmo_r", "sentence_index": 44, "text": "Edit: Based on the feedback from the authors, I changed my rating from a 'weak accept' to an 'accept'.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1eaptK5hm", "sentence_index": 0, "text": "The paper proposes to learn task-level modules progressively to perform the task of VQA.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1eaptK5hm", "sentence_index": 1, "text": "Such task-level modules include object/attribute prediction, image captioning, relationship detection, object counting, and finally VQA model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1eaptK5hm", "sentence_index": 2, "text": "The benefit of using modules for reasoning allows one to visualize the reasoning process more easily to understand the model better.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "r1eaptK5hm", "sentence_index": 3, "text": "The results are mainly shown on VQA 2.0 set, with a good amount of analysis.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "r1eaptK5hm", "sentence_index": 4, "text": "- I think overall this is a good paper, with clear organization, detailed description of the approach, solid analysis of the approach and cool visualization.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "r1eaptK5hm", "sentence_index": 5, "text": "I especially appreciate that analysis is done taking into consideration of extra computation cost of the large model; the extra data used for visual relationship detection.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "r1eaptK5hm", "sentence_index": 6, "text": "I do not have major comments about the paper itself, although I did not check the technical details super carefully.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1eaptK5hm", "sentence_index": 7, "text": "- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1eaptK5hm", "sentence_index": 8, "text": "- I am in general curious to see if it will be beneficial to fine-tune the modules themselves can further improve performance. It maybe hard to do it entirely end-to-end, but maybe it is fine to fine-tune just a few top layers (like what Jiang et al did)?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "r1eaptK5hm", "sentence_index": 9, "text": "- One great benefit of having a module-based model is feed in the *ground truth* output for some of the modules.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1eaptK5hm", "sentence_index": 10, "text": "For example, what benefit we can get if we have perfect object detection? Where can we get if we have perfect relationships?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "r1eaptK5hm", "sentence_index": 11, "text": "This can help us not only better understand the models, but also the dataset (VQA) and the task in general.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1eF1-xjh7", "sentence_index": 0, "text": "This paper presents a new synthetic dataset to evaluate the mathematical reasoning ability of sequence-to-sequence models.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1eF1-xjh7", "sentence_index": 1, "text": "It consists of math problems in various categories such as algebra, arithmetic, calculus, etc.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1eF1-xjh7", "sentence_index": 2, "text": "The dataset is designed carefully so that it is very unlikely there will be any duplicate between train/test split and the difficulty can be controlled.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1eF1-xjh7", "sentence_index": 3, "text": "Several models including LSTM, LSTM + Attention, Transformer are evaluated on the proposed dataset.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1eF1-xjh7", "sentence_index": 4, "text": "The result showed some interesting insights about the evaluated models.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1eF1-xjh7", "sentence_index": 5, "text": "The evaluation of mathematical reasoning ability is an interesting perspective.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1eF1-xjh7", "sentence_index": 6, "text": "However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "r1eF1-xjh7", "sentence_index": 7, "text": "The paper is relatively well-written, although the description of the neural models can be improved.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1eF1-xjh7", "sentence_index": 8, "text": "The generation process of the dataset is well thought out.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "r1eF1-xjh7", "sentence_index": 9, "text": "The insights from the analysis of the failure cases are intriguing, but it also points out that the neural networks models are not really performing mathematical reasoning since the generalization is very limited.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1eF1-xjh7", "sentence_index": 10, "text": "One suggestion is that it might be useful to also release the structured (parsed) form besides the freeform inputs and outputs, for analysis and for evaluating structured neural network models like the graph networks.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "r1eF1-xjh7", "sentence_index": 11, "text": "My main concerns are about the evaluation and comparison of standard neural models.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "r1eF1-xjh7", "sentence_index": 12, "text": "The use of \u201cblank inputs (referred to as \u201cthinking steps\u201d)\u201d in \u201cSimple LSTM\u201d and \u201cAttentional LSTM\" doesn\u2019t seem to be a standard approach.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1eF1-xjh7", "sentence_index": 13, "text": "In the attentional LSTM, the use of \u201cparse LSTM\u201d is also not a standard approach in seq2seq models and doesn\u2019t seem to work well in the experiment (similar result to \u201cSimple LSTM\").", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1eF1-xjh7", "sentence_index": 14, "text": "I think these issues are against the goal of evaluating standard neural models on the benchmark and will raise doubts about the comparison between different models.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "r1eF1-xjh7", "sentence_index": 15, "text": "With some improvements in the evaluation and comparison, I believe this paper will be more complete and much stronger.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1eF1-xjh7", "sentence_index": 16, "text": "typo:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1eF1-xjh7", "sentence_index": 17, "text": "page 3: \u201cfreefrom inputs and outputs\u201d -> \u201cfreeform inputs and outputs\u201d", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1gk4ck25B", "sentence_index": 0, "text": "This paper proposes variational selective autoencoders (VSAE) to learn the joint distribution model of full data (both observed and unobserved modalities) and the mask information from arbitrary partial-observation data.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1gk4ck25B", "sentence_index": 1, "text": "To infer latent variables from partial-observation data, they introduce the selective proposal distribution that switches encoders depending on whether each input modality is observed.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1gk4ck25B", "sentence_index": 2, "text": "This paper is well written, and the method proposed in this paper is nice.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "r1gk4ck25B", "sentence_index": 3, "text": "In particular, the idea of the selective proposal distribution is interesting and provides an effective solution to deal with the problem of missing modality in conventional multimodal learning.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "r1gk4ck25B", "sentence_index": 4, "text": "The experiment is also well structured and shows higher performance than the existing models.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "r1gk4ck25B", "sentence_index": 5, "text": "However, I have some questions and comments, so I\u2019d like you to answer them.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1gk4ck25B", "sentence_index": 6, "text": "Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1gk4ck25B", "sentence_index": 7, "text": "- The authors state that x_j is sampled from the \"prior network\" to calculate E_x_j in Equation 10, but I didn\u2019t understand how this network is set up. Could you explain it in detail?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "r1gk4ck25B", "sentence_index": 8, "text": "- The authors claim that adding p(m|z) to the objective function (i.e., generating m from the decoder) allows the latent variable to have mask information.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "r1gk4ck25B", "sentence_index": 9, "text": "However, I don\u2019t know how effective this is in practice.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1gk4ck25B", "sentence_index": 10, "text": "Specifically, how performance differs compared to when p (m | z) is not used and the decoder p (x | z, m) is conditioned by the mask included in the training set instead of the generated mask?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "r1gk4ck25B", "sentence_index": 11, "text": "- Why did you not do image inpainting in higher-dimensional experiments like Ivanov et al. (2019), i.e., considering each pixel as a different modality? Of course, I know that Ivanov et al. require the full data as input during training, but I\u2019m interested in whether VSAE can perform inpainting properly even if trained given imperfect images.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "r1gzmeTDnm", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1gzmeTDnm", "sentence_index": 1, "text": "The authors present a video prediction model called SAVP that combines a Variational Auto-Encoder (VAE) model with a Generative Adversarial Network (GAN) to produce more realistic and diverse future samples.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1gzmeTDnm", "sentence_index": 2, "text": "Deterministic models and certain loss functions such as Mean Squared Error (MSE) will produce blurry results when making uncertain predictions.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1gzmeTDnm", "sentence_index": 4, "text": "GAN predictions on the other hand usually are more visually appealing but often lack diversity, producing just a few modes.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1gzmeTDnm", "sentence_index": 5, "text": "The authors propose to combine a VAE model with a GAN objective to combine their strengths: good quality samples (GAN) that cover multiple possible futures (VAE).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1gzmeTDnm", "sentence_index": 6, "text": "Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1gzmeTDnm", "sentence_index": 7, "text": "[+] GANs are notoriously unstable to train, especially for video.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1gzmeTDnm", "sentence_index": 8, "text": "The authors formulate a VAE-GAN model and successfully implement it.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "r1gzmeTDnm", "sentence_index": 9, "text": "Weaknesses:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1gzmeTDnm", "sentence_index": 10, "text": "[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018).", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "r1gzmeTDnm", "sentence_index": 11, "text": "[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts. For example, the experiment of Figure 5 does not show SAVP being significantly more diverse than GANs for KTH (as compared to VAEs).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1gzmeTDnm", "sentence_index": 13, "text": "Furthermore, Figure 6 and Figure 7 in general show SAVP performing worse than SVG (Denton & Fergus 2018), a VAE model with a significantly less complex generator, including for the metric (VGG cosine similarity) that the authors introduce arguing that PSNR and SSIM do not necessarily indicate prediction quality.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1gzmeTDnm", "sentence_index": 14, "text": "While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better. Since a direct application of video prediction is model-based planning, it seems that plausibility might be as important as sample quality.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1gzmeTDnm", "sentence_index": 16, "text": "This work proposes to combine VAEs and GANs in a single model to get the benefits of both models.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1gzmeTDnm", "sentence_index": 17, "text": "However, the experiments conducted generally show that SAVP offers only a trade-off between the visual quality of GANs and the coverage of VAEs, and does not show a clear advantage over current VAE models (Denton & Fergus, 2018) that with simpler architectures obtain similar results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "r1gzmeTDnm", "sentence_index": 18, "text": "While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "r1gzmeTDnm", "sentence_index": 19, "text": "In order to better assess this model and compare it to its individual parts and other VAE models, could the authors:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1gzmeTDnm", "sentence_index": 20, "text": "1) Compare SAVP to the SVG-LP/FP model on a controlled synthetic dataset such as Stochastic Moving MNIST (Denton & Fergus, 2018)?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "r1gzmeTDnm", "sentence_index": 21, "text": "2) Comment on the plausibility of the samples generated by SAVP? Do some samples show imagined objects \u2013 implausible interactions for the robotic arm dataset? If so, what would be the advantage over blurry but plausible generations of a VAE?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "r1l1CEFwKr", "sentence_index": 0, "text": "The paper attempts to clarify the debate on large-batch neural network training, particularly on the relationship between learning rate, batch sizes and test performance.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1l1CEFwKr", "sentence_index": 1, "text": "The authors claim two contributions towards understanding how the hyper-parameters of SGD affect final training and test performance: (1) SGD exhibits two regimes with different behaviours and (2) large-batch training leads to degradation of test performance even with same step budgets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1l1CEFwKr", "sentence_index": 2, "text": "Overall, the authors did a comprehensive study on large-batch training with the support of extensive experiments.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1l1CEFwKr", "sentence_index": 3, "text": "But I'm concerned with the novelty and contributions of this paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1l1CEFwKr", "sentence_index": 4, "text": "I tend to reject this paper because (1) the first contribution of the paper is not new as it has already been recognized by a few paper that SGD exhibits two different regimes; (2) this paper makes the debate of large-batch training even muddier.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1l1CEFwKr", "sentence_index": 5, "text": "Main argument:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1l1CEFwKr", "sentence_index": 6, "text": "The paper does not do a great job in clarify the debate.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "r1l1CEFwKr", "sentence_index": 7, "text": "Particularly, the authors mixed their observations up with the results of published works, making it hard to identify the contributions of this paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1l1CEFwKr", "sentence_index": 8, "text": "For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1l1CEFwKr", "sentence_index": 9, "text": "Also, I find the experiments done in section 3 and 4 are similar to previous works and even the conclusions are similar.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1l1CEFwKr", "sentence_index": 10, "text": "The only new observation I'm aware of in these two sections is that the training loss and test accuracy are independent of batch size in the noise dominated regime.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "r1l1CEFwKr", "sentence_index": 11, "text": "Back to introduction section, the goal of this paper (as claimed in the beginning of second paragraph) is to clarify the debate.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1l1CEFwKr", "sentence_index": 12, "text": "But does this paper really achieves this goal?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "r1l1CEFwKr", "sentence_index": 13, "text": "In terms of learning rate scaling, this paper gets similar conclusions as Shallue et al. (2018).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1l1CEFwKr", "sentence_index": 14, "text": "In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1l1CEFwKr", "sentence_index": 15, "text": "I think the authors should instead focus on the discussion of generalization performance and the observation that training loss and test accuracy are independent of batch size in noise dominated regime.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1l1CEFwKr", "sentence_index": 16, "text": "To my knowledge, this part is novel and interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "r1l1CEFwKr", "sentence_index": 17, "text": "In summary, I'm inclined to reject this paper given the current version.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "r1l1CEFwKr", "sentence_index": 18, "text": "However, I think the paper is still worth reading if the authors can reorganize the paper and I might increase my score if my concerns get resolved.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 0, "text": "==============Final Evaluation================", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 1, "text": "I have gone through the other reviews as well as the author response.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 2, "text": "Firstly, I would like to thank the authors for providing detailed responses to my questions.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 3, "text": "In general, I agree with R2 that the paper generally has some potentially interesting ideas and results but the manner in which the current draft is organized and presented makes it hard to grasp them and there is a lack of coherent message about what the paper is about.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "r1l7E6X22m", "sentence_index": 4, "text": "Moreover, from my understanding the analysis in David McKay\u2019s book (Chapter 41) concerns a single neuron (and the number of parameters for a single neuron) .", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 6, "text": "As pointed out by R2, with depth there are a lot more number of possible ways in which one could carve out decision boundaries to separate data points, thus, it is not clear that the loose linear upper bound holds Specifically, as one might expect with depth it could be possible that linear capacity increase is a lower bound (I am not suggesting that it is, but that possibility should be considered and explained in the paper).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 7, "text": "Similarly, it would be good to formally connect the capacity to the rate of memorization before making a statement about them being related (as suggested in the initial review).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 8, "text": "In general, I feel this section could use some tighter formalism and justifications.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 9, "text": "I also remain unconvinced by the response to my issue with the claim \u201cOur experiments show that our networks can remember a large number of images and distinguish them from unseen images\u201d, where the negative images are also seen by the memorization model, so they are not unseen.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 10, "text": "The authors address this by saying 3M of the 15 M negatives have been seen.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 11, "text": "That does not seem like a small enough percentage to claim that these are \u201cunseen\u201d images.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 12, "text": "In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 13, "text": "==================", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 14, "text": "Summary", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 15, "text": "The paper trains classification models to classify a labeling of a subset of images (assigned with label 1) from the rest of the images (assigned with a label 0).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 16, "text": "Firstly, the paper shows that deep learning models are able to learn such classifiers and get low training loss.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 17, "text": "It then proposes to use this model to ``attack\u2019\u2019 task-specific models to perform membership inference, i.e. figuring out if an image provided in a set was used in training or not.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 18, "text": "Strengths", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 19, "text": "+ The paper thoroughly covers related work and provides context.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "r1l7E6X22m", "sentence_index": 20, "text": "+ Results on confidence as a signature of a dataset are interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "r1l7E6X22m", "sentence_index": 21, "text": "Weaknesses", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 22, "text": "[Motivation] 1.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 24, "text": "In general, recent work has found that the raw number of parameters has little to do with the size of the model class or the capacity of a model for deep models, and thus work like [A] has been trying to come up with better complexity measures for models to explain generalization. Thus, without sufficient justification the assertion in the paper that the capacity of the network is well approximated by the number of parameters does not seem correct.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 26, "text": "Also, the claim in Fig. 1 that the transition from \u2018\u2019high capacity\u2019\u2019 to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand, and should be toned down. (*)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 27, "text": "[Capacity]", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 28, "text": "2. Sec. 3.3, Fig. 3: The capacity (in terms of parameters)of both Resnet-18 and VGG-16 is higher than the capcity for YFCC100M dataset for n=10K images (comes to 161K bits), while the capacity of Resnet-18, with 14.7 million parameters (assuming float32 encoding) has 14.7 * 32 bits = 470.4 million bits, thus capacity alone cannot explain why VGG converges faster than Resnet-18, since both networks exceed the capacity, and capacity does not seem to have an established formal connection to rate of memorization. This is something which would need to be explained/ substantiated separately. (*)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 31, "text": "3. Scenario discussed in Sec. 4 seems somewhat impractical. Given a set of m images, it is not clear that a classifier that is trained to detect between train and validation is sufficient, as one might also need to figure out if it is neither train nor val, which is a very practical scenario.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 33, "text": "4. Fig. 3 (right): It is not clear why the fact that the classifier is able to predict which dataset the image \u2018m\u2019 corresponds to is useful or practical, as this seems to be a property of the set \u2018m\u2019 rather than the property of the trained classification model (f_\\theta) .", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 36, "text": "Please clarify.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "r1l7E6X22m", "sentence_index": 37, "text": "On the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the Fig. is very confusing. (*) 6.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 40, "text": "It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model. While the baseline approaches seem to make use of the model confidence, I cannot see how the proposed approach (which uses a classifier) makes use of the original model.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 42, "text": "It is also not clear why Table. 3 does not report the Bayes baseline results. Also, does this section use the classifier for predicting the dataset, or is the approach reported in the section, the MAT approach?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 44, "text": "7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images\u2019\u2019 -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen\u2019\u2019 images which it labels as the negative class, thus the negative class is also seen by the memorization model. (*)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 46, "text": "Minor Points", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 47, "text": "1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization\u2019\u2019.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 48, "text": "In addition, the paper would also need to show that such a model does not generalize to a validation set of images. This is probably obvious given the results from Zhang et.al. but should be included as a sanity check.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 50, "text": "2. Figure 3: it is confusing to call the cumulative distribution of the maximum classification score as the CDF of the model (y-axis fig. 3 left) as CDF means something else generally in such contexts, as the CDF of a predictor.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1l7E6X22m", "sentence_index": 51, "text": "References:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 52, "text": "[A]: Blier, L\u00e9onard, and Yann Ollivier. 2018. ``The Description Length of Deep Learning Models.\u2019\u2019 arXiv [cs.LG]. arXiv.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 54, "text": "http://arxiv.org/abs/1802.07044 .", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 56, "text": "Preliminary Evaluation", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 57, "text": "There are numerous issues with the writing and clarity of the paper, while it seems like some of the observations around the confidence of classifiers are interesting, in general the connection between those set of results and the ``memorization\u2019\u2019 capabilities of the classifier trained to remember train vs val images is not clear in general.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1l7E6X22m", "sentence_index": 58, "text": "Important points for the rebuttal are marked with (*).", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1labrqphm", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1labrqphm", "sentence_index": 1, "text": "This paper introduces Amortized Proximal Optimization (APO) that optimizes a proximal objective at each optimization step.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1labrqphm", "sentence_index": 2, "text": "The optimization hyperparameters are optimized to best minimize the proximal objective.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1labrqphm", "sentence_index": 3, "text": "The objective is represented using a regularization style parameter lambda and a distance metric D that, depending on its definition, reduces the optimization procedure to Gauss-Newton, General Gauss Newton or Natural Gradient Descent.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1labrqphm", "sentence_index": 4, "text": "There are two key convergence results which are dependent on the meta-objective being optimized directly which, while not practical, gives some insight into the inner workings of the algorithm.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1labrqphm", "sentence_index": 5, "text": "The first result indicates strong convergence when using the Euclidean distance as the distance measure D.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1labrqphm", "sentence_index": 6, "text": "The second result shows strong convergence when D is set as the Bregman divergence.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1labrqphm", "sentence_index": 7, "text": "The algorithm optimizes the base optimizer on a number of domains and shows state-of-the-art results over a grid search of the hyperparameters on the same optimizer.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1labrqphm", "sentence_index": 8, "text": "Clarity and Quality: The paper is well written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "r1labrqphm", "sentence_index": 9, "text": "Originality: It appears to be a novel application of meta-learning.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "r1labrqphm", "sentence_index": 10, "text": "I wonder why the authors didn\u2019t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1labrqphm", "sentence_index": 11, "text": "Also how does this compare to adaptive hyperparameter training techniques such as population based training?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "r1labrqphm", "sentence_index": 12, "text": "Significance:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1labrqphm", "sentence_index": 13, "text": "Overall it appears to be a novel and interesting contribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "r1labrqphm", "sentence_index": 14, "text": "I am concerned though why the authors didn\u2019t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1labrqphm", "sentence_index": 15, "text": "Also, your convergence results appear to rely on strong convexity of the loss.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1labrqphm", "sentence_index": 16, "text": "How is this a reasonable assumption?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1labrqphm", "sentence_index": 17, "text": "These are my major concerns.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1labrqphm", "sentence_index": 18, "text": "Question: In your experiments, you set the learning rate to be really low. What happens if you set it to be arbitrarily high? Can you algorithm recover good learning rates?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "r1lORJDq3m", "sentence_index": 0, "text": "This paper investigates the effect of the batch normalization in DNN learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1lORJDq3m", "sentence_index": 1, "text": "The mean field theory in statistical mechanics was employed to analyze the progress of variance matrices between layers.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1lORJDq3m", "sentence_index": 3, "text": "As the results, the batch normalization itself is found to be the cause of gradient explosion.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1lORJDq3m", "sentence_index": 4, "text": "Moreover, the authors pointed out that near-linear activation function can improve such gradient explosion.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1lORJDq3m", "sentence_index": 5, "text": "Some numerical studies were reported to confirm theoretical findings.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1lORJDq3m", "sentence_index": 6, "text": "The detailed analysis of the training of DNN with the batch normalization is quite interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "r1lORJDq3m", "sentence_index": 7, "text": "There are some minor comments below.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1lORJDq3m", "sentence_index": 8, "text": "- in page 3, 2line above eq(2): what is delta in the variance of the multivariate normal distribution?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1lORJDq3m", "sentence_index": 9, "text": "- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1lORJDq3m", "sentence_index": 10, "text": "- The randomized weight is not very practical. Though it may be the standard approach of mean field, some comments would be helpful to the readers.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1lsMBCdFB", "sentence_index": 0, "text": "This work is clearly the work of a large team.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1lsMBCdFB", "sentence_index": 1, "text": "the paper clearly defines what is being done.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "r1lsMBCdFB", "sentence_index": 2, "text": "I have spent a lot of effort with MCTS. I can not find the corresponding allowance for stochastic jumps in the latent space long horizon learning.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1lsMBCdFB", "sentence_index": 3, "text": "You have the phrase \"allowing to imagine thousands of trajectories in parallel\". I would like some elaboration on this. I think you have ideas of what is happening in the latent space that I am not following.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1lsMBCdFB", "sentence_index": 4, "text": "You are heavy on the machinery and math. I find the learning in the latent space the important part and there are things like how much simulation is done in the latent learning not clearly spelled out. How does the effort compare to the 1E9 steps of the base line your refer to?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "r1lsMBCdFB", "sentence_index": 5, "text": "Your team is highly competent your style is distinct.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "r1lsMBCdFB", "sentence_index": 6, "text": "Now may be the time to move you to understanding what structures get learned in latent space, are the in fact compact, diverse?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "r1lsMBCdFB", "sentence_index": 7, "text": "Perhaps there is room for memory/memories in the latent space?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "r1lsMBCdFB", "sentence_index": 8, "text": "Massive effort, nice results. Now for learning on our part (the humans).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "r1lTmBYHtB", "sentence_index": 0, "text": "This paper proposed \"F pooling\" for Frequency Pooling, which is a pooling operation satisfying shift equivalence and anti-aliasing properties.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1lTmBYHtB", "sentence_index": 1, "text": "The method is very simple: first, transform the input 1D/2D signal into the spectrum domain based on discrete Fourier transform (DFT), then cut the high-frequencies, then transform back to the time domain using the inverse DFT.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_neutral"}, {"review_id": "r1lTmBYHtB", "sentence_index": 2, "text": "The method can be implemented using FFT and auto differentiation frameworks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "r1lTmBYHtB", "sentence_index": 3, "text": "The method is tested on Resnet/Desnet on CIFAR-100 and subsets of ImageNet, showing better performance than the original models.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "r1lTmBYHtB", "sentence_index": 4, "text": "The reviewer votes for rejection as the method has limited novelty.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1lTmBYHtB", "sentence_index": 5, "text": "Spectrum pooling has been used in the community of computer vision and machine learning.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1lTmBYHtB", "sentence_index": 6, "text": "Taking a random example (there are others by simple searching), in the ECCV paper \"DFT-based Transformation Invariant Pooling Layer for Visual Classification, Ryu et al., 2018\" The DFT magnitude pooling is almost the same as the authors' propositions, where the \"Fourier coefficients are cropped by cutting off high-frequency components\".", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1lTmBYHtB", "sentence_index": 7, "text": "The reviewer encourages the authors to make further new developments and have a more comprehensive literature review. But in the current form, the paper has less value to be published in ICLR.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "r1lvRFUliS", "sentence_index": 0, "text": "This paper proposed an aggregation algorithm (DARN) for the multi-source domain adaptation problem which is highly useful in real-world applications.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1lvRFUliS", "sentence_index": 1, "text": "The proposed method is based on the theoretical extension of the single-source domain discrepancy measure proposed by Mansour et al. 2009 to the multi-source setting.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1lvRFUliS", "sentence_index": 2, "text": "This paper also showed the effectiveness of the proposed method on some real-world datasets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1lvRFUliS", "sentence_index": 3, "text": "Strengths", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1lvRFUliS", "sentence_index": 4, "text": "The paper introduces new technical insights to understand their bound, e.g. effective sample size.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "r1lvRFUliS", "sentence_index": 5, "text": "The paper proposed the way to estimate coefficient, optimal \\alpha, with theoretical justification, and I think this is the biggest contribution of this paper and is interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "r1lvRFUliS", "sentence_index": 6, "text": "The proposed method is also able to be used in the regression task since it is based on the disc which can be estimated in the regression task.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "r1lvRFUliS", "sentence_index": 7, "text": "Weakness", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1lvRFUliS", "sentence_index": 8, "text": "The main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1lvRFUliS", "sentence_index": 9, "text": "A naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1lvRFUliS", "sentence_index": 10, "text": "Experimental results itself are fine but not complete.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1lvRFUliS", "sentence_index": 11, "text": "- Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1lvRFUliS", "sentence_index": 12, "text": "- It would be also better to show the coefficient of existing methods that have no theoretical justification.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1lvRFUliS", "sentence_index": 13, "text": "- It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1lvRFUliS", "sentence_index": 14, "text": "Overall, I like the approach of the proposed method, especially tuning coefficient during training procedure although novelty in the theoretical analysis is somewhat limited.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1lvRFUliS", "sentence_index": 15, "text": "So this work has to be supported with more detailed experimental results to express the potential of this approach fully.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1lvRFUliS", "sentence_index": 16, "text": "For this reason, I think it is okay but not good enough at this time.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "r1lvRFUliS", "sentence_index": 17, "text": "*", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1lvRFUliS", "sentence_index": 18, "text": "***After the authors' response****", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1lvRFUliS", "sentence_index": 19, "text": "Increase rating.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1lvRFUliS", "sentence_index": 20, "text": "[1] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1lvRFUliS", "sentence_index": 22, "text": "[2] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In NeurIPS, 2007.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1lvRFUliS", "sentence_index": 24, "text": "[3] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine Learning, 2010.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1lvRFUliS", "sentence_index": 26, "text": "[4] Seiichi Kuroki, Nontawat Charoenphakdee, Han Bao, Junya Honda, Issei Sato, and Masashi Sugiyama.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1lvRFUliS", "sentence_index": 27, "text": "Unsupervised domain adaptation based on source-guided discrepancy. In AAAI, 2019.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1lYlete9S", "sentence_index": 0, "text": "This paper presents a world model-based approach in which behaviours are optimised by rollouts (i.e. imagination) in latent space.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1lYlete9S", "sentence_index": 1, "text": "The paper achieves impressive results across a large selection of tasks, both in terms of sample efficiency and final performance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "r1lYlete9S", "sentence_index": 2, "text": "I found the paper interesting to read and well written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "r1lYlete9S", "sentence_index": 3, "text": "The main contribution (backpropagating analytic gradients through imagined trajectories?) could potentially be highlighted more but otherwise the paper was clear.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "r1lYlete9S", "sentence_index": 4, "text": "I wonder if the authors ever looked at how much the size of the latent vector determines the performance of the system?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1lYlete9S", "sentence_index": 5, "text": "Is there an optional latent vector size across domains or is that optimal size task dependent?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1lYlete9S", "sentence_index": 6, "text": "Additionally, how much variance is there in the imagined trajectories from a certain starting state? In other words, are the endpoints of most imagined trajectories similar or very different?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1lYlete9S", "sentence_index": 7, "text": "There is actually not too much for me to critique and I would suggest this paper should be accepted.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1lYlete9S", "sentence_index": 8, "text": "Minor comment:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1lYlete9S", "sentence_index": 9, "text": "- On page 2 it says \u201cWe approach this limitation in latenby\u201d, which I assume is a typo?", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1lYlete9S", "sentence_index": 10, "text": "### #After rebuttal# # ##", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1lYlete9S", "sentence_index": 14, "text": "The authors' response addressed my remaining questions.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 0, "text": "This paper explores self-supervised learning in the low-data regime, comparing results to self-supervised learning on larger datasets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 1, "text": "BiGAN, RotNet, and DeepCluster serve as the reference self-supervised methods.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 2, "text": "It argues that early layers of a convolutional neural network can be effectively learned from a single source image, with data augmentation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 3, "text": "A performance gap exists for deeper layers, suggesting that larger datasets are required for self-supervised learning of useful filters in deeper network layers.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 4, "text": "I believe the primary claim of this paper is neither surprising nor novel.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1x7498kcS", "sentence_index": 5, "text": "The long history of successful hand-designed descriptors in computer vision, such as SIFT [Lowe, 1999] and HOG [Dalal and Triggs, 2005], suggest that one can design (with no data at all) features reminiscent of those learned in the first couple layers of a convolutional neural network (local image gradients, followed by characterization of those gradients over larger local windows).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 6, "text": "More importantly, it is already well established that it is possible to learn, from only a few images, filter sets that resemble the early layers of filters learned by CNNs.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 7, "text": "This paper fails to account for a vast amount of literature on modeling natural images that predates the post-AlexNet deep-learning era.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1x7498kcS", "sentence_index": 8, "text": "For example, see the following paper (over 5600 citations according to Google scholar):", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 9, "text": "[1] Bruno A. Olshausen and David J. Field.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 10, "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 11, "text": "Nature, 1996.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 12, "text": "Figure 4 of [1] shows results for learning 16x16 filters using \"ten 512x512 images of natural scenes\".", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 13, "text": "Compare to the conv1 filters in Figure 2 of the paper under review.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 14, "text": "This 1996 paper clearly established that it is possible to learn such filters from a small number of images.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 15, "text": "There is long history of sparse coding and dictionary learning techniques, including multilayer representations, that follows from the early work of [1].", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1x7498kcS", "sentence_index": 16, "text": "The paper should at minimum engage with this extensive history, and, in light of it, explain whether its claims are actually novel.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 0, "text": "The authors consider the few-shot / meta-learning scenario in which the test set of interest is drawn from a different distribution from the training set.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 1, "text": "This scenario is well-motivated by the \"researcher example\" given throughout the paper.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 3, "text": "The authors assume access to a large unlabelled set in test (target) domain, and a large labelled (few-shot) set in the source domain.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 4, "text": "Thus, the paper is concerned with unsupervised version of the meta-learning problem under domain shift (i.e., a large amount of data unlabelled are available from the target domain).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 5, "text": "The key idea is to learn a mapping from the source domain to the target domain.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 6, "text": "This mapping is learned jointly with the meta-learner, who performs the meta-learning in the target domain, on examples from the labelled domain.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 7, "text": "In practice however, it appears from the experimental section that the domain mapping is learned offline, and then frozen for the meta-learning phase.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 8, "text": "Thus, at test time, given examples from the target domain, the meta-learner can perform few-shot learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 9, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 10, "text": "- The paper addresses an important scenario which has not been addressed to this point: namely, meta-learning without the assumption that the train and test sets are drawn from the same domain/distribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 11, "text": "- The authors propose a novel task and experimental framework for considering their method, and show (somewhat unsurprisingly) that their method outperforms standard meta-learning methods that do not properly account for domain shift.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 12, "text": "- The paper reads well and is easy to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 13, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 14, "text": "- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / \"additional improvements\".", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 15, "text": "Further, there a number of experimental details that need to be further elaborated upon.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 16, "text": "e.g., architectures and hyper-parameters used, and training procedures (I encourage the authors to utilize the appendices for this).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 17, "text": "It is unclear to me how difficult/easy these results would be to reproduce. Do the authors intend to release code for their implementations and experiments?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 18, "text": "- Some assumptions are not explicitly stated.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 19, "text": "In particular, it is unclear what the assumption on the size of the unlabelled test set is.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 20, "text": "This is also lacking from the description of the experimental protocol, which does not address the data-splits (how many classes were used for each) and size of the unlabelled test set.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 21, "text": "- While the method is presented as jointly learning all the components, in the experimental section it is stated that the embedding network (the meta-learner) and the GAN-based domain adaptation are done separately. Can the authors comment on this further?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 22, "text": "Is this different from first learning a image translation mapping (using the unlabelled data in the target domain), and then applying existing meta-learning models/algorithms to the labelled data in the target domain?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 23, "text": "- The overall method seems to be not very principled, and requires a lot of \"tweaks and tunes\", with additional losses and regularizers, to work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 24, "text": "Overall, the paper proposes a method combining a number of existing useful works (prototypical networks for meta-learning and image-to-image translation for domain adaptation) to tackle an important problem setting that is not currently addressed in existing meta-learning research.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 25, "text": "Further, it establishes a useful experimental benchmark for this task, and provides what appear to be reasonable results (though this is somewhat difficult to judge due to the lack of baseline approaches).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 26, "text": "Hopefully, such a benchmark will inspire more researchers to explore this setting, and perhaps propose simpler, more principled approaches to perform this task.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "r1x9Ih1c2Q", "sentence_index": 27, "text": "It is my impression that, if the authors elaborate on the experimental protocol and implementation details, this paper would be a good fit for the venue.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "r1xXo7UF3m", "sentence_index": 0, "text": "The paper addresses the problem of producing sensible (high) uncertainties on out of distribution (OOD) data along with accurate predictions on in-distribution data.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1xXo7UF3m", "sentence_index": 1, "text": "The authors consider a model wherein the weights of the network (\\theta) are drawn from a matrix normal distribution whose parameters are in-turn a (non-linear; parameterized by a another network) function of the covariates (x).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1xXo7UF3m", "sentence_index": 2, "text": "Instead of inferring a posterior over theta that then induces the predictive uncertainties, uncertainties here arise from a regularizer that penalizes the distribution over theta from deviating too far from a standard Normal.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1xXo7UF3m", "sentence_index": 3, "text": "Experiments present results on toy data, MNIST/not MNIST as well as on adversarial perturbations of MNIST and CIFAR 10 datasets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1xXo7UF3m", "sentence_index": 4, "text": "The paper is clearly written and addresses an important problem.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "r1xXo7UF3m", "sentence_index": 5, "text": "The paper presents both an alternate model as well as an alternate objective function.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1xXo7UF3m", "sentence_index": 6, "text": "While the authors do report some interesting results, they do a poor job of motivating the proposed extensions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 7, "text": "It isn\u2019t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 8, "text": "1. The proposed model? Is using a conditional weight prior p(\\theta | x) (Eq 3) instead of p(\\theta) (as in BNNs)  necessary for the inflated uncertainties on OOD data?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "none"}, {"review_id": "r1xXo7UF3m", "sentence_index": 9, "text": "2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\\theta) =  MN(0, I, I) in the KL sense.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 10, "text": "Depending on \\lambda, the objective either closely approximates the marginal likelihood or not.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1xXo7UF3m", "sentence_index": 11, "text": "It is unclear how important this particular objective is to the results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 12, "text": "-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \\theta and induce predictive uncertainties.  Were they explored and found to be not effective?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 13, "text": "It would be nice to see how a \u201cgold standard\u201d HMC based inference does on at least the small toy problem of Sec 5.1?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 14, "text": "- There is also a closely related variant of Eq 3 which we can arrive at by switching the log and the expectation in the first term of Eq 5 and applying Jensen\u2019s inequality \u2014> E_p(\\theta| x)[ln p(y | x, \\theta)] - KL (p(\\theta | x) || p(\\theta)).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1xXo7UF3m", "sentence_index": 15, "text": "This would correspond to maximizing a valid lower bound to the marginal likelihood of a BNN model p(y | x, \\theta) p(\\theta), while interpreting p(\\theta | x) as an amortized variational approximation.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1xXo7UF3m", "sentence_index": 16, "text": "This variant has the advantage that it provides a valid lower bound on the marginal likelihood, and exploits the well understood variational inference machinery.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1xXo7UF3m", "sentence_index": 17, "text": "This also immediately suggests, that the variational approximation , p (\\theta | x)  should probably depend on both x and y rather than only on x and the flexibility of the hyper networks g would govern how well the true posterior over weights \\theta can be approximated.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1xXo7UF3m", "sentence_index": 18, "text": "Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 19, "text": "3.  Or simply to a well tuned \\lambda, chosen on a per dataset basis? From the text it appears that \\lambda is manually selected to trade off accuracy against uncertainty on OOD data.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 20, "text": "In the real world, one would not have access to OOD data during training, how is one to pick \\lambda in such cases?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 21, "text": "Detailed comments about experiments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1xXo7UF3m", "sentence_index": 22, "text": "a) The uncertainties produced by CDN in Figure 2 seems strange.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 23, "text": "Why does it go to nearly zero around x = 0, while being higher in surrounding regions with more data?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 24, "text": "b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 25, "text": "This forces the VI solution to tend to the MLE, sacrificing uncertainty in the variational distribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 26, "text": "It would be good to include comparisons against VI with \\lambda = 1.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1xXo7UF3m", "sentence_index": 27, "text": "==========", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1xXo7UF3m", "sentence_index": 28, "text": "There are potentially interesting ideas in this paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "r1xXo7UF3m", "sentence_index": 29, "text": "However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1xY4JK62Q", "sentence_index": 0, "text": "I read the paper and understand it, for the most part.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1xY4JK62Q", "sentence_index": 1, "text": "The idea is to interpret some regularization technics as a from of noisy bottleneck, where the mutual information b tween learned parameters and the data is limited through the injection of noise.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1xY4JK62Q", "sentence_index": 2, "text": "While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "r1xY4JK62Q", "sentence_index": 3, "text": "I'd be interested to hear if the authors see a connection between their formalism and the one of Reference prior in Bayesian inference (Bernardo et al https://arxiv.org/pdf/0904.0156)", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "r1xY4JK62Q", "sentence_index": 4, "text": "Pro: nicely written, clear interpretation of regularization as a noise injection technics, explicit link with information theoery and Shanon capacity.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "r1xY4JK62Q", "sentence_index": 5, "text": "Con: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "r1xYhv5J5H", "sentence_index": 0, "text": "This paper points out that the traditional way of model selection is flawed due to that the validation/test set is often small.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1xYhv5J5H", "sentence_index": 1, "text": "The authors also attribute the existence of adversarial examples to the small validation/test set, which I agree to some degree.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1xYhv5J5H", "sentence_index": 2, "text": "Hence, the authors proposed an alternative approach to comparing different classification models by the notion of inter-model discrepancy.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1xYhv5J5H", "sentence_index": 3, "text": "The main idea is reasonable, but it requires that the models to compare all perform reasonably well.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1xYhv5J5H", "sentence_index": 4, "text": "Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xYhv5J5H", "sentence_index": 5, "text": "Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xYhv5J5H", "sentence_index": 6, "text": "Another question which is not answered in the paper is the number $k$ of images to select for each pair of classifiers. Is this number task-dependent? Is it related to the number of classes?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1xYhv5J5H", "sentence_index": 7, "text": "What is a general guideline for one to choose this number $k$ given a new application scenario?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "r1xYhv5J5H", "sentence_index": 8, "text": "The unlabeled set is not \"unlabeled\" in essence. If my understanding was correct, it cannot contain open-set images which do not belong to any of the classes of interest.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xYhv5J5H", "sentence_index": 9, "text": "It is also nontrivial to control that the images contain only one salient object per image.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "r1xYhv5J5H", "sentence_index": 10, "text": "Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 0, "text": "# Summary", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 1, "text": "This paper proposes a new kind of spherical convolution for use in spherical CNNs, and evaluates it on rigid and non-rigid 3D shape recognition and retrieval problems.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 2, "text": "Previous work has either used general anisotropic convolution or azimuthally isotropic convolution.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 3, "text": "The former produces feature maps on SO(3), which is deemed undesirable because processing 3-dimensional feature maps is costly.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 4, "text": "The latter produces feature maps on the sphere, but requires that filters be circularly symmetric / azimuthally isotropic, which limits modeling capacity.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 5, "text": "This paper proposes an anisotropic spherical convolution that produces 2D spherical feature maps.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 6, "text": "The paper also introduces an efficient way of processing geodesic / icosahedral spherical grids, avoiding complicated spectral algorithms.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 7, "text": "# Strengths", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 8, "text": "The paper has several strong points.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 9, "text": "It is well written, clearly structured, and the mathematics is clear and precise while avoiding unnecessary complexity.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 10, "text": "Much of the relevant related work is discussed, and this is done in a balanced way.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 11, "text": "Although it is not directly measured, it does seem highly likely that the alt-az convolution is more computationally efficient than SO(3) convolution, and more expressive than isotropic S2 convolution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 12, "text": "The most important contribution in my opinion is the efficient data structure presented in section 4, which allows the spherical convolution to be computed efficiently on GPUs for a grid that is much more homogeneous than the lat/lon grids used in previous works (which have very high resolution near the poles, and low resolution at the equator).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 13, "text": "The idea of carving up the icosahedral grid in just the right way, so that the spherical convolution can be computed as a planar convolution with funny boundary conditions, is very clever, elegant, and practical.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 14, "text": "# Weaknesses", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 15, "text": "There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 16, "text": "To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 17, "text": "This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 18, "text": "For instance we can multiply Rz(phi) Ry(nu) by the element Rz(omega)Ry(0) = Rz(omega), which gives the element Rz(phi) Ry(nu) Rz(omega).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 19, "text": "As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 20, "text": ".", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 21, "text": "So the closure axiom of a group is violated.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 22, "text": "This matters, because the notion of equivariance really only makes sense for a group.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 23, "text": "If a layer l satisfies l R = R l  (for R a alt-az rotation), then it automatically satisfies l RR' = RR' l, which means l is equivariant to the whole group generated by the set of alt-az rotations.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 24, "text": "As we saw before, this is the whole rotation group.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 25, "text": "This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 26, "text": "Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 27, "text": "This also becomes apparent in the experiments section, where rotational data augmentation is found to be necessary.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 28, "text": "The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 29, "text": "I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 30, "text": "Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 31, "text": "The south pole can be represented by any pair of coordinates of the form phi in [0, 2pi], nu = +/- pi.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 32, "text": "But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 33, "text": "This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 34, "text": "The set of points on the sphere can only be viewed as the quotient SO(3)/S(2).", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 35, "text": "The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 36, "text": "I agree that local SO(2) invariance is too limiting. But it is not true that rotating filters is not effective in planar/volumetric CNNs, as shown by many recent papers on equivariant networks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 37, "text": "I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJeeoDOinQ", "sentence_index": 38, "text": "# Other comments", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 39, "text": "The experiments show that the method is quite effective.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 40, "text": "For instance, the SHREC17 results are on par with Cohen et al. and Esteves et al., presumably at a significantly reduced computational cost.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 41, "text": "That they do not substantially outperform these and other methods", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 42, "text": "is likely due to the input representation, which is lossy, leading to a maximal performance shared by all three methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 43, "text": "An application to omnidirectional vision might more clearly show the strength of the method, but this would be a lot of work so I do not expect the authors to do that for this paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 44, "text": "It would be nice to see a more direct comparison between the three definitions of spherical convolution (general SO3, isotropic S2, and anisotropic S2).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 45, "text": "Right now, the numbers reported in Cohen et al. and Esteves et al. are copied over, but there are probably many differences between the precise setup and architectures used in these papers.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 46, "text": "It would be interesting to see what happens if one uses the same architecture on a number of problems, changing only the convolution in each case.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 47, "text": "Initially, I was a bit puzzled about why SO(3) augmentation seems to reduce accuracy in table 1.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 48, "text": "I think this is because SO(3) augmentation actually makes the classification problem harder if the input is initially aligned.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 49, "text": "Some more explanation / discussion would be good.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 50, "text": "It would be nice to explain the spherical parameterization in more detail. Is this operation itself rotation equivariant?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJeeoDOinQ", "sentence_index": 51, "text": "Typos & minor issues", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 52, "text": "- Abstract: \"to extract non-trivial features\".", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 53, "text": "The word non-trivial really doesn't add anything here.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 54, "text": "Similarly \"offers multi-level feature extraction capabilities\" is almost meaningless since all DL methods can be said to do so.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 55, "text": "- Below eq. 5, D_R^{-1} should equal D_R(-omega, -nu, -phi).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 56, "text": "The order is reversed when inverting.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 57, "text": "- \"Different notations of convolutions\" -> notions", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 58, "text": "- \"For spherical functions there is no consistent and well defined convolution operators.\" As discussed above, the issue is quite a bit more subtle. There are exactly two well-defined convolution operators, but they have some characteristics deemed undesirable by the authors.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 59, "text": "- \"rationally symmetric\" -> rotationally", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 60, "text": "- \"exact hierarchical spherical patterns\" -> extract", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 61, "text": "- It seems quite likely that the unpacking of the icosahedral/hexagonal grid as done in this paper has been studied before in other fields.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_originality", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 62, "text": "References would be in order. Similarly, hexagonal convolution has a history in DL and outside.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 63, "text": "- Bottom of page 7, capitalize \"for\".", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 64, "text": "- \"principle curvatures\" -> principal.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 65, "text": "- \"deferent augmentation modes\" -> different", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 66, "text": "- \"inspite\" -> in spite", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 67, "text": "- \"reprort\" -> report", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 68, "text": "- \"utlize\" -> utilize", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 69, "text": "- \"computer the convolution\" -> compute", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJeeoDOinQ", "sentence_index": 70, "text": "# Conclusion", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 71, "text": "Although the alt-az convolution lacks the mathematical elegance of the general anisotropic and azimuthally isotropic spherical convolutions, it still seems like a practically useful operation for some kinds of data, particularly when implemented using the homogeneous icosahedral/hexagonal grid and fast algorithm presented in this paper.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 72, "text": "Hence, I would wholeheartedly recommend acceptance of this paper if the authors correct the factual errors (e.g. the claim of SO(3)-equivariance) and provide a clear discussion of the issues.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 73, "text": "For now I will give an intermediate rating to the paper.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJeeoDOinQ", "sentence_index": 74, "text": "[1] Kondor, Trivedi, \"On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups\"", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJeF90yCKS", "sentence_index": 0, "text": "This paper proposed another variant of Langevin dynamics, called \u201cStein self-repulsive dynamics,\u201d which simultaneously decreases the auto-correlation of Langevin dynamics and eliminates the need for running parallel chains in SVGD.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeF90yCKS", "sentence_index": 1, "text": "They combined Langevin dynamics with Stein variational gradient descent and theoretically justified that the proposed method successfully converges to the stationary distribution with only a single chain, unlike SVGD.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeF90yCKS", "sentence_index": 2, "text": "The proposed method decreases the auto-correlation of Langevin dynamics, so the proposed method increases the sample efficiency.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeF90yCKS", "sentence_index": 3, "text": "The paper is well-written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "rJeF90yCKS", "sentence_index": 4, "text": "The idea of the proposed method is natural, which is incorporating the functionality of SVGD to reduce the auto-correlation of Langevin dynamics.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "rJeF90yCKS", "sentence_index": 5, "text": "The idea is intuitive and justified by their theoretical analysis.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJeF90yCKS", "sentence_index": 6, "text": "The authors also well- placed their work in the literature, as described in Section 3.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeF90yCKS", "sentence_index": 7, "text": "The intuitive explanation of the proposed method is given in Section 3.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJeF90yCKS", "sentence_index": 8, "text": "I have one technical question as follows. If the authors reply appropriately, I will raise the score to accept.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJeF90yCKS", "sentence_index": 9, "text": "In Theorem 4.3 , the result holds for any k and M. The authors claim that if we take a limit of M -> \u221e with fixed k, the practical dynamics converges to the discrete-time mean-field limit, in Section 4.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJeF90yCKS", "sentence_index": 11, "text": "However, to state the result of Theorem 4.3, k should be bigger than M c_\\eta from the dentition of \\tilde{\\rho}_k^M, as shown under the equation (4).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJeF90yCKS", "sentence_index": 12, "text": "How do we take a limit of M -> \u221e ? Does k also go \u221e?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rJeF90yCKS", "sentence_index": 13, "text": "Minor comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJeF90yCKS", "sentence_index": 14, "text": "- The definition of g should depend on only \\theta_k^I and \\hat{\\delta}_k^M, not \\theta_k^k.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJeF90yCKS", "sentence_index": 15, "text": "- The equation (1) should hold for any \\theta\u2019, not \\theta.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJeF90yCKS", "sentence_index": 16, "text": "- The equation (1) should contain \\rho, not p.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 0, "text": "The paper proposes using the Frank-Wolfe algorithm for fast adversarial attacks.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJefA1rv3X", "sentence_index": 1, "text": "They prove upper bounds on the Frank-Wolfe gap and show experimentally that they can attack successfully much faster than other algorithms.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJefA1rv3X", "sentence_index": 2, "text": "In general I find the paper novel (to the best of my somewhat limited knowledge), interesting and well written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "rJefA1rv3X", "sentence_index": 3, "text": "However I find the white-box experiments lacking as almost every method has 100% success rate. Fixing this would significantly improve the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 5, "text": "Main remarks:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJefA1rv3X", "sentence_index": 6, "text": "- Need more motivation for faster white-box attack.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 7, "text": "One good motivation for example is adversarial training, e.g. Kurakin et al 2017 \u2018ADVERSARIAL MACHINE LEARNING AT SCALE\u2019 that would benefit greatly from faster attacks", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJefA1rv3X", "sentence_index": 8, "text": "- White-box attack experiments don\u2019t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 9, "text": "Need to compare in more challenging settings where the success rate is meaningful, e.g. smaller epsilon or a more robust NN using some defence.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 10, "text": "Also stating the 100% success rate in the abstract is a bit misleading for the this reason.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 11, "text": "-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other attack seems odd.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 13, "text": "-The average distortion metric (that\u2019s unfavourable to your method anyway) doesn\u2019t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 14, "text": "- Regarding lambda>1, you write that \u201cwe argue this modification makes our algorithm more general, and gives rise to better attack results\u201d.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJefA1rv3X", "sentence_index": 15, "text": "I did not see any theoretical or empirical support for this in the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 16, "text": "Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 17, "text": "Some intuitive explanation on why this should help and/or empirical comparison would be a great addition.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJefA1rv3X", "sentence_index": 18, "text": "- The authors claim that this is the first zeroth-order non-convex FW convergence rate, I am not familiar enough with the field to evaluate this claim and its significance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rJefA1rv3X", "sentence_index": 19, "text": "- Alg. 1 for T>1 is very similar to I-FGM, but also \u2018pulls\u2019 x_t towards x_orig.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJefA1rv3X", "sentence_index": 20, "text": "It would be very useful to write the update more explicitly and compare and contrast this 2 very similar updates. This gives nice insight into why this should intuitively work better.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJefA1rv3X", "sentence_index": 22, "text": "- I am not sure what the authors mean by \u201cthe Frank-Wolfe gap is affine invariant\u201d. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 23, "text": "- I am not sure what you mean in 5.4 \u201cwe omit all grid search/ binary search steps \u2026\u201d", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 25, "text": "Minor remarks:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJefA1rv3X", "sentence_index": 26, "text": "- In remark 4.8 in the end option I and II are inverted by mistake", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 27, "text": "- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJefA1rv3X", "sentence_index": 28, "text": "- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJgbJ0v52m", "sentence_index": 0, "text": "This paper explores maximally expressive linear layers for jointly exchangeable data and in doing so presents a surprisingly expressive model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJgbJ0v52m", "sentence_index": 1, "text": "I have given it a strong accept because the paper takes a very well-studied area (convolutions on graphs) and manages to find a far more expressive model (in terms of numbers of parameters) than what was previously known by carefully exploring the implications of the equivariance assumptions implied by graph data.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJgbJ0v52m", "sentence_index": 2, "text": "The result is particularly interesting because the same question was asked about exchangeable matrices (instead of *jointly* exchangeable matrices) by Hartford et al. [2018] which lead to a model with 4 bases instead of the 15 bases in this model, so the additional assumption of joint exchangeability (i.e. that any permutations applied to rows of a matrix must also be applied to columns - or equivalently, the indices of the rows and columns of a matrix refer to the same items / nodes) gives far more flexibility but without losing anything with respect to the Hartford et al result (because it can be recovered using a bipartite graph construction - described below).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "rJgbJ0v52m", "sentence_index": 3, "text": "So we have a case where an additional assumption is both useful (in that it allows for the definition of a more flexible model) and benign (because it doesn't prevent the layer from being used on the data explored in Hartford et al.).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "rJgbJ0v52m", "sentence_index": 4, "text": "I only have a couple of concerns:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJgbJ0v52m", "sentence_index": 5, "text": "1 - I would have liked to see more discussion about why the two results differ to give readers intuition about where the extra flexibility comes from.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rJgbJ0v52m", "sentence_index": 6, "text": "The additional parameters of this paper come from having parameters associated with the diagonal (intuitively: self edges get treated differently to other edges) and having parameters for the transpose of the matrix (intuitively: incoming edges are different to outgoing edges).", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJgbJ0v52m", "sentence_index": 7, "text": "Neither of these assumptions apply in the exchangeable setting (where the matrix may not be square so the diagonal and transpose can't be used).", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "rJgbJ0v52m", "sentence_index": 8, "text": "Because these differences aren't explained, the synthetic tasks in the experimental section make this approach look artificially good in comparison to Hartford et al.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "rJgbJ0v52m", "sentence_index": 9, "text": "The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, \"here are some simple functions for which we would need the additional parameters that we define\" makes sense; but arguing that Hartford et al. \"fail approximating rather simple functions\" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it's designed for a different setting).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "rJgbJ0v52m", "sentence_index": 10, "text": "2 - Those more familiar of the graph convolution literature will be more familiar with GCN [kipf et al. 2016] / GraphSAGE [Hamilton et al. 2017] / Monti et al [2017] / etc.. Most of these approaches are more restricted version of this work / Hartford et al. so we wouldn't expect them to perform any differently from the Hartford et al.  baseline on the synthetic dataset, but including them will strengthen the author's argument in favour of the work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "rJgbJ0v52m", "sentence_index": 11, "text": "I would have also liked to see a comparison to these methods in the the classification results.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "rJgbJ0v52m", "sentence_index": 12, "text": "3 - Appendix A - the 6 parameters for the symmetric case with zero diagonal reduces to the same 4 parameters from Hartford et al. if we constrained the diagonal to be zero in the output as well as the input.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "rJgbJ0v52m", "sentence_index": 13, "text": "This is the case when you map an exchangeable matrix into a jointly exchangeable matrix by representing it as a bipartite graph [0, X; X^T, 0].", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "rJgbJ0v52m", "sentence_index": 14, "text": "So the two results coincide for the exchangeable case. Might be worth pointing this out.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "rJggFt49nX", "sentence_index": 0, "text": "The authors seek to make it practical to use the full-matrix version of Adagrad\u2019s adaptive preconditioner (usually one uses the diagonal version), by storing the r most recently-seen gradient vectors in a matrix G, and then showing that (GG^T)^(-\u00bd) can be calculated fairly efficiently (at the cost of one r*r matrix inversion, and two matrix multiplications by an r*d matrix).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJggFt49nX", "sentence_index": 1, "text": "This is a really nice trick.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "rJggFt49nX", "sentence_index": 2, "text": "I\u2019m glad to see that the authors considered adding momentum (to adapt ADAM to this setting), and their experiments show a convincing benefit in terms of performance *per iteration*. Interestingly, they also show that the models found by their method also don\u2019t generalize poorly, which is noteworthy and slightly surprising.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJggFt49nX", "sentence_index": 3, "text": "However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rJggFt49nX", "sentence_index": 4, "text": "In Appendix B.1, they report mixed results in terms of wall-clock time, and I strongly feel that these results should be in the main body of the paper.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJggFt49nX", "sentence_index": 5, "text": "One would *expect* the proposed approach to work better than diagonal preconditioning on a per-iteration basis (at least in terms of training loss).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJggFt49nX", "sentence_index": 6, "text": "A reader\u2019s most natural question is whether there is a large enough improvement to offset the extra computational cost, so the fact that wall-clock times are relegated to the appendix is a significant weakness.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJggFt49nX", "sentence_index": 7, "text": "Finally, the proposed approach seems to sort of straddle the line between traditional convex optimization algorithms, and the fast stochastic algorithms favored in machine learning.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJggFt49nX", "sentence_index": 8, "text": "In particular, I think that the proposed algorithm has a more-than-superficial resemblance to stochastic LBFGS: the main difference is that LBFGS approximates the inverse Hessian, instead of (GG^T)^(-\u00bd).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJggFt49nX", "sentence_index": 9, "text": "It would be interesting to see how these two algorithms stack up.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "rJggFt49nX", "sentence_index": 10, "text": "Overall, I think that this is an elegant idea and I\u2019m convinced that it\u2019s a good algorithm, at least on a per-iteration basis.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rJggFt49nX", "sentence_index": 11, "text": "However, it trades-off computational cost for progress-per-iteration, so I think that an explicit analysis of this trade-off (beyond what\u2019s in Appendix B.1) must be in the main body of the paper.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rJghfrNnKS", "sentence_index": 0, "text": "The paper studies the Bayesian inferences with the generative adversarial network (GAN).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJghfrNnKS", "sentence_index": 1, "text": "In the first half of the paper, the general framework of the Bayes estimation is introduced.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJghfrNnKS", "sentence_index": 2, "text": "Then, The authors proposed how to incorporate GAN to the Bayesian inference.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJghfrNnKS", "sentence_index": 3, "text": "Some computational methods for calculating the mean of the statistic under the posterior distribution are described.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJghfrNnKS", "sentence_index": 4, "text": "Then, numerical experiments using MNIST and Celeb-A datasets are presented.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJghfrNnKS", "sentence_index": 5, "text": "Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJghfrNnKS", "sentence_index": 6, "text": "In numerical experiments, there is no comparison with major competitors besides random sampling in the active learning setup.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "rJghfrNnKS", "sentence_index": 7, "text": "Hence, the effectiveness and advantage of the proposed methods are not clear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "rJghfrNnKS", "sentence_index": 8, "text": "- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "rJghfrNnKS", "sentence_index": 9, "text": "- How does the estimation accuracy of GAN relate to the estimation accuracy of the proposed method? Showing a quantitative description would be nice.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJgwCU_82Q", "sentence_index": 0, "text": "This paper introduces a technique using ensembles of models with MC-dropout to perform uncertainty sampling for active learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJgwCU_82Q", "sentence_index": 1, "text": "In active learning, there is generally a trade-off between data efficiency and computational cost.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJgwCU_82Q", "sentence_index": 2, "text": "This paper proposes a combination of existing techniques, not just ensembling neural networks and not just doing MC dropout, but doing both.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJgwCU_82Q", "sentence_index": 3, "text": "The improvements over basic ensembling are rather minimal, at the cost of extra computation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJgwCU_82Q", "sentence_index": 4, "text": "More specifically, the data efficiency (factor improvement in data to achieve some accuracy) of the proposed method over using a deterministic ensemble is around just 10% or so.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJgwCU_82Q", "sentence_index": 5, "text": "On the other hand, the proposed algorithm requires 100x more forward passes when computing the uncertainty (which may be significant, unclear without runtime experiments).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJgwCU_82Q", "sentence_index": 6, "text": "As a concrete experiment to determine the importance, what would be the accuracy and computational comparison of ensembling 4+ models without MC-dropout vs. 3 ensembled models with MC-dropout?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJgwCU_82Q", "sentence_index": 7, "text": "At the point (number of extra ensembles) where the computational time is equivalent, is the learning curve still better?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rJgwCU_82Q", "sentence_index": 8, "text": "The novelty of this method is minimal.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJgwCU_82Q", "sentence_index": 9, "text": "The technique basically fills out the fourth entry in a Punnett square.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJgwCU_82Q", "sentence_index": 10, "text": "The paper is well-written, has good experiments, and has a comprehensive related work section.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "rJgwCU_82Q", "sentence_index": 11, "text": "Overall, this paper is good, but is not novel or important enough for acceptance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJl-0ufypm", "sentence_index": 0, "text": "This paper proposes an algorithm for auxiliary learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJl-0ufypm", "sentence_index": 1, "text": "Given a target prediction task to be learned on training data, the auxiliary learning utilizes external training data to improve learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJl-0ufypm", "sentence_index": 2, "text": "The authors focus on a setup where both target and external training data come from the same distribution but differ in class labels, where each class in the target data is a set of finer-grained classes in the auxiliary data.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJl-0ufypm", "sentence_index": 3, "text": "The authors propose a heuristic for learning from both data sets through minimization of a joint loss function.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJl-0ufypm", "sentence_index": 4, "text": "The experimental results show that the proposed methods works well on this particular setup on CIFAR data set.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJl-0ufypm", "sentence_index": 5, "text": "Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJl-0ufypm", "sentence_index": 6, "text": "+ a new auxiliary learning algorithm", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "rJl-0ufypm", "sentence_index": 7, "text": "+ positive results on CIFAR data set", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJl-0ufypm", "sentence_index": 8, "text": "Weaknesses:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJl-0ufypm", "sentence_index": 9, "text": "- novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJl-0ufypm", "sentence_index": 10, "text": "- there is no attempt to provide a theoretical insight into the performance of the algorithm", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJl-0ufypm", "sentence_index": 11, "text": "- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rJl-0ufypm", "sentence_index": 12, "text": "- experiments are performed using a synthetic setup on a single data set, so it remains unclear if the algorithm would be successful in a real life scenario", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rJl-0ufypm", "sentence_index": 13, "text": "- the paper is poorly written and sentences are generally very hard to parse. For example, section 3.1 is opened by statements such as \"(we use) a multi-task evaluator which trains on the principal and auxiliary tasks, and evaluates the performance of the auxiliary tasks on a meta set\"??", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJldfOy-27", "sentence_index": 0, "text": "This paper studied learning unsupervised node embeddings by considering the structural properties of networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJldfOy-27", "sentence_index": 1, "text": "Experimental results on a few data sets prove the effective of the proposed approaches over existing state-of-the-art approaches for unsupervised node embeddings.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJldfOy-27", "sentence_index": 2, "text": "Strength:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJldfOy-27", "sentence_index": 3, "text": "- important problem and interesting idea", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rJldfOy-27", "sentence_index": 4, "text": "- the proposed approach seems to be effective according to the experiments", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJldfOy-27", "sentence_index": 5, "text": "Weakness:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJldfOy-27", "sentence_index": 6, "text": "- some parts of the paper are quite unclear", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJldfOy-27", "sentence_index": 7, "text": "- the complexity of the proposed algorithm seems to be very high", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJldfOy-27", "sentence_index": 8, "text": "- the data sets used in the experiments are very small", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJldfOy-27", "sentence_index": 9, "text": "Details:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJldfOy-27", "sentence_index": 10, "text": "-In the introduction, \"it is in general impossible to find an embedding in R^d such that ...\", why do we have to make v and v'(and u, and u') far from each other?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rJldfOy-27", "sentence_index": 11, "text": "- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJldfOy-27", "sentence_index": 12, "text": "- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right \uff1f", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rJldfOy-27", "sentence_index": 14, "text": "- In Table 2 and 3, how are the degree and block information leveraged into the model?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rJlsvSSchm", "sentence_index": 0, "text": "This paper presents a gradient estimator for expectation-based objectives, which is called Go-gradient.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJlsvSSchm", "sentence_index": 1, "text": "This estimator is unbiased, has low variance and, in contrast to other previous approaches, applies to either continuous and discrete random variables.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJlsvSSchm", "sentence_index": 2, "text": "They also extend this estimator to problems where the gradient should be \"backpropagated\" through a nested combination of random variables and a (non-linear) functions.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJlsvSSchm", "sentence_index": 3, "text": "Authors present an extensive experimental evaluation of the estimator on different challenging machine learning problems.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJlsvSSchm", "sentence_index": 4, "text": "The paper addresses a relevant problem which appears in many machine learning settings, as it is the problem of estimating the gradient of an expectation-based objective.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rJlsvSSchm", "sentence_index": 5, "text": "In general, the paper is well written and easy to follow. And the experimental evaluation is extensive and compares with relevant state-of-the-art methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "rJlsvSSchm", "sentence_index": 6, "text": "The main problem with this paper is that it is difficult to identify its main and novel contributions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJlsvSSchm", "sentence_index": 7, "text": "1. In the case of continuous random variables, Go-gradient is equal to Implicit Rep gradients (Figurnov et al. 2018) and pathwise gradients (Jankowiack & Obermeyer,2018).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJlsvSSchm", "sentence_index": 8, "text": "Furthermore, for the Gaussian case, Implicit Rep gradients (and Go-gradient too) are equal to the standard reparametrization trick estimator (Kingma & Welling, 2014).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJlsvSSchm", "sentence_index": 9, "text": "This should be made crystal-clear in the paper.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJlsvSSchm", "sentence_index": 10, "text": "What happens is that the authors arrive at this solution using a different approach.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJlsvSSchm", "sentence_index": 11, "text": "In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJlsvSSchm", "sentence_index": 12, "text": "Moreover, I don't think some of the presented experiments are necessary.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJlsvSSchm", "sentence_index": 13, "text": "Simply because for continuous variables similar experiments have been reported before (Figurnov et al. 2018, Jankowiack & Obermeyer,2018).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJlsvSSchm", "sentence_index": 15, "text": "2. It seems that the main novel contribution of the paper is to extend the ideas of (Figurnov et al. 2018, Jankowiack & Obermeyer,2018) to discrete variables. And this is a relevant contribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "rJlsvSSchm", "sentence_index": 16, "text": "And the experimental evaluations of this part are convincing and compare favourably with other state-of-the-art methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJlsvSSchm", "sentence_index": 17, "text": "3. Authors should be much more clear about which is their original contribution to the problems stated in Section 4 and Section 5. As authors acknowledge in Section 6 .", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJlsvSSchm", "sentence_index": 20, "text": "<<Stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015), focusing mainly on re-parameterizable Gaussian random variables and deep latent Gaussian models, exploits the product rule for an integral to derive gradient backpropagation through several continuous random variables. >>", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJlsvSSchm", "sentence_index": 22, "text": "This is exactly what authors do in these sections.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJlsvSSchm", "sentence_index": 23, "text": "Again it seems that the real contribution of this paper here is to extend this stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015) ideas to discrete variables.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJlsvSSchm", "sentence_index": 24, "text": "Although this extension seems to be easily derived using the contributions made at point 2.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJlsvSSchm", "sentence_index": 25, "text": "Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 0, "text": "The paper proposes a technique to perform reasoning on mathematical formulas in a latent space.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 1, "text": "The model is trained to predict whether a rewrite rule can be applied to a formula given its latent representation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 2, "text": "When the rewrite is possible, the model also predicts the embedding of the resulting formula.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 3, "text": "Experiments show that the network can be applied multiple steps in a row, while operating only in the embedding space.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 4, "text": "1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \\sigma and \\alpha become unnecessary and we only need to train \\omega.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 5, "text": "Did you try to have a single network?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 6, "text": "This seems a much more natural approach to me, and I'm surprised that you did not start with that.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 7, "text": "From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 8, "text": "The role of \\sigma seems very redundant given \\omega.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 9, "text": "2. If you consider \\sigma, why do you also predict the rewrite success with \\omega? Couldn't it be simply a function from S x S -> L ?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "none"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 10, "text": "3. The graph neural networks used in the model are not described in the paper, only a reference to Paliwal et al (2019) is given.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 11, "text": "It would be helpful to have a brief paragraph describing this architecture, for readers not familiar with the referenced paper.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 12, "text": "4. How large is the training set of (T, P) pairs? I don't think this is mentioned in the paper.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 13, "text": "5. To train \\sigma and \\omega, the negative instances are selected randomly.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 14, "text": "You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 15, "text": "6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 16, "text": "I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 17, "text": "This could be a possibility to remove the noise you have when doing multi-step operations (and potentially go way beyond 4 steps).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 18, "text": "Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJxGlK6Ttr", "sentence_index": 19, "text": "Overall, the model is a bit complicated (e.g. question 1.), but the results are promising, the paper is well written, and the ability to manipulate formula embeddings is probably going to be useful in the context of theorem proving.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJxkq6waYr", "sentence_index": 0, "text": "This paper studies the properties of SGD as a function of batch size and learning rate.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxkq6waYr", "sentence_index": 1, "text": "Authors argue that SGD has two regimes:  a noise dominated regime (small batch size) and curvature dominated regime (large batch size).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxkq6waYr", "sentence_index": 2, "text": "Authors conduct through numerical experiments highlighting how learning rate changes as a function of batch size (initially linear growth and then saturates).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxkq6waYr", "sentence_index": 3, "text": "The critical contribution of this work appears to be the observation that large batch size can be worse than small under same number of steps demonstrating implicit regularization of small batch size.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxkq6waYr", "sentence_index": 4, "text": "The two regime claim of the paper is not really novel.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJxkq6waYr", "sentence_index": 5, "text": "These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJxkq6waYr", "sentence_index": 6, "text": "When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJxkq6waYr", "sentence_index": 7, "text": "The interesting part in my opinion is the experiments on constant steps.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "rJxkq6waYr", "sentence_index": 8, "text": "Authors verify large batch size reduces test accuracy while improving train.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJxkq6waYr", "sentence_index": 9, "text": "I believe these experiments are novel and the results are interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "rJxkq6waYr", "sentence_index": 10, "text": "Besides CIFAR 10, authors test this hypothesis in two other datasets while tuning the learning rate.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxkq6waYr", "sentence_index": 11, "text": "On the other hand, contribution is somewhat incremental given observations made by related literature (Keskar et al and others).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_neutral"}, {"review_id": "rJxkq6waYr", "sentence_index": 12, "text": "Some remarks:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJxkq6waYr", "sentence_index": 13, "text": "1) In Table 1, batch size 16k has effective LR of 32.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxkq6waYr", "sentence_index": 14, "text": "However in Figure 1c SGD with momentum at batch size 8k uses an effective LR of 4.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJxkq6waYr", "sentence_index": 15, "text": "Can you explain this inconsistency i.e. why is there such a huge jump from 4 to 32 (in reality we expect the effective LR to stay constant in the curvature regime).", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJxkq6waYr", "sentence_index": 16, "text": "I also understand that one is constant epoch and other is constant step.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxkq6waYr", "sentence_index": 17, "text": "However 4 to 32 seems a bit inconsistent.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJxkq6waYr", "sentence_index": 18, "text": "2) Does momentum help in constant step budget (with sufficiently large steps so that training loss is small)?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJxkq6waYr", "sentence_index": 19, "text": "3) Readability: Consider explaining what is meant by \"warm-up\", \"epoch budget\", \"step budget\" clearly and upfront.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJxKsYxycS", "sentence_index": 0, "text": "The authors propose to meta-learn, using MAML, the mean of an elementwise, input-dependent, multiplicative noise to improve generalization in few-shot learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxKsYxycS", "sentence_index": 1, "text": "The motivation is that meta-learning the noise allows to learn how to best perturb examples in order to improve generlization.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxKsYxycS", "sentence_index": 2, "text": "This claim is supported by ample experimental evidence and comparisons against many baselines, as well as additional ablation studies w.r.t design choices of the algorithm itself.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "rJxKsYxycS", "sentence_index": 3, "text": "The paper is well written and easy to read.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "rJxKsYxycS", "sentence_index": 4, "text": "Consequently, I think this is a nice paper and should be accepted.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJxKsYxycS", "sentence_index": 5, "text": "Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJxKsYxycS", "sentence_index": 6, "text": "Edit:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJxKsYxycS", "sentence_index": 7, "text": "Thank you for your response.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxKsYxycS", "sentence_index": 8, "text": "I will leave my score as is.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxKsYxycS", "sentence_index": 9, "text": "I would strongly encourage the authors to incorporate the baseline \"(1)\" as proposed by R3 in a future version of the paper as I agree with them that this is a relevant baseline.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rJxLRp2s9H", "sentence_index": 0, "text": "This paper presents a method for unsupervised representation learning of speech.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxLRp2s9H", "sentence_index": 1, "text": "The idea is to first learn discrete representation (vector quantization is done by Gumbel softmax or k-means) from audio samples with contrastive prediction coding type objective, and then perform BERT-style pre-training (borrowed from NLP).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxLRp2s9H", "sentence_index": 2, "text": "The BERT features are used as inputs to ASR systems, rather than the usual log-mel features.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxLRp2s9H", "sentence_index": 3, "text": "The idea, which combines those of previous work (wav2vec and BERT) synergetically, is intuitive and clearly presented, significant improvements over log-mel and wav2vec were achieved on ASR benchmarks WSJ and TIMIT.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "rJxLRp2s9H", "sentence_index": 4, "text": "Based on these merits, I suggest this paper to be accepted.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "rJxLRp2s9H", "sentence_index": 5, "text": "On the other hand, I would suggest directions for investigation and improvements as follows.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJxLRp2s9H", "sentence_index": 6, "text": "1. While I understand that vector quantization makes the use of NLP-style BERT-training possible (as the inputs to NLP models are discrete tokens),  there are potential disadvantages as well.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxLRp2s9H", "sentence_index": 7, "text": "One observation from the submission is that the token set may need to very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive (I noticed that the BERT model is trained on 128 GPUs) .", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJxLRp2s9H", "sentence_index": 9, "text": "Also, without BERT pre-training, using directly the discrete tokens seems to consistently give worse performance for ASR.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxLRp2s9H", "sentence_index": 10, "text": "I think some more motivations or explorations (what kind of information did BERT learn) are needed to understand why that is the case.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rJxLRp2s9H", "sentence_index": 11, "text": "2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJxLRp2s9H", "sentence_index": 12, "text": "A more economical approach is to use BERT-trained model as initialization for acoustic model training, which is the classical way how RBMs pre-training were used in ASR.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rJxLRp2s9H", "sentence_index": 13, "text": "3. One concern I have with discrete representation is how robust they are wrt different dataset.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJxLRp2s9H", "sentence_index": 14, "text": "The ASR datasets used in this work are relatively clean (but there does exists domain difference between them).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxLRp2s9H", "sentence_index": 15, "text": "It remains to see how the method performs with more acoustically-challenging speech data, and how universally useful the learned features are (as is the case for BERT in NLP).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxLRp2s9H", "sentence_index": 16, "text": "4. Another curious question is whether the features would still provide as much improvement when a stronger ASR system than AutoSeg (e.g., Lattice-free MMI) is used.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJxLRp2s9H", "sentence_index": 17, "text": "Overall, while I think the computational cost of the proposed method is high, rendering it less practical at this point, I believe the approach has potential and the result obtained so far is already significant.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rJxPUFHc3m", "sentence_index": 0, "text": "In this paper the authors introduce a new technique for softmax inference.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxPUFHc3m", "sentence_index": 1, "text": "In a multiclass setting, the idea is to take the output of a NN and turn it into a gating function to choose one expert.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxPUFHc3m", "sentence_index": 2, "text": "Then, given the expert, output a particular category.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxPUFHc3m", "sentence_index": 3, "text": "The first level of sparsity comes from the first expert.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxPUFHc3m", "sentence_index": 4, "text": "The second level of sparsity comes from every expert only outputting a limited set of output categories.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxPUFHc3m", "sentence_index": 5, "text": "The paper is easy to understand but several sections (starting from section 2) could use an english language review (e.g. \"search right\" -> \"search for the right\", \"predict next word\" -> \"predict the next word\", ...) In section 3, can you be more specific about the gains in training versus inference time?", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJxPUFHc3m", "sentence_index": 6, "text": "I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJxPUFHc3m", "sentence_index": 7, "text": "You motivate some of the work by the fact that the experts have overlapping outputs.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxPUFHc3m", "sentence_index": 8, "text": "Maybe in section 3.7 you can address how often that occurs as well?", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJxPUFHc3m", "sentence_index": 9, "text": "Nits:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJxPUFHc3m", "sentence_index": 10, "text": "- it wasn't clear how the sparsity percentage on page 3 was defined?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rJxPUFHc3m", "sentence_index": 11, "text": "- can you motivate why you are not using perplexity in section 3.2?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 0, "text": "This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxr_fmNFS", "sentence_index": 1, "text": "The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxr_fmNFS", "sentence_index": 2, "text": "Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxr_fmNFS", "sentence_index": 3, "text": "Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxr_fmNFS", "sentence_index": 4, "text": "The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxr_fmNFS", "sentence_index": 5, "text": "This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxr_fmNFS", "sentence_index": 6, "text": "There are some general discussions on how to adapt gyrovector space theory into spherical spaces.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxr_fmNFS", "sentence_index": 7, "text": "This is interesting but no formal results are presented.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxr_fmNFS", "sentence_index": 8, "text": "I vote for rejection for four major weaknesses explained as follows.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 9, "text": "(1) The experimental results cannot show the usefulness of the proposed GCN.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 10, "text": "The results on real datasets are similar to the regular GCN.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 11, "text": "As the authors themselves remarked, \"it can be seen that our models sometimes outperform the two Euclidean GCN\".", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "rJxr_fmNFS", "sentence_index": 12, "text": "The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 13, "text": "(2) The method is not well motivated.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 14, "text": "The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "rJxr_fmNFS", "sentence_index": 15, "text": "This is too general and not enough as a motivation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 16, "text": "After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 17, "text": "(3) A large body of graph neural network literature is omitted.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 18, "text": "The authors start from a very high-level description of machine learning in constant curvature spaces.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxr_fmNFS", "sentence_index": 19, "text": "Such high-level introductions require more comprehensive literatures to support.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 20, "text": "In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxr_fmNFS", "sentence_index": 21, "text": "This is misleading.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 22, "text": "For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 23, "text": "The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 24, "text": "This is a thriving area that requires a careful literature review.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 25, "text": "(4) The writing quality is not satisfactory.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 26, "text": "Here are a few examples: The ICLR citation style needs to use sometimes \\citep.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 27, "text": "The authors instead used \\cite everywhere, making the paper hard to read.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 28, "text": "The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 29, "text": "The introduction can start at a lower level (such as flat/hyperbolic neural networks).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJxr_fmNFS", "sentence_index": 30, "text": "Section 3.4, as the main technical innovation, can be extended and includes some demonstrations.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "none", "pol": "none"}, {"review_id": "rJxS7urch7", "sentence_index": 0, "text": "While the invertible model structure itself is essentially the same as Real-NVP, the use of observation variables in the framework with theoretically sound bidirectional training for safe use of the seemingly na\u00efve inclusion of y (i.e., y and z can be independent).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxS7urch7", "sentence_index": 1, "text": "Its abilities to model the posterior distributions of the inputs are supported by both quantitative and qualitative experiments.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxS7urch7", "sentence_index": 2, "text": "The demonstration on practical examples is a plus.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJxS7urch7", "sentence_index": 3, "text": "The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJxS7urch7", "sentence_index": 4, "text": "This is an interesting paper overall, so I am looking forward for further discussions.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxS7urch7", "sentence_index": 5, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJxS7urch7", "sentence_index": 6, "text": "1.\tExtensive analyses of the possibility of modeling posterior distributions with an INN have been shown.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJxS7urch7", "sentence_index": 7, "text": "Detailed experiment setups are provided in the appendix.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rJxS7urch7", "sentence_index": 8, "text": "2.\tThe theoretical guarantee (with some assumptions) of the true posterior might be beneficial in practice for relatively low-dimensional or less complex tasks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rJxS7urch7", "sentence_index": 9, "text": "Comments/Questions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJxS7urch7", "sentence_index": 10, "text": "1.\tFrom the generative model point of view, could the authors elaborate on the comparison against cGAN (aside from the descriptions in Appendix 2)? It is quoted \u201ccGAN\u2026often lack satisfactory diversity in practice\u201d.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "rJxS7urch7", "sentence_index": 11, "text": "Also, can cGAN be used estimate the density of X (posterior or not)?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJxS7urch7", "sentence_index": 12, "text": "2. For the bidirectional training, did the ratios of the losses (L_z, L_y, L_x) have to be changed, or the iterations of forward/backward trainings have to be changed (e.g., 1 forward, 1 backward vs. 2 forward, 1 backward)?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJxS7urch7", "sentence_index": 14, "text": "This question comes from my observation that the nature of the losses, especially for L_y vs. L_y,L_x (i.e., SL vs. USL) seem to be different.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxS7urch7", "sentence_index": 15, "text": "3.\t\u201cwe find it advantageous to pad both the in- and output of the network with equal number of zeros\u201d: Is this to effectively increase the intermediate network dimensions?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJxS7urch7", "sentence_index": 16, "text": "Also, does this imply that for both forward and inverse process those zero-padded entries always come out to be zero?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rJxS7urch7", "sentence_index": 17, "text": "It seems that there needs some way to enforce them to be zero to ensure that the propagation happens only among the entries belonging to the variables of interests (x, y and z).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxS7urch7", "sentence_index": 18, "text": "4.\tIt seems that most of the experiments are done in relatively small dimensional data.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxS7urch7", "sentence_index": 19, "text": "This is not necessarily a drawback, I am curious if this model could succeed on higher dimensional data (e.g., image), especially with the observation y.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rJxUPOqhhX", "sentence_index": 0, "text": "[Summary]:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 1, "text": "This paper tackles the problem of automatic robot design.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 2, "text": "The most popular approach to doing this has been evolutionary methods which work by evolving morphology of agents in a feed-forward manner using a propagation and mutation rules.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 3, "text": "This is a non-differentiable process and relies on maintaining a large pool of candidates out of which best ones are chosen with the highest fitness.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 4, "text": "In robot design for a given task using rewards, training each robot design using RL with rewards is an expensive process and not scalable.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 5, "text": "This paper uses graph network to train each morphology using RL.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 6, "text": "Thereby, allowing the controller to share parameters and reuse information across generations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 7, "text": "This expedites the score function evaluation improving the time complexity of the evolutionary process.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 8, "text": "[Strengths]:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 9, "text": "This paper shows some promise when graph network-based controllers augmented with evolutionary algorithms.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rJxUPOqhhX", "sentence_index": 10, "text": "Paper is quite easy to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "rJxUPOqhhX", "sentence_index": 11, "text": "[Weaknesses and Clarifications]:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 12, "text": "=> Robot design area has been explored extensively in classical work of Sims (1994) etc. using ES.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 13, "text": "Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rJxUPOqhhX", "sentence_index": 14, "text": "=> Environment: The experimental section of the paper can be further improved.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rJxUPOqhhX", "sentence_index": 15, "text": "The approach is evaluated only in three cases: fish, walker, cheetah. Can it be applied to more complex morphologies? Humanoid etc. maybe?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rJxUPOqhhX", "sentence_index": 16, "text": "=> Baselines: The comparison provided in the paper is weak.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "rJxUPOqhhX", "sentence_index": 17, "text": "At first, it compares to random graph search and ES.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 18, "text": "But there are better baselines possible.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "rJxUPOqhhX", "sentence_index": 19, "text": "One such example would be to have a network for each body part and share parameters across each body part.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 20, "text": "This network takes some identifying information (ID, shape etc.) about body part as input.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 21, "text": "As more body parts are added, more such network modules can be added.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 22, "text": "How would the given graph network compare to this?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJxUPOqhhX", "sentence_index": 23, "text": "This baseline can be thought of a shared parameter graph with no message passing.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 24, "text": "=> The results shown in Figure-4 (Section-4.2) seems unclear to me.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rJxUPOqhhX", "sentence_index": 25, "text": "As far as I understand, the model starts with hand-engineered design and then finetuned using evolutionary process.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 26, "text": "However, the original performance of the hand-engineered design is surprisingly bad (see first data point in any plot in Figure-4).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJxUPOqhhX", "sentence_index": 27, "text": "Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rJxUPOqhhX", "sentence_index": 28, "text": "[Recommendation]:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 29, "text": "I request the authors to address the comments raised above.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "rJxUPOqhhX", "sentence_index": 30, "text": "Overall, this is a reasonable paper but experimental section needs much more attention.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rkepvb7c2Q", "sentence_index": 0, "text": "This paper builds upon the assumption that GANs successfully approximate the data manifold, and uses this assumption to regularize semi-supervised learning process.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkepvb7c2Q", "sentence_index": 1, "text": "The proposed regularization strategy enforces that a discriminator or a given classifier should be invariant to small perturbations on the data manifold z. It is empirically shown that naively enforcing such a constraint by randomly adding noise to z could lead to under-smoothing or over-smoothing in some cases which can harm the final classification performance.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkepvb7c2Q", "sentence_index": 2, "text": "Consequently, the proposed regularization technique takes a step of tunable size in the direction of the manifold gradient, which has the effect of smoothing along the direction of the gradient while ignoring its norm.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkepvb7c2Q", "sentence_index": 3, "text": "Extensive experiments have been conducted, showing that the proposed approach outperforms or is comparable with recent state-of-the-art approaches on cifar 10, especially in presence of fewer labelled data points.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkepvb7c2Q", "sentence_index": 5, "text": "On SVHN however, the proposed approach fails in comparison with (Kumar et al 2017) but performs better than other approaches.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkepvb7c2Q", "sentence_index": 6, "text": "Furthermore, it has been shown that adding the proposed manifold regularization technique to the training of GAN greatly improves the image quality of generated images (in terms of FID scores and inception scores).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkepvb7c2Q", "sentence_index": 7, "text": "Also, by combining the proposed regularizer with a classical supervised classifier (via pre-training a GAN and using it for regularization) decreases classification error by 2 to 3%.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkepvb7c2Q", "sentence_index": 8, "text": "Finally, it has also been shown that after training a GAN using the manifold regularization, the algorithm is able to produce similar images giving a low enough perturbation of the data manifold z.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkepvb7c2Q", "sentence_index": 9, "text": "Overall, this paper is well written and show significant improvements especially for image generation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkepvb7c2Q", "sentence_index": 10, "text": "However, the novelty is rather limited as similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rkepvb7c2Q", "sentence_index": 11, "text": "The paper would be improved if the following points are taken into account:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rkepvb7c2Q", "sentence_index": 12, "text": "A comparison with Graph Convolutional Network based techniques seems appropriate (e.g. Kipf and Welling 2017).", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "rkepvb7c2Q", "sentence_index": 13, "text": "How do the FID/Inception improvements compare to (Mescheder et al 2018)?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rkepvb7c2Q", "sentence_index": 14, "text": "It would be interesting to discuss why the FID score for SVHN gets worse in presence of 1000 labels.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rkepvb7c2Q", "sentence_index": 15, "text": "Although there is a clear improvement in FID scores for Cifar10.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rkepvb7c2Q", "sentence_index": 16, "text": "It would be informative to show the generated images w/ and w/o manifold regularization.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rkepvb7c2Q", "sentence_index": 17, "text": "More analysis should be provided on why (Kumar et al 2017) perform so well on SVHN.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rkepvb7c2Q", "sentence_index": 18, "text": "It should be stated that bold values in tables do not represent best results (as it is usually the case) but rather results for the proposed approach.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rkeqvIaJor", "sentence_index": 0, "text": "The paper presents an approach to extract a character from a video and then maneuver that character in the plane, optionally with other backgrounds.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkeqvIaJor", "sentence_index": 1, "text": "The character is then redrawn into the background with a neural net, and all of this is done in real time.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkeqvIaJor", "sentence_index": 2, "text": "All in all, this paper was well structured and extensively detailed wrt how it engineered this solution (and why).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "rkeqvIaJor", "sentence_index": 3, "text": "If I had a complaint, it would be that I did not learn anything scientifically from the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkeqvIaJor", "sentence_index": 4, "text": "There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkeqvIaJor", "sentence_index": 5, "text": "Those are important as well for the field, and I suspect that this direction could be pushed a lot more.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rkeqvIaJor", "sentence_index": 6, "text": "For example, it's not close to getting realistic spatial movement relative to the plane nor is the control that impressive wrt limbs.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkeqvIaJor", "sentence_index": 7, "text": "However, as a next-contribution, this work deserves to be seen more widely.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rkeqvIaJor", "sentence_index": 8, "text": "Hence, I rate it as a weak accept.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_positive"}, {"review_id": "rkeucgq15r", "sentence_index": 0, "text": "The authors introduce the idea of distributed backdoor attacks in the FL framework, in which the dishonest participants in FL add local triggers to their training data to influence the global model to classify triggered images in a desired way.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkeucgq15r", "sentence_index": 1, "text": "They show empirically that the learned models then are more likely to be successfully forced to misclassified images in which all the local triggers are present at test time, than are models learned using centralized backdoor attacks, where all attackers use the same trigger pattern (one of the same size as the concatenation of the local triggers, to be fair in the comparison).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkeucgq15r", "sentence_index": 2, "text": "They then demonstrate that because the local triggers cause smaller corruptions in the model coefficients, these distributed attacks survive robust FL training algorithms (namely FoolsGold, and a recent robust regression based method) more often than centralized attacks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkeucgq15r", "sentence_index": 3, "text": "Similar experiments are conducted on the Loan text dataset, using appropriate analogs of local triggers, with similar results.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkeucgq15r", "sentence_index": 4, "text": "The paper contributes a novel model for conducting backdoor attacks in the FL setup, and shows that this model is more successful at attacking when training using robust FL algorithms than the standard centralized backdoor attack model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkeucgq15r", "sentence_index": 5, "text": "I lean towards accept, as this is a realistic attack model, and as such can further stimulate research into the robustification of FL model aggregation algorithms.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rkeUDe_i37", "sentence_index": 0, "text": "This paper tackles the question generation problem from a logical form and proposes an addition called Scratchpad Encoder to the standard seq2seq framework.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkeUDe_i37", "sentence_index": 1, "text": "The new model has been tested on the WebQuestionsSP and the WikiSQL datasets, with both automatic and human evaluation, compared to the baselines with copy and coverage mechanisms.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkeUDe_i37", "sentence_index": 2, "text": "Major points:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rkeUDe_i37", "sentence_index": 3, "text": "Overall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rkeUDe_i37", "sentence_index": 4, "text": "I don\u2019t recommend to accept this paper, at least in the current format.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rkeUDe_i37", "sentence_index": 5, "text": "The paper states two major contributions (the last paragraph of Introduction), one is the new model Scratchpad Encoder, and the other is \u201cpossible to generate a large high quality (SPARQL query, local form) dataset\u201d.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkeUDe_i37", "sentence_index": 6, "text": "For the second contribution, there isn\u2019t any evaluation/justification about the quality of the generated questions and how useful this dataset would be in any KB-QA applications.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkeUDe_i37", "sentence_index": 7, "text": "I believe that this paper is not the first one to study question generation from logical form (cf. Guo et al, 2018 as cited), so it is unclear what is the contribution of this paper in that respect.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "rkeUDe_i37", "sentence_index": 8, "text": "For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn\u2019t explain well the intuition of this \u201cwrite\u201d operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkeUDe_i37", "sentence_index": 9, "text": "In general I find Section 3 pretty difficult to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkeUDe_i37", "sentence_index": 10, "text": "What does \u201ckeeping notes\u201d mean? It seems that the goal of this model is to keep updating the encoder hidden vectors (h_0, .., h_T) instead of fixing them at the decoder stage.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkeUDe_i37", "sentence_index": 12, "text": "I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkeUDe_i37", "sentence_index": 13, "text": "\\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkeUDe_i37", "sentence_index": 14, "text": "Minor points:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rkeUDe_i37", "sentence_index": 15, "text": "- tau Yih et al, 2016 --> Yih et al, 2016", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rkeUDe_i37", "sentence_index": 16, "text": "- It is unclear why the results on WikiSQL is presented in Appendix. Combining the results on both datasets in the experiments section would be more convincing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkeUDe_i37", "sentence_index": 17, "text": "- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkg2K06S2m", "sentence_index": 0, "text": "This work presents Backpropamine, a neuromodulated plastic LSTM training regime.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkg2K06S2m", "sentence_index": 1, "text": "It extends previous research on differentiable Hebbian plasticity by introducing a neuromodulatory term to help gate information into the Hebbian synapse.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkg2K06S2m", "sentence_index": 2, "text": "The neuromodulatory term is placed under network control, allowing it to be time varying (and hence to be sensitive to the input, for example).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkg2K06S2m", "sentence_index": 3, "text": "Another variant proposes updating the Hebbian synapse with modulated exponential average of the Hebbian product.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkg2K06S2m", "sentence_index": 4, "text": "This average is linked to the notion of an eligibility trace, and ties into some recent biological work that shows the role of dopamine in retroactively modulating synaptic plasticity.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkg2K06S2m", "sentence_index": 5, "text": "Overall the work is nicely motivated and clearly presented.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "rkg2K06S2m", "sentence_index": 6, "text": "There are some interesting ties to biological work -- in particular, to retroactive plasticity phenomena.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rkg2K06S2m", "sentence_index": 7, "text": "There should be sufficient details for a reader to implement this model, thought there are some minor details missing regarding the experimental setup, which will be addressed below.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "rkg2K06S2m", "sentence_index": 8, "text": "The authors test their model on three tasks: cue-award association, maze learning, and Penn Treebank (PTB).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkg2K06S2m", "sentence_index": 9, "text": "In the cue-award association task the retroactive and simple modulation networks perform well, while the non-modulated and non-plastics fail.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkg2K06S2m", "sentence_index": 10, "text": "For the maze navigation task the modulated networks perform better than the non-modulated networks, though the effect is less pronounced.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkg2K06S2m", "sentence_index": 11, "text": "Finally, on PTB the authors report improvements over baseline LSTMs.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkg2K06S2m", "sentence_index": 12, "text": "One of the main claims of this paper is that neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task\u201d, and that therefore \u201cdifferentiable neuromodulation of plasticity offers a powerful new framework for training neural networks\u201d.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "rkg2K06S2m", "sentence_index": 13, "text": "This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rkg2K06S2m", "sentence_index": 14, "text": "The authors cite such models in the appendix (Melor et al), but claim that \u201cmuch larger models\u201d are needed, potentially with other mechanisms, such as dropout.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "rkg2K06S2m", "sentence_index": 15, "text": "Though this may be true, these models still undermine the claim that \u201cneuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task\u201d.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rkg2K06S2m", "sentence_index": 16, "text": "This claim is simply not true, and more care is needed in reporting the results here in the wider context of the literature.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rkg2K06S2m", "sentence_index": 17, "text": "Also, I am left wondering what are considered the parameters of the models -- are only the neuromodulatory terms considered as the additional trainable parameters compared to baseline LSTMs? How are the Hebbian synapses themselves considered in this calculation?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rkg2K06S2m", "sentence_index": 18, "text": "If the Hebbian synapses are not considered, then the authors need a control with matched memory-capacities to account for the extra capacity afforded by the Hebbian synapses.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rkg2K06S2m", "sentence_index": 19, "text": "Given the ties between Hebbian synapses and attention (see Ba et al), an important control here could be an LSTM with Bahdanau (2014) style attention.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rkg2K06S2m", "sentence_index": 20, "text": "Finally, the style (font) of the paper does not adhere to the ICLR style template, and must be changed.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkg2K06S2m", "sentence_index": 21, "text": "Overall, the ideas presented in the paper are intriguing, and further research down this line is encouraged.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rkg2K06S2m", "sentence_index": 22, "text": "However, in its current state the work lacks sufficiently strong baselines to support the paper\u2019s claims; thus, the merits of this approach cannot yet be properly assessed.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkgfiCmT5r", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 1, "text": "Many prior works have found that the features output by the final layer of neural networks can often be used as informative representations for many tasks despite being trained for one in particular.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 2, "text": "These feature representations, however, are learned transformations of low-level input representations, e.g. RGB values of an image.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 3, "text": "In this paper, they aim to learn useful feature representations without meaningful low-level input representations, e.g. just an instance ID.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 4, "text": "Instead, meaningful representations are learned through gathered triplet comparisons of these IDs, e.g. is instance A more similar to instance B or instance C? Similar existing techniques fall in the realm of learning ordinal embeddings, but this technique demonstrates speed-ups that allow it to scale to large real world datasets.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 5, "text": "The two primary contributions of the paper are given as:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 6, "text": "- a showcase of the power of neural networks as a tool to approximately solve NP-hard optimization problems with discrete inputs", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 7, "text": "- a scalable approach for the ordinal embedding problem", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 8, "text": "After experimentation on synthetic data, they compare the effectiveness of their proposed method Ordinal Embedding Neural Network (OENN) against the baseline techniques of Local Ordinal Embedding (LOE) and t-distributed Stochastic Triplet Embedding (TSTE).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 9, "text": "The test error given by the systems is comparable, but there are clear speed benefits to the proposed method OENN as the other techniques could not be run for a dataset size of 20k, 50k, or 100k.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 10, "text": "Then, they gathered real-world data using MTurk applied to a subset of ImageNet and applied OENN to learning embeddings of different image instances using only the MTurk triplet information rather than the input RGB input features.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 11, "text": "Decision: Weak Reject", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 12, "text": "1. Interesting technique to take advantage of neural networks to efficiently learn ordinal embeddings from a set of relationships without a low-level feature representation, but I believe the experiments could be improved.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rkgfiCmT5r", "sentence_index": 13, "text": "One of the main advantages of this approach is efficiency, which allows it to be used on large real-world datasets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "rkgfiCmT5r", "sentence_index": 14, "text": "The MTurk experiment gives a qualitative picture, but it could be improved with comparisons to pairwise distances learned through alternative means using the RGB image itself (given that images would permit such a comparison). By this I mean, that you may be able to use relationships learned using conventional triplet methods which use input RGB features as ground truth, and test your learned relationships against those.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "rkgfiCmT5r", "sentence_index": 16, "text": "However, since quantitative exploration of large real-world datasets may be challenging and expensive to collect, the synthetic experiments could have been more detailed.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkgfiCmT5r", "sentence_index": 17, "text": "The message of synthetic experiments would be stronger if more of them were available and if the comparison between LOE, TSTE, and OENN was made on more of them.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "rkgfiCmT5r", "sentence_index": 18, "text": "2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem. This claim can be made secondarily or as motivation for continued exploration along this direction, but I think listing them as two distinct contributions is necessary.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkgfiCmT5r", "sentence_index": 20, "text": "Additional feedback:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 21, "text": "Since quantitative real-world results are challenging to obtain, improved presentation of the qualitative results would be helpful as well.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkgfiCmT5r", "sentence_index": 22, "text": "You may be able to show more plots which help display the quality of the embedding space varying with the number of triplets used. For example, an additional plot after Figure 5 (b) which shows a few scatter plots of points (color coded by class) for training with different numbers of collected triplets.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rkgfiCmT5r", "sentence_index": 24, "text": "Also, since it should be fairly easy to distinguish between cars and animals or cars and food, it may be more interesting to focus on the heat-maps from along the block diagonal of Figure 5 (a) and talk about what relationships may have been uncovered within the animal or food subsets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rkgfiCmT5r", "sentence_index": 25, "text": "Very minor details:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rkgfiCmT5r", "sentence_index": 26, "text": "In Figure 5, a legend indicating the relationship between color intensity and distance would be helpful.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkgfiCmT5r", "sentence_index": 27, "text": "In Figure 6 there seem to be unnecessary discrepancies between the y-axis and colorbar of subplots (a) and (b), and keeping those more consistent would improve readability.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkgoA2UCKB", "sentence_index": 0, "text": "This paper presents 4D convolutional neural networks for video-level representations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkgoA2UCKB", "sentence_index": 1, "text": "To learn long-range evolution of spatio-temporal representation of videos, the authors proposed V4D convolution layer.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkgoA2UCKB", "sentence_index": 2, "text": "Benchmark on several video classification dataset shows improvement.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rkgoA2UCKB", "sentence_index": 3, "text": "1. In section 3.1, the authors selected a snippet from each section, but this was not rigorously defined.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkgoA2UCKB", "sentence_index": 4, "text": "Same for action units.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkgoA2UCKB", "sentence_index": 5, "text": "It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkgoA2UCKB", "sentence_index": 6, "text": "2. In section 3.2, the authors argued that 3D kernel suffers from trade-off between receptive field and cost of computation.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rkgoA2UCKB", "sentence_index": 7, "text": "At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkgoA2UCKB", "sentence_index": 8, "text": "3D convolution is already expensive and not scalable, but 4D operation sounds even more expensive and more prohibitive.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rkgoA2UCKB", "sentence_index": 9, "text": "3. In the paper, the authors argued that clip-level feature learning is limited as it is hard to learn long-range spatio-temporal dependency.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rkgoA2UCKB", "sentence_index": 10, "text": "It makes sense, and I expect the proposed model may benefit from its design for long-range spatio-temporal feature learning.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rkgoA2UCKB", "sentence_index": 11, "text": "However, what I see in the experiments is on ~300 frames for Mini-Kinetics and 36-72 frames for Something-Something dataset.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rkgoA2UCKB", "sentence_index": 12, "text": "Assuming that a second is represented with 15-30 frames, this corresponds to 10-20 sec and 1-4 sec, respectively.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rkgoA2UCKB", "sentence_index": 13, "text": "I'd say these short videos are still clips.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkgoA2UCKB", "sentence_index": 14, "text": "The paper presents an interesting idea, but there are some issues that need to be addressed before published on ICLR.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rklz9YLKh7", "sentence_index": 0, "text": "The paper design a low variance gradient for distributions associated with continuous or discrete random variables.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rklz9YLKh7", "sentence_index": 1, "text": "The gradient is designed in the way to approximate the  property of reparameterization gradient.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rklz9YLKh7", "sentence_index": 2, "text": "The paper is comprehensive and includes mathematical details.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rklz9YLKh7", "sentence_index": 3, "text": "I have following comments/questions", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rklz9YLKh7", "sentence_index": 4, "text": "1. What is the \\kappa in \u201cvariable-nabla\u201d stands for? What is the gradient w.r.t. \\kappa?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rklz9YLKh7", "sentence_index": 5, "text": "2. In Eq(8), does the outer expectation w.r.t . y_{-v} be approximated by one sample? If so, it is using the local expectation method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rklz9YLKh7", "sentence_index": 6, "text": "How does that differs from Titsias & Lazaro-Gredilla(2015) both mathematically and experimentally?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rklz9YLKh7", "sentence_index": 7, "text": "3. Assume y_v is M-way categorical distribution, Eq(8) evaluates f by 2*V*M times which can be computationally expensive.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rklz9YLKh7", "sentence_index": 8, "text": "What is the computation complexity of GO? How to explain the fast speed shown in the experiments?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rklz9YLKh7", "sentence_index": 9, "text": "4. A most simple way to reduce the variance of REINFORCE gradient is to take multiple Monte-Carlo samples at the cost of more computation with multiple function f evaluations.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rklz9YLKh7", "sentence_index": 10, "text": "Assume GO gradient needs to evaluate f N times, how does the performance compared with the REINFORCE gradient with N Monte-Carlo samples?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rklz9YLKh7", "sentence_index": 11, "text": "5. In the discrete VAE experiment, upon brief checking the results in Grathwohl(2017), it shows validation ELBO for MNIST as (114.32,111.12), OMNIGLOT as (122.11,128.20) from which two cases are better than GO.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "rklz9YLKh7", "sentence_index": 12, "text": "Does the hyper parameter setting favor the GO gradient in the reported experiments?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rklz9YLKh7", "sentence_index": 13, "text": "Error bar may also be needed for comparison.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "rklz9YLKh7", "sentence_index": 14, "text": "What about the performance of GO gradient in the 2 stochastic layer setting in Grathwohl(2017)?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rklz9YLKh7", "sentence_index": 15, "text": "6. The paper claims GO has less parameters than REBAR/RELAX. But in Figure 9, GO has more severe overfitting. How to explain this contradicts between the model complexity and overfitting?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rkx6mDnd37", "sentence_index": 0, "text": "The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkx6mDnd37", "sentence_index": 1, "text": "Unfortunately the paper falls short in two main areas:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rkx6mDnd37", "sentence_index": 2, "text": "- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "rkx6mDnd37", "sentence_index": 3, "text": "- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkx6mDnd37", "sentence_index": 4, "text": "However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "rkxBU7f0qH", "sentence_index": 0, "text": "The paper presents a Neural Network based method for learning ordinal embeddings only from triplet comparisons.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkxBU7f0qH", "sentence_index": 1, "text": "A nice, easy to read paper, with an original idea.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "rkxBU7f0qH", "sentence_index": 2, "text": "Still, there are some issues the authors should address:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rkxBU7f0qH", "sentence_index": 3, "text": "- for the experiment with Imagenet images, it is not very clear how many pictures are used. Is this number 2500?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 4, "text": "- the authors state that they use \"the power of DNNs\" while they are experimenting with a neural network with only 4 layers. While there is no clear line between shallow and deep neural networks, I would argue that a 4 layer NN is rather shallow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 5, "text": "- the authors fix the number of layers of the used network based on \"our experience\". For the sake of completeness, more experiments in this area would be nice.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 6, "text": "- for Figure 6, there is not a clear conclusion.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 7, "text": "While, it supports that \" that logarithmic growth of the layer width respect to n is enough to obtain desirable performance.\"  I don't see a clear conclusion of how to pick the width of hidden layers, maybe a better representation could be used.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 8, "text": "- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 9, "text": "- in section 4.4 when comparing the proposed approach with another methods why not use more complex datasets (like those used in section 4.3)", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 11, "text": "- in section 4.3, there is no guarantee that the intersection between the training set and test set is empty.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 12, "text": "- in section 4.3 how is the reconstruction built (Figure 3b)?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 13, "text": "A few typos found:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rkxBU7f0qH", "sentence_index": 14, "text": "- In figure 3 (c) \"number |T of input\" should be  \"number |T| of input\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 15, "text": "- In figure 5 (a) \"cencept\" should be \"concept\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 16, "text": "- In figure 8 \"Each column corresponds to ...\" should be \"Each row corresponds to ...\".", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 17, "text": "- In the last paragraph of A1 \"growth of the layer width respect\" should be \"growth of the layer width with respect\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 18, "text": "- In the second paragraph of A2 \"hypothesize the that relation\" should be \"hypothesize that the relation\".", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxBU7f0qH", "sentence_index": 19, "text": "- In section 4.3 last paragraph, first sentence: \"with the maximunm number\" should be \"with the maximum number\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxUMYW6hX", "sentence_index": 0, "text": "The paper proposes a recurrent knowledge graph (bipartite graph between entities and location nodes) construction & updating mechanism for entity state tracking datasets such as (two) ProPara tasks and Recipes.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkxUMYW6hX", "sentence_index": 1, "text": "The model goes through the following three steps: 1) it reads a sentence at each time step t and identifies the location of each entity via machine reading comprehension model such as DrQA (entities are predefined).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkxUMYW6hX", "sentence_index": 2, "text": "2) Co-reference module adjusts relationship scores (soft adjacency matrix) among nodes, including possibly new nodes introduced by the MRC model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkxUMYW6hX", "sentence_index": 3, "text": "3) to propagate the relational information across all the nodes, the model performs L layers of LSTM for each entity that attend on other nodes via attention (where the weights come from the adjacency matrix).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkxUMYW6hX", "sentence_index": 4, "text": "The model repeats the three steps for each sentence.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkxUMYW6hX", "sentence_index": 5, "text": "The model is trained by directly supervising for the correct span by the MRC model at each time step, which is possible because the data provides strong supervision for each sentence (not just the answer at the end).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkxUMYW6hX", "sentence_index": 6, "text": "The model achieves the state of the art in the two tasks of ProPara and Recipes dataset.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkxUMYW6hX", "sentence_index": 7, "text": "Strengths: The paper provides an elegant solution for tracking relationship between entities as time (sentence) progresses.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rkxUMYW6hX", "sentence_index": 8, "text": "I also agree with the authors that this line of work (dynamic KG construction and modification) is an important area of research.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rkxUMYW6hX", "sentence_index": 9, "text": "While the model shares a similar spirit to EntNet, I think the model has enough distinctions / contributions, especially given that it outperforms EntNet by a large margin.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "rkxUMYW6hX", "sentence_index": 10, "text": "The model also obtains non-trivial improvement over previous SOTA models.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "rkxUMYW6hX", "sentence_index": 11, "text": "Weaknesses: Paper could have been written better. I had hard time understanding it.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxUMYW6hX", "sentence_index": 12, "text": "The notations are overall confusing and not explained well.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxUMYW6hX", "sentence_index": 13, "text": "Also there are a few unclear parts which I discuss in questions below.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rkxUMYW6hX", "sentence_index": 14, "text": "Questions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rkxUMYW6hX", "sentence_index": 15, "text": "1. Are e_{i,t} and lambda_{i,t} vectors? Scalars? Abstract node notations? It is not clear in the model section.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxUMYW6hX", "sentence_index": 18, "text": "Also, it took me a long time to figure out that \u2018i\u2019 is used to index each entity (it is mentioned later).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxUMYW6hX", "sentence_index": 19, "text": "2. The paper says v_i (initial representation of each entity) is obtained by looking at the contextualized representations (LSTM outputs) of entity mention in the context.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "rkxUMYW6hX", "sentence_index": 20, "text": "What happens if there are multiple mentions in the text? Which one does it look at?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rkxUMYW6hX", "sentence_index": 21, "text": "3. For the LSTM in the graph update, why does it have only one input? Shouldn\u2019t it have two inputs, one for previous hidden state and the other for input?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rkxUMYW6hX", "sentence_index": 22, "text": "4. Regarding Recipe experiments, the paper says it reaches a better performance than the baseline using just 10k examples out of 60k.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "rkxUMYW6hX", "sentence_index": 23, "text": "This is great, but could you also report the number when the full dataset is used?", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "rkxUMYW6hX", "sentence_index": 24, "text": "5. What does it mean that in training time the model \u201cupdates\u201d the location node representation with the encoding of correct span. Do you mean you use the encoding instead?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rkxUMYW6hX", "sentence_index": 25, "text": "6. For ProPara task 2, what threshold did you choose to obtain the P/R/F1 score?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "rkxUMYW6hX", "sentence_index": 26, "text": "Is it the threshold that maximizes F1?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rkxwCK8chQ", "sentence_index": 0, "text": "The paper introduces a generative model for video prediction.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkxwCK8chQ", "sentence_index": 1, "text": "The originality stems from a new training criterion which combines a VAE and a GAN criteria.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkxwCK8chQ", "sentence_index": 2, "text": "At training time, the GAN and the VAE are trained simultaneously with a shared generator; at test time, prediction conditioned on initial frames is performed by sampling from a latent distribution and generating the next frames via an enhanced conv LST .", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkxwCK8chQ", "sentence_index": 3, "text": "Evaluations are performed on two movement video datasets classically used for benchmarking  this task - several quantitative evaluation criteria are considered.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkxwCK8chQ", "sentence_index": 4, "text": "The paper clearly states the objective and provides a nice general description of the method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rkxwCK8chQ", "sentence_index": 5, "text": "The proposed model extends previous work by adding an adversarial loss to a VAE video prediction model.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rkxwCK8chQ", "sentence_index": 6, "text": "The evaluation compares different variants of this model to two recent VAE baselines.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_positive"}, {"review_id": "rkxwCK8chQ", "sentence_index": 7, "text": "A special emphasis is put on the quantitative evaluation: several criteria are introduced for characterizing different properties of the models with a focus on diversity. w.r.t. the baselines, the model behaves well for the \u201crealistic\u201d and \u201cdiversity\u201d measures. The results are more mitigated for measures of accuracy.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rkxwCK8chQ", "sentence_index": 10, "text": "As for the qualitative evaluation, the model corrects the blurring effect of the reference SV2P baseline, and produces quite realistic predictions on these datasets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "rkxwCK8chQ", "sentence_index": 11, "text": "The difference with the other reference model (SVG) is less clear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxwCK8chQ", "sentence_index": 12, "text": "While the general description of the model is clear, details are lacking.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxwCK8chQ", "sentence_index": 13, "text": "It would probably help to position the VAE component more precisely w.r.t. one of the two baselines, by indicating the differences.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "rkxwCK8chQ", "sentence_index": 14, "text": "This would also help to explain the difference of performance/ behavior  w.r.t. these models (Fig. 5).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rkxwCK8chQ", "sentence_index": 15, "text": "It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rkxwCK8chQ", "sentence_index": 16, "text": "Similarly, you did not indicate what the deterministic version of your model is.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxwCK8chQ", "sentence_index": 17, "text": "The generator model with its warping component makes a strong hypothesis on the nature of the videos: it seems especially well suited for translations or for other simple geometric transformations characteristics of the benchmarking videos .", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rkxwCK8chQ", "sentence_index": 18, "text": "Could you comment on the importance of this component? Did you test the model on other types of videos where this hypothesis is less relevant?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rkxwCK8chQ", "sentence_index": 19, "text": "It seems that the baseline SVG makes use of simpler ConLSTM for example.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rkxwCK8chQ", "sentence_index": 20, "text": "The description of the generator in the appendix is difficult to follow. I missed the point in the following sentence: \u201cFor each one-step prediction, the network has the freedom to choose to copy pixels from the previous frame, used transformed versions of the previous frame, or to synthesize pixels from scratch\u201d .", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxwCK8chQ", "sentence_index": 22, "text": "Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rkxwCK8chQ", "sentence_index": 23, "text": "Overall, the paper proposes an extension of VAE based video prediction models and produces an extensive evaluation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkxwCK8chQ", "sentence_index": 24, "text": "While the model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rye5LcLAYH", "sentence_index": 0, "text": "This paper proposed to train a network with training curves and corresponding parameters, and use policy search to find optimal parameter to replace hundreds or thousands of training in real case scenario, and it is clearly much faster using the trained network to infer parameters, instead of tuning the network manually.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rye5LcLAYH", "sentence_index": 1, "text": "The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rye5LcLAYH", "sentence_index": 2, "text": "The cited paper 'Learning an adaptive learning rate schedule' does not appear online.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "ryeSkAjN37", "sentence_index": 0, "text": "Paper summary - This paper extends the differentiable plasticity framework of Miconi et al. (2018) by dynamically modulating the plasticity learning rate.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryeSkAjN37", "sentence_index": 1, "text": "This is accomplished via an output unit of the network which defines the plasticity learning rate for the next timestep.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryeSkAjN37", "sentence_index": 2, "text": "A variation on this dynamic learning rate related to eligibility traces is also proposed.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryeSkAjN37", "sentence_index": 3, "text": "Both dynamic modulation variations strikingly outperform non-plastic and plastic non-modulated recurrent networks on a cue-reward association task with high-dimensional cues.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryeSkAjN37", "sentence_index": 4, "text": "The methods marginally outperform plastic non-modulated recurrent networks on a 9x9 water maze task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryeSkAjN37", "sentence_index": 5, "text": "Finally, the authors show that adding dynamic plasticity to a small LSTM without dropout improves performance on Penn Treebank.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryeSkAjN37", "sentence_index": 6, "text": "The paper motivates dynamic plasticity by analogy to the hypothesized role of dopamine in reward-driven learning in humans and animals.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryeSkAjN37", "sentence_index": 7, "text": "Clarity -  The paper is very clear and well written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "ryeSkAjN37", "sentence_index": 8, "text": "The introduction provides useful insights, motivates the work convincingly, and provides interesting connections to past work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "ryeSkAjN37", "sentence_index": 9, "text": "Originality - I don't know of any other work that models the role of dopamine in quite this way, or that applies dynamic plasticity modulation in settings like these.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "ryeSkAjN37", "sentence_index": 10, "text": "Quality - The experiments are well chosen and seem technically sound.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ryeSkAjN37", "sentence_index": 11, "text": "Significance - The results show that meta-learning by gradient descent to modulate the plasticity learning rate is a promising direction -- a significant contribution in my view.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "ryeSkAjN37", "sentence_index": 12, "text": "Other Comments - The citation to Zaremba et al. in Table 1 made it seem like the perplexity result on that line of the table was directly from Zaremba et al's paper. I'd recommend removing the citation from that line to avoid confusion.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "ryeSkAjN37", "sentence_index": 13, "text": "One thing I would have loved to see from this paper is a comparison of modulated-plasticity LSTMs with the sota from Melis et al., 2017.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "ryeSkAjN37", "sentence_index": 14, "text": "I gather that Experiment 3 presents small LSTMs without recurrent dropout instead because combining plasticity and dropout proved challenging (or at least the authors haven't tried it yet).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "ryeSkAjN37", "sentence_index": 15, "text": "I think the paper is solid as-is; positive results in this comparison would take it to the next level.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "ryeSkAjN37", "sentence_index": 16, "text": "Questions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ryeSkAjN37", "sentence_index": 17, "text": "Why were zero-sequences necessary in Experiment 1?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "ryeSkAjN37", "sentence_index": 18, "text": "This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "ryeSkAjN37", "sentence_index": 19, "text": "Perhaps the authors could clarify on what a confounding \"time-locked scheduling strategy\" would look like in this task?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "ryeSkAjN37", "sentence_index": 20, "text": "Why does Experiment 1 present pairs of stimuli, rather than high-dimensional individual stimuli?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "ryeSkAjN37", "sentence_index": 21, "text": "Why is non-plastic rnn left out of Figure 2b?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "ryeSkAjN37", "sentence_index": 22, "text": "Typos", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ryeSkAjN37", "sentence_index": 23, "text": "\"However, in Nature,\" -- no caps", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "ryeSkAjN37", "sentence_index": 24, "text": "in appendix: \"(see Figure A.4)\" -- the figure is labeled \"Figure 3\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 0, "text": "This paper proposes an extension to the continual learning framework using existing variational continual learning (VCL) as the base method.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 1, "text": "In particular, it proposes to use the weight of evidence (WE) (from Zintgraf et al 2017) for each task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 2, "text": "Firstly, this WE can be used to visualize the learned model (as used in Zintgraf et. al. 2017).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 3, "text": "The novelty of this paper is:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 4, "text": "1. to use this WE from the current task to generate a silence map (by smoothing the WE) for the next task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 5, "text": "This is interpreted the learned the learned attention region.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 6, "text": "Such an approach is named Interpretable COntinual Learning (ICL)", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 7, "text": "2. The paper proposes a metric for the saliency map naming FSM which is an extension of existing metric SSR.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 8, "text": "The extension is to take pixel count to compute the area instead of using rectangular region area, as well as taking the distance between pixels into account.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 9, "text": "This metric can be used to evaluate the level of catastrophic forgetting.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 10, "text": "Pro:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 11, "text": "In general, the idea is very intuitive and make sense.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 12, "text": "The paper also demonstrates superior performance with the proposed method on continual learning on all classic tasks comparing with VCL and EWC.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 13, "text": "The presentation is very easy to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 14, "text": "It seems like a valid and flexible extension that can be used in other continual learning frameworks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 15, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 16, "text": "The theoretical contribution is very limited.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 17, "text": "The work is rather incremental from current state-of-the-art methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 18, "text": "There should be a better discussion of related work on the topic.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 19, "text": "The paper currently only mentions the most related work for the proposed method,  using the whole section 2 to describe VCL and use section 3 to describe FSM and half of section 5 to describe SSR.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 20, "text": "A general overview of related work in these directions are needed.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 21, "text": "Other:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 22, "text": "1. The paper should also consider more recently proposed evaluation metrics such as discussed in https://arxiv.org/pdf/1805.09733.pdf", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "ryg4Fgr9hX", "sentence_index": 23, "text": "2. The author should try to avoid using yellow color in plots.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "rygRU5E5nm", "sentence_index": 0, "text": "Overview:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rygRU5E5nm", "sentence_index": 1, "text": "This paper proposes modifications to the original Differentiable Neural Computer architecture in three ways.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rygRU5E5nm", "sentence_index": 2, "text": "First by introducing a masked content-based addressing which dynamically induces a key-value separation .", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rygRU5E5nm", "sentence_index": 4, "text": "Second, by modifying the de-allocation system by also multiplying the memory contents by a retention vector before an update .", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rygRU5E5nm", "sentence_index": 6, "text": "Finally, the authors propose a modification in the link distribution, through renormalization.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rygRU5E5nm", "sentence_index": 7, "text": "They provide some theoretical motivation and empirical evidence that it helps avoiding memory aliasing.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rygRU5E5nm", "sentence_index": 8, "text": "The authors test their approach in the some algorithm task from the DNC paper (Copy, Associative Recall and Key-Value Retrieval), and also in the bAbi dataset.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rygRU5E5nm", "sentence_index": 9, "text": "Strengths: Overall I think the paper is well-written, and proposes simple adaptions to the DNC architecture which are theoretically grounded and could be effective for improving general performance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rygRU5E5nm", "sentence_index": 10, "text": "Although the experimental results seem promising when comparing the modified architecture to the original DNC, in my opinion there are a few fundamental problems in the empirical session (see weakness discussion bellow).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rygRU5E5nm", "sentence_index": 11, "text": "Weaknesses: Not all model modifications are studied in all the algorithmic tasks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rygRU5E5nm", "sentence_index": 12, "text": "For example, in the associative recall and key-value retrieval only DNC and DNC + masking are studied.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "rygRU5E5nm", "sentence_index": 13, "text": "For the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "rygRU5E5nm", "sentence_index": 14, "text": "Moreover, the sparse DNC (Rae el at., 2016) is already a much better performant in this task. (mean error DNC: 16.7 \\pm 7.6, DNC-MD (this paper) 9.5 \\pm 1.6, sparse DNC 6.4 \\pm 2.5).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rygRU5E5nm", "sentence_index": 16, "text": "Although the authors mention in the conclusion that it's future work to merge their proposed changes into the sparse DNC, it is hard to know how relevant the improvements are, knowing that there are much better baselines for this task.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rygRU5E5nm", "sentence_index": 17, "text": "It would also be good if besides the mean error rates, they reported best runs chosen by performance on the validation task, and number of the tasks solve (with < 5% error) as it is standard in this dataset.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rygRU5E5nm", "sentence_index": 18, "text": "Smaller Notes.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rygRU5E5nm", "sentence_index": 19, "text": "1) In the abstract, I find the message for motivating the masking from the sentence  \"content based look-up results... which is not present in the key and need to be retrieved.\"  hard to understand by itself. When I first read the abstract, I couldn't understand what the authors wanted to communicate with it. Later in 3.1 it became clear. 2) page 3, beta in that equation is not defined", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rygRU5E5nm", "sentence_index": 21, "text": "3) First paragraph in page 5 uses definition of acronyms DNC-MS and DNC-MDS before they are defined.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rygRU5E5nm", "sentence_index": 22, "text": "4) Table 1 difference between DNC and DNC (DM) is not clear. I am assuming it's the numbers reported in the paper, vs the author's implementation?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rygRU5E5nm", "sentence_index": 23, "text": "5)In session 3.1-3.3, for completeness. I think it would be helpful to explicitly compare the equations from the original DNC paper with the new proposed ones.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rygRU5E5nm", "sentence_index": 24, "text": "--------------", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rygRU5E5nm", "sentence_index": 25, "text": "Post rebuttal update: I think the authors have addressed my main concern points and I am updating my score accordingly.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryg_N1TK2Q", "sentence_index": 0, "text": "This paper studies the dynamics-based curiosity intrinsic reward where the agent is rewarded highly in states where the forward dynamic prediction errors are high in an embedding space (either due to complexity of the state or unfamiliarity).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryg_N1TK2Q", "sentence_index": 1, "text": "Overall I like the paper, it's systematic and follows a series of practical considerations and step-by-step experimentations.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "ryg_N1TK2Q", "sentence_index": 2, "text": "One of the main area which is missing in the paper is the comparison to two other class of RL methods: count-based exploration and novelty search.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "ryg_N1TK2Q", "sentence_index": 3, "text": "While the section 4 has a discussion on related papers, there's no systematic experimental comparison across these methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "ryg_N1TK2Q", "sentence_index": 4, "text": "In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "ryg_N1TK2Q", "sentence_index": 5, "text": "Another area of improvement is the experiments around VAE.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "ryg_N1TK2Q", "sentence_index": 6, "text": "While the paper shows experimentally that they aren't as successful as the RFs or IDFs, there's no further discussion on the reasons for poor performance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "ryg_N1TK2Q", "sentence_index": 7, "text": "Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "ryg_N1TK2Q", "sentence_index": 8, "text": "An interesting area for future work could be on early stopping techniques for embedding training - it seems that RFs perform well without any training while in some scenarios the IDFs work overall the best.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "ryg_N1TK2Q", "sentence_index": 9, "text": "So it would be interesting to explore how much training is needed for the embedding model.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "ryg_N1TK2Q", "sentence_index": 10, "text": "RFs are never trained and IDFs are continuously trained.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "ryg_N1TK2Q", "sentence_index": 11, "text": "So maybe somewhere in between could be the sweet spot with training for a short while and then fixing the features.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 1, "text": "This paper proposes Direct Sparse Optimization (DSO)-NAS, which is a method to obtain neural architectures on specific problems, at a reasonable computational cost.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 2, "text": "The main idea is to treat all architectures as a Directed Acyclic Graph (DAG), where each architecture is realized by a subgraph.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 3, "text": "All architectures in the search space thus share their weights, like ENAS (Pham et al 2018) and DARTS (Liu et al 2018a).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 4, "text": "The DAG\u2019s edges can be pruned via a sparsity regularization term.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 5, "text": "The optimization objective of DSO-NAS is thus:", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 6, "text": "Accuracy + L2-regularization(W) + L1-sparsity(\\lambda),", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 7, "text": "where W is the shared weights and \\lambda specifies which edges in the DAG are used.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 8, "text": "There are 3 phases of optimization:", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 9, "text": "1. All edges are activated and the shared weights W are trained using normal SGD.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 10, "text": "Note that this step does not involve \\lambda.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 11, "text": "2. \\lambda is trained using Accelerated Proximal Gradient (APG, Huang and Wang 2018).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 12, "text": "3. The best architecture is selected and retrained from scratch.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 13, "text": "This procedure works for all architectures and objectives.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 14, "text": "However, DSO-NAS further proposes to incorporate the computation expense of architectures into step (2) above, leading to their found architectures having fewer parameters and a smaller FLOP counts.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 15, "text": "Their experiments confirm all the hypotheses (DSO-NAS can find architectures, having small FLOP counts, having good performances on CIFAR-10 and ImageNet).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 16, "text": "Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 17, "text": "1. Regularization by sparsity is a neat idea.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "rylgsNqchQ", "sentence_index": 18, "text": "2. The authors claim to be the first NAS algorithm to perform direct search on ImageNet.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 19, "text": "Honestly, I cannot confirm this claim (not sure if I have seen all NAS papers out there), but if it is the case, then it is impressive.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "rylgsNqchQ", "sentence_index": 20, "text": "3. Incorporating architecture costs into the search objective is nice.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "rylgsNqchQ", "sentence_index": 21, "text": "However, this contribution seems to be orthogonal to the sparsity regularization, which, I suppose, is the main point of the paper.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 22, "text": "Weaknesses:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 23, "text": "1. Some experimental details are missing. I\u2019m going to list them here:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 24, "text": "- Was the auxiliary tower used during the training of the shared weights W?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 25, "text": "- Figure 4 does not illustrate M=4 and N=4, e.g. which operation belongs to which layer?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 26, "text": "- Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule [1]? If or if not, either way, you should specify it in a revised version of this paper, e.g. did you use the cosine schedule in the first 120 steps to train the shared parameters W, did you use it in the retraining from scratch?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rylgsNqchQ", "sentence_index": 27, "text": "- In Section 3.3, it is written that \u201cThe sparse regularization of \\lambda induces great difficulties in optimization\u201d.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 28, "text": "This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rylgsNqchQ", "sentence_index": 29, "text": "2. Missed citation: MnasNet [2] also incorporates the cost of architectures in their search process.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "rylgsNqchQ", "sentence_index": 30, "text": "On ImageNet, your performance is similar to theirs.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 31, "text": "I think this will be a good comparison.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rylgsNqchQ", "sentence_index": 32, "text": "3. The paper has some grammatical errors. I obviously missed many, but here are the one I found:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 33, "text": "- Section 3.3: \u201cDifferent from pruning, which the search space is usually quite limited\u201d. \u201cwhich\u201d should be \u201cwhose\u201d?", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rylgsNqchQ", "sentence_index": 34, "text": "- Section 4.4.1: \u201cDSO-NAS can also search architecture [...]\u201d  -> \u201cDSO-NAS can also search for architectures [...]\u201d", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rylgsNqchQ", "sentence_index": 35, "text": "References.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 36, "text": "[1] SGDR: Stochastic Gradient Descent with Warm Restarts. https://arxiv.org/pdf/1608.03983.pdf", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylgsNqchQ", "sentence_index": 37, "text": "[2] MnasNet: Platform-Aware Neural Architecture Search for Mobile. https://arxiv.org/pdf/1807.11626.pdf", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylJA7G82Q", "sentence_index": 0, "text": "The authors present an architecture search method where connections are removed with sparse regularization.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylJA7G82Q", "sentence_index": 1, "text": "It produces good network blocks relatively quickly that perform well on CIFAR/ImageNet.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylJA7G82Q", "sentence_index": 2, "text": "There are a few grammatical/spelling errors that need ironing out.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "rylJA7G82Q", "sentence_index": 3, "text": "e.g. \"In specific\" --> \"Specifically\" in the abstract, \"computational budge\" -> \"budget\" (page 6) etc.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rylJA7G82Q", "sentence_index": 4, "text": "A few (roughly chronological comments).", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rylJA7G82Q", "sentence_index": 5, "text": "- Pioneering work is not necessarily equivalent to \"using all the GPUs\"", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rylJA7G82Q", "sentence_index": 6, "text": "- There are better words than \"decent\" to describe the performance of DARTS, as it's very similar to the results in this work!", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "rylJA7G82Q", "sentence_index": 7, "text": "- From figure 2 it's not clear why all non-zero connections in (b) are then equally weighted in (c). Would keeping the non-zero weightings be at all helpful?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rylJA7G82Q", "sentence_index": 8, "text": "-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rylJA7G82Q", "sentence_index": 9, "text": "- How do you specifically encode the number of surviving connections? Is it entirely dependent on budget?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rylJA7G82Q", "sentence_index": 10, "text": "- You should add DARTS 1st order to table 1.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rylJA7G82Q", "sentence_index": 11, "text": "- Measuring in GPU days is only meaningful if you use the same GPU make for every experiment. Which did you use?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rylJA7G82Q", "sentence_index": 12, "text": "- The ablation study is good, and the results are impressive.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rylJA7G82Q", "sentence_index": 13, "text": "I propose a marginal acceptance for this paper as it produces impressive results in what appears to be a short amount of search time.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylJA7G82Q", "sentence_index": 14, "text": "However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified. ------------", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rylJA7G82Q", "sentence_index": 16, "text": "UPDATE: Score changed based on author resposne ------------", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylJdHwn2Q", "sentence_index": 0, "text": "The paper explores how well different visual reasoning models can learn systematic generalization on a simple binary task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylJdHwn2Q", "sentence_index": 1, "text": "They create a simple synthetic dataset, involving asking if particular types of objects are in a spatial relation to others.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylJdHwn2Q", "sentence_index": 2, "text": "To test generalization, they lower the ratio of observed  combinations of objects in the training data.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylJdHwn2Q", "sentence_index": 3, "text": "The authors show the result that tree structured neural module networks generalize very well, but other strong visual reasoning approaches do not.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylJdHwn2Q", "sentence_index": 4, "text": "They also explore whether appropriate structures can be learned.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylJdHwn2Q", "sentence_index": 5, "text": "I think this is a very interesting area to explore, and the paper is clearly written and presented.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylJdHwn2Q", "sentence_index": 6, "text": "As the authors admit, the main result is not especially surprising.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "rylJdHwn2Q", "sentence_index": 7, "text": "I think everyone agrees that we can design models that show particular kinds of generalization by carefully building inductive bias into the architecture, and that it's easy to make these work on the right toy data.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylJdHwn2Q", "sentence_index": 8, "text": "However, on less restricted data, more general architectures seem to show better generalization (even if it is not systematic).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylJdHwn2Q", "sentence_index": 9, "text": "What I really want this paper to explore is when and why this happens.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "rylJdHwn2Q", "sentence_index": 10, "text": "Even on synthetic data, when do or don't we see generalization (systematic or otherwise) from NMNs/MAC/FiLM?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rylJdHwn2Q", "sentence_index": 11, "text": "MAC in particular seems to have an inductive bias that might make some forms of systematic generalization possible.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylJdHwn2Q", "sentence_index": 12, "text": "It might be the case that their version of NMN can only really do well on this specific task, which would be less interesting.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylJdHwn2Q", "sentence_index": 13, "text": "All the models show very high training accuracy, even if they do not show systematic generalization.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "rylJdHwn2Q", "sentence_index": 14, "text": "That suggests that from the point of view of training, there are many equally good solutions, which suggests a number of interesting questions. If you did large numbers of training runs, would the models occasionally find the right solution? Could you somehow test for if a given trained model will show systematic generalization?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rylJdHwn2Q", "sentence_index": 15, "text": "Is there any way to help the models find the \"right\" (or better) solutions - e.g. adding regularization, or changing the model size?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rylJdHwn2Q", "sentence_index": 16, "text": "Overall, I do think the paper has makes a contribution in experimentally showing a setting where tree-structured NMNs can show better systematic generalization than other visual reasoning approaches.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rylJdHwn2Q", "sentence_index": 17, "text": "However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rylkjAtu2m", "sentence_index": 0, "text": "The paper examines an architectural feature in GAN generators -- self-modulation -- and presents empirical evidence supporting the claim that it helps improve modeling performance.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylkjAtu2m", "sentence_index": 1, "text": "The self-modulation mechanism itself is implemented via FiLM layers applied to all convolutional blocks in the generator and whose scaling and shifting parameters are predicted as a function of the noise vector z.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylkjAtu2m", "sentence_index": 2, "text": "Performance is measured in terms of Fr\u00e9chet Inception Distance (FID) for models trained with and without self-modulation on a fairly comprehensive range of model architectures (DCGAN-based, ResNet-based), discriminator regularization techniques (gradient penalty, spectral normalization), and datasets (CIFAR10, CelebA-HQ, LSUN-Bedroom, ImageNet).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylkjAtu2m", "sentence_index": 3, "text": "The takeaway is that self-modulation is an architectural feature that helps improve modeling performance by a significant margin in most settings.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylkjAtu2m", "sentence_index": 4, "text": "An ablation study is also performed on the location where self-modulation is applied, showing that it is beneficial across all locations but has more impact towards the later layers of the generator.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylkjAtu2m", "sentence_index": 5, "text": "I am overall positive about the paper: the proposed idea is simple, but is well-explained and backed by rigorous evaluation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rylkjAtu2m", "sentence_index": 6, "text": "Here are the questions I would like the authors to discuss further:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rylkjAtu2m", "sentence_index": 7, "text": "- The proposed approach is a fairly specific form of self-modulation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rylkjAtu2m", "sentence_index": 8, "text": "In general, I think of self-modulation as a way for the network to interact with itself, which can be a local interaction, like for squeeze-and-excitation blocks.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylkjAtu2m", "sentence_index": 9, "text": "In the case of this paper, the self-interaction allows the noise vector z to interact with various intermediate features across the generation process, which for me appears to be different than allowing intermediate features to interact with themselves.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylkjAtu2m", "sentence_index": 10, "text": "This form of noise injection at various levels of the generator is also close in spirit to what BigGAN employs, except that in the case of BigGAN different parts of the noise vector are used to influence different parts of the generator.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylkjAtu2m", "sentence_index": 11, "text": "Can you clarify how you view the relationship between the approaches mentioned above?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "rylkjAtu2m", "sentence_index": 12, "text": "- It\u2019s interesting to me that the ResNet architecture performs better with self-modulation in all settings, considering that one possible explanation for why self-modulation is helpful is that it allows the \u201cinformation\u201d contained in the noise vector to better propagate to and influence different parts of the generator.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylkjAtu2m", "sentence_index": 13, "text": "ResNets also have this ability to \u201cpropagate\u201d the noise signal more easily, but it appears that having a self-modulation mechanism on top of that is still beneficial.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylkjAtu2m", "sentence_index": 14, "text": "I\u2019m curious to hear the authors\u2019 thoughts in this.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rylkjAtu2m", "sentence_index": 15, "text": "- Reading Figure 2b, one could be tempted to draw a correlation between the complexity of the dataset and the gains achieved by self-modulation over the baseline (e.g., Bedroom shows less difference between the two approaches than ImageNet). Do the authors agree with that?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rylog8i62m", "sentence_index": 0, "text": "The paper is well written and easy to follow. The topic is apt.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "rylog8i62m", "sentence_index": 1, "text": "I don\u2019t have any comments except the following ones.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rylog8i62m", "sentence_index": 2, "text": "Lemma 2.4, Point 1: The proof is confusing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rylog8i62m", "sentence_index": 3, "text": "Consider the one variable vector case.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylog8i62m", "sentence_index": 4, "text": "Assuming that there is only one variable w, then \\nabla L(w) is not perpendicular to w in general.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylog8i62m", "sentence_index": 5, "text": "The Rayleigh quotient example L(w)  = w\u2019*A*w/ (w\u2019*w) for a symmetric matrix A, then \\nabla L(w) = (2/w\u2019*w)(Aw - L(w)*w), which is not perpendicular to w.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylog8i62m", "sentence_index": 6, "text": "Even if we constrain ||w ||_2 = 1 , then also  \\nabla L(w)  is not perpendicular to w.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "rylog8i62m", "sentence_index": 8, "text": "Am I missing something?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "rylog8i62m", "sentence_index": 9, "text": "What is G_t in Theorem 2.5. It should be defined in the theorem itself.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rylog8i62m", "sentence_index": 10, "text": "There is another symbol G_g which is a constant.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "rylvbv_93Q", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rylvbv_93Q", "sentence_index": 1, "text": "The role of auxiliary tasks is to improve the generalization performance of the principal task of interest.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylvbv_93Q", "sentence_index": 2, "text": "So far, hand-crafted auxiliary tasks are generated, tailored for a problem of interest.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylvbv_93Q", "sentence_index": 3, "text": "The current work addresses a meta-learning approach to automatically generate auxiliary tasks suited to the principal task, without human knowledge.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylvbv_93Q", "sentence_index": 4, "text": "The key components of the method are: (1) meta-generator; (2) multi-task evaluator.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylvbv_93Q", "sentence_index": 5, "text": "These two models are trained using the gradient-based meta-learning technique (for instance, MAML).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rylvbv_93Q", "sentence_index": 6, "text": "The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "rylvbv_93Q", "sentence_index": 7, "text": "Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rylvbv_93Q", "sentence_index": 8, "text": "- To my best knowledge, the idea of applying the meta-learning to the automatic generation of auxiliary tasks is novel.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "rylvbv_93Q", "sentence_index": 9, "text": "- The paper is well written and easy to read.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "rylvbv_93Q", "sentence_index": 10, "text": "- The method nicely blends a few components such as self-supervised learning, meta-learning, auxiliary tasks into a single model to tackle the meta auxiliary learning.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "rylvbv_93Q", "sentence_index": 11, "text": "Weakness:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "rylvbv_93Q", "sentence_index": 12, "text": "- The performance gain is not substantial in experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rylvbv_93Q", "sentence_index": 13, "text": "I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks. You can refer to the state-of-the-arts performance on CIFAR.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "rylvbv_93Q", "sentence_index": 15, "text": "- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "ryxs4ypg9r", "sentence_index": 0, "text": "This paper proposes two new architectures for processing set-structured data: An RNN with an accumulator on its output, and an RNN with gating followed by an accumulator on its output.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxs4ypg9r", "sentence_index": 1, "text": "While sensible, this seems to me to be too minor a contribution to stand alone as a paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "ryxs4ypg9r", "sentence_index": 2, "text": "Additionally, I believe the experimental tasks are new, and as a result all implementations of competing techniques are by the paper authors. This makes it difficult to have confidence in the higher reported performance of the proposed techniques.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "ryxUOGLHcH", "sentence_index": 0, "text": "This paper proposed a few-shot graph classification algorithm based on graph neural networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxUOGLHcH", "sentence_index": 1, "text": "The learning is based on a large set of base class labeled graphs and a small set of novel class labeled graphs.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxUOGLHcH", "sentence_index": 2, "text": "The goal is to learn a classification algorithm over the novel class based on the sample from the base class and novel class.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxUOGLHcH", "sentence_index": 3, "text": "The learning process constitutes of the following steps.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxUOGLHcH", "sentence_index": 4, "text": "First, the base class is classified into K super classes based on the spectral embedding of the graph (onto distributions over the corresponding graph spectrum) and the k-means algorithm with the Wasserstein metric.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxUOGLHcH", "sentence_index": 5, "text": "Second, for each super class, the classification is done through a feature extractor and a classifier.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxUOGLHcH", "sentence_index": 6, "text": "In the training of the feature extractor and classifier, the author introduces a super-graph with each node representing a super class.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxUOGLHcH", "sentence_index": 7, "text": "Finally, in the fine-tuning stage, the feature extractor is fixed, and the classifier is trained based on the novel class.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxUOGLHcH", "sentence_index": 8, "text": "This work seems to be the first attempt to adopt the few-shot learning in graph classification tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxUOGLHcH", "sentence_index": 9, "text": "The architecture is novel, and the classification of graph based on spectral embedding together with the Wasserstein metric is novel to me.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "ryxUOGLHcH", "sentence_index": 10, "text": "I vote for rejecting this submission for the following concerns.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ryxUOGLHcH", "sentence_index": 11, "text": "1. The classification of base class into super classes seems questionable to me.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "ryxUOGLHcH", "sentence_index": 12, "text": "In the meta-learning language, the author attempts to learn a good representation of graphs based on different graph classification tasks generated by a task distribution.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxUOGLHcH", "sentence_index": 13, "text": "In terms of graph classification, the task distribution is supported on the joint distributions (G, Y).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxUOGLHcH", "sentence_index": 14, "text": "Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "ryxUOGLHcH", "sentence_index": 15, "text": "2. Though seemingly very important to the architecture, the purpose of constructing the super-graph g^{sup} in the training of C^{CAT} seems to be unclear to me.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "ryxUOGLHcH", "sentence_index": 16, "text": "I would appreciate it if the author could provide more explanation on the introduction of the super-graph in training.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 0, "text": "The paper extends previous work on differentiable placticity to include neuro modulation by parameterizing the learning rate of Hebbs update rule.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 1, "text": "In addition, the authors introduce retroactive modulation that basically allows the system to delay incorporation of plasticity updates via so eligibility traces.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 2, "text": "Experiments are performaed on 2 simple toy datasets and a simple language modeling task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 3, "text": "A newly developed cue-reward association task shows the clear limitations of basic plasticity and how modulation can resolve this.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 4, "text": "Slight improvements can also be seen on a simple maze navigation task as well as on a basic language modeling dataset.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 5, "text": "Overall I like the motivation, provided background information and simplicity of the approach.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 6, "text": "Furthermore, the cue-reward experiment seems to be a well designed show case for neuro-modulation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 7, "text": "However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 8, "text": "Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 9, "text": "Therefore, although I would like to see an extended version of this paper at the conference, without further experiments and analysis I see the current version rather as an interesting workshop contribution.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 10, "text": "Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 11, "text": "- motivation: the natural extension of previous work on differentiable plasticity based on existing knowledge from neuro science is an important next step", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 12, "text": "- cue reward experiment exemplifies limitations of current plasticity approaches and clearly shows the potential benefits of neuro modulation", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 13, "text": "- maze navigation shows incremental benefits over non-modulated plasticity", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 14, "text": "- thorough experimentation", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 15, "text": "- clipping-trick is a neat observation", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 16, "text": "Weaknesses:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 17, "text": "- evaluation: only on toy tasks (which includes PTB), no real world tasks", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 18, "text": "- very incremental improvements on PTB over a very simple baseline (far from SotA)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 19, "text": "- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 20, "text": "- no qualitative analysis on how modulation is actually use by the systems.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 21, "text": "E.g., when is modulation strong and when is it not used", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 22, "text": "Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 23, "text": "- perplexity improvements of less than 1.3 points over plasticity alone (which is the actual baseline for this paper) can hardy be called \"significant\". Even though they might be statistically significant (meaning nothing more than the two models being statistically different), minor architectural changes can lead to such improvements.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "ryxWDI_Gsm", "sentence_index": 24, "text": "Furthermore PTB is not a \"challenging\" LM benchmark.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1e1sc_GqB", "sentence_index": 0, "text": "This paper proposes meta dropout, which leverages adaptive dropout training for regularizing gradient based meta learning models, e.g., MAML and MetaSGD.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1e1sc_GqB", "sentence_index": 1, "text": "Experiments on few shot learning show that meta dropout achieves better performance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "S1e1sc_GqB", "sentence_index": 2, "text": "Overally, I think this paper is well motivated and experiments on few shot learning are impressive.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "S1e1sc_GqB", "sentence_index": 3, "text": "I have only two major concerns.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1e1sc_GqB", "sentence_index": 4, "text": "1. Sec 3.2. According to my understanding, Meta dropout introduces a learnable prior for latent $z$, but the training objective does not require posterior inference and thus no variational inference is needed.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1e1sc_GqB", "sentence_index": 5, "text": "I think it is ok to say that meta dropout tries to optimize a lower bound of log p(Y|X;\\theta,\\phi^*), but meta dropout does not regularize the variational framework because there is no variational inference framework.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "S1e1sc_GqB", "sentence_index": 6, "text": "2.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1e1sc_GqB", "sentence_index": 7, "text": "Experiments on adversarial robustness can be further improved.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1e1sc_GqB", "sentence_index": 8, "text": "(1) the settings and the analysis of adversarial robustness experiment can be discussed in details.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1e1sc_GqB", "sentence_index": 9, "text": "For example, how to build ''adversarial learning baseline'' in meta learning settings and why the result implies the perturbation directions for generalization and robustness relates to each other; (2) how other regularization methods (e.g., Mixup, VIB and Information dropout) perform on adversarial robustness? Does Meta dropout performs better than them?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1e1sc_GqB", "sentence_index": 10, "text": "(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1e1sc_GqB", "sentence_index": 11, "text": "I suggest trying some other STOA attack methods (e.g., iterative methods).", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1e1sc_GqB", "sentence_index": 12, "text": "Some typos:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1e1sc_GqB", "sentence_index": 13, "text": "Page 3, Regularization methods, 3rd line, ````wwwdiscuss", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "S1e1sc_GqB", "sentence_index": 14, "text": "Page 7, 2nd line from the bottom, FSGM->FGSM", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "S1eIP8MpYB", "sentence_index": 0, "text": "-- This paper seeks to combine several ideas together to propose an approach for image classification based continual learning tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1eIP8MpYB", "sentence_index": 1, "text": "In this effort, the paper combines previously published approaches from generative modeling with VAEs, mutual information regularization and domain adaptation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1eIP8MpYB", "sentence_index": 2, "text": "I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 3, "text": "--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don\u2019t believe 1 and 2 are equal.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 4, "text": "--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 5, "text": "The following is the concern:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1eIP8MpYB", "sentence_index": 6, "text": "--In the second line of Equation 5, the KL term appears to be measuring a distance between distributions on two different variables; z|c and c|z.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1eIP8MpYB", "sentence_index": 7, "text": "If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 8, "text": "--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 9, "text": "It is also not clear how the loss function proposed differs from that of the CDVAE, etc.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 10, "text": "If the novelty is in applying to continual learning and new datasets, it is not clear that this is sufficient.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 11, "text": "Additional feedback for authors (not part of the main decision reasoning):", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1eIP8MpYB", "sentence_index": 12, "text": "- What is dt in Algorithm 1 description?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 13, "text": "Figure 1:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1eIP8MpYB", "sentence_index": 14, "text": "-typo \u201cimplmented\u201d", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 15, "text": "-What\u2019s the 3d plot supposed to represent?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 16, "text": "Doesn't the classification loss have a dependency on the input condition?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 17, "text": "--What does a \"heavy classifier\" imply concretely?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 18, "text": "--\u201cRedundant weights\u201d seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 19, "text": "--The notation for the proposed parameters theta, theta\u2019, phi, phi\u2019 are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 20, "text": "In later sections they use theta and theta\u2019 for encoder/decoder resp.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1eIP8MpYB", "sentence_index": 21, "text": "-- \u201cWhen the encoder and decoder networks are sufficiently complex, it is enough to implement each the prior and classification network as one fully-connected layer\u201d", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "S1eIP8MpYB", "sentence_index": 22, "text": "\u2192 what do the authors mean \u201c when \u2026 networks are sufficiently complex\u201d or do they actually mean when the \u201cwhen the problem is simple enough\u201d?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 1, "text": "The authors present a empirical investigation of methods for scaling GANs to complex datasets, such as ImageNet, for class-conditioned image generation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 2, "text": "They first build and describe a strong baseline based on recently proposed techniques for GANs and push the performance on large datasets with several modifications presented sequentially, to obtain strong state-of-the-art IS/FID scores, as well as impressive visual results.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 3, "text": "The authors propose a simple truncation trick to control the fidelity/variance which is interesting on its own but cannot always scale with the architecture.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 4, "text": "The authors further propose a orthogonalization-based regularization to mitigate this problem.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 5, "text": "An investigation of training collapse at large scale is also performed; the authors investigate some regularization schemes based on gathered empirical evidence.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 6, "text": "As a result, they explore and discard Spectral Normalization of the generator as a way to prevent collapse and show that a severe tradeoff between stability and quality can be controlled when using zero-centered gradient penalties in the Discriminator.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 7, "text": "In the end, no solution that can ensure quality and stability is found, except having prohibitively large amounts of data (~300M images).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 8, "text": "Models are evaluated on the ImageNet and on this internal, bigger dataset.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 9, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 10, "text": "- This investigation gives a significant amount of insights on GAN stability and performance at large scales, which should be useful for anyone working with GANs on complex datasets (and that have access to great computational resources).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 11, "text": "- Even though commonly used evaluations metrics for GANs are still not fully adequate, the authors obtain quantitative performance significantly beyond previous work, which seems indeed correlated with remarkable visual results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 12, "text": "- The baseline and added modifications are well presented and clearly explained.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 13, "text": "The Appendices also have great value in that regard.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 14, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 15, "text": "- Discussions sometimes lack depth or are absent.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 16, "text": "For example, it is unclear to me why some larger models are not amenable to truncation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 17, "text": "Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts? Were samples from those networks better without using truncation? Why would this be?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 19, "text": "Authors report how wider networks perform best, and how deeper networks degrade performance.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 20, "text": "Again, discussions are lacking, and it doesn\u2019t seem the authors tried to understand why such behaviors were shown.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 21, "text": "Even though this is mostly an empirical investigation, I think some more efforts should be put in understanding and explaining why some of those behaviors are shown, as I think it can bootstrap future work more easily.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 22, "text": "- In Section 3.1 : \u201cAcross runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.\u201d For me, this is not particularly clear. Is this something the reader should understand from Table 1?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 24, "text": "- I question the choice of sections chosen to be in the main paper/appendices.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 25, "text": "I greatly appreciated the negative results reported in the main text as well as in the appendices and this has significant value.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 26, "text": "However, as this is to me mostly a detailed empirical investigation and presentation of high-performance GANs on large scales, I would be likely to share this with colleagues who want to tackle similar problems.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 27, "text": "In this case, if future readers limit themselves to the main text, I think it can have more value to present some content form Appendix B and C than to have more than a full page on stability investigations and attempted tricks that turned out not to be used to reach maximal performance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 28, "text": "However I do not want to discourage publishing of negative results, and I definitely wish to see this investigation in the paper, but I merely question the positioning of such information.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 29, "text": "With regard to my first negative point above about the lack of discussions, it seems the analysis of Section 4 is disproportionate compared to other places.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 30, "text": "Suggestions/Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 31, "text": "- Regarding the diversity/fidelity tradeoff using different truncation thresholds, I think constraining the norm of the sampled noise vectors to the exact threshold value (by projecting the samples on the 0-centered hyper-sphere of radius = threshold) could yield even more interesting or more informative Figures, as obtained scores or samples on the edge of that hyper-sphere might provide information on the \u2018guaranteed\u2019 (not proven) quality/fidelity of samples mapped from inside that hyper-sphere.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 32, "text": "- In Appendix D, the Figures could be slightly clarified by using a colored heatmap to color the curve, with colors corresponding to the threshold values.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 33, "text": "Similar curves could also be produced with the hyper-sphere projection proposed above to have a slightly clearer idea of the behavior on the limit of that hyper-sphere.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 34, "text": "- In Section 4.2, in the second paragraph, you refer to Appendix F and describe \u201csharp upward jump at collapse\u201d in D\u2019s loss.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 35, "text": "However, it seems the only Figure showing D\u2019s loss when unconstrained is Figure 26, in which it is hard to notice any significant jump in the loss.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 36, "text": "- In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says \u201closses\u201d.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 37, "text": "This investigation of GAN scalability is successful results-wise even though the inability to stabilize training without sacrificing great performance on ImageNet is disappointing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 38, "text": "The improvement over previous SOTA is definitely significant.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 39, "text": "This work thus shows a modern GAN architecture for complex datasets that could be a strong basis for future work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 40, "text": "However, I think the paper could and should be improved with some more detailed analysis and discussions of exhibited behaviors in order to further guide and encourage future work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 41, "text": "It could also be clarified on some aspects, and potentially re-structured a bit to be better align with its probable impact directions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 42, "text": "I would also be curious to see the proposed techniques applied on simpler datasets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "S1gaWerP2X", "sentence_index": 43, "text": "Can this be useful for someone having less compute power and working on something similar to CelebA?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1gQAUDB27", "sentence_index": 0, "text": "This paper proposed a framework to incorporate GAN into MAP inference process for general image restoration.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gQAUDB27", "sentence_index": 1, "text": "First, the motivation of the proposed framework is not convincing for me.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "S1gQAUDB27", "sentence_index": 2, "text": "That is, authors assumed that they have a degradation function F and all the inference process is just based on this known function.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "S1gQAUDB27", "sentence_index": 3, "text": "However, in real world scenarios, it is actually challenging to obtain exact degradation information.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "S1gQAUDB27", "sentence_index": 4, "text": "Thus we may only apply the proposed model on a few tasks with exactly known F.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "S1gQAUDB27", "sentence_index": 5, "text": "Second, due to the norm based constraints, authors actually need to optimize a highly nonconvex optimization problem.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gQAUDB27", "sentence_index": 6, "text": "Moreover, due to the trace based loss function, the computational cost will also be very high.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gQAUDB27", "sentence_index": 7, "text": "Please notice that standard MAP based methods only need to solve a simple convex optimization model (e.g., TV) and these methods can also be applied for different restoration tasks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1gQAUDB27", "sentence_index": 8, "text": "Actually, we only need to specify particular fidelity terms for different tasks.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gQAUDB27", "sentence_index": 9, "text": "Moreover, very recent works have also successfully incorporate both generative and discriminative network architectures (e.g., [1,2]) into the optimization process.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "S1gQAUDB27", "sentence_index": 10, "text": "Therefore, I cannot find any advantage in the proposed method, compared with these existing MAP based image restoration approaches.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "S1gQAUDB27", "sentence_index": 11, "text": "Finally, the experimental part is also too weak to evaluate the proposed method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gQAUDB27", "sentence_index": 12, "text": "As I have mentioned above, actually a lot of methods have been developed to address general image restoration tasks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "S1gQAUDB27", "sentence_index": 13, "text": "Some works actually also incorporate generative and/or discriminative networks into MAP inference process for these tasks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "S1gQAUDB27", "sentence_index": 14, "text": "Thus I believe authors must compare their method with these state-of-the-art approaches.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "S1gQAUDB27", "sentence_index": 15, "text": "Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gQAUDB27", "sentence_index": 16, "text": "This is because the digitals images in MNIST do not have rich texture and detail structures, thus are not very challenging for standard image restoration methods.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1gQAUDB27", "sentence_index": 17, "text": "[1]. Kai Zhang, Wangmeng Zuo, Shuhang Gu, Lei Zhang: Learning Deep CNN Denoiser Prior for Image Restoration. CVPR 2017: 2808-2817", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gQAUDB27", "sentence_index": 18, "text": "[2]. Jiawei Zhang, Jin-shan Pan, Wei-Sheng Lai, Rynson W. H. Lau, Ming-Hsuan Yang: Learning Fully Convolutional Networks for Iterative Non-blind Deconvolution. CVPR 2017: 6969-6977", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 0, "text": "*", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 1, "text": "Description", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 2, "text": "The work is motivated by the empirical performance of Batch Normalization and in particular the observed better robustness of the choice of the learning rate.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 3, "text": "Authors analyze theoretically the asymptotic convergence rate for objectives involving normalization, not necessarily BN, and show that for scale-invariant groups of parameters (appearing as a result of normalization) the initial learning rate may be set arbitrary while still asymptotic convergence is guaranteed with the same rate as the best known in the general case.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 4, "text": "Offline gradient descent and stochastic gradient descent cases are considered.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 5, "text": "* Strengths", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 6, "text": "The work addresses better theoretical understanding of successful heuristics in deep learning, namely batch normalization and other normalizations.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 7, "text": "The technical results obtained are non-trivial and detailed proofs are presented.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 8, "text": "Also I did not verify the proofs the paper appears technically correct and technically clear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 9, "text": "The result may be interpreted in the following form: if one chooses to use BN or other normalization, the paper gives a recommendation that only the learning rate of scale-variant parameters need to be set, which may have some practical advantages.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 10, "text": "Perhaps more important than the rate of convergence, is the guarantee that the method will not diverge (and will not get stuck in a non-local minimum).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 11, "text": "* Criticism", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 12, "text": "This paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 13, "text": "-- Concerns regarding the clarity of presentation and interpretation of the results.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 14, "text": "The properties of BN used as motivation for the study, are observed non-asymptotically with constant or empirically decreased learning rate schedules for a limited number of iterations.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 15, "text": "In contrast, the studied learning rates are asymptotic and there is a big discrepancy.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 16, "text": "SGD is observed to be significantly faster than batch gradient when far from convergence (experimental evidence), and this is with or without normalization.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 18, "text": "In practice, the training is stopped much before convergence, in the hope of finding solutions close to minimum with high probability.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 19, "text": "There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 20, "text": "It makes a nice story that the theoretical properties justify the observations, but they may be as well completely unrelated.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 21, "text": "As seen from the formal construction, the theoretical results apply equally well to all normalization methods.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 22, "text": "It occludes the clarity that BN is emphasized amongst them.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 23, "text": "Considering theoretically, what advantages truly follow from the paper for optimizing a given function? Let\u2019s consider the following cases. 1. For optimizing a general smooth function with all parameters forming a single scale-invariant vector.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 25, "text": "In this case, the paper proves that no careful selection of the learning rate is necessary.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 26, "text": "This result is beyond machine learning and unfortunately I cannot evaluate its merit.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 27, "text": "Is it known / not known in optimization?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 28, "text": "2. The case of data-independent normalization (such as weight normalization).", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 29, "text": "Without normalization, we have to tune learning rate to achieve the optimal convergence.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 30, "text": "With normalization we still have to tune the learning rate (as scale-variant parameters remain or are reintroduced with each invariance to preserve the degrees of freedom), then we have to wait for the phase two of Lemma 3.2 so that the learning rate of scale-invariant parameters adapts, and from then on the optimal convergence rate can be guaranteed.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 31, "text": "3. The case of Batch Normalization.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 32, "text": "Note that there is no direct correspondence between the loss of BN-normalized network (2) and the loss of the original network because of dependence of the normalization on the batches.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 33, "text": "In other words, there is no setting of parameters of the original network that would make its forward pass equivalent to that of BN network (2) for all batches.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 34, "text": "The theory tells the same as in case 2 above but with an additional price of optimizing a different function.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 35, "text": "These points remain me puzzled regarding either practical or theoretical application of the result. It would be great if authors could elaborate.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 36, "text": "-- Difference from Wu et al. 2018", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 37, "text": "This works is cited as a source of inspiration in several places in the paper.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 38, "text": "As the submission is a theoretical result with no immediate applicability, it would be very helpful if the authors could detail the technical improvements over this related work.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 39, "text": "Note, ICLR policy says that arxiv preprints earlier than one month before submission are considered a prior art. Could the authors elaborate more on possible practical/theoretical applications?", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 40, "text": "* Side Notes (not affecting the review recommendation)", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 41, "text": "I believe that the claim that \u201cBN reduces covariate shift\u201d (actively discussed in the intro) was an imprecise statement in the original work.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 42, "text": "Instead, BN should be able to quickly adapt to the covariate shift when it occurs.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 43, "text": "It achieves this by using the parameterization in which the mean and variance statistics of neurons (the quantities whose change is called the covariate shift) depend on variables that are local to the layer (gamma, beta in (1)) rather than on the cumulative effect of all of the preceding layers.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 44, "text": "* Revision", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 45, "text": "I took into account the discussion and the newly added experiments and increased the score.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 46, "text": "The experiments verify the proven effect and make the paper more substantial.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 47, "text": "Some additional comments about experiments follow.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 48, "text": "Training loss plots would be more clear in the log scale.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 49, "text": "Comparison to \"SGD BN removed\" is not fair because the initialization is different (application of BN re-initializes weight scales and biases). The same initialization can be achieved by performing one training pass with BN with 0 learning rate and then removing it, see e.g. Gitman, I. and Ginsburg, B. (2017).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 51, "text": "Comparison of batch normalization and weight normalization algorithms for the large-scale image classification.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 52, "text": "The use of Glorot uniform initializer is somewhat subtle. Since BN is used, Glorot initialization has no effect for a forward pass. However, it affects the gradient norm.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gZOpsFnQ", "sentence_index": 55, "text": "Is there a rationale in this setting or it is just a more tricky method to fix the weight norm to some constant, e.g. ||w||=1?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1llxt2cnQ", "sentence_index": 0, "text": "In this paper, the authors propose a method for dimensionality reduction of image data.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1llxt2cnQ", "sentence_index": 1, "text": "They provide a structured and deterministic function G that maps a set of parameters C to an image X = G(C).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1llxt2cnQ", "sentence_index": 2, "text": "The number of parameters C is smaller than the number of free parameters in the image X, so this results in a predictive model that can be used for compression, denoising, inpainting, superresolution and other inverse problems.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1llxt2cnQ", "sentence_index": 3, "text": "The structure of G is as follows: starting with a small fixed, multichannel white noise image, linearly mix the channels, truncate the negative values to zero and upsample.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1llxt2cnQ", "sentence_index": 4, "text": "This process is repeated multiple times and finally the output is squashed through a sigmoid function for the output to remain in the 0..1 range.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1llxt2cnQ", "sentence_index": 5, "text": "This approach makes sense and the model is indeed more principled than the one taken by Ulyanov et al. In fact, the DIP of Ulyanov et al. can hardly be considered \"a model\" (or a prior, for that matter), and instead should be considered \"an algorithm\", since it relies on the early stopping of a specific optimization algorithm.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "S1llxt2cnQ", "sentence_index": 6, "text": "This means that we are not interested in the minimum of the cost function associated to the model, which contradicts the very concept of \"cost function\".", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "S1llxt2cnQ", "sentence_index": 7, "text": "If only global optimizers were available, DIP wouldn't work, showing its value is in the interplay of the \"cost\" function and a specific optimization algorithm.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1llxt2cnQ", "sentence_index": 8, "text": "None of these problems exist with the presented approach.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1llxt2cnQ", "sentence_index": 9, "text": "The exposition is clear and the presented inverse problems as well as demonstrated performance are sufficient.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "S1llxt2cnQ", "sentence_index": 10, "text": "One thing that I missed while reading the paper is more comment on negative results.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1llxt2cnQ", "sentence_index": 11, "text": "Did the authors tried any version of their model with convolutions or pooling and found it not to perform as well? Measuring the number of parameters when including pooling or convolutions can become tricky, was that part of the reason?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "S1llxt2cnQ", "sentence_index": 12, "text": "Minor:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1llxt2cnQ", "sentence_index": 13, "text": "\"Regularizing by stopping early for regularization,\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1llxt2cnQ", "sentence_index": 14, "text": "In this paper \"large compression ratios\" means little compression, which I found confusing.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1lNOhec27", "sentence_index": 0, "text": "The recent work of Schott et al (which the authors compare results to) proposed the use of Bayes rule inversion as a more robust mechanism for classification under different types of adversarial attacks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lNOhec27", "sentence_index": 1, "text": "The probabilities are approximated with variational autoencoders.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lNOhec27", "sentence_index": 2, "text": "During training the inference network is used, but during testing optimization based inference is carried out to compute loglikelihoods.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lNOhec27", "sentence_index": 3, "text": "This paper focuses on the second part, with a different model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lNOhec27", "sentence_index": 4, "text": "Specifically, it proposes a specific Boltzmann machine to be used as a first layer of neural networks for MNIST classification.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lNOhec27", "sentence_index": 5, "text": "This Boltzmann machine is pre-trained in two-stages using mean field inference of the binary latent variables and gradient estimation of the parameters.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lNOhec27", "sentence_index": 6, "text": "This pre-trained model is then incorporated into the neural net for MNIST classification.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lNOhec27", "sentence_index": 7, "text": "The existence of couplings J_h among the hidden units means that we have to carry out mean field inference over several iterations to compute the output activations of the model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lNOhec27", "sentence_index": 8, "text": "This is basically analogous to the optimization-based inference proposed by Schott et al. (As a detail, this optimization can be expressed as computation over several layers of a neural net.)", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lNOhec27", "sentence_index": 9, "text": "The authors compare to the work of Schott for one type of attack.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lNOhec27", "sentence_index": 10, "text": "It would be nice to see more detailed experiments as done in Schott.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1lNOhec27", "sentence_index": 11, "text": "Questions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1lNOhec27", "sentence_index": 12, "text": "1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1lNOhec27", "sentence_index": 13, "text": "2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "S1lNOhec27", "sentence_index": 14, "text": "3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1lNOhec27", "sentence_index": 15, "text": "4- Could you please add the found J_h's to the appendix.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "S1lNOhec27", "sentence_index": 16, "text": "This architecture reminds me of the good old MRFs for image denoising.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1lNOhec27", "sentence_index": 17, "text": "Could it be that what we are seeing is the attack being denoised?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1lNOhec27", "sentence_index": 18, "text": "I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1lNOhec27", "sentence_index": 19, "text": "Thanks in advance. I will re-adjust the review rating following your reply.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1lzrrz637", "sentence_index": 0, "text": "The authors introduce a  novel  on-policy  temporally  consistent  exploration  strategy, named Neural  AdaptiveDropout Policy Exploration (NADPEx), for deep reinforcement learning agents.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1lzrrz637", "sentence_index": 1, "text": "The main idea is to sample from a distribution of plausible subnetworks modeling the temporally consistent exploration.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lzrrz637", "sentence_index": 2, "text": "For this, the authors use the ideas of the standard dropout for deep networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lzrrz637", "sentence_index": 3, "text": "Using the proposed  dropout transformation that is differentiable, the authors show that the KL regularizers on policy-space play an important role in stabilizing its learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lzrrz637", "sentence_index": 4, "text": "The experimental validation is performed on continuous control learning tasks, showing the benefits of the proposed.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1lzrrz637", "sentence_index": 5, "text": "This paper is very well written, although very dense and not easy to follows, as many methods are referenced and assume that the reviewer is highly familiar with the related works.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1lzrrz637", "sentence_index": 6, "text": "This poses a challenge in evaluating this paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1lzrrz637", "sentence_index": 7, "text": "Nevertheless, this paper clearly explores and offers a novel approach for more efficient on-policy exploration which allows for more stable learning compared to traditional approaches.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "S1lzrrz637", "sentence_index": 8, "text": "Even though the authors answer positively to each of their four questions in the experiments section , it would like that the authors provide more intuition why these improvements occur and also outline the limitations of their approach.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1x0HKfqh7", "sentence_index": 0, "text": "This paper proposes an evaluation method for confidence thresholding defense models, as well as a new approach for generating of adversarial examples by choosing the wrong class with the most confidence when employing targeted attacks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1x0HKfqh7", "sentence_index": 1, "text": "Although the idea behind this paper is fairly simple, the paper is very difficult to understand.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1x0HKfqh7", "sentence_index": 2, "text": "I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "S1x0HKfqh7", "sentence_index": 3, "text": "Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1x0HKfqh7", "sentence_index": 4, "text": "However, in Figure 2, it is used for evaluating defense schemes.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x0HKfqh7", "sentence_index": 5, "text": "Again, this confuses me on what is the main topic of this paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1x0HKfqh7", "sentence_index": 6, "text": "Indeed, why the commonly used attack success ratio or other similar measures cannot be used in the case?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1x0HKfqh7", "sentence_index": 7, "text": "Intuitively, it should provide similar results to the success-failure curve.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x0HKfqh7", "sentence_index": 8, "text": "The paper also lacks experimental results, and the main conclusion from these results seems to be \"MNIST is not suitable for benchmarking of adversarial attacks\".", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1x0HKfqh7", "sentence_index": 9, "text": "If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "S1x0HKfqh7", "sentence_index": 10, "text": "Meanwhile, the computational cost on large dataset such as ImageNet could be huge, the authors should further develop the method to make sure it works in all situations.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1x4ca-chQ", "sentence_index": 0, "text": "Summary", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1x4ca-chQ", "sentence_index": 1, "text": "This paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1x4ca-chQ", "sentence_index": 2, "text": "The proposed approaches are evaluated on two few-shot datasets and achieves the state-of-the-art results.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1x4ca-chQ", "sentence_index": 3, "text": "Pros.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1x4ca-chQ", "sentence_index": 4, "text": "-This paper is well-motivated.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "S1x4ca-chQ", "sentence_index": 5, "text": "Studying label propagation in the meta-learning setting is interesting and novel.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "S1x4ca-chQ", "sentence_index": 6, "text": "Intuitively, transductive label propagation should improve supervised learning when the number of labeled instances is low.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x4ca-chQ", "sentence_index": 7, "text": "-The empirical results show improvement over the baselines, which are expected.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "S1x4ca-chQ", "sentence_index": 8, "text": "Cons.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1x4ca-chQ", "sentence_index": 9, "text": "-Some technical details  are missing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1x4ca-chQ", "sentence_index": 10, "text": "In Section 3.2.2, the authors only explain how they learn example-based \\sigma, but details on how to make graph construction end-to-end trainable are missing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "S1x4ca-chQ", "sentence_index": 11, "text": "Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "S1x4ca-chQ", "sentence_index": 12, "text": "-Does episode training help label propagation? How about the results of label propagation without the episode training?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "S1x6zNA92m", "sentence_index": 0, "text": "Following the suggested rubric:", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1x6zNA92m", "sentence_index": 1, "text": "1. Briefly establish your personal expertise in the field of the paper.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x6zNA92m", "sentence_index": 2, "text": "2. Concisely summarize the contributions of the paper.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x6zNA92m", "sentence_index": 3, "text": "3. Evaluate the quality and composition of the work.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x6zNA92m", "sentence_index": 4, "text": "4. Place the work in context of prior work, and evaluate this work's novelty.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x6zNA92m", "sentence_index": 5, "text": "5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper's novelty and quality.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x6zNA92m", "sentence_index": 6, "text": "6. Provide a summary judgment if the work is significant and of interest to the community.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x6zNA92m", "sentence_index": 7, "text": "1.  I work at the intersection of machine learning and biological vision", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x6zNA92m", "sentence_index": 8, "text": "and have worked on modeling word representations.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x6zNA92m", "sentence_index": 9, "text": "2. This paper develops a new representation system for object representations from training on data collected from odd-one-out human judgements of images.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1x6zNA92m", "sentence_index": 12, "text": "The vector representation for objects is designed to be sparse and low dimensional (and ends up being about 49D) .", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1x6zNA92m", "sentence_index": 16, "text": "Similarity is measured by dot products in the space and probabilities of which pair of items will be paired are modeled as the exponential of the similarity.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1x6zNA92m", "sentence_index": 19, "text": "3,5", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1x6zNA92m", "sentence_index": 20, "text": "The resulting embedding\tdoes a good job\tof predicting human similarity judgements and seems to cover similar features to those named by humans.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "S1x6zNA92m", "sentence_index": 25, "text": "They also explain typicality judgements and cluster semantic categories well.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "S1x6zNA92m", "sentence_index": 27, "text": "The creation of the upper limit based on noise between and within subjects was a nice addition.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "S1x6zNA92m", "sentence_index": 29, "text": "4. Some relevant related work is discussed and this seems like a novel and interesting contribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "S1x6zNA92m", "sentence_index": 31, "text": "The authors might also want to compare to similar work that looked at similarities among triplets (Similarity Comparisons for Interactive Fine-Grained Categorization http://ttic.uchicago.edu/~smaji/papers/similarity-cvpr14.pdf; Conditional Similarity Networks https://arxiv.org/abs/1603.07810 ).", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1x6zNA92m", "sentence_index": 36, "text": "6. While this paper is not especially surprising or ground breaking, the number and quality of the comparisons make it a worthwhile contribution and the resulting embeddings are worth further exploration and could be very useful for future research.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1xAWnjRFH", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1xAWnjRFH", "sentence_index": 1, "text": "This paper makes an observation that most of the neural network architectures do not learn the mutual exclusivity (ME) bias: if an object has one label, then it does not need another.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1xAWnjRFH", "sentence_index": 2, "text": "Authors demonstrate this in both synthetic tasks and real-world tasks like object recognition and machine translation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1xAWnjRFH", "sentence_index": 3, "text": "Authors argue that ME bias could help the model to handle new classes and rare events better.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1xAWnjRFH", "sentence_index": 4, "text": "My comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1xAWnjRFH", "sentence_index": 5, "text": "I very much enjoyed reading this paper.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1xAWnjRFH", "sentence_index": 6, "text": "I support accepting this paper.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1xAWnjRFH", "sentence_index": 7, "text": "It highlights one of the missing inductive biases in ML and proposes it as a challenge.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "S1xAWnjRFH", "sentence_index": 8, "text": "As the authors also agree, ME bias is missing not just in DNNs.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1xAWnjRFH", "sentence_index": 9, "text": "It is the issue of MLE.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1xAWnjRFH", "sentence_index": 10, "text": "It would be good to have some non-NN results too.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "S1xAWnjRFH", "sentence_index": 11, "text": "I see this is a challenge for MLE than DNNs.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1xAWnjRFH", "sentence_index": 12, "text": "1. In figure-4 you mention that entropy regularizer helps to keep the initial ME score. Can you elaborate more about the way in which entropy regularizer is used with regular MLE training?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1xAWnjRFH", "sentence_index": 13, "text": "2. It is not very clear how is the base rate computed in Figure 5. I have a guess. But it is better to explain it in detail.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1xAWnjRFH", "sentence_index": 14, "text": "3. Section 4.2 need more clarity. For example, what do you mean by classifying the image as \u201cnew\u201d? Is \u201cnew\u201d a class name? Also, how is P(N|t) computed? Please explain.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1xAWnjRFH", "sentence_index": 16, "text": "4. Are the authors willing to release the code and data to reproduce the results?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1xAWnjRFH", "sentence_index": 17, "text": "Minor comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1xAWnjRFH", "sentence_index": 18, "text": "1. Page 3: second para, line 4: \u201cour aim is to study\u201d", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1xAWnjRFH", "sentence_index": 19, "text": "2. Page 5: last line: estimate for -> estimated for", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1xAWnjRFH", "sentence_index": 20, "text": "3. Section 4.2: 3rd line: \u201cthe class for the from\u201d", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "S1xAWnjRFH", "sentence_index": 21, "text": "=====================================================", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1xAWnjRFH", "sentence_index": 22, "text": "After rebuttal: I have read the authors' response and  I stand by my decision.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 0, "text": "i have change my rating from 5 to 6 after reading the numerous and thorough rebuttals from the authors.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 1, "text": "I hope they will incorporate these clarifications and additional experiments into the final version of the paper if accepted.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 2, "text": "The purpose of this paper is presumably to approximate the margin of a sample as accurately as possible.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 3, "text": "This is clearly an intractable problem.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 4, "text": "Thus all attacks make some kind of approximation, including this paper.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 5, "text": "I am still a bit confused about the difference between \"zero-confidence attacks\" and those that don't fall into that category such as PGD.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 6, "text": "Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 7, "text": "The authors spend two paragraphs in the introduction trying to draw a distinction but I am still not convinced.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 8, "text": "The proofs provided by the authors assume that convexity and many assumptions, which makes it not very useful for the real world case.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 9, "text": "What would have been helpful is to show the accuracy of their margin for simple binary toy 2D problems, where the true margin and their approximation can be visualized.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "none"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 10, "text": "This was not done.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 11, "text": "This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 12, "text": "Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 13, "text": "Thus their novelty rests on the definition of zero confidence attack, and of the importance of such a attack.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1xcRXhFnQ", "sentence_index": 14, "text": "So clarifying the above question will help to judge the paper's novelty.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 0, "text": "The authors proposed meta domain adaptation to address domain shift scenario in meta learning setup.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 1, "text": "The proposed model combines few shot meta-learning with the adversarial domain adaptation to demonstrate performance improvements in several experiments.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 2, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 3, "text": "1. A new few shot learning with domain shift problem is studied in the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 4, "text": "2. A new model combining prototypical network with GAN and cycle-consistency loss for addressing meta-learning domain shift scenario.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 5, "text": "The experimental improvements on omniglot seem quite substantial.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 6, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 7, "text": "1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 8, "text": "It seems that both are using meta-learning with domain adaptation technique.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 9, "text": "What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 10, "text": "I feel the baseline in domain adaptation area is a bit limited.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 11, "text": "2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 12, "text": "3. It seems the domain shift in the paper is less dramatic.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 13, "text": "i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 14, "text": "4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 15, "text": "Minor:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 16, "text": "1. Where is L_da in Figure 2? In Figure 2, what\u2019s the unlabelled data from which testing tasks are drawn? Is it from meta-test data training set?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "S1xGt0Rq3m", "sentence_index": 17, "text": "2. In the caption of figure 2, there should be a space after `\":\".", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1xUCqYpYH", "sentence_index": 0, "text": "The paper proposes 4d convolution and an enhanced inference strategy to improve the feature interaction for video classification.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1xUCqYpYH", "sentence_index": 1, "text": "State-of-the-art performance is achieved on several datasets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1xUCqYpYH", "sentence_index": 2, "text": "However, there are still details and concerns.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1xUCqYpYH", "sentence_index": 3, "text": "1. The paper should also talk about the details of ARNet and discuss the difference, as I assume they are the most related work", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "S1xUCqYpYH", "sentence_index": 4, "text": "2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "S1xUCqYpYH", "sentence_index": 5, "text": "3. can you provide the training memory, inference speed, and total training time?", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "S1xUCqYpYH", "sentence_index": 6, "text": "4. Most importantly, for table4, authors are comparing to the ResNet18 ARNet instead of ResNet50? which is better than the proposed method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "S1xUCqYpYH", "sentence_index": 7, "text": "5. lack of related work:  4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks, CVPR 2019.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "SJe4yM7lcr", "sentence_index": 0, "text": "This paper proposes a distributed backdoor attack strategy, framed differently from the previous two main approches (1) the centralised backdoor approach and (2) the (less discussed in the paper) distributed fault tolerance approach (often named \"Byzantine\").", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJe4yM7lcr", "sentence_index": 1, "text": "The authors show through experiments how their attack is more persistent than centralised backdoor attack.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJe4yM7lcr", "sentence_index": 2, "text": "The authors also compare two aggregation rules for federated learning schemes, (Fung et al 2018 & Pillutla et al 2019), suggesting that both rules are bypassed by the proposed distributed backdoor attack.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJe4yM7lcr", "sentence_index": 3, "text": "Strength:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJe4yM7lcr", "sentence_index": 4, "text": "what I found most interesting in the paper is Section 3.4, presenting an appreciable attempt to \"interpret\" poisoning. Together with Section 4.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SJe4yM7lcr", "sentence_index": 5, "text": "This kind of fine-grained analysis of poisoning is highly needed.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SJe4yM7lcr", "sentence_index": 6, "text": "Weakness:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJe4yM7lcr", "sentence_index": 7, "text": "in section 3.3, the authors compare against RFA and take what is claimed in Pillulata et al as granted (that RFA detects more nuanced outliers than the wort-case of the Byzantine setting", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJe4yM7lcr", "sentence_index": 8, "text": "(Blanchard et al 2017) ) .", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJe4yM7lcr", "sentence_index": 10, "text": "In fact, there is more to the Byzantine setting than that, see e.g. Draco (Chen et al 2018 SysML), Bulyan (El Mhamdi et al 2018 ICML) and SignSGD (Bernstein et al 2019 ICLR) which have proposed more sophisticated approches to distributed robustness.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJe4yM7lcr", "sentence_index": 12, "text": "Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJe4yM7lcr", "sentence_index": 13, "text": "post rebuttal: thank your for your detailed reply, I acknowledge your new comparisons with the distributed robustness mechanisms of Krum and Bulyan, too bad time was short to compare with the other measures such as Draco and SignSGD.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJeDlWsjj7", "sentence_index": 0, "text": "This paper develops a framework for evaluating the ability of neural models on answering free-form mathematical problems.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJeDlWsjj7", "sentence_index": 1, "text": "The contributions are i) a publicly available dataset, and ii) an evaluation of two existing model families, recurrent networks and the Transformer.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJeDlWsjj7", "sentence_index": 2, "text": "I think that this paper makes a good contribution by establishing a benchmark and providing some preliminary results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "SJeDlWsjj7", "sentence_index": 3, "text": "I am biased because I once did exactly the same thing as this paper, although at a much smaller scale; I am thus happy to see such a public dataset.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJeDlWsjj7", "sentence_index": 4, "text": "The paper is a reasonable dataset/analysis paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "SJeDlWsjj7", "sentence_index": 5, "text": "Whether to accept it or not depends on what standard ICLR has towards such papers (ones that do not propose a new model/new theory).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJeDlWsjj7", "sentence_index": 6, "text": "I think that the dataset generation process is well-thought-out.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "SJeDlWsjj7", "sentence_index": 7, "text": "There are a large variety of modules, and trying to not generate either trivial or impossible problems is a plus in my opinion.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "SJeDlWsjj7", "sentence_index": 8, "text": "The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJeDlWsjj7", "sentence_index": 9, "text": "I think the authors should move a portion of the big bar plot (too low resolution, btw) into the main text and discuss it thoroughly.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJeDlWsjj7", "sentence_index": 10, "text": "Details on how to generate the dataset, however, can be moved into the appendix.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SJeDlWsjj7", "sentence_index": 11, "text": "I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a \"soft\", secondary metric?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJeDlWsjj7", "sentence_index": 12, "text": "One other thing I want to see is a test set with multiple different difficulty levels.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJeDlWsjj7", "sentence_index": 13, "text": "The authors try to do this with composition, which is good, but I am not sure whether that captures the real important thing - the ability to generalize, say learning to factorise single-variable polynomials and test it on factorising polynomials with multiple variables? And what about the transfer between these tasks (e.g., if a network learns to solve equations with both x and y and also factorise a polynomial with x, can it generalize to the unseen case of factorising a polynomial with both x and y)?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "SJeDlWsjj7", "sentence_index": 14, "text": "Also, is there an option for \"unsolvable\"?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "SJeDlWsjj7", "sentence_index": 15, "text": "For example, the answer being a special \"this is impossible\" character for \"factorise x^2 - 5\" (if your training set does not use \\sqrt, of course).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJekKHZ93Q", "sentence_index": 0, "text": "In this work the authors propose an extension of mixture density networks to the continuous domain, named compound density networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJekKHZ93Q", "sentence_index": 1, "text": "Specifically the paper builds on top of the idea of the ensemble neural networks (NNs) and introduces a stochastic neural network for handling the mixing components.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJekKHZ93Q", "sentence_index": 2, "text": "The mixing distribution is also parameterised by a neural network.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJekKHZ93Q", "sentence_index": 3, "text": "The authors claim that the proposed model can result in better uncertainty estimates and the experiments attempt to demonstrate the benefits of the approach, especially in cases of having to deal with adversarial attacks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJekKHZ93Q", "sentence_index": 4, "text": "The paper in general is well written and easy to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "SJekKHZ93Q", "sentence_index": 5, "text": "I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 6, "text": "Let me elaborate.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJekKHZ93Q", "sentence_index": 7, "text": "First of all, I don\u2019t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 8, "text": "I also find weird the way that the authors arrive to their final objective in Equation (5).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 9, "text": "They start from Equation (4) which is incorrectly denoted as the log-marginal distribution while it is the same conditional distribution introduced in Equation (3) with the extra summation for all the available data points.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 10, "text": "Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 11, "text": "However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\\theta) = p(\\theta | g(x_n; \\psi).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 12, "text": "Is there a reason why the authors do not introduce their objective by following the variational framework?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 13, "text": "Furthermore, in the beginning of Section 3.1 the authors present their idea on probabilistic hypernetoworks which \u201cmaps x to a distribution over parameters instead of specific value \\theta.\u201d How is this different from the case that we were considering so far? If we had a point estimate for \\theta we would not require to take an expectation in Equation (3) in the first place.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 14, "text": "My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 15, "text": "The Kronecker product between two diagonal matrices results in another diagonal matrix, i.e., diagonal covariance, which implies that the weights within a layer are given by an independent multivariate Gaussian.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJekKHZ93Q", "sentence_index": 16, "text": "What is the purpose then for introducing the matrix variate Gaussian?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 17, "text": "I would expect that you would like to impose additional structure to the weights.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJekKHZ93Q", "sentence_index": 18, "text": "I expect the authors to comment on that.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 19, "text": "Regarding the experimental evaluation of the model rather confusing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 20, "text": "The authors have proposed a model that due to the mixing is better suited for predictions with heteroscedastic noise and can better quantify the aleatoric uncertainty.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJekKHZ93Q", "sentence_index": 21, "text": "However, the selected experiments on the cubic regression toy data (Section 5.1) and the out-of-distribution classification (Section 5.2) are clear examples of system\u2019s noise, i.e. epistemic uncertainty.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 22, "text": "The generative process of the toy data clearly states that there is no heteroscedastic noise to handle.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJekKHZ93Q", "sentence_index": 23, "text": "The same applies for the notMNIST data which belong to a completely different data set compared to MNIST and thus out of sample prediction cannot benefit from the mixing; i.e., variations have to be explained by system\u2019s noise.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJekKHZ93Q", "sentence_index": 24, "text": "So overall I have the feeling that the authors have not succeeded to evaluate the model\u2019s power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 25, "text": "To continue with the experimental evaluation, I found the plots with the predictive uncertainty in Figure 3 a bit confusing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 26, "text": "The plot by itself, as I understood, quantifies the model\u2019s uncertainty in in- and out-of sample prediction.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJekKHZ93Q", "sentence_index": 27, "text": "While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 28, "text": "There is no value in being very confident if you are wrong and vice-versa, so unless there is an accompanying plot/table reporting the accuracy I see not much value from this plot alone.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJekKHZ93Q", "sentence_index": 29, "text": "Finally, it is unclear how the authors have picked the best \\lambda parameter for their approach? On page 5 they state that they \u201cpick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.\u201d Does this mean that you get to observe the performance in the test in order to select the appropriate value for \\lambda? If this is the case this is completely undesirable and is considered a bad practice.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SJeRQb-oFH", "sentence_index": 0, "text": "Paper summary.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 1, "text": "The paper proposes Dreamer, a model-based RL method for high-dimensional inputs such as images.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 2, "text": "The main novelty in Dreamer is to learn a policy function from latent representation-and-transition models in an end-to-end manner.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 3, "text": "Specifically, Dreamer is an actor-critic method that learns an optimal policy by backpropagating re-parameterized gradients through a value function, a latent transition model, and a latent representation model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 4, "text": "This is unlike existing methods which use model-free or planning methods on simulated trajectories to learn the optimal policy.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 5, "text": "Meanwhile, Dreamer learns the remaining components, namely a value function, a latent transition model, and a latent representation model, based on existing methods (the world models and PlaNet).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 6, "text": "Experiments on a large set of continuous control tasks show that Dreamer outperforms existing model-based and model-free methods.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 7, "text": "Comments.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 8, "text": "Efficiently learning a policy from visual inputs is an important research direction in RL.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 9, "text": "This paper takes a step in this direction by improving existing model-based methods (the world models and PlaNet) using the actor-critic approach.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "SJeRQb-oFH", "sentence_index": 10, "text": "I am leaning towards weak accepting the paper.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 11, "text": "I am reluctant to give a higher score due to its incremental contribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SJeRQb-oFH", "sentence_index": 12, "text": "Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SJeRQb-oFH", "sentence_index": 13, "text": "The main difference between Dreamer and SVG is that Dreamer incorporates a latent representation model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 14, "text": "From this viewpoint, the actor-critic component in Dreamer is an incremental contribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SJeRQb-oFH", "sentence_index": 15, "text": "Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SJeRQb-oFH", "sentence_index": 16, "text": "Besides the above comments, I have these additional comments.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 17, "text": "- Effectiveness on very long horizon trajectories:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 18, "text": "Simulating long-horizon trajectories with a probabilistic model is known to be unsuitable for model-based RL due to accumulated errors.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 19, "text": "This is an open issue in model-based RL.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 20, "text": "The paper attempts to solve this issue by backpropagating policy gradients through the transition model, which is known to be more robust against model errors (see e.g., PILCO (Deisenroth et al., 2011)).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 21, "text": "However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJeRQb-oFH", "sentence_index": 22, "text": "This horizon length is still short compared to the entire horizon length of many MDPs (e.g., 1000).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 23, "text": "I think this point should be discussed in the paper.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "SJeRQb-oFH", "sentence_index": 24, "text": "That is, the issue still exists, and Dreamer is less effective with very long horizon.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJeRQb-oFH", "sentence_index": 25, "text": "- Inapplicability to discrete controls:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 26, "text": "One restriction of re-parameterized gradients is that the technique is not applicable to discrete random variables.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 27, "text": "This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJeRQb-oFH", "sentence_index": 28, "text": "Still, such approximations would make learning more challenging, especially with long-horizon backpropagation.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 29, "text": "This restriction should be noted in the paper.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "SJeRQb-oFH", "sentence_index": 30, "text": "- There is no mention about variance of policy gradient estimates.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 31, "text": "Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJeRQb-oFH", "sentence_index": 32, "text": "- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SJeRQb-oFH", "sentence_index": 33, "text": "Also, I suggest moving Section 4 to be right after Section 2, since Section 4 presents existing techniques similarly to Section 2, while Section 3 presents the main contribution.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SJeRQb-oFH", "sentence_index": 34, "text": "Update after authors' response.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 35, "text": "I read the response.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 36, "text": "The paper is more clear after authors' clarification.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJeRQb-oFH", "sentence_index": 37, "text": "Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SJeRQb-oFH", "sentence_index": 38, "text": "Nonetheless, I am keen to acceptance. I would increase the rating from 6 to 7, but I will keep the rating of 6 since the rating of 7 is not possible.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 0, "text": "This is an novel, interesting paper on an important topic: semi-supervised learning.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 1, "text": "Even though the proposed approach seems to have significant potential, the experimental is somewhat disorganized,", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 3, "text": "and it also includes some weak claims that should be removed.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 4, "text": "For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 5, "text": "In this reviewer's opinion, it would be a lot more reasonable to have instead a learning curve showing the results for, say, 100, 500, 1K, 5K, and 10K labeled examples for all three domains.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 6, "text": "In 4.1, you are using different epsilon policies for synthetic vs organic datasets; why?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 7, "text": "The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for \"stratified SSL.\" Without this extra work, your claim is just a conjecture.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 8, "text": "You should also show the performance of regular SSL methods in the setup on Table 4.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 9, "text": "Last but not least, you have repeatedly made the claim combining SST and other SSL may further improve the performance;", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 10, "text": "however, you do not provide any evidence for it, so you should avoid making such claims.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 11, "text": "Other comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 12, "text": "- on page 2, the two terms classification & selection network appear \"out of the blue;\" it would be quite helpful to make it clear from the abstract that the proposed implementation is for neural networks.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 13, "text": "- figures 2 & 3 should be a lot larger in order to be readable", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 14, "text": "- 4.1.2 top of page 7: claims such as \"SST could have obtained better performance\" have no place in such a paper; you could instead make a note about the method being \"prohibitively CPU intensive for the time being\"", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJg9GkF9nQ", "sentence_index": 15, "text": "- lower on the same page you say: \"SST may get better performance\" - see above", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 1, "text": "This paper addresses the computational aspects of Viterbi-based encoding for neural networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 2, "text": "In usual Viterbi codes, input messages are encoded via a convolution with a codeword, and then decoded using a trellis.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 3, "text": "Now consider a codebook with n convolutional codes, of rate 1/k.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 4, "text": "Then a vector of length n is represented by inputing a message of length k and receiving n encoded bits.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 5, "text": "Then the memory footprint (in terms of messages) is reduced by rate k/n.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 6, "text": "This is the format that will be used to encode the row indices in a matrix, with n columns.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 7, "text": "(The value of each nonzero is stored separately.)", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 8, "text": "However, it is clear that not all messages are possible, only those in the \"range space\" of my codes. (This part is previous work Lee 2018.)", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 9, "text": "The \"Double Viterbi\" (new contribution) refers to the storage of the nonzero values themselves.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 10, "text": "A weakness of CSR and CSC (carried over to the previous work) is that since each row may have a different number of nonzeros, then finding the value of any particular nonzero requires going through the list to find the right corresponding nonzero, a sequential task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 11, "text": "Instead, m new Viterbi decompressers are included, where each row becomes (s_1*codeword_1 + s_2*codeword2 + ...) cdot mask, and the new scalar are the results of the linear combinations of the codewords.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 12, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 13, "text": "- I think the work addressed here is important, and though the details are hard to parse and the new contributions seemingly small, it is important enough for practical performance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 14, "text": "- The idea is theoretically sound and interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 15, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 16, "text": "- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 17, "text": "Compressability is evaluated, but that was already present in the previous work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 18, "text": "Therefore the novel contribution of this paper over Lee 2018 is not clearly outlined.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 19, "text": "- It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SJgAEEpDhQ", "sentence_index": 20, "text": "- Minor grammatical mistakes (missing \"a\" or \"the\" in front of some terms, suggest proofread.)", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SJgzc8XTnX", "sentence_index": 0, "text": "The paper considers the problem of dictionary learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzc8XTnX", "sentence_index": 1, "text": "Here the model that we are given samples y, where we know that y = Ax where A is a dictionary matrix, and x is a random sparse vector.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzc8XTnX", "sentence_index": 2, "text": "The goal is typically to recover the dictionary A, from which one can also recover the x under suitable conditions on A. The paper shows that there is an alternating optimization-based algorithm for this problem that under standard assumptions provably converges exactly to the true dictionary and the true coefficients x (up to some negligible bias).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzc8XTnX", "sentence_index": 3, "text": "The main comparison with prior work", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzc8XTnX", "sentence_index": 4, "text": "is with [1].", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzc8XTnX", "sentence_index": 5, "text": "Both give algorithms of this type for the same problem, with similar assumptions (although there is some difference; see below).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzc8XTnX", "sentence_index": 6, "text": "In [1], the authors give two algorithms: one with a better sample complexity than the algorithm presented here, but which has some systematic, somewhat large, error floor which it cannot exceed, and another which can obtain similar rates of convergence to the exact solution, but which requires polynomial sample complexity (the explicit bound is not stated in the paper).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzc8XTnX", "sentence_index": 7, "text": "The algorithm here seems to build off of the former algorithm; essentially replacing a single hard thresholding step with an IHT-like step.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzc8XTnX", "sentence_index": 8, "text": "This update rule is able to remove the error floor and achieve exact recovery.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzc8XTnX", "sentence_index": 9, "text": "However, this makes the analysis substantially more difficult.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzc8XTnX", "sentence_index": 10, "text": "I am not an expert in this area, but this seems like a nice and non-trivial result.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "SJgzc8XTnX", "sentence_index": 11, "text": "The proofs are quite dense and I was unable to verify them carefully.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SJgzc8XTnX", "sentence_index": 12, "text": "Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJgzc8XTnX", "sentence_index": 13, "text": "- The analysis in [1] handles the case of noisy updates, whereas the analysis given here only works for exact updates.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJgzc8XTnX", "sentence_index": 14, "text": "The authors claim that some amount of noise can be tolerated, but do not quantify how much.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJgzc8XTnX", "sentence_index": 15, "text": "- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJgzc8XTnX", "sentence_index": 16, "text": "[1] Arora, S. Ge, R., Ma, T. and Moitra, A. Simple, Efficient, and Neural Algorithms for Sparse Coding. COLT 2015.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 0, "text": "This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 1, "text": "The problem is important to understand in the theoretical machine learning community.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 2, "text": "The paper is written well overall, clearly explaining the results obtained.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 3, "text": "I would like to raise several important points:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 4, "text": "1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 5, "text": "The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 6, "text": "2. Vacuous bounds in the regime \\beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \\beta >1.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 7, "text": "A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 8, "text": "They can indeed be subsumed by generalization bounds based on VC theory.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 9, "text": "The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 10, "text": "3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 11, "text": "4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the \"model complexity\" introduced upto numerical constants.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 12, "text": "It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 13, "text": "[1] Koiran, Pascal, and Eduardo D. Sontag. \"Vapnik-Chervonenkis Dimension of Recurrent Neural Networks.\" Discrete Applied Mathematics 86.1 (1998): 63-79.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 14, "text": "[2] Dasgupta, Bhaskar, and Eduardo D. Sontag. \"Sample complexity for learning recurrent perceptron mappings.\" Advances in Neural Information Processing Systems. 1996.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJgzW2Vqh7", "sentence_index": 15, "text": "[3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. \"Spectrally-normalized margin bounds for neural networks.\" Advances in Neural Information Processing Systems. 2017.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 0, "text": "adaptive versions of sgd are commonly used in machine learning.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 1, "text": "adagrad, adadelta are both popular adaptive variations of sgd.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 2, "text": "These algorithms can be seen as preconditioned versions of gradient descent where the preconditioner applied is a matrix of second-order moments of the gradients.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 3, "text": "However, because this matrix turns out to be a pxp matrix where p is the number of parameters in the model, maintaining and performing linear algebra with this pxp matrix is computationally intensive.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 4, "text": "In this paper, the authors show how to maintain and update this pxp matrix by storing only smaller matrices of size pxr and rxr, and performing 1. an SVD of a small matrix of size rxr", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 5, "text": "2. matrix-vector multiplication between a pxr matrix and rx1 vector.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 6, "text": "Given that rxr is a small constant sized matrix and that matrix-vector multiplication can be efficiently computed on GPUs, this matrix adapted SGD can be made scalable.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 7, "text": "The authors also discuss how to adapt the proposed algorithm with Adam style updates that incorporate momentum.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 8, "text": "Experiments are shown on various architectures (CNN, RNN) and comparisons are made against SGD, ADAM.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 9, "text": "General comments: THe appendix has some good discussion and it would be great if some of that discussion was moved to the main paper.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 10, "text": "Pros:  Shows how to make full matrix preconditioning efficient, via the use of clever linear algebra, and GPU computations.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 11, "text": "Shows improvements on LSTM tasks, and is comparable with SGD, matching accuracy with time.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 12, "text": "Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 13, "text": "This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 14, "text": "It might be possible that if one performs few steps of GGT optimizer in the initial stages and then switches to SGD/ADAM in the later stages, then some of the computational concerns that arise are eliminated.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJgzxEO5hQ", "sentence_index": 15, "text": "Have the authors tried out such techniques?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SJl0rjD2tr", "sentence_index": 0, "text": "The study is motivated by the observation that the Q-value matrix in reinforcement learning problems often has a low-rank structure.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "SJl0rjD2tr", "sentence_index": 1, "text": "The paper proposes an approach called structured value-based planning or learning, where the Q matrix or the Q function is estimated from incomplete observations based on the prior that it is low-rank.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJl0rjD2tr", "sentence_index": 2, "text": "The proposed strategy is demonstrated in stochastic control tasks and reinforcement learning applications.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJl0rjD2tr", "sentence_index": 3, "text": "The paper is clearly written and the experimental results show that the proposed strategy leads to performance gains especially in problems where the Q matrix indeed conforms to a low-rank model.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "SJl0rjD2tr", "sentence_index": 4, "text": "A few comments and questions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJl0rjD2tr", "sentence_index": 5, "text": "- The assumption that the Q matrix should be low-rank is demonstrated with several experiments.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJl0rjD2tr", "sentence_index": 6, "text": "Is there any theoretical motivation or guarantee for this assumption as well?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJl0rjD2tr", "sentence_index": 7, "text": "- The experimental results show that the proposed strategy performs well in problems that are low-rank, while the performance may degrade in problems where the low-rank assumption is not met. Would it be possible to detect the rank of the problem in a dynamical manner (i.e., during the learning), so that the number of incomplete observations of Q can be increased to improve the performance, or the solution strategy (e.g. whether to use the low-rank assumption or not) can be adapted to the nature of the problem?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJl0rjD2tr", "sentence_index": 8, "text": "- The Q-value matrices and functions considered in the problem have a special structure as they result from Markov Decision Processes. Would it be possible to go beyond the low-rank assumption and propose and use a more elaborate type of prior that employs the special structure of MDPs?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJl0rjD2tr", "sentence_index": 9, "text": "- Please clearly define the notation used in Section 4.2.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SJl68_Hx37", "sentence_index": 0, "text": "This paper present extensions of the Self-Attention Generative Adversarial Network approach SAGAN, leading to impressive images generations conditioned on imagenet classes.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJl68_Hx37", "sentence_index": 1, "text": "The key components of the approach are :", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJl68_Hx37", "sentence_index": 2, "text": "- increasing the batch size by a factor 8", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJl68_Hx37", "sentence_index": 3, "text": "- augmenting the width of the networks by 50%", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJl68_Hx37", "sentence_index": 4, "text": "These first two elements result in an Inception score (IS) boost from 52 to 93.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJl68_Hx37", "sentence_index": 5, "text": "- the use of shared embeddings for the class conditioned batch norm layers, orthonormal regularization and hierarchical latent space bring an additional boost of IS 99.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJl68_Hx37", "sentence_index": 6, "text": "The core novel element of the paper is the truncation trick: At train time, the input z is sampled from a normal distribution but at test time, a truncated normal distribution is used: when the magnitude of elements of z are above a certain threshold, they are re-sampled.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJl68_Hx37", "sentence_index": 7, "text": "Variations of this threshold lead to variations in FD and IS, as shown in insightful experiments.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJl68_Hx37", "sentence_index": 8, "text": "The comments that more data helps (internal dataset experiments) is also informative.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJl68_Hx37", "sentence_index": 9, "text": "Very nice to have included negative results and detailed parameter sweeps.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJl68_Hx37", "sentence_index": 10, "text": "This is a very nice work with impressive results, a great progress achievement in the field of image generation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "SJl68_Hx37", "sentence_index": 11, "text": "Very well written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "SJl68_Hx37", "sentence_index": 12, "text": "Suggestions/questions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJl68_Hx37", "sentence_index": 13, "text": "- it would be nice to also propose unconditioned experiments.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SJl68_Hx37", "sentence_index": 14, "text": "It would be good to give an idea in the text of TPU-GPU equivalence in terms of feasibility of a standard GPU implementation - computation time it would involve.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_replicability", "pol": "pol_positive"}, {"review_id": "SJl68_Hx37", "sentence_index": 15, "text": "- I understand that no data augmentation was used during training?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SJl68_Hx37", "sentence_index": 16, "text": "- clarification of the truncation trick: if the elements of z are re-sampled and are still above the threshold, are they re-sampled again and again until they are all below the given threshold?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SJl68_Hx37", "sentence_index": 17, "text": "- A sentence could be added to explain the truncation trick in the abstract directly since it is simple to understand and is key to the quality of the results.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "SJl68_Hx37", "sentence_index": 18, "text": "- A reference to Appendix C could be given at the beginning of the Experiments section to help the reader find these details more easily.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SJl68_Hx37", "sentence_index": 19, "text": "- It would be nice to display more Nearest neighbors for the dog image.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SJl68_Hx37", "sentence_index": 20, "text": "- It would be nice to add a figure of random generations.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJl68_Hx37", "sentence_index": 21, "text": "- make the bib uniform: remove unnecessary doi - url - cvpr page numbers", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SJl8R-eTnQ", "sentence_index": 0, "text": "The paper proposes an approach to adapt hyperparameters online.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJl8R-eTnQ", "sentence_index": 1, "text": "When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJl8R-eTnQ", "sentence_index": 2, "text": "A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJl8R-eTnQ", "sentence_index": 3, "text": "Thus, it is hard to say whether the results are applicable in practice.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJl8R-eTnQ", "sentence_index": 4, "text": "B. You don't schedule learning rates for your baseline methods except for a single experiment for some initial learning rate.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJl8R-eTnQ", "sentence_index": 5, "text": "C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJl8R-eTnQ", "sentence_index": 6, "text": "This hyperparameter itself benefits from (requires?) some scheduling.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJl8R-eTnQ", "sentence_index": 7, "text": "It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJl8R-eTnQ", "sentence_index": 8, "text": "Online tuning of  hyperparameters is an important functionality and I hope your paper will make it more straightforward to use it in practice.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJl8R-eTnQ", "sentence_index": 9, "text": "* Minor notes:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJl8R-eTnQ", "sentence_index": 10, "text": "You mention that \"APO converges quickly from different starting points on the Rosenbrock surface\" but 10000 iterations is not quick at all for the 2-dimensional Rosenbrock, it is extremely slow compared to 100-200 function evaluations needed for Nelder-Mead to solve it. I guess you mean w.r.t. the original RMSprop.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJllCbbahQ", "sentence_index": 0, "text": "This paper studies \"Noisy Information Bottlenecks\".", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJllCbbahQ", "sentence_index": 1, "text": "The overall idea is that, if the mutual information between learned parameters and the data is limited, then this prevents overfitting.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJllCbbahQ", "sentence_index": 2, "text": "It proposes to create a \"bottleneck\" to limit the mutual information.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJllCbbahQ", "sentence_index": 3, "text": "Specifically, the bottleneck is created by having the data depend on a noisy version of the parameters, rather than the true parameters and invoking the information processing inequality.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJllCbbahQ", "sentence_index": 4, "text": "The paper gives an example of Gaussian mean field inference.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJllCbbahQ", "sentence_index": 5, "text": "Ultimately, the analysis boils down to looking at a signal-to-noise ratio of the algorithm, which looks very much like regularization.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJllCbbahQ", "sentence_index": 6, "text": "I think this is a very interesting direction, but the present paper is somewhat unclear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SJllCbbahQ", "sentence_index": 7, "text": "In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have \"training algorithms that are exactly equivalent.\" I think this example needs to be clarified.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SJllCbbahQ", "sentence_index": 8, "text": "Many of the parameters here are also unclear and not properly defined/introduced.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SJllCbbahQ", "sentence_index": 9, "text": "What is the relationship between $\\theta$ and $\\tilde\\theta$ exactly?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SJllCbbahQ", "sentence_index": 10, "text": "In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SJllCbbahQ", "sentence_index": 11, "text": "The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJllCbbahQ", "sentence_index": 12, "text": "This paper is giving an information-theoretic perspective on existing variational inference methods.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJllCbbahQ", "sentence_index": 13, "text": "Such a perspective is interesting, but needs to be further developed and explained.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJllCbbahQ", "sentence_index": 14, "text": "Specifically, how can mutual information in this context be formally linked to generalization/overfitting?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SJllCbbahQ", "sentence_index": 15, "text": "Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJllCbbahQ", "sentence_index": 16, "text": "As a result, constraining the model will alter the mutual information and I think the effect of this should be remarked on.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "SJllCbbahQ", "sentence_index": 17, "text": "Overall, I think this paper has some interesting ideas, but those need to be fleshed out and clearly explained in a future revision.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SJlsCX2CFr", "sentence_index": 0, "text": "The authors propose PARCUS (\"Pattern Representations on Continuous Spaces\"), a model which computes a soft-matching probability for all words in an input sequence with so-called prototypes in order to predict a label for the input.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJlsCX2CFr", "sentence_index": 1, "text": "Furthermore, for training, PARCUS makes use of rationales.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJlsCX2CFr", "sentence_index": 2, "text": "Those are indicators of input importance, and help to boost the loss for relevant tokens.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJlsCX2CFr", "sentence_index": 3, "text": "The main motivation to use PARCUS is that it works better in a low-resource setting than recent state-of-the-art models for the high-resource case.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "SJlsCX2CFr", "sentence_index": 4, "text": "This is due to it having relatively few parameters and to it having a strong inductive bias.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "SJlsCX2CFr", "sentence_index": 5, "text": "However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SJlsCX2CFr", "sentence_index": 6, "text": "Looking at the experiments, the results on HATESPEECH show less differences between models than for SPOUSE or MOVIEREVIEW.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJlsCX2CFr", "sentence_index": 7, "text": "Another selling point of PARCUS is that it's interpretable. While neural networks can also be analyzed in different ways, I agree with the authors that this is nice to have.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SJlsCX2CFr", "sentence_index": 8, "text": "Overall, the paper seems solid.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SJlsCX2CFr", "sentence_index": 9, "text": "==========", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJlsCX2CFr", "sentence_index": 10, "text": "Update: After reading the other reviews and the responses by the authors, I lowered my score from 6 to 3.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "SJx0zByAYr", "sentence_index": 0, "text": "The main contributions of the submission are:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJx0zByAYr", "sentence_index": 1, "text": "1. A comprehensive empirical comparison of deep learning optimizers, with their performance compared under different amount of hyper-parameter tuning (they perform hyper-parameter tuning using random search).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJx0zByAYr", "sentence_index": 2, "text": "2. The introduction of a novel metric that tries to capture the \"tunability\" of an optimizer.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJx0zByAYr", "sentence_index": 3, "text": "This metric attempts to trade off the performance of an optimizer when tuned only with a small number of hyper-parameter trials, and its performance when carefully tuned.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJx0zByAYr", "sentence_index": 4, "text": "The metric is defined as a weighted average of the performance after tuning with i random trials, with i that goes from 1 to K.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJx0zByAYr", "sentence_index": 5, "text": "The weights of this weighted average and K are \"hyper-parameters\" of the metric itself.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJx0zByAYr", "sentence_index": 6, "text": "They use K=100 and suggest 3 possible choices of weights.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJx0zByAYr", "sentence_index": 7, "text": "The paper appears to treat 2. as the main contribution.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJx0zByAYr", "sentence_index": 8, "text": "However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJx0zByAYr", "sentence_index": 9, "text": "The reason is that simpler methods provide just as much information, and do not rely on the need of interpreting the choice of the weights and K. This point is proven in the paper itself, where for example Figure 2 provides a more concrete and easier to interpret information than the tunability metric, similar graphs could be easily provided per dataset.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJx0zByAYr", "sentence_index": 10, "text": "Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJx0zByAYr", "sentence_index": 11, "text": "Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJx0zByAYr", "sentence_index": 12, "text": "A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SJx0zByAYr", "sentence_index": 13, "text": "Comparisons which, while mentioned, should perhaps have been discussed and compared more in detail in this work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SJx0zByAYr", "sentence_index": 14, "text": "Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "SJx0zByAYr", "sentence_index": 15, "text": "They also suggest that when the tuning budget is low, using Adam but tuning only the learning rate is beneficial, which could be a valuable and practical suggestion.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "SJx0zByAYr", "sentence_index": 16, "text": "I enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJx0zByAYr", "sentence_index": 17, "text": "Other comments/notes:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJx0zByAYr", "sentence_index": 18, "text": "* One aspects that is mostly left out of the discussion (except from one side comment) is the wallclock time, as some optimizers might be on average quicker to train (for example due to quicker convergence), this can easily lead it to be quicker to tune even though it requires a higher budget of trials.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJx0zByAYr", "sentence_index": 19, "text": "I think it would be worth discussing this more.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJx0zByAYr", "sentence_index": 20, "text": "* minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJx0zByAYr", "sentence_index": 21, "text": "* similarly to the above, if the configurations are always sampled from the same 100, confidence intervals in the graphs become less reliable as the budget increases.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJxb8_g0FS", "sentence_index": 0, "text": "The paper proposes a neural-network-based estimation of mutual information, following the earlier line of work in [A].", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJxb8_g0FS", "sentence_index": 1, "text": "The main focus has been to develop an estimator that can reliably work with small dataset sizes.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJxb8_g0FS", "sentence_index": 2, "text": "They first reduce the sample complexity of estimating mutual information by decoupling the network learning problem and the estimation problem by creating a training and validation set and then using the validation set for estimating mutual information.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJxb8_g0FS", "sentence_index": 3, "text": "Of course, there is still the problem of learning the network with smaller sized data.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJxb8_g0FS", "sentence_index": 4, "text": "For this, they propose the strategy of creating multiple tasks from the same dataset, where the dataset is run through transformations that do not affect mutual information.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJxb8_g0FS", "sentence_index": 5, "text": "I am inclined to accept (weak) the paper for the following reasons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJxb8_g0FS", "sentence_index": 6, "text": "1. The paper uses some nice ideas to reduce the variance of the MI estimates and to be able to work with smaller dataset sizes.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "SJxb8_g0FS", "sentence_index": 7, "text": "Both splitting data into training and validation and then using task augmentation to make learning robust are pretty nice ideas.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "SJxb8_g0FS", "sentence_index": 8, "text": "2. The results on the synthetic datasets show that the resulting estimator does have low variance and the estimates are less than or equal to the true MI value, which is consistent with the fact that it is a lower bound estimation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "SJxb8_g0FS", "sentence_index": 9, "text": "3. The results on fMRI dataset were interesting and showed that the method gives improvements over baselines were the estimates were made on a smaller sized dataset.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SJxb8_g0FS", "sentence_index": 10, "text": "Some improvement suggestions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJxb8_g0FS", "sentence_index": 11, "text": "1. I don't see why MINE cannot be applied to the fMRI dataset and results be reported.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJxb8_g0FS", "sentence_index": 12, "text": "I know that the variance in estimation is large, but it would still be useful to look at the performance of MINE in comparison to DEMINE.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJxb8_g0FS", "sentence_index": 13, "text": "2. There are several errors in the writing - hyperparamters in Abstract, repetition of the word \"Section\" in fMRI experiment section etc. - which needs to be fixed.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SJxb8_g0FS", "sentence_index": 14, "text": "[A] Mutual Information Neural Estimation, ICML 2018", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 0, "text": "This paper proposes a self-supervised reinforcement learning approach, Mutual Information-based State-Control (MISC), which maximizes the mutual information between the context states (i.e. robot states) and the states of interest (i.e. states of an object to manipulate).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 1, "text": "For this, they first split the entire state into two mutually exclusive sets of the context states and the states of interest.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 2, "text": "Then, the neural discriminator is trained to estimate the (lower-bound of) mutual information between the two states.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 3, "text": "The (mutual-information) intrinsic reward is computed by the trained neural discriminator, which is used for policy pre-training.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 4, "text": "Experimental results show that MISC helps to improve the performance of DDPG/SAC and the learned discriminator can be transferred to different environments.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 5, "text": "Detailed comments and questions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 6, "text": "- In the paper, the states are represented by only object positions (x, y, z). Is this sufficient? (e.g. velocity is unnecessary?)", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 7, "text": "- For MISC, the additional assumption is required: the agent should know that which parts of the states are its own controllable state and object's state respectively.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 8, "text": "Is this additional assumption realistic enough and has it been adopted in other previous works? Is there any way to discriminate robot states and object states automatically?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 9, "text": "- Can MISC deal with the problems where the number of objects of interest is more than two? In this case, how can we define mutual information?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 10, "text": "- In Eq. (4), T(x_1:N, y_1:N) is assumed to be decomposable into the sum of T(x_t, y_t) / N. Can this make the lower bound (Eq. (3)) arbitrarily loose since the class of functions becomes very limited?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 11, "text": "- Detailed experimental setups are missing. e.g. network architecture, hyper-parameters (e.g. I_tran^max), and how they were searched.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 13, "text": "- Similarly to the problem of sparse reward, if the robot and the object are far apart and it is difficult to reach the object with random exploration, it would also be difficult to train the mutual information discriminator. How was the discriminator trained? How many time steps were used to train MI discriminator?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 14, "text": "- It seems that the MI discriminator learns to estimate the 'proximity' between the robot and the object.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 15, "text": "Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 16, "text": "- For the MISC+DIAYN, what if we train the agent using MISC and DIAYN at the same time, instead of pre-training MISC first and fine-tuning DIAYN later?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 17, "text": "- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 18, "text": "- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 19, "text": "- It seems that MISC is beneficial when the robot should get closer to the object for the success of the task.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 20, "text": "Then, how about the opposite situation? What if the task requires that the robot should 'avoid' the object of interest? Does MISC still work? Is it helpful for the improvement of sample efficiency?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 21, "text": "- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SJxy9Ndy5r", "sentence_index": 22, "text": "- In section 4.3, what happens if we transfer the learned discriminator to Pick&Place from Push that has a gripper fixed to be closed, rather than the opposite direction (i.e. from Pick&Place to Push)? Does the MISC-t still well work? Can the learned MI discriminator be transferred to different tasks even when the state space is different?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 0, "text": "Update 11/21", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 1, "text": "With the additional experiments (testing a new image, testing fine-tuning of hand-crafted features), additions to related work, and clarifications, I am happy to raise my score to accept.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 2, "text": "Overall, I think this paper is a nice sanity check on recent self-supervision methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 3, "text": "In the future, I am quite curious about how these mono-image learned features would fare on more complex downstream tasks (e.g., segmentation, keypoint detection) which necessarily rely less on texture.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 4, "text": "Summary", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 5, "text": "This paper seeks to understand the role of the *number of training examples* in self-supervised learning with images.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 6, "text": "The usefulness of the learned features is evaluated with linear probes at each layer for either ImageNet or CiFAR image classification.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 7, "text": "Empirically, they find that a single image along with heavy data augmentation suffices for learning the first 2-3 layers of convolutional weights, while later layers improve with more self-supervised training images.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 8, "text": "The result holds for three state-of-the-art self-supervised methods, tested with two single-image training examples.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 9, "text": "In my view, learning without labels is an important problem, and it is interesting what can be learned from a single image and simple data augmentation strategies.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 10, "text": "Comments / Questions", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 11, "text": "It seems to me that for completeness, Table 4 should include the result of training a supervised network on top of random conv1/2 and Scattering network features, because this experiment is actually testing what we want - performance of the features when fine-tuned for a downstream task.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 12, "text": "So for example, even if a linear classifier on top of Scattering features does poorly, if downstream fine-tuning results in the same performance as another pre-training method, then Scattering is a perfectly fine approach for initial features.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 13, "text": "Could the authors please either correct this logic or provide the experiments?", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 14, "text": "Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 15, "text": "I wonder if the learned features require fewer fully supervised images to obtain the same performance on the downstream task?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 16, "text": "Can the authors clarify how the neural style transfer experiment is performed?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 17, "text": "The method from Gatys et al. requires features from different layers of the feature hierarchy, including deeper layers.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 18, "text": "Are all these features taken directly from the self-supervised network or is it fine-tuned in some way?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 19, "text": "While I appreciate the computational burden of testing more images, it does feel that Image A and B are quite cherry-picked in being very visually diverse.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 20, "text": "Because of this, it seems like a precise answer to what makes a good single training image remains unknown.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 21, "text": "I wonder how feasible it is to find a proxy metric that corresponds to the performance on downstream tasks which is expensive to compute.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 22, "text": "It might be interesting to try to generate synthetic images (or modify real ones) that are good for this purpose and observe their properties.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 23, "text": "I disagree with the claim of practicality in the introduction (page 2, top).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 24, "text": "While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn\u2019t seem likely that *any* image would work for this method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 25, "text": "Finally, more images are needed to learn the deeper layers for the downstream task anyway.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Ske6OaZ-qr", "sentence_index": 26, "text": "The paper is well-written and clear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "SkedBT5ntB", "sentence_index": 0, "text": "*** Increased to weak accept after discussion of merits of ME bias was improved in the paper ***", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkedBT5ntB", "sentence_index": 1, "text": "This paper investigates whether neural networks exhibit a \u2018mutual exclusivity (ME) bias\u2019, whereby novel inputs tend to be associated with previously unseen outputs, an inductive bias that is cited to be present in children when learning to associate new words and objects. Via three different sets of experiments, the authors conclude that, under standard training procedures, neural networks in fact display an anti-ME bias: when faced with a novel input, they tend to assign less probability mass to unobserved outputs than is justified by the incoming data.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkedBT5ntB", "sentence_index": 3, "text": "The authors go on to argue that explicitly designing neural networks to reason by mutual exclusivity could lead to faster and more flexible learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkedBT5ntB", "sentence_index": 4, "text": "While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SkedBT5ntB", "sentence_index": 5, "text": "The authors pose an interesting hypothesis, but it would gain a lot of credibility if they could provide an empirical analysis of an algorithm that uses ME reasoning to improve learning in a realistic setting or if they could at least perform some quantitative analysis of the effects of ME bias on task performance for a toy example.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "SkedBT5ntB", "sentence_index": 6, "text": "Comments / questions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkedBT5ntB", "sentence_index": 7, "text": "* The authors duly acknowledge that ME bias is not necessarily desirable in all circumstances, namely in tasks that feature many-to-one mappings, citing polysemy and synonymy in machine translation as examples.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkedBT5ntB", "sentence_index": 8, "text": "Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SkedBT5ntB", "sentence_index": 9, "text": "* For the synthetic dataset one-to-one mapping of one-hot vectors it is clear that an ME bias would incur an advantage for classification on a zero-shot basis, i.e. an increased an accuracy for the first time a new input is observed. Is ME bias considered to be useful here because (i) we care directly about improving this type of zero-shot classification, or is it (ii) because it's implicitly assumed that a better initial prediction will lead to faster learning on these examples? * If it is (i) we care about, then it would be useful to quantify the advantage gained either empirically or analytically.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SkedBT5ntB", "sentence_index": 12, "text": "It\u2019s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SkedBT5ntB", "sentence_index": 13, "text": "So at the beginning of training, while there are lots of new classes to benefit from the ME bias, the maximum advantage per example is small, and vice versa at the end of training - how do these conflicting effects balance each other out over the course of training and how do the benefits of ME bias scale with the number of classes?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SkedBT5ntB", "sentence_index": 14, "text": "Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SkedBT5ntB", "sentence_index": 15, "text": "* The authors say that \u201cME can be generalized from applying to 'novel versus familiar\u2019 stimuli to instead handling \u2018rare versus frequent\u2019 stimuli\u201d and they cite the fact that neural networks take longer to learn from rare stimuli, suggesting that ME could help for reason (ii) above.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkedBT5ntB", "sentence_index": 16, "text": "It\u2019s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SkedBT5ntB", "sentence_index": 17, "text": "* It would be interesting to see a more detailed analysis of the predictions of the image classifiers in section 4.2. When a new image is presented, how is the probability mass distributed across previously seen classes versus across unseen ones? From a strategic standpoint, in a scenario where it is difficult to determine whether a new image belongs to a new class or not, could it plausibly be more sensible for the network to make a strong prediction on its best guess out of previously seen classes (that it knows more about) rather than a scattered prediction on the unseen classes? Admittedly this is a difficult question to quantify, but my point is to question whether the anti-ME bias shown in Figure 6 is necessarily suboptimal, given the difficulty of classifying a new image as a new class or not.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SkedBT5ntB", "sentence_index": 21, "text": "Minor comments /questions not affecting review:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkedBT5ntB", "sentence_index": 22, "text": "* Is the acronym ME pronounced like the word \u201cme\u201d or is it spelled out \u201cM-E\u201d? If the latter, then all cases of \u201ca ME bias\u201d should be corrected to \u201can ME bias\u201d.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SkedBT5ntB", "sentence_index": 23, "text": "* Section 4.2 line 3: \u201csample the class [for the] from a power law distribution\"", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SkeOpvO227", "sentence_index": 0, "text": "In this paper, the authors proposed a multi-domain adversarial learning approach, MULANN, to improve the classification accuracy on three datasets-DIGITS, OFFICE and CELL-in the semi-supervised DA setting.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkeOpvO227", "sentence_index": 1, "text": "It\u2019s contributions include: i) using the H-divergence to bound both the risk across all domains and the worst-domain risk (imbalance on a specific domain); ii) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; iii) the experimental validation of the approach, improving on the state-of-the-art on two standard image benchmarks, and a novel bioimage dataset, CELL.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkeOpvO227", "sentence_index": 2, "text": "In addition, this paper has a clear logic to explain and prove the problem to be solved, and has ample experimental evidence.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkeOpvO227", "sentence_index": 3, "text": "Above on, this paper did a meaningful work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkeOpvO227", "sentence_index": 4, "text": "But there are some errors of expression, so it should be checked.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SkeXPX7hhm", "sentence_index": 0, "text": "The paper first provides a generalization bounds for adversarial training, showing that the error bound depends on Lipschitz constant.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkeXPX7hhm", "sentence_index": 1, "text": "This motivates the use of spectral regularization (similar to Miyato et al 2018) in adversarial training.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkeXPX7hhm", "sentence_index": 2, "text": "Using spectral regularization to improve robustness is not new, but it's interesting to combine spectral regularization and adversarial training.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkeXPX7hhm", "sentence_index": 3, "text": "Experimental results show significant improvement over vanilla adversarial training.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "SkeXPX7hhm", "sentence_index": 4, "text": "The paper is nicely written and the experimental results are quite strong and comprehensive. I really like the paper but I have two questions about the results:", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "SkeXPX7hhm", "sentence_index": 5, "text": "1. The numbers reported in Figure 5 do not match with the performance of adversarial training in previous paper.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkeXPX7hhm", "sentence_index": 6, "text": "In PGM L_inf adversarial training/attack (column 3 of Figure 5), the prediction accuracy is roughly 50% under 0.1 infinity norm perturbation.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkeXPX7hhm", "sentence_index": 7, "text": "However, previous papers (e.g., \"Obfuscated Gradients Give a False Sense of Security\") reported 55% accuracy under 0.031 infinity norm perturbation.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkeXPX7hhm", "sentence_index": 8, "text": "I wonder why the numbers are so different.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "SkeXPX7hhm", "sentence_index": 9, "text": "Maybe it's because of different scales? Previous works usually scale each pixel to [0,1] or [-1,1], maybe the authors use the [0, 255] scale? But 0.1/255 will be much smaller than 0.031.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SkeXPX7hhm", "sentence_index": 10, "text": "Another factor might be the model structure. If Alexnet has much lower accuracy, it's probably worthwhile to conduct experiments on the same structure with previous works (Madry et al and Athalye et al) to make the conclusion more clear.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkeXPX7hhm", "sentence_index": 11, "text": "2. What's the training time of the proposed method compared with vanilla adversarial training?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SkeXPX7hhm", "sentence_index": 12, "text": "3. The idea of using SN to improve robustness has been introduced in the following paper:", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkeXPX7hhm", "sentence_index": 13, "text": "\"Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks\"", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkeXPX7hhm", "sentence_index": 14, "text": "(but this paper did not combine it with adv training).", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 0, "text": "The authors consider the setting of a RL agent that exclusively receives intrinsic reward during training that is intended to model curiosity; technically, \u2018curiosity\u2019 is quantified by the ability of the agent to predict its own forward dynamics [Pathak, et al., ICML17].", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 1, "text": "This study primarily centers around an initially somewhat surprising result that non-trivial policies can be learned for many \u2019simpler\u2019 video games (e.g., Atari, Super Mario, Pong) using just curiosity as reward.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 2, "text": "While this is primarily an empirical study, one aspect considered was the observation representation (raw pixels, random features, VAE, and inverse dynamics features [Pathak, et al., ICML17]).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 3, "text": "In examining reward curves (generally extrinsic during testing), \u2018curiosity-based\u2019 reward generally works with the representation effectiveness varying across different testbeds.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 4, "text": "They also conduct more in-depth experiments on specific testbeds to study the dynamics (e.g., Super Mario, Juggling, Ant Robot, Multi-agent Pong) \u2014 perhaps most interestingly showing representation-based transfer of different embeddings across levels in Super Mario.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 5, "text": "Finally, they consider the Unity maze testbed, combining intrinsic rewards with the end-state goal reward to generate a more dense reward space.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 6, "text": "From a high level perspective, this is an interesting result that ostensibly will lead to a fair amount of discussion within the RL community (and already has based on earlier versions of this work).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "Ske_-TWah7", "sentence_index": 7, "text": "However, it isn\u2019t entirely clear if the primary contribution is showing that \u2018curiosity reward\u2019 is a potentially promising approach or if game environments aren\u2019t particularly good testbeds for practical RL algorithms \u2014 given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with \u2018simulator artifact\u2019 based explanations). And honestly, I think the paper reads as if leaning toward the same conclusion.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 9, "text": "Regardless, given the prevalence of these types of testbed environments, either is a useful discussion to have. Maybe the end result could minimally be a new baseline that can help quantify the \u2018difficulty\u2019 of a particular environment.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Ske_-TWah7", "sentence_index": 10, "text": "From the perspective of a purely technical contribution, there are fewer exciting results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 11, "text": "The basic method is taken from [Parthak, et al., ICML17] (modulo some empirical choices such as using PPO).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 12, "text": "The comparison of different observation representations doesn\u2019t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 13, "text": "The testbeds all existed previously and this is mostly the effort of pulling then together.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 14, "text": "Even the \u2018focused experiments\u2019 can be explained with the intuitive narrative that in the state/action space, there is always more uncertainty the farther one goes from the starting point and this is more of a result of massive computation being applied primarily to problems that are designed to provide some level of novelly (the Roboschool examples are a bit more interesting, but also less conclusive).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Ske_-TWah7", "sentence_index": 15, "text": "Finally, Figure 5 is interesting in showing that \u2018curiosity + extrinsic\u2019 improves over extrinsic rewards \u2014 although this isn\u2019t particularly surprising for maze navigation that has such sparse rewards and can be viewed as something like \u2018active exploration\u2019.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Ske_-TWah7", "sentence_index": 16, "text": "With respect to this specific setting, the authors may want to consider [Mirowski, et al., Learning to Navigate in Complex Environments, ICLR17] with respect to auxiliary loss + RL extrinsic rewards to improve performance (in this case, also in maze environments).", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "Ske_-TWah7", "sentence_index": 17, "text": "In just considering the empirical results, they clearly entail a fair amount of effort and just a dump of the code and experiments on the community will likely lead to new findings (even if they are that game simulators are weaker testbeds than previously thought).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 18, "text": "It is easy to ask for additional experiments (i.e., other mechanisms of uncertainty such as the count-based discussed in related work, other settings in 2.2) \u2014 but the quality seems high enough that I basically trust the settings and findings.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Ske_-TWah7", "sentence_index": 19, "text": "Beyond the core findings, the other settings are less convincingly supported by seem more like work in progress and this paper is really just a scaling-up of [Pathak, et al., ICML17] without generating any strong results regarding questions around representation, what to do about stochasticity (although the discussion regarding something like \u2018curiosity honeypots\u2019 is interesting).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 20, "text": "Thus, it reads like one interesting finding around curiosity-driven RL working in games plus a bunch of preliminary findings trying to grasp at some explanations and potential future directions.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 21, "text": "Evaluating the paper along the requested dimensions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 22, "text": "= Quality: The paper is well-written with a large set of experiments, making the case that exclusively using curiosity-based reward is very promising for the widely-used game RL testbeds.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Ske_-TWah7", "sentence_index": 23, "text": "Modulo a few pointers, the work is well-contextualized and makes reasonable assumptions in conducting its experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Ske_-TWah7", "sentence_index": 24, "text": "The submitted code and videos result in a high-quality presentation and trustworthiness of the results. (7/10)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Ske_-TWah7", "sentence_index": 26, "text": "= Clarity: The paper is very clearly written. (7/10)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Ske_-TWah7", "sentence_index": 27, "text": "= Originality: The algorithmic approach is a combination of [Parthak, et al., ICML17] and [Schulman, et al. 2017] (with some experiments using [Kingma & Welling, 2013]).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 28, "text": "All of the testbeds have been used previously.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 29, "text": "Other than completely relying on curiously-based reward exclusively, there is little here.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 30, "text": "In considering combining with extrinsic rewards, I would also consider [Mirowski, et al., ICLR17], which is actually more involved in this regard. (4/10)", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "Ske_-TWah7", "sentence_index": 32, "text": "= Significance: Primarily, this \u2018finishes\u2019 [Parthak, et al., ICML17] to its logical conclusion for game-based environments and should spur interesting conversations and further research.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "Ske_-TWah7", "sentence_index": 33, "text": "In terms of actual technical contributions, I believe much less significant. (5/10)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 35, "text": "=== Pros ===", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 36, "text": "+ demonstrates that curiosity-based reward works in simpler game environments", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Ske_-TWah7", "sentence_index": 37, "text": "+ (implicitly) calls into question the value of these testbed environments", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "Ske_-TWah7", "sentence_index": 38, "text": "+ well written, with a large set of experiments and some interesting observations/discussions", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Ske_-TWah7", "sentence_index": 39, "text": "=== Cons ===", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 40, "text": "- little methodological innovation or analytical explanations", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 41, "text": "- offers minimal (but some) evidence that curiosity-based reward works in more realistic settings", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 42, "text": "- doesn\u2019t answer the one question regarding observation representation that it set out to evaluate", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 43, "text": "- the more interesting problem, RL + auxiliary loss isn\u2019t evaluated in detail", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 44, "text": "- presumably, the sample complexity is ridiculous", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 45, "text": "Overall, I am ambivalent.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 46, "text": "I think that more casual ML/RL researchers will find these results controversial and surprising while more experienced researchers will see curiosity-driven learning to be explainable primarily by the intuition of the \u201cThe fact that the curiosity reward is often sufficient\u201d paragraph of page 6, demanding more complex environments before accepting that this form of curiosity is particularly useful.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 47, "text": "The ostensible goal of learning more about observation representations is mostly preliminary \u2014 and this direction holds promise of for a stronger set of findings.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Ske_-TWah7", "sentence_index": 48, "text": "Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Ske_-TWah7", "sentence_index": 49, "text": "However, as I said previously, this is probably a discussion worth having given the popularity and visibility of game-based testbeds \u2014 so, coupled with the overall quality of the paper, I lean toward a weak accept.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skg1tSgV5S", "sentence_index": 0, "text": "This paper proposes to relax the assumption of disentangled representation and encourage the model to learn linearly manipulable representations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Skg1tSgV5S", "sentence_index": 1, "text": "The paper assumes that the latent canonicalizers are predefined for each task and that it is possible to obtain the ground-truth image of different latent canonicalizations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Skg1tSgV5S", "sentence_index": 2, "text": "I find these assumptions too strong for the task of learning disentangled representation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skg1tSgV5S", "sentence_index": 3, "text": "Firstly, most prior works such as beta-vae, info-gan do not assume that the factors / canonicalizers are known beforehand.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skg1tSgV5S", "sentence_index": 4, "text": "In fact, this is a very difficult part of learning disentangled representation.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skg1tSgV5S", "sentence_index": 5, "text": "Secondly, if it is possible to obtain the ground-truth image of different latent canonicalizations, you can simply train a network to predict the canonicalizations by simple supervised learning.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skg1tSgV5S", "sentence_index": 6, "text": "Hence, these overly simplified and unrealistic assumptions make the task too trivial.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skg1tSgV5S", "sentence_index": 7, "text": "The proposed method is very simple and frames the problem basically as a supervised learning problem.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skg1tSgV5S", "sentence_index": 8, "text": "Although experiments show that learning such representations are beneficial for low-shot setting of SVHN, it is not clear whether such improvement generalizes to more realistic datasets such as ImageNet.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skg1tSgV5S", "sentence_index": 9, "text": "If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skg1tSgV5S", "sentence_index": 10, "text": "[1] Noroozi, Mehdi, and Paolo Favaro. \"Unsupervised learning of visual representations by solving jigsaw puzzles.\" European Conference on Computer Vision. Springer, Cham, 2016.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skg1tSgV5S", "sentence_index": 11, "text": "[2] Zhang, Richard, Phillip Isola, and Alexei A. Efros. \"Colorful image colorization.\" European conference on computer vision. Springer, Cham, 2016.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skg1tSgV5S", "sentence_index": 12, "text": "[3] Gidaris, Spyros, Praveer Singh, and Nikos Komodakis. \"Unsupervised representation learning by predicting image rotations.\" arXiv preprint arXiv:1803.07728 (2018).", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skg9OOEf6X", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Skg9OOEf6X", "sentence_index": 1, "text": "In the semi-supervised self-training setting, this paper proposes to select a certain subset of unlabelled data for training rather than all unlabelled data, where the ensemble of confidence scores of the trained model in iterations is used to guide the selection.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Skg9OOEf6X", "sentence_index": 2, "text": "Strong points:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Skg9OOEf6X", "sentence_index": 3, "text": "It is a good idea to conduct an ensemble based on the confidence scores of trained models in iterations, although the authors did not mention any theoretical explanation or guarantee behind this.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "Skg9OOEf6X", "sentence_index": 4, "text": "Weak points:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Skg9OOEf6X", "sentence_index": 5, "text": "1) Although the ensemble idea is new, the idea of selective self-training is not novel in self-training or co-training of SSL as in the following survey.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "Skg9OOEf6X", "sentence_index": 6, "text": "Considering the selection based on highest-confidence, the in or out of class unlabeled data in most cases does not matter.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skg9OOEf6X", "sentence_index": 7, "text": "Therefore, the technical contribution of this paper is moderate.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "Skg9OOEf6X", "sentence_index": 8, "text": "Zhu, Xiaojin. \"Semi-supervised learning literature survey.\" Computer Science, University of Wisconsin-Madison 2.3 (2006): 4.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "Skg9OOEf6X", "sentence_index": 9, "text": "2) The writing is poor and hard to follow.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Skg9OOEf6X", "sentence_index": 10, "text": "First, many details are missing, especially in the experiments, which makes the proposed method suspicious and non-convincing.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Skg9OOEf6X", "sentence_index": 11, "text": "For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skg9OOEf6X", "sentence_index": 12, "text": "From the plots in Figure 2 and 3, it is hard to find the convergence of the method within 100 iterations.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skg9OOEf6X", "sentence_index": 13, "text": "The descriptions of the datasets used are not clear, e.g., the number of classes for each data.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skg9OOEf6X", "sentence_index": 14, "text": "Second, many typos and grammar errors need to fix, e.g., \"the proposed SST is suitable for lifelong learning which make use...\", \"the error 21.44% was lower than\" 18.97?", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skg9OOEf6X", "sentence_index": 15, "text": "3) The overall performance of the proposed SST in the experiments is not convincing and not promising.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skg9OOEf6X", "sentence_index": 16, "text": "First, the labeled data portion is fixed and is relatively high compared to most standard semi-supervised learning settings .", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skg9OOEf6X", "sentence_index": 19, "text": "Second, SST itself is only comparable with or even worse than the state-of-art methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skg9OOEf6X", "sentence_index": 20, "text": "Combining SST with other existing techniques can help.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Skg9OOEf6X", "sentence_index": 21, "text": "However, the additional cost is expensive.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skg9OOEf6X", "sentence_index": 22, "text": "Further demonstrations are necessary for the proposed SST method.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SkggB79t2X", "sentence_index": 0, "text": "This paper uses constrained Markov decision processes to solve a multi-objective problem that aims to find the correct trade-off between cost and return in continuous control.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkggB79t2X", "sentence_index": 1, "text": "The main technique is Lagrangian relaxation and experiments are focus on cart-pole and locomotion task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkggB79t2X", "sentence_index": 2, "text": "Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkggB79t2X", "sentence_index": 3, "text": "1) How to solve the constrained problem (8) is unclear. It is prefer to provide detailed description or pseudocode for this step.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SkggB79t2X", "sentence_index": 5, "text": "2) In equation (8), lambda is a trade-off between cost and return.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkggB79t2X", "sentence_index": 6, "text": "Optimization on lambda reduces burdensome hyperparameter selection, but a new hyperparameter beta is introduced.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkggB79t2X", "sentence_index": 7, "text": "How do we choose a proper beta, and will the algorithm be sensitive to beta?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SkggB79t2X", "sentence_index": 8, "text": "3) The paper only conducts comparison experiments with fixed-alpha baselines. The topic is similar to safe reinforcement learning. Including the comparison with safe reinforcement learning algorithms is more convincing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 0, "text": "In this paper, the authors focus on alleviating the catastrophic forgetting problem in continual learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkgI3OVTYH", "sentence_index": 1, "text": "The authors propose a discriminative variational autoencoder (DiVA) to solve this problem under the generative replay framework.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkgI3OVTYH", "sentence_index": 2, "text": "DiVA modifies the objective function of VAE by introducing an additional term that maximizes the mutual information between the latent variables and the class labels.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkgI3OVTYH", "sentence_index": 3, "text": "The authors do not thoroughly explain the motivation of this paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 4, "text": "The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 5, "text": "It is also not clear to me why these problems are important.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 6, "text": "The idea that introduces labels in VAE is not novel.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 7, "text": "For example, Narayanaswamy et al. [1] also propose to utilize labels to VAE.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 8, "text": "I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 9, "text": "It is also not clear to me how domain translation is relevant to continual learning.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 10, "text": "In terms of modeling, since the input into the prior network has finite possible discrete values, we do not need a fully connected network to generate $\\hat{\\mu}_c$ and $\\hat{\\sigma}_c$. Instead, we can directly optimize $\\hat{\\mu}_c$ and $\\hat{\\sigma}_c$ for each $c$ as parameters.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 11, "text": "The paper provides some good experimental results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkgI3OVTYH", "sentence_index": 12, "text": "But the problem settings are not clear to me.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 13, "text": "I do not understand how the model is trained to solve multiple tasks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 14, "text": "Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 15, "text": "It is also not clear to me why CIFAR datasets involve two domains and how these domains are relevant in each of the tasks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 16, "text": "In summary, since DiVA gives a good experimental performance, the proposed method might be promising.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkgI3OVTYH", "sentence_index": 17, "text": "However, it looks to me that the authors need to better explain the motivation of DiVA, the differences of DiVA from existing supervised VAE, and the experimental settings, before the acceptance of this paper.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "SkgI3OVTYH", "sentence_index": 18, "text": "References", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkgI3OVTYH", "sentence_index": 19, "text": "[1]Narayanaswamy, Siddharth, T. Brooks Paige, Jan-Willem Van de Meent, Alban Desmaison, Noah Goodman, Pushmeet Kohli, Frank Wood, and Philip Torr. \"Learning disentangled representations with semi-supervised deep generative models.\" In Advances in Neural Information Processing Systems, pp. 5925-5935. 2017.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skgof-pjiX", "sentence_index": 0, "text": "This paper proposes a new application of embedding techniques for mathematical problem retrieval in adaptive tutoring.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Skgof-pjiX", "sentence_index": 1, "text": "The proposed method performs much better than baseline sentence embedding methods.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Skgof-pjiX", "sentence_index": 2, "text": "Another contribution is on using negative pre-training to deal with an imbalanced training dataset.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Skgof-pjiX", "sentence_index": 3, "text": "To me this paper is just not good enough - the method essentially i) use \"a professor and two teaching assistants\" to build a \"rule-based concept extractor\" for problems, then ii) map problems into this \"concept space\" and simply treat them as words. There are several problems with this approach.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skgof-pjiX", "sentence_index": 4, "text": "First, doing so does not touch the core of the proposed application.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skgof-pjiX", "sentence_index": 5, "text": "For tutoring applications, the most important thing is to select a problem that can help students improve; even if you can indeed select a problem that is the most similar to another problem, is it the best one to show a student? There are no evaluations on real students in the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skgof-pjiX", "sentence_index": 6, "text": "Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skgof-pjiX", "sentence_index": 7, "text": "Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skgof-pjiX", "sentence_index": 8, "text": "I am not sure how this will generalize to a larger number of problem spanning many different domains.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skgof-pjiX", "sentence_index": 9, "text": "I also had a hard time going through the paper - there aren't many details.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Skgof-pjiX", "sentence_index": 10, "text": "Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the \"rule-based concept extractor\", which is the key technical innovation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 0, "text": "- Summary", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 1, "text": "This paper proposes a neural architecture search method based on a direct sparse optimization, where the proposed method provides a novel model pruning view to the neural architecture search problem.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 2, "text": "Specifically, the proposed method introduces scaling factors to connections between operations, and impose sparse regularizations to prune useless connections in the network.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 3, "text": "The proposed method is evaluated on CIFAR-10 and ImageNet dataset.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 4, "text": "- Pros", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 5, "text": "- The proposed method shows competitive or better performance than existing neural architecture search methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 6, "text": "- The experiments are conducted thoroughly in the CIFAR-10 and ImageNet.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 7, "text": "The selection of the datasets is appropriate.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 8, "text": "Also, the selection of the methods to be compared is appropriate.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 9, "text": "- The effect of each proposed technique is appropriately evaluated.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 10, "text": "- Cons", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 11, "text": "- The search space of the proposed method, such as the number of operations in the convolution block, is limited.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 12, "text": "- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 13, "text": "- The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SkgrQMiFhQ", "sentence_index": 14, "text": "Overall, if we focus on the balance between the classification accuracy and computational efficiency, the proposed method is promising.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 1, "text": "The manuscript proposes a multi-domain adversarial learning (MDL) method called MULANN, to leverage multiple datasets with overlapping but distinct class sets, in a semi-supervised setting.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 2, "text": "The authors define a new discrimination task to discriminate, within each domain, labeled samples from unlabeled ones that most likely belong to extra classes (classes with no labeled or unlabeled samples in the domain).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 3, "text": "They also introduce a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 4, "text": "Strengths:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 5, "text": "-\u00a0The idea of using discriminators for separating the labeled samples from unlabeled ones that most likely belong\u00a0to extra classes is interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 6, "text": "-\u00a0A new generalization bound for MDL is introduced.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 7, "text": "-\u00a0The paper was clear, well written, well-motivated and nicely structured.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 8, "text": "-\u00a0The authors perform numerous empirical experiments on several types of problems on various datasets (Digit, OFFICE,CELL) successfully showing how the MULANN can reduce the nasty effects of the adversarial domain\u00a0discriminator and repulse (a fraction of) unlabeled examples from labeled ones in each domain.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 9, "text": "Weaknesses:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 10, "text": "-\u00a0all the experiments except the last row of Table 2 concern adaptation between two domains.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 11, "text": "Given the paper title, the reviewer would have expected more experiments in a multiple domain context.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 12, "text": "More precisely, for the digit datasets, the reviewer was interested to see how the proposed MDL performs on jointly adapting SVHN, MNIST, MNIST-M, and USPS or jointly adapting DSLR, Amazon, and Webcam for OFFICE dataset.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 13, "text": "Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 14, "text": "-\u00a0The authors propose to rank the unlabeled samples of each domain according to the entropy of their classification of the current classifier.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 15, "text": "Obviously there must be some false ranking (specially at the initial stages of updating the classifier) for the unlabeled samples (e.g. the classifier may output high entropy for the unlabeled samples of the classes with labeled samples) and they may harm the performance of adaptation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 16, "text": "It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 17, "text": "-\u00a0Although the paper introduces the generalization bound for MDL, it does not give new formulation or algorithm to handle MDL (MULANN handles only the class asymmetry when domains involve distinct sets of classes and it has nothing to do with MDL).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 18, "text": "hence, there is no connection between the theoretical results on MDL generalization bound and the proposed method MULANN.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 19, "text": "-", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 20, "text": "Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 21, "text": "The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 22, "text": "[1]\u00a0Tzeng, Eric, et al. \"Adversarial discriminative domain adaptation.\"\u00a0Computer Vision and Pattern Recognition (CVPR). Vol. 1. No. 2. 2017.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 23, "text": "[2]", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 24, "text": "Bousmalis, Konstantinos, et al. \"Domain separation networks.\"\u00a0Advances in Neural Information Processing Systems. 2016.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 25, "text": "[3]\u00a0Zhao, Han, et al. \"Multiple Source Domain Adaptation with Adversarial Learning.\" Advances in Neural Information Processing Systems. 2018.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkgS2HI5hQ", "sentence_index": 26, "text": "[4]\u00a0Hoffman, Judy, Mehryar Mohri, and Ningshan Zhang. \"Algorithms and Theory for Multiple-Source Adaptation.\"\u00a0\u00a0Advances in Neural Information Processing Systems. 2018.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skl1-09T2m", "sentence_index": 0, "text": "This paper is clearly written and in an interesting domain.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Skl1-09T2m", "sentence_index": 1, "text": "The question asked is whether or not pretrained mean-field RBMs can help in preventing adversarial attacks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Skl1-09T2m", "sentence_index": 2, "text": "However, there are some key issues with the paper that are not clear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Skl1-09T2m", "sentence_index": 3, "text": "The first is regarding the structure of the paper.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Skl1-09T2m", "sentence_index": 4, "text": "The authors combine two ideas, 1: the training of MF RBMs and 2: the the ability to prevent adversarial attacks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Skl1-09T2m", "sentence_index": 5, "text": "The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "Skl1-09T2m", "sentence_index": 6, "text": "It would make the paper much stronger if the authors perform quantitative + qualitative evaluation on the MF training of RBMs first.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skl1-09T2m", "sentence_index": 7, "text": "Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skl1-09T2m", "sentence_index": 8, "text": "In a related note, using MF for training BMs have been proposed previously and found to not work due to various reasons:", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skl1-09T2m", "sentence_index": 9, "text": "see paragraph after equation 8 of the Deep BM paper: http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skl1-09T2m", "sentence_index": 10, "text": "It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skl1-09T2m", "sentence_index": 11, "text": "It is also unclear how the calculation of relative entropy \"D\" was performed in figure 3.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "Skl1-09T2m", "sentence_index": 12, "text": "Obtaining the normalized marginal density in a BM is very challenging due to the partition function.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skl1-09T2m", "sentence_index": 13, "text": "The second part of the paper associate good performance in preventing adversarial attacks with the possibility of denoising by the pretrained BM.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Skl1-09T2m", "sentence_index": 14, "text": "This is a very good point, however the paper do not compare or contrast with existing methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skl1-09T2m", "sentence_index": 15, "text": "For example, it is curious to see how denoising Auto encoders would perform.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skl1-09T2m", "sentence_index": 16, "text": "In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skl1-09T2m", "sentence_index": 17, "text": "- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skl1-09T2m", "sentence_index": 18, "text": "Defending against black box attacks is considerably easier than defending against white-box attacks.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skl1-09T2m", "sentence_index": 19, "text": "In summary, the paper is interesting, however, more experiments could be added to concretely demonstrate the advantage of the proposed MF BMs in increasing robustness against adversarial attacks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklBkB-f9r", "sentence_index": 0, "text": "This work is focused on learning 3D object representations (decoders) that can be computed more efficiently than existing methods.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklBkB-f9r", "sentence_index": 1, "text": "The computational inefficiency of these methods is that you learn a (big) fixed decoder for all objects (all z latents), and then need to apply it individually on either each point cloud point you want to produce, or each voxel in the output (this problem exists for both the class of methods that deform a uniform distribution R^3 -> R^3 a la FoldingNet, or directly predict the 3D function R^3 -> R e.g. DeepSDF).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklBkB-f9r", "sentence_index": 2, "text": "The authors propose that the encoder directly predict the weights and biases of a decoder network that, since it is specific to the particular object being reconstructed, can be much smaller and thus much cheaper to compute.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklBkB-f9r", "sentence_index": 3, "text": "The authors then note the fact that their method lacks a continuous latent space that allows for interpolation, as provided by existing (VAE-like) methods.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklBkB-f9r", "sentence_index": 4, "text": "They propose to solve this by learning an MLP that produces the output by recurrent application, and then composing subapplications of different networks as a type of interpolation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklBkB-f9r", "sentence_index": 5, "text": "-------------------", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SklBkB-f9r", "sentence_index": 6, "text": "I like this work, it addresses a real problem in a number of models for 3D representation learning (similar models are also used for e.g. cryo-EM reconstruction).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "SklBkB-f9r", "sentence_index": 7, "text": "While the fast weights approach is not totally original, its application to this problem is novel and very well-suited to it.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "SklBkB-f9r", "sentence_index": 8, "text": "I was a bit surprised by just how much the decoder network could be shrunk by using fast weights.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "SklBkB-f9r", "sentence_index": 9, "text": "The paper is also quite well written.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "SklBkB-f9r", "sentence_index": 10, "text": "I especially like how Section 2 synthesizes existing work into model categories which make it easier to think about their relationships.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "SklBkB-f9r", "sentence_index": 11, "text": "I also think the explanation in Sec. 3.2, while kind of obvious, is a nice way think about decoder vs. fast weights.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "SklBkB-f9r", "sentence_index": 12, "text": "I like that the authors are straightforward about the deficiency of the method (i.e. that you can't interpolate in latent space).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "SklBkB-f9r", "sentence_index": 13, "text": "Their proposed solution of functional composition is exceedingly clever but in my opinion too impractical to really be useful.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SklBkB-f9r", "sentence_index": 14, "text": "It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SklBkB-f9r", "sentence_index": 15, "text": "The purpose of generative models is not to interpolate per se; the interpolation is really a sanity check that the model is capturing the underlying distribution rather than just memorizing training examples.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklBkB-f9r", "sentence_index": 16, "text": "The function composition doesn't capture that.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SklBkB-f9r", "sentence_index": 17, "text": "I think the authors should just acknowledge that you can't soundly *sample* from their generative model the way e.g. VAE or GAN allows (their function composition is not a sampling method).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "SklBkB-f9r", "sentence_index": 18, "text": "But I think there are lots of useful things you can do without that capability, e.g. do 3D point cloud completion, go image -> structure, etc.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklBkB-f9r", "sentence_index": 19, "text": "I think this function composition angle should be deemphasized in the title/abstract, but I think the paper stands reasonably on its own without that .", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "SklBkB-f9r", "sentence_index": 23, "text": "Nits:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SklBkB-f9r", "sentence_index": 24, "text": "- In Figure 2 it's pretty hard to see the differences between the methods. What exactly is being visualized here? DeepSDF shold be visualizing surface normals vs. HOF which is point clouds, right?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SklBkB-f9r", "sentence_index": 25, "text": "- For predicting a deformation R^3 -> R^3 function composition sort of makes sense, but how generalizable is this approach e.g. to directly predicting a function R^3 -> R (a la DeepSDF)?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SklBkB-f9r", "sentence_index": 27, "text": "I think there are ways this function composition approach could generalize, e.g. using skip connections and layer dropout (which encourages layers to be composable).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklCKPP1h7", "sentence_index": 0, "text": "I raised my rating. After the rebuttal.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklCKPP1h7", "sentence_index": 1, "text": "- the authors address most of my concerns.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklCKPP1h7", "sentence_index": 2, "text": "- it's better to show time v.s. testing accuracy as well.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SklCKPP1h7", "sentence_index": 3, "text": "the per-epoch time for each method is different.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SklCKPP1h7", "sentence_index": 4, "text": "- anyway, the theory part acts still more like a decoration. as the author mentioned, the assumption is not realistic.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklCKPP1h7", "sentence_index": 5, "text": "-------------------------------------------------------------", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SklCKPP1h7", "sentence_index": 6, "text": "This paper presents a method to update hyper-parameters (e.g. learning rate) before updating of model parameters.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklCKPP1h7", "sentence_index": 7, "text": "The idea is simple but intuitive. I am conservative about my rating now, I will consider raising it after the rebuttal.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklCKPP1h7", "sentence_index": 8, "text": "1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SklCKPP1h7", "sentence_index": 9, "text": "- no need to write so much in section 2.1, the surrogate is simple and common in optimization for parameters.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SklCKPP1h7", "sentence_index": 10, "text": "After all, newton method and natural gradients method are not used in experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "SklCKPP1h7", "sentence_index": 11, "text": "- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SklCKPP1h7", "sentence_index": 12, "text": "2. No need to write so much decorated bounds in section 3.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "SklCKPP1h7", "sentence_index": 13, "text": "The convergence analysis is on Z, not on parameters x and hyper-parameters theta.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SklCKPP1h7", "sentence_index": 14, "text": "So, bounds here can not be used to explain empirical observations in Section 5.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SklCKPP1h7", "sentence_index": 15, "text": "3. Could authors explain the time complexity of inner loop in Algorithm 1? Does it take more time than that of updating model parameters?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SklCKPP1h7", "sentence_index": 16, "text": "4. Authors have done a good comparison in the context of deep nets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SklCKPP1h7", "sentence_index": 17, "text": "However, - could the authors compare with changing step-size?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SklCKPP1h7", "sentence_index": 19, "text": "In most of experiments, the baseline methods, i.e. RMSProp are used with fixed rates.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklCKPP1h7", "sentence_index": 20, "text": "Is it better to decay learning rates for toy data sets?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SklCKPP1h7", "sentence_index": 21, "text": "It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SklCKPP1h7", "sentence_index": 22, "text": "- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., \"For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01\", \"while for RMSprop-APO, the best lambda was 0.0001 \" .", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SklCKPP1h7", "sentence_index": 25, "text": "What are reasons for these?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SklCKPP1h7", "sentence_index": 26, "text": "- In Section 5.2, it is said lambda is tuned by grid-search.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklCKPP1h7", "sentence_index": 27, "text": "Tuning a good lambda v.s. tuning a good step-size, which one costs more?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Sklcn-_c3m", "sentence_index": 0, "text": "The paper addresses a challenging problem of predicting the states of entities over the description of a process.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "Sklcn-_c3m", "sentence_index": 1, "text": "The paper is very well written, and easily understandable.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "Sklcn-_c3m", "sentence_index": 2, "text": "The authors propose a graph structure for entity states, which is updated at each step using the outputs of a machine comprehension system.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Sklcn-_c3m", "sentence_index": 3, "text": "The approach is novel and well motivated.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "Sklcn-_c3m", "sentence_index": 4, "text": "I will suggest a few improvements:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Sklcn-_c3m", "sentence_index": 5, "text": "1. the NPN model seems a good alternative, will be good to have a discussion about why your model is better than NPN.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "Sklcn-_c3m", "sentence_index": 6, "text": "Also, NPN can probably be modified to output spans of a sentence.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Sklcn-_c3m", "sentence_index": 7, "text": "I will be curious to know how it performs.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "Sklcn-_c3m", "sentence_index": 8, "text": "2. A more detailed illustration of the system / network is needed. Would have made it much easier to understand the paper.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Sklcn-_c3m", "sentence_index": 9, "text": "3. What are the results when using the whole training set of Recipes ?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "SklHkeMohX", "sentence_index": 0, "text": "The paper proposes doubly sparse, which is a sparse mixture of sparse experts and learns a two-level class hierarchy, for efficient softmax inference.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklHkeMohX", "sentence_index": 1, "text": "[+] It reduces computational cost compared to full softmax.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SklHkeMohX", "sentence_index": 2, "text": "[+] Ablation study is done for group lasso, expert lasso and load balancing, which help understand the effect of different components of the proposed", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SklHkeMohX", "sentence_index": 3, "text": "[-] It seems to me the motivation is similar to that of Sparsely-Gated MoE (Shazeer et al. 2017), but it is not clear how the proposed two-hierarchy method is superior to the Sparsely-Gated MoE. It would be helpful the paper discuss more about this.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SklHkeMohX", "sentence_index": 4, "text": "Besides, in evaluation, the paper only compares Doubly Sparse with full softmax.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SklHkeMohX", "sentence_index": 5, "text": "Why not compare with Sparsely-Gated MoE?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SklHkeMohX", "sentence_index": 6, "text": "Overall, I think this paper is below the borderline of acceptance due to insufficient comparison with Sparsely-Gated MoE.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SklswyoFnX", "sentence_index": 0, "text": "The paper focuses on the generalization performance of RNNs and its variant in a theoretical perspective.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklswyoFnX", "sentence_index": 1, "text": "Compared to the previous result (Zhang et al., 2018) for RNNs, this paper refines the generalization bounds for vanilla RNNs in all cases and fills the blank for RNN variants like MGU and LSTM.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklswyoFnX", "sentence_index": 2, "text": "Specifically, in the work of Zhang et al. (2018), the complexity term quadratically depends on the layer (or say, current sequence length, denoted by t in original paper), making it less instructive.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklswyoFnX", "sentence_index": 3, "text": "This paper improves it to at most linear dependence and can achieve at logarithmic dependence in some cases, which should be accredited.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklswyoFnX", "sentence_index": 4, "text": "The key step in the proof is Lemma 2.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklswyoFnX", "sentence_index": 5, "text": "In Lemma 2, the spectral norms of weight matrices and the number of weight parameters are decoupled.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklswyoFnX", "sentence_index": 6, "text": "With Lemma 2, it is natural to construct a epsilon-net for RNNs and then upper bound the empirical Rademacher complexity by Dudley\u2019s entropy integral, since such methodology is not so novel.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklswyoFnX", "sentence_index": 7, "text": "Bartlett, et al. (2017) developed this technique to analyze the generalization bound for neural networks in a margin-based multiclass classification.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklswyoFnX", "sentence_index": 8, "text": "However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SklswyoFnX", "sentence_index": 9, "text": "I wonder the motivation of analyzing generalization of RNNs by the techniques established by Bartlett.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skx-jEBr5B", "sentence_index": 0, "text": "This paper proposed a new pooling method (Frequency pooling) which is strict shift equivalent and anti-aliasing in theory.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Skx-jEBr5B", "sentence_index": 1, "text": "The authors first derived the theory of F-pooling to be optimal anti-aliasing down sampling and is shift-equivalent in sec 2, and then demonstrated the experimental results of 1D signals and image classification tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Skx-jEBr5B", "sentence_index": 2, "text": "The experimental results are actually less impressive than what are claimed in contribution and conclusion.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skx-jEBr5B", "sentence_index": 3, "text": "The authors stated that \"F-pooling remarkably increases accuracy and robustness w.r.t. shifts of moderns CNNs\"; however, in Table 1-3, the winning margin of accuracy is actually quite small (<2%), and the consistency (<3.5% compared to the second best baseline except resnet-18 on CIFAR 100 has large improvement ~7-8%).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skx-jEBr5B", "sentence_index": 4, "text": "Questions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Skx-jEBr5B", "sentence_index": 5, "text": "1. For the experiment of 1D signal on sine wave, the AA-pooling and F-pooling give the same result?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "none", "pol": "pol_negative"}, {"review_id": "Skx-jEBr5B", "sentence_index": 6, "text": "2. Compared to AA-pooling, it seems that F-pooling has a better theoretical guarantee (i.e. the optimal anti-aliasing down sampling operation given U).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Skx-jEBr5B", "sentence_index": 7, "text": "But other than this, the empirical performance seem not showing particular advantage over AA-pooling.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skx-jEBr5B", "sentence_index": 8, "text": "Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skx-jEBr5B", "sentence_index": 9, "text": "3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skx-jEBr5B", "sentence_index": 10, "text": "- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skx7vDii3X", "sentence_index": 0, "text": "This paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Skx7vDii3X", "sentence_index": 1, "text": "Semi-supervised few-shot learning is important considering the limitation of the very few labeled instances.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Skx7vDii3X", "sentence_index": 2, "text": "This is an interesting work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "Skx7vDii3X", "sentence_index": 3, "text": "The merits of this paper lie in the following aspects: (1) It is the first to learn label propagation for transductive few-shot learning.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "Skx7vDii3X", "sentence_index": 4, "text": "(2) The proposed approach produced effective empirical results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Skx7vDii3X", "sentence_index": 5, "text": "The drawbacks  of the work include the following: (1) There is not much technical contribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skx7vDii3X", "sentence_index": 6, "text": "It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skx7vDii3X", "sentence_index": 7, "text": "Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Skx7vDii3X", "sentence_index": 8, "text": "(2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "Skx7vDii3X", "sentence_index": 9, "text": "Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Skx7vDii3X", "sentence_index": 10, "text": "For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Skx7vDii3X", "sentence_index": 11, "text": "This is a major concern.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkxAD0ECtB", "sentence_index": 0, "text": "This paper proposes new pre-training strategies for GNN with both a node-level and a graph-level pretraining.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkxAD0ECtB", "sentence_index": 1, "text": "For the node-level pretraining, the goal is to map nodes with similar surrounding structures to nearby context (similarly to word2vec).", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkxAD0ECtB", "sentence_index": 2, "text": "The main problem is that directly predicting the context is intractable because of combinatorial explosion.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SkxAD0ECtB", "sentence_index": 3, "text": "The main idea is then to use an additional GNN to encode the context and to learn simultaneously the main GNN and the context GNN via negative sampling.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkxAD0ECtB", "sentence_index": 4, "text": "Another method used is attribute masking where some masked node and edge attributes need to be predicted by the GNN.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkxAD0ECtB", "sentence_index": 5, "text": "For graph-level pretraining, some general graph properties need to be predicted by the graph.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkxAD0ECtB", "sentence_index": 6, "text": "Experiments are conducted on datasets in the chemistry domain and the biology domain showing the benefit of the pre-training.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkxAD0ECtB", "sentence_index": 7, "text": "The paper addresses an important and timely problem.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "SkxAD0ECtB", "sentence_index": 8, "text": "It is a pity that the code is not provided.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SkxAD0ECtB", "sentence_index": 9, "text": "In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SkxAD0ECtB", "sentence_index": 10, "text": "In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SkxED7_nKS", "sentence_index": 0, "text": "I take issue with the usage of the phrase \"skill discovery\".", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SkxED7_nKS", "sentence_index": 1, "text": "In prior work (e.g. VIC, DIAYN), this meant learning a skill-conditional policy.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkxED7_nKS", "sentence_index": 2, "text": "Here, there is only a single (unconditioned) policy, and the different \"skills\" come from modifications of the environment -- the number of skills is tied to the number of environments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SkxED7_nKS", "sentence_index": 3, "text": "This is not to say that this way of doing things is wrong, but rather that it is misleading in the context of prior work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SkxED7_nKS", "sentence_index": 4, "text": "Skill discovery in this context implies being able to have a single agent execute a variety of learned skills, rather than having one agent per environment with each environment designed to elicit a specific skill.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkxED7_nKS", "sentence_index": 5, "text": "Rather than \"skill discovery\", I suggest the authors position MISC relative to earlier work on empowerment, wherein a single policy was used to maximize mutual information of the form I(a; s_t | s_{t-1}).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SkxED7_nKS", "sentence_index": 6, "text": "Modifying the objective to incorporate domain knowledge (as done in your DIAYN baseline) yields I(a; s_i | s_{t-1}) and is amenable to maximization by either of the lower bounds considered here.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SkxED7_nKS", "sentence_index": 7, "text": "Indeed, your DIAYN baseline with skill length set to 1 and the number of skills equal to the number of actions (or same parameterization in the case of continuous actions) should recover this approach.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "SkxED7_nKS", "sentence_index": 8, "text": "I believe this would be a much more appropriate baseline, and I'd be curious to hear the intuition for why I(s_c ; s_i) should be superior.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SkxED7_nKS", "sentence_index": 9, "text": "Apart from this missing baseline, the experimental results seem convincing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "SkxED7_nKS", "sentence_index": 10, "text": "However, it is unclear whether or not VIME and PER were modified to incorporate domain knowledge (i.e. s_i/s_c distinction).", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SkxED7_nKS", "sentence_index": 11, "text": "Indeed, an appendix would be greatly appreciated, as many experimental details were omitted.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "SkxED7_nKS", "sentence_index": 12, "text": "Ideally, an experimental setup with previously published results (e.g. control suite for DIAYN, Seaquest for DISCERN) would be considered, but I can understand why this wasn't done as incorporating domain knowledge is the main contribution of the paper.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SkxED7_nKS", "sentence_index": 13, "text": "That said, the claims should be weakened to reflect this gap, and domain knowledge should be mentioned more prominently (e.g. states of interest vs context are given, not learned).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "SkxED7_nKS", "sentence_index": 14, "text": "Rebuttal EDIT:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SkxED7_nKS", "sentence_index": 15, "text": "The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SkxED7_nKS", "sentence_index": 16, "text": "Needing new environment variations to obtain new skills is a large step backwards from things like DIAYN (the MISC/DIAYN combination needs more evidence to be considered a possible solution), and the s_i/s_c distinction is non-trivial to specify or learn for harder problems (e.g. pixel observations).", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkxED7_nKS", "sentence_index": 17, "text": "That said, in the sort of settings under consideration (low dimensional state variables and environmental variations are simple to create) MISC does appear to be superior to prior work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkxED7_nKS", "sentence_index": 18, "text": "The empowerment baseline is much appreciated, and while modifications of PER and VIME that incorporate prior knowledge would've also been nice, the experimental results pass the bar for acceptance in my view.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkxZmGP027", "sentence_index": 0, "text": "The paper proposes an approach to construct surrogate objectives for the effective application of policy gradient methods to combinatorial optimization without known neighborhood structure.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkxZmGP027", "sentence_index": 1, "text": "The surrogate is constructed with the goal of reducing the need of hyper-parameter tuning and evaluated on a clique finding task.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkxZmGP027", "sentence_index": 2, "text": "The proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SkxZmGP027", "sentence_index": 3, "text": "The proposed sampling distributions assumes independence between the random variables over which the authors optimize \u2014 I find it surprising that this leads to good empirical results are relatively little structure can be captured using this distribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SkxZmGP027", "sentence_index": 5, "text": "Can the authors elaborate on this?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "SkxZmGP027", "sentence_index": 6, "text": "However, as also observed by the authors, the sampling distribution can also be replaced by more sophisticated distributions.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SkxZmGP027", "sentence_index": 7, "text": "The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Sye1Z22R5B", "sentence_index": 0, "text": "The paper presents algorithms for solving computational problems in a datastream model augmented with an oracle learned from data.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Sye1Z22R5B", "sentence_index": 1, "text": "The authors show that under this model, there exist algorithms that have significantly better time and space complexity than the current best known algorithms that do not use an oracle.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Sye1Z22R5B", "sentence_index": 2, "text": "The authors support their theoretical analysis with experiments in which the oracle is represented by a deep neural network and demonstrate improvement over classical algorithms that do not use machine learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Sye1Z22R5B", "sentence_index": 3, "text": "Overall, this paper seems like a solid contribution to the literature.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "Sye1Z22R5B", "sentence_index": 4, "text": "However, in its current state it does seem to be presented and motivated in a way that is appropriate for the audience of ML researchers at ICLR.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "Sye1Z22R5B", "sentence_index": 5, "text": "It reads very much like a STOC theory paper, and a lot of the key ML details that would be relevant to audience at this conference seem to have been shoved under the rug in a way.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Sye1Z22R5B", "sentence_index": 6, "text": "Therefore my score for now is a weak reject, but I am very happy to increase the score if the authors address my presentations concerns.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Sye1Z22R5B", "sentence_index": 7, "text": "Major comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Sye1Z22R5B", "sentence_index": 8, "text": "* The oracle-augmented datasteam model needs to be contextualized better.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Sye1Z22R5B", "sentence_index": 9, "text": "I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me. For example, how do I even know that the oracle in question exists? What are the particular assumptions under which it exists? What are the requirements on the training data, optimization ability, generalization error, etc. How do we know that we can create in practice ML learning models that are sufficiently accurate to serve as an oracle?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Sye1Z22R5B", "sentence_index": 11, "text": "* The connections to deep learning seem arbitrary in some of the experiments. In one of the experiments, the authors train neural networks over a concatenation of IP address embeddings. Why do we need to use deep learning here?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Sye1Z22R5B", "sentence_index": 14, "text": "What is the benefit of using DL algorithms within the oracle-augmented datastream model?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Sye1Z22R5B", "sentence_index": 15, "text": "Is a simple algorithm enough? What algorithms should we ideally use in practice?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Sye1Z22R5B", "sentence_index": 16, "text": "What if you used simpler online learning algorithms with formal accuracy guarantees?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Sye1Z22R5B", "sentence_index": 17, "text": "Minor comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Sye1Z22R5B", "sentence_index": 18, "text": "* I thought there was a bit over-selling in intro.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Sye1Z22R5B", "sentence_index": 19, "text": "The authors say that they match the theoretical lower bounds for several problems.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Sye1Z22R5B", "sentence_index": 20, "text": "However, you are in a different computational model in which you now have access to an oracle. This needs to be made more explicitly, and language could be a bit toned down (e.g. in this model, we can obtain runtime that match or improve over lower bounds...)", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "Syee2fmjYH", "sentence_index": 0, "text": "This paper presents a method for single image 3D reconstruction.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syee2fmjYH", "sentence_index": 1, "text": "It is inspired by implicit shape models, like presented in Park et al. and Mescheder et al., that given a latent code project 3D positions to signed distance, or occupancy values, respectively.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syee2fmjYH", "sentence_index": 2, "text": "However, instead of a latent vector, the proposed method directly outputs the network parameters of a second (mapping) network that displaces 3D points from a given canonical object, i.e., a unit sphere.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syee2fmjYH", "sentence_index": 3, "text": "As the second network maps 3D points to 3D points it is composable, which can be used to interpolate between different shapes.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syee2fmjYH", "sentence_index": 4, "text": "Evaluations are conducted on the standard ShapeNet dataset and the yields results close to the state-of-the-art, but using significantly less parameters.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syee2fmjYH", "sentence_index": 5, "text": "Overall, I am in favour of accepting this paper given some clarifications and improving the evaluations.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Syee2fmjYH", "sentence_index": 6, "text": "The core contribution of the paper is to estimate the network parameters conditioned on the input (i.e., the RGB image).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syee2fmjYH", "sentence_index": 7, "text": "As noted in the related work section this is not a completely new idea (cf. Schmidhuber, Ha et al.).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syee2fmjYH", "sentence_index": 8, "text": "There are a few more references that had similar ideas and might be worth adding: Brabandere et al. \"Dynamic Filter Networks\", Klein et al. \"A dynamic convolutional layer for short range weather prediction\", Riegler et al. \"Conditioned regression models for non-blind single image super-resolution\", and maybe newer works along the line of Su et al. \"Pixel-Adaptive Convolutional Neural Networks\".", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "Syee2fmjYH", "sentence_index": 9, "text": "The input 3D points are sampled from a unit sphere.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syee2fmjYH", "sentence_index": 10, "text": "Does this imply any topological constraints?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Syee2fmjYH", "sentence_index": 11, "text": "Is this the most suitable shape to sample from? How do you draw samples from the sphere (Similarly, how are the points sampled for the training objects)?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Syee2fmjYH", "sentence_index": 12, "text": "What happens if you instead densely sample from a 3D box (similar to the implicit shape models)?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Syee2fmjYH", "sentence_index": 13, "text": "On page 4 the mapping network is described as a function that maps c-dimensional points to 3D points.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "Syee2fmjYH", "sentence_index": 14, "text": "What is c? Isn't it always 3, or how else is it possible to composite the mapping network?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Syee2fmjYH", "sentence_index": 15, "text": "Regarding the main evaluation: The paper follows the \"standard\" protocol on ShapeNet.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Syee2fmjYH", "sentence_index": 16, "text": "Recently, Tatarchenko et al. showed in \"What Do Single-view 3D Reconstruction Networks Learn?\" shortcomings of this evaluation scheme and proposed alternatives.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Syee2fmjYH", "sentence_index": 17, "text": "It would be great if this paper could follow those recommendations to get better insights in the results.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "Syee2fmjYH", "sentence_index": 18, "text": "Further, I could not find what k was set to in the evaluation of Tab. 1.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Syee2fmjYH", "sentence_index": 19, "text": "It did also not match any numbers in Tab. 4 of the appendix.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Syee2fmjYH", "sentence_index": 20, "text": "Tab. 4 shows to some extend the influence of k, but I would like to see a more extensive evaluation.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "Syee2fmjYH", "sentence_index": 21, "text": "How does performance change for larger k, and what happens if k is larger at testing then on at training, etc.?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Syee2fmjYH", "sentence_index": 22, "text": "Things to improve the paper that did not impact the score:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Syee2fmjYH", "sentence_index": 23, "text": "- The tables will look a lot nicer if booktab is used in LaTeX", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SyeR04XiuS", "sentence_index": 0, "text": "This work focuses on learning a good policy for hyperparameters schedulers, for example learning rate or weight decay, using reinforcement learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeR04XiuS", "sentence_index": 1, "text": "The main contributions include 1) a discretization on the learning curves such that transformer can be applied to predict the them; 2) an empirical evaluations using the predicted learning curves to train the policy.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeR04XiuS", "sentence_index": 2, "text": "The main novelties are two folds.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyeR04XiuS", "sentence_index": 3, "text": "On the methodology side, using predicted learning curves instead of real ones can speed up training significantly.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeR04XiuS", "sentence_index": 4, "text": "On the technical side, the author presented a discretization step to use transformer for learning curve predictions.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeR04XiuS", "sentence_index": 5, "text": "The results are mixed, we see slightly advantage over human baseline on one task but worse in the other.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeR04XiuS", "sentence_index": 6, "text": "Human baseline does not need any training!", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeR04XiuS", "sentence_index": 7, "text": "On the writing part, it would be nice to provide more context for both transformer, Proximal Policy Optimization and Simulated Policy Learning to make the paper more self-complete.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "SyeR04XiuS", "sentence_index": 8, "text": "I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyeR04XiuS", "sentence_index": 9, "text": "* Baker, Bowen, et al. \"Accelerating neural architecture search using performance prediction.\" arXiv preprint arXiv:1705.10823 (2017).", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyeR04XiuS", "sentence_index": 10, "text": "* Domhan, Tobias, Jost Tobias Springenberg, and Frank Hutter. \"Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves.\" Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyeR04XiuS", "sentence_index": 11, "text": "Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyeR04XiuS", "sentence_index": 12, "text": "How does the transformer based method comparing to others?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyeS7CQ83m", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 1, "text": "This paper proposes a generative point cloud model based on adversarial learning and definitti\u2019s representation theorem of exchangeable variables.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 2, "text": "The main focus in experiments and the exposition is on 3D point clouds representing object shapes (seems the surface, but could also be the interior of objects, please clarify).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 3, "text": "The main idea is to represent a point cloud using a global latent variable that captures the overall shape, and a collection of local latent variables that code for the position of a point on the shape.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 4, "text": "The model consists of thee components: (i) an \u201cencoder\u201d that takes a point cloud as input and maps it to a (point estimate of) the global latent variable of the shape represented by the input cloud, a point-net architecture is used here (ii) a \u201cdecoder\u201d that takes the estimated global latent variable, and a local latent variable, and maps it to an \u201coutput\u201d point in the cloud to be produced by the model. (iii) a \u201cdiscriminator\u201d network that aims to distinguish points from a *given* shape, and the points produced by pipe-lining the encoder and decoder. Critically different from conventional GANs, the discriminator is optimized *per shape*, ie each point cloud is considered as a *distribution* over R^3 specific to that shape. (iv) a \u201cshape prior\u201d that, once the encoder-decoder model from above is trained, is used to model the distribution over the global latent variables.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 10, "text": "This model is trained, presumably, in a conventional GAN style using the global latent variable representations inferred across the different training point clouds.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 11, "text": "As compared to prior work by Achiloptas et al (2017), the proposed approach has the advantage to allow for sampling an arbitrary number of points from the target shape, rather than a fixed pre-defined number.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 12, "text": "In addition, the authors propose to minimize a weighted average of a lower bound and upper bound on the Wasserstein distance between the distributions of points corresponding to given shapes. This approach translates to improved quantitative evaluation measures, Experiments are conducted on a simple toy data set, as  a proof of concept, and on data from ModelNet10 and ModelNet40.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 15, "text": "Two performance metrics are introduced to assess the auto-encoding ability of the model: to what extent does the encoder-decoder pipeline result in point clouds similar to the shape from which the input point-cloud is generated.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 16, "text": "Overall I find the idea of the paper interesting and worth publishing, but the exposition of the paper is less than ideal and needs further work.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 17, "text": "The experimental validation of the proposed approach can also be further improved, see more specific comments below.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 18, "text": "Specific comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 19, "text": "- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyeS7CQ83m", "sentence_index": 20, "text": "- The notation in section 3 (before 3.1) is rather sloppy. For example, - please define P and G, the elements of the divergence D(P||G) that appears in the first paragraph of section 3. - it is not defined in which space theta lives, it is not clear what the authors intend with the notation G_theta(u) \\sim p(theta).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyeS7CQ83m", "sentence_index": 24, "text": "- what prior distributions p(z) and p(u) are used? What is the choice based on?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyeS7CQ83m", "sentence_index": 25, "text": "- abbreviation IPM is referred several times in the paper, but remains undefined in the paper until end of page 4, please define earlier.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyeS7CQ83m", "sentence_index": 26, "text": "- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyeS7CQ83m", "sentence_index": 27, "text": "- Lack of clarity in the following passage: \u201cIn our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images\u201d", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SyeS7CQ83m", "sentence_index": 28, "text": "- The notion of divergence D(P|G) is not made concrete in section 3 and 3.1, which makes the notation of rather little use.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "SyeS7CQ83m", "sentence_index": 29, "text": "- The following paper merits a discussion in the related work section:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 30, "text": "\u201cTOWARDS A NEURAL STATISTICIAN\u201d, ICLR\u201917,", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 31, "text": "https://openreview.net/pdf?id=HJDBUF5le", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 32, "text": "- The manuscript contains many typos. For example", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 34, "text": "\u201cvedio\u201d op page 4, \u201ccircile\u201d on page 5, \u201ccondct\u201d on page 8, etc.", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_neutral"}, {"review_id": "SyeS7CQ83m", "sentence_index": 35, "text": "Please proof read your paper and fix these.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 36, "text": "The refenence to  Bengio 2018 is incomplete: what do you refer to precisely?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SyeS7CQ83m", "sentence_index": 37, "text": "- There seems to be no mention of the dimension of the \u201clocal\u201d latent variables z_i.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyeS7CQ83m", "sentence_index": 38, "text": "Please comment on the choice, and its impact on the behavior of the model.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyeS7CQ83m", "sentence_index": 39, "text": "- The quantitative evaluation in table 1 is interesting and useful.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "SyeS7CQ83m", "sentence_index": 40, "text": "It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SyeS7CQ83m", "sentence_index": 41, "text": "Quantitative evaluation of generative modeling performance is unfortunately missing from this paper, as it is in much of the GAN literature. Could you please comment on how this can/will be fixed?", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "SyeS7CQ83m", "sentence_index": 43, "text": "- The toy data set experiments could be dropped  to make room for experiments suggested below.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SyeS7CQ83m", "sentence_index": 44, "text": "- An experimental study of the effect of the mixing parameter \u201cs\u201d would be useful to include. For example, by taking s on a grid from 0 to 1, one could plot the coverage and distance-to-face measures.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SyeS7CQ83m", "sentence_index": 46, "text": "- Experimental evaluation of auto-encoding using a variable number of input points is interesting to add: ie how do the two evaluation measures evolve as a function of the number of points in the input point cloud?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SyeS7CQ83m", "sentence_index": 47, "text": "- Similar, it is interesting to evaluate how auto encoding performs when non-uniform decimation of the input cloud is performed, eg what happens if we \u201cchop off\u201d part of the input point cloud (eg the legs of the chair), does the model recover and add the removed parts? This is potentially useful to practitioners which have to deal with incomplete point clouds acquired by range scanners.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SyeS7CQ83m", "sentence_index": 49, "text": "- Analysis of shapes with different genus and dimensions would be interesting.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SyeS7CQ83m", "sentence_index": 50, "text": "Does the model manage to capture that some shapes have holes, or consists of a closed 2D surface (ball) vs an open surface (disk), despite a simple prior on the local latent variables z?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SygmVm7XpQ", "sentence_index": 0, "text": "This paper proposes an approach for automatic robot design based on Neural graph evolution.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SygmVm7XpQ", "sentence_index": 1, "text": "The overall approach has a flavor of genetical algorithms, as it also performs evolutionary operations on the graph, but it also allows for a better mechanism for policy sharing across the different topologies, which is nice.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SygmVm7XpQ", "sentence_index": 2, "text": "My main concern about the paper is that, currently, the experiments do not include any strong baseline (the ES currently is not a strong baseline, see comments below).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 3, "text": "The experiments currently demonstrate that optimizing both controller and hardware is better than optimizing just the controller, which is not surprising and is a phenomenon which has been previously studied in the literature.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 4, "text": "What instead is missing is an answer to the question: Is it worth using a neural graph? what are the advantages and disadvantages compared to previous approaches?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 5, "text": "I would like to see additional experiments to answer this questions.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SygmVm7XpQ", "sentence_index": 6, "text": "In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 7, "text": "You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 8, "text": "If you can also compare against one or two algorithms of your choice from the recent literature it would also give more value to the comparison.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "SygmVm7XpQ", "sentence_index": 9, "text": "Detailed comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SygmVm7XpQ", "sentence_index": 10, "text": "- in the abstract you say that \"NGE is the first algorithm that can automatically discover complex robotic graph structures\".", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "SygmVm7XpQ", "sentence_index": 11, "text": "This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 12, "text": "- in the introduction you mention that automatic robot design had limited success.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "SygmVm7XpQ", "sentence_index": 13, "text": "This is rather subject, and I would tend to disagree.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 14, "text": "Moreover, the same limitations that apply to other algorithms to make them successful, in my opinion, apply to your proposed algorithm (e.g., difficulty to move from simulated to real-world).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SygmVm7XpQ", "sentence_index": 15, "text": "- The digression at the bottom of the first page about neural architecture search seem out of context and interrupts the flow of the introduction.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 16, "text": "What is the point that you are trying to make? Also, note that some of the algorithms that you are citing there have indeed applied beyond architecture search, eg. Bayesian optimization is used for gait optimization in robotics, and Genetic algorithms have been used for automatic robot design.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 17, "text": "- The stated contributions number 3 and 5 are not truly contributions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 18, "text": "#3 is so generic that a large part of the previous literature on the topic fall under this category -- not new.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 19, "text": "#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 20, "text": "- Sec 2.2: \"(GNNs) are very effective\" effective at what? what is the metric that you consider?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 21, "text": "- Sec 3 \"(PS), where weights are reused\" can you already go into more details or refer to later sections?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 22, "text": "- First line page 4 you mention AF, without introducing the acronym ever before.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 23, "text": "- Sec 3.1: the statements about MB and MF algorithms are inaccurate.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 24, "text": "Model-based RL algorithms can work in real-time (e.g. http://proceedings.mlr.press/v78/drews17a/drews17a.pdf) and have been shown to have same asymptotic performance of MB controllers for simple robot control (e.g. https://arxiv.org/abs/1805.12114)", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SygmVm7XpQ", "sentence_index": 25, "text": "- \"to speed up and trade off between evaluating fitness and evolving new species\" Unclear sentence. speed up what ? why is this a trade-off?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 27, "text": "- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 28, "text": "- Sec 4.1: would argue that computational cost is rarely a concern among evolutionary algorithms.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SygmVm7XpQ", "sentence_index": 30, "text": "The cost of evaluating the function is typically more pressing, and as a result it is important to have algorithms that can converge within a small number of iterations/generations.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SygmVm7XpQ", "sentence_index": 31, "text": "- Providing the same computational budget seem rather arbitrary at the moment, and it heavily depends from implementation. How many evaluations do you perform for each method? why not having the same budget of experiments?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Syl0HVschm", "sentence_index": 0, "text": "Authors propose a novel combination of RBM feature extractor and CNN classifiers to gain robustness toward adversarial attacks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syl0HVschm", "sentence_index": 1, "text": "They first train a small mean field boltzmann machine on 4x4 patches of MNIST, then combine 4 of these into a larger 8x8 feature extractor.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syl0HVschm", "sentence_index": 2, "text": "Authors use the RBM 8x8 feature representation as a fixed convolutional layer and train a CNN on top of it.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syl0HVschm", "sentence_index": 3, "text": "The intuition behind the idea is that since RBMs are generative, the RBM layer will act as a denoiser.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syl0HVschm", "sentence_index": 4, "text": "One question which is not addressed is the reason for only one RBM layer.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Syl0HVschm", "sentence_index": 5, "text": "In \"Stacks of convolutional Restricted Boltzmann Machines for shift-invariant feature learning\" by Norouzi et al, several RBM layers are trained greedily (same as here, only difference is contrastive loss vs mean field) and they achieve 0.67% error on MNIST.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Syl0HVschm", "sentence_index": 6, "text": "Attacking CRBMs is highly relevant and should be included as a baseline.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "Syl0HVschm", "sentence_index": 7, "text": "The only set of experiments are comparisons on first 500 MNIST test images.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Syl0HVschm", "sentence_index": 8, "text": "If the test set is not shuffled (by emphasis on first I assume not) these images are from training NIST (cleaner) set and may not include samples of all digits.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "Syl0HVschm", "sentence_index": 9, "text": "Authors should clarify the justification behind experimenting only on 'first 500 test images'.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "Syl0HVschm", "sentence_index": 10, "text": "Furthermore, as authors discussed the iterative weight sharing which increases the depth can vanish the gradient toward input.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "Syl0HVschm", "sentence_index": 11, "text": "Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Syl0HVschm", "sentence_index": 12, "text": "The iterative architecture is similar to the routing in CapsNet (Hinton 2018) in terms of weight sharing between successive layers.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Syl0HVschm", "sentence_index": 13, "text": "Although their network was resilient toward white box attacks they suffered from black box attacks.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Syl0HVschm", "sentence_index": 14, "text": "The boundary method on MNIST could be  weaker than a black box attack.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyllU9Iq37", "sentence_index": 0, "text": "This paper provide a method to produce adversarial attack using a Frank-Wolfe inspired method.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyllU9Iq37", "sentence_index": 1, "text": "I have some concerns about the motivation of this method:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyllU9Iq37", "sentence_index": 2, "text": "- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SyllU9Iq37", "sentence_index": 3, "text": "- Consequently why did not you compare simple projected gradient method ?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyllU9Iq37", "sentence_index": 4, "text": "(BIM) is not equivalent to the projected gradient method since the direction chosen is the sign of the gradient and not the gradient itself (the first iteration is actually equivalent because we start at the center of the box but after both methods are no longer equivalent).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyllU9Iq37", "sentence_index": 5, "text": "- There is no motivations for the use of $\\lambda >1$ neither practical or theoretical since the results are only proven for $\\lambda =1$ whereas the experiments are done with \\lambda = 5,20 or 30.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SyllU9Iq37", "sentence_index": 6, "text": "- What is the difference between the result of Theorem 4.3 and the result from (Lacoste-Julien 2016)?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyllU9Iq37", "sentence_index": 7, "text": "Depending on the answer to these questions I'm planning to move up or down my grade.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyllU9Iq37", "sentence_index": 8, "text": "In the experiment there is no details on how you set the hyperparameters of CW and EAD.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SyllU9Iq37", "sentence_index": 9, "text": "They use a penalized formulation instead of a constrained one.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyllU9Iq37", "sentence_index": 10, "text": "Consequently the regularization hyperparameters have to be set differently.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyllU9Iq37", "sentence_index": 11, "text": "The only new result seem to be Theorem 4.7 which is a natural extension to theorem 4.3 to zeroth-order methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SyllU9Iq37", "sentence_index": 12, "text": "Comment:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyllU9Iq37", "sentence_index": 13, "text": "- in the whole paper there is $y$ which is not defined.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SyllU9Iq37", "sentence_index": 14, "text": "I guess it is the $y_{tar}$ fixed in the problem formulation Sec 3.2.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyllU9Iq37", "sentence_index": 15, "text": "In don't see why there is a need to work on any $y$. If it is true", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyllU9Iq37", "sentence_index": 16, "text": ",  case assumption 4.5 do not make any sense since $y = y_{tar}$ (we just need to note $\\|\\nabla f(O,y_{tar})\\| = C_g$) and some notation could be simplified setting for instance $f(x,y_{tar})  = f(x)$.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "SyllU9Iq37", "sentence_index": 17, "text": "- In Theorem 4.7 an expectation on g(x_a) is missing", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SyllU9Iq37", "sentence_index": 18, "text": "Minor comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyllU9Iq37", "sentence_index": 19, "text": "- Sec 3.1 theta_i -> x_i", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SyllU9Iq37", "sentence_index": 20, "text": "- Sec 3.3 the argmin is a set, then it is LMO $\\in$ argmin.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SyllU9Iq37", "sentence_index": 21, "text": "===== After rebuttal ======", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyllU9Iq37", "sentence_index": 22, "text": "The authors answered some of my questions but I still think it is a borderline submission.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "SylvLhvph7", "sentence_index": 0, "text": "The authors of this paper are proposing a neural network approach for learning diffusion dynamics in networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SylvLhvph7", "sentence_index": 1, "text": "The authors argue that the main advantage of their framework is that it incorporates the structure of independent cascades into the model which predicts the diffusion process.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SylvLhvph7", "sentence_index": 2, "text": "The primary difficulty in reviewing this paper is the poor presentation of the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SylvLhvph7", "sentence_index": 3, "text": "There are many typos and mistakes (e.g., the last paragraph of the paper starts with a sentence that does not make any sense), missing references (e.g., there is an empty parenthesis at the end of the second paragraph on the second page) and in at least two cases, there are references to a formula that is not in the manuscript (e.g., reference to formula 15 on line 3 of page 5).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SylvLhvph7", "sentence_index": 4, "text": "This issues makes reviewing this paper very difficult.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SylvLhvph7", "sentence_index": 5, "text": "In the modeling section, authors use p(I|D) as q^D(.) in the Eq. 12, where p(I|D) is the conditional probability that a particular node infected an observed infected node first.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "SylvLhvph7", "sentence_index": 6, "text": "Plugging p(I|D) in Eq. 12 and using decomposition of p(D ,I) used in Eq. 10, we arrive at a formulation which drops all p(I|D) terms.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "SylvLhvph7", "sentence_index": 7, "text": "This results in an objective function which only involves infected nodes (and no term associated with the parent node), weighted by likelihood of each node j infecting the node at step i. This should make the training more simplified than what is discussed in Algorithm 2.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SylvLhvph7", "sentence_index": 8, "text": "Beyond this simplification, I am not clear if that is actually intended by the authors.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SylvLhvph7", "sentence_index": 9, "text": "The experiments demonstrate a superior performance of the proposed model compared to alternative benchmarks.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "SylvLhvph7", "sentence_index": 10, "text": "The authors explain how they trained their own model but there is no mention on how they trained benchmark models.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SylvLhvph7", "sentence_index": 11, "text": "However, given that the datasets used in the experiments were not used in the associated benchmark papers, it is necessary for authors to explain how they trained competing models.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SylvLhvph7", "sentence_index": 12, "text": "Due to several shortcomings of the paper, most important of which is on presentation of the paper, this manuscript requires a significant revision by the authors to reach the necessary standards for publication, moreover it would be helpful to clarify the modeling choices and consequences of these choices more clearly.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 0, "text": "The paper proposes a new method for robustifying a pre-trained model improving its decision boundaries.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 1, "text": "The goal is to defend the model from mistakes in training labels and to be more robust to adversarial examples at test time.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 2, "text": "The main idea is to train a LDA on top of the last-layer, or many layers in its ensemble version, making use of a small set of clean labels after training the main model.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 3, "text": "Additionally, robustness to outliers is achieved by the minimum covariance determinant estimator for the LDA covariance matrix.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 4, "text": "While I find this idea interesting and of potential practical use, I have concerns about novelty and the experimental results and overall I recommend rejection.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 5, "text": "== Method", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 6, "text": "At a high level, the idea of imposing a mixture of gaussian structure in the feature space of a deep neural network classifier is not new.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 7, "text": "See for example [A, B].", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 8, "text": "In particular, [B] performs experiments on adversarial examples.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 9, "text": "Moreover, in spite of the authors writing that their goal is \u201ccompletely different\u201d from [Lee at al 18a, Ma et al 18a], I found the two cited papers having a similar intent and approach to the problem, but a comparison is completely missing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 10, "text": "Without a proper comparison (formal and experimental) with these lines of work, the paper is incomplete.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 11, "text": "Theorem 1 well supports the proposed method and it is well explained. I did not check the proofs in appendix.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "SyxhCjde3Q", "sentence_index": 12, "text": "Regarding the presentation, I found odd having some experimental results (page 5) before the Section on experience even have started.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 13, "text": "== Experiments", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 14, "text": "The authors did not comment on the computational overhead of training their LDA.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 15, "text": "But I assume it is very cheap compared to training e.g. the ResNet, correct?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SyxhCjde3Q", "sentence_index": 16, "text": "I also did not find an explanation of which version backward/forward losses [Patrini et al. 17] is used in the experiments: are the noise transition matrices estimated on the data or assumed to be known (for fair comparison, I would do the former).", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SyxhCjde3Q", "sentence_index": 17, "text": "I disagree on the importance of the numbers reported on the abstract: DenseNet on Cifar10 with 60% goes from 53.34 to 74.72.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 18, "text": "This is the improvement with the weakest possible baseline, i.e. no method to defend for noise!", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 19, "text": "Looking at Table 3, which is on ResNets, I will make this point clear.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 20, "text": "Noise 60% on CIFAR10, DDGC improves 60.05-> 71.38, while (hard) bootstrap and forward do better.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 21, "text": "Even more, it seems that forward does always better than DDGC with noise 60% on every dataset.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 22, "text": "Therefore, I don\u2019t find interesting to report how DDGC improve upon \u201cno baseline\u201d, because known methods do even better.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 23, "text": "Yet, it is interesting --- and I find this to be a contribution of the paper --- that DDGC can be used in combination with prior work to boost performance even further.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "SyxhCjde3Q", "sentence_index": 24, "text": "A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 25, "text": "An additional column on the table showing that the algorithm can also work in this case would improve the confidence that the proposed method is useful in practice.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SyxhCjde3Q", "sentence_index": 26, "text": "Uniform noise is the least realistic assumption for label noise.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 27, "text": "Regarding the experiments on adversarial examples, I am not convinced of their relevance at all.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 28, "text": "There are now dozens of defence methods that work (partially) for improving robustness.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 29, "text": "I don\u2019t think it is of any practical use to show that a new algorithm (such at DDGD) provide some defence compared to no defence.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 30, "text": "A proper baseline should have been compared.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 31, "text": "One more unclear but important point: is Table 3 obtained by white-box attacks on the Resnet/Denset but oblivious of the MCD? Is so, I don\u2019t think such an experiment tells the whole story: as the the MCD would arguably also be deployed for classification, the attacker would also target it.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 32, "text": "Additionally, the authors state \u201cwe remark that accessing the parameters of the generative classifiers [\u2026] is not a mild assumption since the information about training data is required to compute them\u201d.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 33, "text": "I don\u2019t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what is the difference here?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 35, "text": "Table 8 rises some concerns.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 36, "text": "I appreciate the idea of testing full white-box adversarial attacks here. But I don\u2019t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SyxhCjde3Q", "sentence_index": 37, "text": "[A] Wen, Yandong, et al. \"A discriminative feature learning approach for deep face recognition.\" European Conference on Computer Vision. Springer, Cham, 2016.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyxhCjde3Q", "sentence_index": 38, "text": "[B] Wan, Weitao, et al. \"Rethinking feature distribution for loss functions in image classification.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyxO5eTc3X", "sentence_index": 0, "text": "The authors propose a generative model of networks by learning embeddings and pairing the embeddings with a prior distribution over networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyxO5eTc3X", "sentence_index": 1, "text": "The idea is that the prior distribution may explain structure that the embeddings would not have to capture.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyxO5eTc3X", "sentence_index": 2, "text": "The motivation for doing this is that this structure is typically hard to model for network embeddings.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_neutral"}, {"review_id": "SyxO5eTc3X", "sentence_index": 3, "text": "The authors propose a clean -if improper- prior on networks and proceed to perform maximum likelihood inference on it.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SyxO5eTc3X", "sentence_index": 4, "text": "The experiments show that the approach works fine for link porediction and can be used for visualization.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "SyxO5eTc3X", "sentence_index": 5, "text": "Two points:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyxO5eTc3X", "sentence_index": 6, "text": "a) Why not try to do this with Variational inference? It should conceptually still work and be fast and potentially more robust.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SyxO5eTc3X", "sentence_index": 7, "text": "b) The prior seems to be picked according to properties of the observed data and expressed in a product of constraints.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SyxO5eTc3X", "sentence_index": 8, "text": "This seems clunky, I would have been more impressed with a prior structure that ties in closer with the embeddings and requires less hand-engineering.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SyxO5eTc3X", "sentence_index": 9, "text": "A key point of interest is the following: very exciting recent work (GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models by You et al ICML2018) has proposed neural generative models of networks with a high degree of fidelity and much less hand-picked features.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "SyxO5eTc3X", "sentence_index": 10, "text": "The work here tries to not learn a lot of these structures but impose them.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyxO5eTc3X", "sentence_index": 11, "text": "Do the authors think that ultimately learning priors with models like GraphRNN might be more promising for certain applications?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SyxO5eTc3X", "sentence_index": 12, "text": "The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SyxO5eTc3X", "sentence_index": 13, "text": "A more predictive generative model that makes less hard assumptions on graph data would be interesting.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SyxO5eTc3X", "sentence_index": 14, "text": "Update After rebuttal:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyxO5eTc3X", "sentence_index": 15, "text": "Given the authors' rebuttal to all reviews, I am upgrading my score to a 6.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyxO5eTc3X", "sentence_index": 16, "text": "I still feel that more learning (as inGraphRNN) to build a fuller generative model of the graph would be interesting, but the authors make a strong case for the usefulness and practicality of their approach.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyxPPFpDhm", "sentence_index": 0, "text": "This paper presents a new way to represent a dense matrix in a compact format.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyxPPFpDhm", "sentence_index": 1, "text": "First, the method prunes a dense matrix based on the Viterbi-based pruning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyxPPFpDhm", "sentence_index": 2, "text": "Then, the pruned matrix is quantized with alternating multi-bit quantization.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyxPPFpDhm", "sentence_index": 3, "text": "Finally, the binary vectors produced by the quantization algorithm are further compressed with the Viterbi-based algorithm.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyxPPFpDhm", "sentence_index": 4, "text": "It spots the problem of each existing approach and solve the problems by combining each method.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SyxPPFpDhm", "sentence_index": 5, "text": "The combination is new and the result is encouraging.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "SyxPPFpDhm", "sentence_index": 6, "text": "I find this paper is interesting and I like the strong results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "SyxPPFpDhm", "sentence_index": 7, "text": "It is an interesting combination of methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SyxPPFpDhm", "sentence_index": 8, "text": "However, the experiments are not enough to show that the proposed method is really needed to achieve the results. If these are answered well, I'd be happy to change my evaluation.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "SyxPPFpDhm", "sentence_index": 9, "text": "1. The method should be compared with other combinations of components.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "SyxPPFpDhm", "sentence_index": 10, "text": "At least, it should be compared with \"Multi-bit quantization only (Xu et al., 2018)\" and \"Multi-bit-quantization + Viterbi-based binary code encoding\".", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "SyxPPFpDhm", "sentence_index": 11, "text": "2. The experiments with \"Don't Care\" should go to the experiment section, and the end-to-end results should be present but not the ratio of incorrect bits.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "SyxPPFpDhm", "sentence_index": 12, "text": "3. Similarly, the paper will become stronger if it has some experimental results that compare quantization methods.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_meaningful-comparison", "pol": "pol_neutral"}, {"review_id": "SyxPPFpDhm", "sentence_index": 13, "text": "In Section 3.3. , it mentions that the conventional k-bit quantization was tried and significant accuracy drops were observed.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyxPPFpDhm", "sentence_index": 15, "text": "I feel that this is a kind of things which support the proposed method if it is properly assessed.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyxPPFpDhm", "sentence_index": 16, "text": "4. When you say \"slow\" form something and propose a method to address it, I'd like to see some benchmark numbers. There is an experiment with simulation, but that does not seem to simulate the slow \"sequential sparse matrix decoding process\".", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SyxPPFpDhm", "sentence_index": 17, "text": "Minor comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "SyxPPFpDhm", "sentence_index": 18, "text": "* It was a bit hard to understand how a matrix is processed through the flowchart in Fig. 1 at first glance.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "SyxPPFpDhm", "sentence_index": 19, "text": "It would help readers to understand it better if it has a corresponding figure which shows how a matrix is processed through the flowchart.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 0, "text": "This paper proposes a method for mathematical problem embedding, which firstly decomposes problems into concepts by an abstraction step and then trains a skip-gram model to learn concept embedding.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 1, "text": "A problem can be represented as the average concept (corresponding to those in the problem) embeddings.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 2, "text": "To handle the imbalanced dataset, a negative pre-training method is proposed to decrease false and false positives.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 3, "text": "Experimental results show that the proposed method works much better than baselines in similar problem detection, on an undergraduate probability data set.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 4, "text": "Strong points:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 5, "text": "(1)\tThe idea of decomposing problems into concepts is interesting and also makes sense.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 6, "text": "(2) The training method for imbalanced datasets is impressive.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 8, "text": "Concerns or suggestions:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 9, "text": "1.\tThe main idea of using contents to represent a problem is quite simple and straightforward.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 10, "text": "The contribution of this paper seems more on the training method for imbalanced data sets.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 11, "text": "But there are no comparisons between the proposed training method and previous related works.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 12, "text": "Actually, imbalance data sets are common in machine learning problems and there are many related works.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 13, "text": "The comparisons are also absent in experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 14, "text": "2.\tThe experimental data set is too small, with only 635 problems.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 15, "text": "It is difficult to judge the performance of the proposed model based on so small data set.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 16, "text": "3.\tThe proposed method, which decomposes a problem into multiple concepts, looks general for many problem settings.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 17, "text": "For example, representing a movie or news article by tags or topics.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 18, "text": "In this way, the proposed method can be tested in a broader domain and on larger datasets.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "Syxz3gT_2Q", "sentence_index": 19, "text": "4.\tFor the final purpose, comparing problem similarity, I am wondering what the result will be if we train a supervised model based problem-problem similarity labels?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}], "rebuttal_sentences": [{"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 0, "text": "Dear Reviewer,", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 1, "text": "Thank you for your valuable comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 2, "text": "We have revised our writing in the revision, and will further improve its clarity.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 3, "text": "Please find our response as follows.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 4, "text": "- Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 5, "text": "Mitosis training can be considered as executing Algorithm 1 for multiple times with an increasing number of experts and inherited initialization from last round by changing W^e and W^g.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 6, "text": "Also, training with mitosis achieves similar performance as training without it shown in Appendix B, Figure (a).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 7, "text": "- How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 8, "text": "The hyper-parameters related to DS-softmax (such as lambda) are tuned according to the performance on a validation dataset.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 9, "text": "Also, as we mentioned in the paper, only one hyper-parameter (group lasso lambda) needs to be tuned.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 10, "text": "The heuristic we use to tune group lasso lambda is to increase lambda, starting from a small value, until it hurts the performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 11, "text": "Also threshold and balancing lambda variables are kept fixed as (0.01 and 10).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 12, "text": "- Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 13, "text": "In terms of baselines, SVD-softmax (NIPS\u201917) was chosen since it is a recent method that provides a significant inference speedup for softmax.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 14, "text": "Other alternatives, such as D-softmax and adaptive-softmax, focus on training instead of inference speedup.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 15, "text": "Furthermore, as claimed in their papers, they achieve limited speedup (around 5x) in language modeling, which is much worse than ours.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 16, "text": "With regards to Sparsely Gated MoE, it cannot speed up inference, since they select expert with full softmax.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 17, "text": "We would like to emphasize that most existing methods for inference speedup focus on approximating trained softmax layer, which usually suffers a loss on performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "B1euHOqi37", "rebuttal_id": "B1e8djk9Tm", "sentence_index": 18, "text": "Our model allows the adaptive adjustment of the softmax layer, achieves speedup through capturing the two-level overlapped hierarchy during training, which is novel and does not suffer from the performance loss.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 0, "text": "Thank you for your thoughtful review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 1, "text": "We appreciate that you think our problem and approach are interesting! We have made some substantial revisions to clarify the presentation, we hope that these will help address your concerns.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 2, "text": "We respond to some specific comments below:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 3, "text": "> No comparisons were provided to any baselines/alternative methods.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 4, "text": "We do compare to a number of lesions in the supplement, and to an alternative method (performing tasks from a description alone).", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [6]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 5, "text": "We have moved this latter comparison to the main text at the suggestion of Reviewer 1.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 6, "text": "> in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 7, "text": "Also, I felt that the proposed approach in Section 5 is very similar to MAML intuitively.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 8, "text": "This makes a comparison with MAML even more desirable. Without any comparisons it\u2019s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 9, "text": "Our main contribution is to propose a meta-mapping framework for zero-shot task performance, and parsimonious method for performing these meta-mappings.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 10, "text": "MAML as such is not a method of zero-shot task performance, it requires examples to learn", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 11, "text": "from", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 12, "text": ".", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 13, "text": "We could therefore compare to MAML for our basic-meta-learning results, but those are simply a sanity check.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 14, "text": "We also compare to a variety of baselines, including chance and optimal performance, untransformed representations, and the most correlated task experienced (in the cards domain).", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 15, "text": "However, if our paper is accepted, and you feel that the comparison to MAML for basic meta-learning is useful, we will run MAML on our tasks before the camera-ready submission.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 16, "text": "One might take inspiration from our framework try to use MAML for zero-shot task performance by transforming task representations would require adopting our meta-mapping framework, as well as a number of ideas of our architecture (where do the task representations come from, and how are they used?), and so its not clear to us that this is an appropriate baseline, rather than simply another implementation of our technique.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 17, "text": "Instead, we feel the more appropriate baseline is performing tasks from a natural language description, as many prior zero-shot works have, and so we have moved this result to the main text, as noted above.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 18, "text": "> The experimental results presented were all done on small synthetic datasets and it\u2019s hard to evaluate whether the method is practically useful.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 19, "text": "We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 20, "text": "We remained with synthetic tasks for two reasons: 1) to illustrate the method in settings we thought would be clearer, 2) because as we highlight in the future directions, there is a lack of meta-learning datasets that contain as structured of relationships between tasks as we consider.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 21, "text": "(Taskonomy, for example, has at best a notion of \"similarity\" in terms of transfer.) We are working on creating several such datasets, but we think that this paper as it stands is a useful contribution that illustrates the concept and ideas -- the datasets themselves will also require further description, and including them in a paper of this length would likely result in even more material being cut, and so a less clear presentation.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 22, "text": "We do think this is an important direction, but we think this paper makes a useful contribution by highlighting this new perspective. We hope that you agree.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [5]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 23, "text": "> Also, the work by Platanios, et al. on contextual parameter generation is very relevant to this work as it tackles multi-task learning using HyperNetworks.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 24, "text": "It may be worth adding a short discussion/comparison to that work as it also considers zero-shot learning.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 25, "text": "Thanks, we weren't familiar with this very interesting work! We have added a reference and a brief discussion of the relationship.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 26, "text": "- Capitalize: \u201csection\u201d -> \u201cSection\u201d, \u201cappendix\u201d -> \u201cAppendix\u201d, \u201cfig.\u201d -> \u201cFigure\u201d.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 27, "text": "Sometimes these are capitalized, but the use is inconsistent throughout the paper.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 28, "text": "Thanks for pointing this out, fixed.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 29, "text": "- \u201cHold-out\u201d vs \u201cheld-out\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18, 18, 20]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 30, "text": ".", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18, 18, 20]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 31, "text": "Be consistent and use \u201cheld-out\u201d throughout.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18, 18, 20]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 32, "text": "We are making a grammatical distinction here -- \"held-out\" is an adjective and \"hold-out\" is a noun. That is, one might refer to a \"held-out task\" or to a \"a hold-out.\" Hopefully this clarifies things.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [18, 18, 20]]}, {"review_id": "B1ez1LvJcB", "rebuttal_id": "rygJHT4DoB", "sentence_index": 33, "text": "We hope that this clarifies things, and will help to address your concerns. Please let us know if further changes would be useful.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 0, "text": "We thank our second reviewer for his comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 1, "text": "We first refer to your main comments and then answer each point in part.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 2, "text": "The work of Lakshminarayanan et al. indeed showed that deterministic ensembles can improve on the performance of MC-dropout techniques and provides a foundation for ours.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 3, "text": "And as Beluch et al. (2018) showed, this can be valuable in an active learning setting.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8, 9]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 4, "text": "However, our work differs in two major ways:", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 5, "text": "i) We focus on showing the uncertainty representation in these methods suffer from overconfident predictions and that combining the two methods into a stochastic ensemble can be of great benefit and improve on the quality of the uncertainty.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 6, "text": "ii) We believe the true novelty to be in applying them in an active learning setting, and in particular on a small dataset problem (i.e. the size of the final dataset acquired during AL is only a small fraction of the entire available unlabelled dataset).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 7, "text": "As you mentioned, data is notoriously scarce and deep learning methods rarely work on small dataset problems.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 8, "text": "We thank the reviewer for pointing us to the work of Huand et al. Indeed this is an interesting method that would allow us to most likely achieve similar or better results with less computational overhead.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8, 9]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 9, "text": "This is definitely something we will consider for future work, but it is somehow out of the main scope of the paper, which was to show the power of combining MC-dropout with ensembles in the active learning setting.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [5, 6, 7, 8, 9]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 10, "text": "Taking into account more advanced ensemble methods is definitely of interest.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8, 9]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 11, "text": "In terms of the Bayesian Optimization literature, this is definitely of interest if we are to focus on hyper-parameter tuning for our models, but we fail to see the connection of the work you mentioned to our active learning examples.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [5, 6, 7, 8, 9]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 12, "text": "Our focus was not on fine-tuning our models.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [5, 6, 7, 8, 9]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 13, "text": "In relation to your specific points, we answer these below:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 14, "text": "1) Gal has already showed in his PhD thesis that MC-Dropout almost always performs best in terms of prediction accuracy and uncertainty quality assessment when compared to alternative Bayesian neural network approaches such as Probabilistic Back Prop and other variants of stochastic gradient MCMC methods.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 15, "text": "The aim of our paper was to improve upon MC-Dropout in the context of active learning, which would invariably translate into better performance w.r.t. other Bayesian NN approaches.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [11]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 16, "text": "2) Beluch et al. (2018) showed that going beyond 3 networks in their deterministic ensemble method does not add any significant improvements in terms of performance.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 17, "text": "Therefore, we used this number when benchmarking against their method.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 18, "text": "3) The aim of the paper was to improve upon the state-of-the-art in active learning for the image classification task.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 19, "text": "We specifically chose this task due to its relevance to the real world especially in the medical imaging industry.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 20, "text": "We agree that a more comprehensive study could be done in order to asses the viability of our method for ML tasks other than image classification.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 21, "text": "As for other neural network architectures, we chose the one used in the benchmarked methods.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 22, "text": "4) Results are averaged over 5 multiple independent runs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "B1g0bJk5h7", "rebuttal_id": "rylwcjykCX", "sentence_index": 23, "text": "We will include both this and confidence scores in a revised version of our paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [15]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 0, "text": "Thank you for your comments. Please find below our response to your questions and concerns.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 1, "text": "1) Technical contributions", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 2, "text": "We are glad that the reviewer agrees that we are tackling a long standing and important problem and acknowledge the fact that neither the definition of constrained MDPs nor the application of Lagrangian relaxation to solve these problems is novel by itself.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 3, "text": "We should have stated our exact technical contributions more clearly and have adapted the paper to do so.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 4, "text": "For completeness we will list these below:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 5, "text": "a) We introduce pointwise, per-state constraints to learn more consistent behavior compared a single global constraint, and regress the resulting state-dependent Lagrangian multipliers using a neural network to exploit generalization across similar states.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 6, "text": "b) Instead of recombining the reward and cost directly on the environment side and learning a single value estimate, we train a critic network to output both return and penalty value estimates as well as the Lagrangian multipliers themselves, effectively providing more structure to the critic.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 7, "text": "We only combine the different terms appropriately for the actor update.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 8, "text": "c) We show that we can train a single, bound-conditional policy that can optimize penalty across a range of bounds and can be used to dynamically trade off reward and penalty.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 9, "text": "2) Comparison with the original benchmark reward", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 10, "text": "We have extended the results on Cartpole to include the original reward as defined in the DM Control Suite (incl. bonus for low control).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 11, "text": "We found that compared to the original setting, our method is able to reduce the average control norm by over 50% across the entire episode, and by over 80% after the swingup phase, without significant reduction in the average return as measured without control bonus.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 12, "text": "3) Claims about bang-bang control in continuous RL", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 17, 17]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 13, "text": "The reviewer is right in that the claim of RL often leading to bang-bang control is too strongly worded.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [17, 17, 17]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 14, "text": "This is only the case when the objective function is not well-designed and one is naively optimizing for success only.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 17, 17]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 15, "text": "Designing a proper objective function is however often not trivial and more of an art, requiring several iterations to achieve the desired behavior.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 17, 17]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 16, "text": "This work tries to remove some of the complexities in designing such a function.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 17, 17]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 17, "text": "4) State-dependent lower bound", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 18, "text": "Defining a state-dependent bound is indeed not trivial and requires knowledge of what is feasible in the system, and as such we leave this up to future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 19, "text": "In this paper we have made the approximation that the state distribution is stationary and the discount is large enough to assume that the value is more or less constant.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "B1g4z20Vs7", "rebuttal_id": "SJgudF2KCX", "sentence_index": 20, "text": "While this holds for locomotion tasks, this does not apply in e.g. the swingup phase of the cartpole task and as a result the penalty is completely ignored during this phase.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 0, "text": "We very much appreciate your valuable comments, efforts and times on our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 1, "text": "Our responses for all your questions are provided below. Our major revisions in the new draft are colored by red.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 2, "text": "Q1. More related works", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 3, "text": "We updated the introduction by including more recent works [1, 2, 3, 4, 5] related to deep learning with noisy labels.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10, 11, 13]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 4, "text": "In the previous draft, we only included the relevant literature which involves a single network/classifier.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10, 11, 13]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 5, "text": "The updated related works utilize multiple networks, e.g., an ensemble of classifiers or meta-learning model.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10, 11, 13]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 6, "text": "We also added new experimental results for them in Table 4 of the revised draft, as we mentioned in our common response to all reviewers.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10, 11, 13]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 7, "text": "Thank you very much for the suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [10, 11, 13]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 8, "text": "Q2. Comparison with VAT [6].", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 9, "text": "We remark that a targeted setting of VAT [6] is different from ours in that it is designed for improving the performance on semi-supervised learning, while our main goal is handling noisy labels in the training dataset.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [14]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 10, "text": "Due to this, we skip the comparison with VAT.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [14]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 11, "text": "Instead, as we mentioned in our common response to all reviewers, we consider more training baselines (such as MentorNet [2] and Co-teaching [3]) focusing on handling noisy labels, and show that our inference method can improve all of them.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 12, "text": "Q3. L-FBGS adversarial attacks [8].", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 13, "text": "We remark that L-FBGS [8] is known to fail easily due to the near-zero gradient of loss function [7].", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [15]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 14, "text": "Instead, we consider CW attack [7] which is known to be much stronger.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 15, "text": "[1] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 16, "text": "[2] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Regularizing very deep neural networks on corrupted labels. In ICML, 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 17, "text": "[3] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: robust training deep neural networks with extremely noisy labels. In NIPS, 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 18, "text": "[4] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In ICML, 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 19, "text": "[5] Eran Malach and Shai Shalev-Shwartz. Decoupling\u201d when to update\u201d from\u201d how to update\u201d. In NIPS, 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 20, "text": "[6] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 21, "text": "[7] C. Nicholas and W. David. Towards evaluating the robustness of neural networks. In IEEE Symposium on SP, 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 22, "text": "[8] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In ICLR, 2013.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 23, "text": "Thanks a lot,", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1gcyfVOn7", "rebuttal_id": "H1xd_45SCm", "sentence_index": 24, "text": "Authors", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 0, "text": "Except for the learning rate, all the hyper-parameters were chosen according to the values suggested by the authors of AdaGrad, and Adam.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 1, "text": "The learning rate was chosen as 1/K, with K=100 being the number of examples used to estimate the CDF.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 2, "text": "As our stated goal is to present an algorithm which can be blindly applied with some fixed set of hyper-parameters to any possible objective, one of the goals of the experiments is to show that in such a setting some methods will work, while others will fail.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 3, "text": "Thus, as a controlled experiment for this hypothesis, we first fixed the set of all hyper-parameters for all methods, and then proceeded to apply them to various problems.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 4, "text": "In this setting therefore, tuning the learning rate or any other hyper-parameter for that matter will compromise the validity of our results.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 5, "text": "Regarding table 3, we accept the reviewer\u2019s suggestion, this is a good point. We particularly like the suggestion of writing NA or some such value, and we will use it to correct the paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [23, 24, 25]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 6, "text": "We thank the reviewer for the evaluation.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 7, "text": "Please see our detailed response to several recurring issues at https://openreview.net/forum?id=Hkx-ii05FQ&noteId=HygFbNmL6X.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 8, "text": "In that response we address the following issues:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 9, "text": "(1) We emphasize fundamental differences between Cakewalk and CE.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 10, "text": "These go beyond the differences the reviewer mentions.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 11, "text": "(2) How the sampling distribution should not be considered as a part of Cakewalk, and that it is mostly provided as an example, and a basis for the reported experiments.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 12, "text": "(3) The experiments include results two tasks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 13, "text": "Nonetheless, it appears the paper doesn\u2019t convey this clearly, and we suggest two possible ways how to update the paper in this regard.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 14, "text": "Next, we try to answer the specific issues the reviewer mentions.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 15, "text": "First, we address the suggestion of introducing Cakewalk as a generalization of CE.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11, 12, 13]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 16, "text": "While we were writing the paper we in fact considered presenting Cakewalk as the reviewer suggests.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11, 12, 13]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 17, "text": "We eventually decided against this approach as CE is a method for adapting an importance sampler, and its convergence guarantees only apply when it is treated as such.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11, 12, 13]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 18, "text": "The convergence guarantees of REINFORCE on the other hand still apply under our surrogate objective framework.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11, 12, 13]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 19, "text": "This property allows us to explore various surrogates, where one such construction allows us to interpret CE as a policy gradient method, and another makes the basis for Cakewalk.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11, 12, 13]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 20, "text": "Second, we address the issue of using a sampling distribution that assumes independence between the different dimensions.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 21, "text": "As the author correctly states, such a distribution will not always be useful, and one can design a problem for which this distribution will lead to a poor local optimum.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 22, "text": "Note however that a global maximizer for the objective suggested by the reviewer can be easily found just by random sampling: sampling such a maximizer has the same probably as sampling an odd integer - half.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 23, "text": "Nonetheless, for the clique problem such a distribution can be effective.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 24, "text": "Intuitively, if some node i is part of a large clique, then sampling x_i=1 is likely to result in a good objective as there are many nodes that are connected to i, and the chance of not sampling any of them decreases with the clique size.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 25, "text": "In this way, over time the probability for sampling such nodes becomes higher, and the chance of sampling all of them together increases.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 26, "text": "A similar reasoning applies for the k-medoids problem.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 27, "text": "We note that these kind of factorized distributions have a long history of being useful in machine learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 28, "text": "In a similar context to the one studied in the paper, such distributions have been studied by Rubinstein in his paper which discusses CE as an algorithm for combinatorial optimization, and in the classical bandit papers Exp3 is applied independently to several dimensions to study game theoretic problems.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 29, "text": "In different contexts, such distributions have also been used as naive mean field approximations in variational inference.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 30, "text": "Next, we address the question regarding the gradient update types.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 31, "text": "One intuitive explanation for why an algorithm that maintains a \u2018memory\u2019 of previous gradient updates like AdaGrad or Adam is required", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 32, "text": "is that they protect against sampling biases.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 33, "text": "Consider for example the case when the execution is at the start, and the sampling distribution still has maximum entropy.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 34, "text": "Due to the combinatorial nature of the solution space, the examples that have been sampled thus far create a distorted representation of the solution space.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 35, "text": "In this case we could get that some x_i=j will occur few times, while some other x_k will not receive the value j at all.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 36, "text": "Now if we apply vanilla gradient updates this can skew the sampling distribution in random directions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 37, "text": "Gradient updates such as those of AdaGrad and Adam on the other hand will lessen the impact of such deviations as the importance of each case is inversely proportional to the number of previous observations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 38, "text": "As such deviations will inevitably occur whenever we rely on polynomially sized samples to represent a combinatorial solution space, without such corrections a gradient based adaptive sampling algorithm will almost surely fail.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 39, "text": "Indeed, as can be seen in tables 1,2 and 4, SGA almost never leads to a locally optimal solution.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 40, "text": "Furthermore, this reasoning explains why AdaGrad is superior to Adam: AdaGrad corrects against sampling biases that entail all the examples that have been encountered, while Adam does this only within some exponentially moving time window.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1l3zjA_h7", "rebuttal_id": "SyxFgYgiTQ", "sentence_index": 41, "text": "Indeed, this phenomenon is studied in detail in the AdaGrad paper (though without assuming a data distribution), and sparse data like ours (one can say our data points are N indicator vectors of length M) is the first motivating example in their paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 0, "text": "Dear Reviewer 2, thank you for your constructive feedback and for taking the time to review our work.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 1, "text": "Your specific points are addressed below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 2, "text": "> \u201cThe results are not strong. And, unfortunately, the model contribution currently is too modest.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 3, "text": "Indeed, the model is a minor contribution and, especially in light of a more thorough evaluation of RGCN, the sum attention RGAT results do not improve on those in Schlichtkrull et al. (2017).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 4, "text": "However, we would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 5, "text": "We plan to make our implementation public to aid future research in the area.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [2]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 6, "text": "To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 7, "text": "(Zhang et al. 2018)", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 8, "text": ".", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 9, "text": "We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance model contributions of the paper.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 10, "text": "We also consider the poor results on MUTAG to be significant and informative for the rest of the community in developing more powerful models that can be applied to relational graphs.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 11, "text": "> \u201cWe should see the performance on other datasets  (e.g., some of the other datasets in Wu et al. (2018)\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 12, "text": "We agree that including experiments on the other data sets presented in Wu et al. (2018) would be a valuable addition to the paper.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 13, "text": "Unfortunately, we are unable to perform the required thorough hyperparameter exploration required to draw meaningful conclusions within the remaining time of the rebuttal period.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [4]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 14, "text": "This is something we will investigate in the future.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [4]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 15, "text": "> \u201cMy introduction suggestion: do not talk about Convolutional neural networks (CNNs).\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 16, "text": "Thank you for this stylistic critique.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 17, "text": "We see how the approach taken did not provide an intuition about the problem as well as it could have.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 18, "text": "We agree with your point regarding the wealth of graph neural network studies.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 19, "text": "We feel that the field of geometric deep learning, from which part of the direction of this work originated, is important to keep as part of the development of graph-based machine learning models.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 20, "text": "Some recent publications in the area of graph based ML have put less emphasis on geometric deep learning, the generalisations of convolutions from grids to graph, and the modifications of convolutions to achieve non-basis dependent methods.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 21, "text": "We felt that it is important to keep these concepts associated with the field of graph based ML.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 22, "text": "This introduction could, however, talk less in detail about CNNs themselves, and deal more with graphs - the main focus of the paper.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 23, "text": "We will produce a reworked introduction where graphs play a larger role.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6]]}, {"review_id": "B1lK2M8jnX", "rebuttal_id": "HJes1ogi67", "sentence_index": 24, "text": "This should provide a more intuitive introduction to our work, whilst maintaining cornerstone concepts.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6]]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 0, "text": "We thank reviewer #3 for the extensive feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 1, "text": "We have incorporated it as stated below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 2, "text": "We have significantly improved the exposition in this paper, ameliorated the discussion of the related work and added the suggested new interesting references, added theorems and more formal statements in the main text, compressed the appendix and enhanced the description of the experimental setup.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 3, "text": "Proofs that the space \"interpolates smoothly with curvature\": we added formal proofs (see theorems 2 and 3) that all the operations are differentiable, i.e. the gradients are equal from both the left and right at 0, w.r.t. curvature, for the chosen models of hyperbolic and spherical geometry.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17, 18]]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 4, "text": "k-addition definiteness in the spherical setting: we have added the formal condition that the k-addition be well-defined, and a proof that for two points this condition indeed recovers x != y / (k ||y||^2) - see Theorem 1.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [23, 24, 25, 26]]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 5, "text": "Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [23, 24, 25, 26]]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 6, "text": "This is a desirable property for Riemannian vector averaging.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [23, 24, 25, 26]]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 7, "text": "Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [23, 24, 25, 26]]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 8, "text": "Other comments:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 9, "text": "-Synthetic tree: contains |V| - 1 edges.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [27]]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 10, "text": "We corrected this mistake.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [27]]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 11, "text": "-Curvatures: are learned as we state in the paper section 4 and appendix F.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [28, 29, 30]]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 12, "text": "-Citations: fixed.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [31, 32, 33]]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 13, "text": "-Working with the Poincare ball as opposed to the hyperboloid model: this allows us to use gyrovector spaces which are defined either for the Poincare ball or the Klein model, as well as to connect those with the Riemannian geometry of the space.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [34, 35, 35, 35]]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 14, "text": "Moreover, as we show in the paper, we can now smoothly interpolate between all constant curvature spaces which is beneficial for learning curvatures without a priori deciding on their signs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [34, 35, 35, 35]]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 15, "text": "-Statement about \u201cgeneral class of trees\u201d replaced by \u201call weighted or unweighted trees\u201d.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [40]]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 16, "text": "[1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips\u201919", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1lrh6UpYS", "rebuttal_id": "BJlcvhU8iS", "sentence_index": 17, "text": "[2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips\u201919", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 0, "text": "We are grateful to the reviewer for the comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 1, "text": "In this revision, we have corrected the minor typos, added additional comparisons, and added a proof map for easier navigation of the results.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 2, "text": "Specific comments are addressed below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 3, "text": "1. Regarding exact recovery guarantees \u2014 NOODL converges geometrically to the true factors.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 4, "text": "Therefore, the error drops exponentially with iterations t. In other words, as t \u2014> infinity A_i \u2014> A^*_i for i in [1,m] and x_j \u2014> x^*_j for j in [1,m], where x_j is in R^m.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 5, "text": "We have added this clarification in Section 1.1.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 6, "text": "2. On tuning parameters \u2014 There are primarily three tuning parameters, namely eta_x (step-size for the IHT step), tau (threshold for the IHT step), and eta_A (step-size for the dictionary update step.) Our main result prescribes the theoretical values of these as shown in assumptions A.5 and A.6.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 7, "text": "Here, eta_x = Omega_tilde(k/sqrt(n)), tau = Omega_tilde(k\u02c62/n), and eta_A = Theta(m/k).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 8, "text": "We have updated A.6. to include the order of these parameters.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 9, "text": "The specific choices of these parameters, like other similar problems, depend on some a priori unknown parameters (e.g. the sparsity k, and the incoherence mu) which makes some level of tuning unavoidable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 10, "text": "This is true for Arora '15 and Mairal '09, as well, where tuning is required for the choice of step-size for dictionary update, and for choice of regularization parameter and the step-size for coefficient estimation via FISTA.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 11, "text": "Note that, in our experiments we fix the step-size for FISTA as 1/L, where L is the estimate of the Lipschitz constant (since A is not known exactly).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 12, "text": "Alternately, since NOODL involves gradient-based updates for the coefficients and the dictionary, tuning (the step-sizes and the threshold) is relatively straightforward in practice, since it is based on a gradient descent strategy.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 13, "text": "In fact, to compile the experiments presented in this paper, we fixed step-size, eta_x, and threshold, tau, and tuned the step-size parameter eta_A only (Theta(m/k)).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 14, "text": "The choices of eta_A are 30 for k = 10,20 and eta_A = 15 for k =50,100, as shown in Fig.2.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 15, "text": ", eta_A mostly effects the convergence rate as long as it is chosen in Theta(m/k).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 16, "text": "Also, as shown in Table 4 (Appendix E), the tuning process for l1-based algorithms (i.e. FISTA) takes more time, since one needs to scan over the range of the regularization parameter to find one that works.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 17, "text": "This (a) adds to the computational time, and (b) since the dictionary is not known exactly, may guarantee recovery of coefficients only in terms of closeness in l2-norm sense, due to the error-in-variables (EIV) model for the dictionary.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 18, "text": "In this sense, NOODL is (a) simple to tune, (b) assures guaranteed recovery of both factors, and (c) is fast due to its geometric convergence properties.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 19, "text": "These factors highlight its applicability in practical DL problems.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 20, "text": "3. Definition of Hard Thresholding (HT) \u2014 As per the recommendation of the reviewer, we have repeated the definition of hard-thresholding (HT) initially presented in the \"Notation\" sub-section, in Section 2 for clarity.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 21, "text": "4. Comparison to other Online DL algorithms \u2014 As correctly observed by the reviewer, the overall structure of NOODL is similar to successful online DL algorithms.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 22, "text": "These successful algorithms (such as Mairal '09) leverage the progress made on both factors for convergence, however, do not guarantee recovery of the factors.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 23, "text": "On the other hand, the state-of-the-art provable DL algorithms focus on the progress made on only one of factors (the dictionary), and do not have good performance in practice, since they incur a non-negligible bias; see Section 5 and Appendix E.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 24, "text": "NOODL bridges the gap between these two.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 25, "text": "In addition to our main theoretical result, which establishes conditions for exact recovery of both factors at a geometric rate, NOODL also has superior empirical performance, leading to a neurally-plausible practical online DL algorithm with strong guarantees; see Section 3 and 4.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 26, "text": "Our work also paves way for the development and analysis of related alternating optimization-based techniques.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 27, "text": "On reviewer's recommendation, we compare the performance of NOODL with one of the most popular alternating minimization-based online DL algorithm used in practice -- Mairal `09 -- in Fig. 2 and Table 4 (Appendix E).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 28, "text": "In this work, the authors show that alternating between a l1-based sparse approximation step and dictionary update based on block co-ordinate descent converges to a stationary point.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 29, "text": "The other comparable techniques shown in Table 1, are not ``online\u2019\u2019 and/or require stringent initializations, in terms of closeness to the true dictionary, as compared to NOODL.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 30, "text": "Our experiments show that due to the geometric convergence to the true factors, NOODL outperforms competing state-of-the-art provable online DL techniques both in terms of overall computational time, and convergence performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "B1x81eSE2m", "rebuttal_id": "ByeGb44y0X", "sentence_index": 31, "text": "These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 0, "text": "Regarding your first concern on the comparison with CW: In short, MarginAttack is able to achieve a higher attack success rate than CW AND a shorter running time.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 1, "text": "The paper may not make this point obvious enough probably because the curves are too thick to reveal the difference.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 2, "text": "To show this point clearly, we would like to refer you to the results in our response to reviewer 3, where we scanned through the number of binary search steps and measure the success rate and running time.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 3, "text": "As can be seen, MarginAttack has a higher success rate than all the versions of CW.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 4, "text": "There is a success-rate-efficiency tradeoff in CW, as a smaller binary search step number leads to a lower success rate.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 5, "text": "However, even with 10 binary search steps, CW is still unable to outperform MarginAttack in terms of success rate.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 6, "text": "On the other hand, with very small numbers of binary search steps, CW still runs slower than MarginAttack.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 7, "text": "Hope these results will clarify your major concern.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [2]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 8, "text": "Regarding your minor concern:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 9, "text": "In the theorem, we did not assume convexity.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 10, "text": "The assumption with the name 'convexity' is saying that the constraint set should not be 'too concave'.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 11, "text": "Please check the following figure where we listed what decision boundaries are permitted by our theorem and what not.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 12, "text": "https://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure2.pdf", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 13, "text": "As can be seen, the convexity assumption permits a wide variety of decision boundaries.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 14, "text": "Among the few cases that it does not permit is the case where the decision boundary bends more than the L2 ball does.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "B1xOut6DpQ", "rebuttal_id": "S1loX4IlA7", "sentence_index": 15, "text": "In this case, the critical point becomes a local maximum rather than a local minimum.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 0, "text": "We thank the reviewer for the comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 1, "text": "As correctly observed by the reviewer, Arora et. al. 2015 suffers from a bias in estimation both in the analysis and in the empirical evaluations.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 2, "text": "The source of this bias term is an irreducible error in the coefficient estimate (formed using the hard-thresholding step).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 3, "text": "NOODL overcomes this issue by introducing a iterative hard-thresholding (IHT)-based coefficient update step, which removes the dependence of the error in estimated coefficient on this irreducible error, and ultimately the dictionary estimate.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 4, "text": "Intuitively, this approach highlights the symbiotic relationship between the two unknown factors \u2014 the dictionary and the coefficients.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 5, "text": "In other words, to make progress on one, it is imperative to make progress on the other.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 6, "text": "To this end, in Theorem 1 we first show that the coefficient error only depends on the dictionary error (given an appropriate number of IHT iterations R), i.e. we remove the dependence on x_0 which is the source of bias in Arora et. al. 2015.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 7, "text": "We have added the intuition corresponding to this in the revised paper after the statement of Theorem 1 in Section 3.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10]]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 8, "text": "Analysis of Computational Time \u2014 We have added the average per iteration time taken by various algorithms considered in our analysis in Table~4 and Appendix E.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11]]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 9, "text": "The primary takeaway is that although NOODL takes marginally more time per iteration as compared to other methods when accounting for just one (Lasso-based) sparse recovery for coefficient update, it (a) is in fact faster per iteration since it does not involve any computationally expensive tuning procedure to scan across regularization parameters; owing to its geometric convergence property (b) achieves orders of magnitude superior error at convergence, and as a result, (c) overall takes significantly less time to reach such a solution; see Appendix E for details.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 10, "text": "We would like to add that since NOODL involves simple separable update steps, this computation time can be further lowered by distributing the processing of individual samples across cores of a GPU (e.g. via TensorFlow) by utilizing the architecture shown in Fig. 1.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 11, "text": "We plan to release all the relevant code as a package in the future.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [11]]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 12, "text": "In this revision, we have added comparison to Mairal '09, a popular online DL algorithm.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1xtSy6t2Q", "rebuttal_id": "rJlnpeEk0m", "sentence_index": 13, "text": "Further, we have also added a proof map, in addition to the Table 3, for easier navigation of the results.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 0, "text": "We thank Reviewer 3 for the constructive feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 1, "text": "Here is our point-to-point response to the comments and questions raised in this review:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 2, "text": "1. \u201cThe novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 3, "text": "GAN inference and adversarial training seek different goals.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 4, "text": "Adversarial training addresses a supervised learning task while GAN inference focuses on an unsupervised learning problem.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 5, "text": "Due to the inherent difference between supervised and unsupervised learning problems, the notion of generalization is defined differently between them.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 6, "text": "Arora et al. (2017) provide the standard definition of generalization error for GANs which is very different from the standard generalization error considered in supervised learning.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 7, "text": "Furthermore, no work in the literature theoretically guarantees that spectral normalization closes the generalization gap for either adversarial supervised learning or GAN unsupervised learning.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 8, "text": "2. \u201cIt is not clear to me that these are some novel results that can better help adversarial training\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 9, "text": "Our work\u2019s main contribution is the theoretical generalization guarantees for spectrally-normalized adversarially-trained DNNs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 10, "text": "Introducing the adversary can significantly grow the capacity of a DNN.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 11, "text": "Therefore, existing DNN generalization bounds are not applicable to adversarial training settings.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 12, "text": "Our work, to our best knowledge, is the first to show that the adversarial learning capacity of a DNN for FGM, PGM, WRM training schemes can be effectively controlled by regularizing the spectral norm of the DNN\u2019s weight matrices.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "B1xw3F5KhQ", "rebuttal_id": "HyxOkadJAX", "sentence_index": 13, "text": "Our numerical results further support our theoretical contribution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 0, "text": "Thank you for your fruitful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 1, "text": ">>", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [1]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 2, "text": "What would make it stronger imo is to address the issue of how much is gained from a discrete vs. continuous representation.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [1]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 3, "text": "Discrete representations by themselves are not better than continuous ones (cf. Table 1, wav2vec vs. vq-wav2vec).", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [1]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 4, "text": "However, discretization enables the application of existing algorithms from the NLP literature which were designed for discrete inputs.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [1]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 5, "text": "We show that the BERT model can be directly applied to discretized speech.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [1]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 6, "text": "BERT can better model context than (vq-)wav2vec.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [1]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 7, "text": ">> The authors take it as a given that discrete is good because it allows us to leverage work in NLP. That makes sense -- but at what cost?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 8, "text": "Chaining vq-wav2vec and BERT requires more computational effort than just wav2vec, however, it does improve accuracy as our results show (cf. Table 1).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 9, "text": "Running BERT requires roughly as much computational overhead as just vq-wav2vec.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 10, "text": ">> The state of the art on LibriSpeech is not Mohamed at al. 2019.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 11, "text": "See e.g. Irie et al. Interspeech 2019 for better result.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 12, "text": "Thanks for pointing this out, we fixed this in the updated version of the paper we just posted.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 13, "text": ">> The Conclusion is very sparse.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "B1xXOITI9H", "rebuttal_id": "BJlsGxtnsS", "sentence_index": 14, "text": "We broadened conclusion and delineated additional future work.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5]]}, {"review_id": "BJe2djKE3X", "rebuttal_id": "HklKrfHc0X", "sentence_index": 0, "text": "Thank you for your thoughtful review. We will address your concerns in turn.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJe2djKE3X", "rebuttal_id": "HklKrfHc0X", "sentence_index": 1, "text": "Q1: So you provide the general framework where somebody has to specify only the F?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "BJe2djKE3X", "rebuttal_id": "HklKrfHc0X", "sentence_index": 2, "text": "A1: Yes, and that is the motivation of this work, to avoid training new models for slightly different situations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJe2djKE3X", "rebuttal_id": "HklKrfHc0X", "sentence_index": 3, "text": "Q2: The efficiency of the method is highly based on the ability of the GAN to approximate well the prior distribution of the noise-free images.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "BJe2djKE3X", "rebuttal_id": "HklKrfHc0X", "sentence_index": 4, "text": "A2: Yes, so we use WGAN-GP, a strong and elegant implementation, as our trained GAN.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BJe2djKE3X", "rebuttal_id": "HklKrfHc0X", "sentence_index": 5, "text": "Q3: Is parameter Omega estimated individually for each degraded image?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "BJe2djKE3X", "rebuttal_id": "HklKrfHc0X", "sentence_index": 6, "text": "A3: Yes.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "BJe2djKE3X", "rebuttal_id": "HklKrfHc0X", "sentence_index": 7, "text": "Thank you again for your positive reviews which give me some confidence, I really appreciate it.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJe3wuvchQ", "rebuttal_id": "BylNJ83Z0Q", "sentence_index": 0, "text": "We thank the reviewer for their positive evaluation of our study.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJe3wuvchQ", "rebuttal_id": "BylNJ83Z0Q", "sentence_index": 1, "text": "We agree that the prediction from CSLB features is particularly interesting, and we are currently working on improving this further by interpolating to other objects in a semi-supervised manner (similar to what was proposed by the reviewer).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "BJe3wuvchQ", "rebuttal_id": "BylNJ83Z0Q", "sentence_index": 2, "text": "We also strongly agree that testing additional embeddings would be very interesting!", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJe3wuvchQ", "rebuttal_id": "BylNJ83Z0Q", "sentence_index": 3, "text": "For the present work, we focused on synset embeddings because they represent a closer match to the meaning of each individual object than word embeddings would and provide a one-to-one match for the meanings.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "BJe3wuvchQ", "rebuttal_id": "BylNJ83Z0Q", "sentence_index": 4, "text": "For example, our list contains four different meanings for the object named by the word \u201cbaton\u201d, referring to (1) an item in relay races, (2) in twirling, (3) a weapon used by police, and (4) an item used by a musical conductor.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "BJe3wuvchQ", "rebuttal_id": "BylNJ83Z0Q", "sentence_index": 5, "text": "Due to the novelty of this line of research, to our knowledge there are no other synset embeddings available than the ones we used, and we included both a 50d dense and a 300d dense version.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "BJe3wuvchQ", "rebuttal_id": "BylNJ83Z0Q", "sentence_index": 6, "text": "In addition, we would have liked to include sparse positive synset embeddings as a reference, however those are currently not available; for that reason, we included NNSE word embeddings instead.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "BJe3wuvchQ", "rebuttal_id": "BylNJ83Z0Q", "sentence_index": 7, "text": "In the future, we would like to add sparse positive synset embeddings and test their interpretability relative to our similarity embedding.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [6]]}, {"review_id": "BJe3wuvchQ", "rebuttal_id": "BylNJ83Z0Q", "sentence_index": 8, "text": "We hope this will underline the unique contribution of a behavior-based similarity embedding presented here.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "BJe3wuvchQ", "rebuttal_id": "BylNJ83Z0Q", "sentence_index": 9, "text": "In addition, we would like to thank the reviewer for their idea on how to extend the embedding.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [10]]}, {"review_id": "BJe3wuvchQ", "rebuttal_id": "BylNJ83Z0Q", "sentence_index": 10, "text": "Indeed, we are currently working on predicting similarities for other concepts and images from pretrained synset vectors and activations in deep convolutional neural networks.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [10]]}, {"review_id": "BJe3wuvchQ", "rebuttal_id": "BylNJ83Z0Q", "sentence_index": 11, "text": "However, this effort is still in its early stages and beyond the scope of the present work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [10]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 0, "text": "We thank the reviewer for acknowledging the novelty of our work and for noting that our experiments are thorough.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 1, "text": "Thank you for pointing out a related preprint by Z. Hu et al. [arXiv:1905.13728].", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 2, "text": "We note the work by Z. Hu et al. was developed independently and concurrently to our work here, and we were not aware of it at the time of writing our paper.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 3, "text": "We shall cite the preprint and include a discussion in our paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 4, "text": "Briefly, the key difference between our work and that of Hu et al. is that Hu et al. consider a more restrictive setting where graphs are completely unlabeled (i.e., graphs have no node features).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 5, "text": "Hu et al. then focus on extracting generic graph properties of unlabeled graphs by pre-training on randomly-generated graphs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 6, "text": "While the approach is interesting, the limitation of such an approach is that it improves performance only marginally over ordinary supervised classification of the original attributed graphs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 7, "text": "This is because it is hard for random unlabeled graphs to capture domain-specific knowledge that is useful for a specific application.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 8, "text": "Moreover, in practice, graphs tend to have labels together with rich node and edge attributes, but Hu et al.\u2019s approach cannot naturally leverage such attribute information, which then results in limited gains.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 9, "text": "In principle, we could compare our approach against Hu et al., however, right now, this would be extremely challenging because of the following reasons.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 10, "text": "(1) We cannot find a public implementation of Hu et al.\u2019s approach for reliable comparison.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 11, "text": "(2) Reimplementing their method requires knowledge of many specific implementational details and design choices (feature extraction, graph generation, etc.), which are not discussed in their preprint.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 12, "text": "(3) Finally, their pre-trained GNN operates on unlabeled graphs, and so it cannot be directly applied to our datasets of labeled graphs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 13, "text": "Lastly, in contrast to Hu et al., our work focuses on important real-world domains, where one wants to pre-train GNNs by utilizing the abundant graph, node, and edge attributes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 14, "text": "Importantly, our approach is able to learn a domain-specific data distribution that is useful for downstream prediction.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "BJeDySijKr", "rebuttal_id": "rJgKcidXir", "sentence_index": 15, "text": "We demonstrate on two application domains that such practical settings (i.e., labeled graphs with naturally-given node and edge attributes) are very important to consider and that our pre-training can substantially improve model performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 0, "text": "Many thanks for the detailed review!", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 1, "text": "Main comments:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 2, "text": "1/ The DIP approach critically relies on regularization in order to make the method work (both by adding random noise in each optimization step to the input, as well as early stopping).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 3, "text": "As the first reviewer noted ``In fact, the DIP of Ulyanov et al. can hardly be considered \"a model\" (or a prior, for that matter), and instead should be considered \"an algorithm\", since it relies on the early stopping of a specific optimization algorithm''.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 4, "text": "However we follow the reviewers' suggestion and made clear that the idea to use a deep network without learning as an image model is not new and rewrote the item to ``The network itself acts as a natural data model.  Not only does the network require no training (just as the DIP); it also does not critically rely on regularization, for example by early stopping (in contrast to the DIP).''", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 5, "text": "Before that, in the introduction, in the original and revised version, we have a paragraph devoted to the DIP explaining that Ulyanov et al. introduced the idea of using a deep neural network without learning as an image model.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 6, "text": "2/ Regarding the theoretical contribution: We fully agree that a limitation of the theorem is that it pertains to a one layered version of the decoder.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 7, "text": "We are currently extending this to the multilayer case, but still have to address a technical difficulty in counting the number of different sign pattern matrices.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 8, "text": "Regarding the assumptions: The proposition uses the assumption that k^2 log(n_0)  / n <= 1/32.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 9, "text": "Here, the constant 1/32 is not optimal.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 10, "text": "k^2 is essentially the number of parameters of the model, and n is the output dimension.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 11, "text": "The proposition is only interesting if k^2 log(n_0)  / n <= 1/20 even without this assumption (due to the right hand side of the lower bound) therefore this assumption is not restrictive.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 12, "text": "The bound is applicable if the number of parameters, k^2 is smaller than a logarithmic term times the number of output parameters, i.e., it allows the number of parameters to scale almost linearly in the output dimension.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 13, "text": "This is the regime in which the deep decoder operates throughout the paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 14, "text": "We agree that many natural noise patterns have structure, and that those can be better approximated with deep models, and are thus more difficult to remove.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 15, "text": "3/ We have added the sentence ``In the default architectures with $d=6$ and $k=64$ or $k=128$, we have that N = 25,536 (for k=64) and N = 100,224 (k=128)", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 16, "text": "out of an RGB image space of dimensionality 512\\times512\\times3=786,432 parameters.'' to specify the number of parameters.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 17, "text": "Thanks for the suggestion to try second order method like LBFGS; we have tried LBFGS as a response to the reviewer's comment.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 18, "text": "It converges in significantly fewer iterations, but each iterations is so much more expensive that overall it optimizes slower than ADAM or gradient descent.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 19, "text": "Minor comments:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 20, "text": "1/ Figure 4: We have added labels and the sentence ``Early stopping can mildly enhance the performance of DD; to see this note that in panel (a), the minimum is obtained at around 5000 iterations and not at 50,000.'' in the caption to clarify.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16, 17]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 21, "text": "Also, we have added the sentence ``Models are fitted independently for the noisy image, the noiseless image, and the noise.'', and rewrote the paragraph", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16, 17, 18, 19]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 22, "text": "Thanks for pointing this out!", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 16, 17, 18, 19]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 23, "text": "We agree that here we present only results for one image, but we did carry out simulations for many images, and those plots are qualitatively the same for all the images considered.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 24, "text": "Thus our conclusions about the model do not only hold for one image.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 25, "text": "2/ Normalization is applied channel wise.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [23]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 26, "text": "Let z{ij} be the j-th column in the i-th layer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [23]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 27, "text": "Then z{ij} is normalized independently of any of the other channels.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [23]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 28, "text": "3/ We have reworded the corresponding paragraphs to make clear that while we do not use convolutions, and thus this is not strictly speaking a convolutional neural network, it shares many structural similarities with a conventional neural network, as pointed out by the reviewer.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [24, 25, 27]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 29, "text": "4/ The equation is correct in that the parameter choices in the paper are such that the deep decoder has much fewer model parameters N than its output dimension. Thus N is much less than n.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [28, 29]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 30, "text": "5/ We agree that it is not optimal to use unintroduced notation at this point, but we made this compromise so that we can illustrate the performance of the deep decoder without introducing its details, but wanted to give a reader the chance to later see exactly what parameters we used.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [31, 32]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 31, "text": "6/ Unfortunately choosing k=6 is too small to have a small representation error, i.e., to represent the image well.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [33]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 32, "text": "We have, however not hand-selected the 8 images shown out of the 64, and the other 64-8 images look very similar.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [33]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 33, "text": "We have all the images in the jupyter notebook that comes with the paper.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [33]]}, {"review_id": "BJeYeRM0jm", "rebuttal_id": "r1gzyKy_a7", "sentence_index": 34, "text": "7/ Great question, it is faster to optimize the deep decoder since the adam/SGD steps are cheaper, but it indeed seems to require slightly more iterations for best performance than the DIP.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [34, 35]]}, {"review_id": "BJgF7g7jFr", "rebuttal_id": "HyeEUJbMsr", "sentence_index": 0, "text": "Thank you very much for your review! Please see our responses below regarding your comments:", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJgF7g7jFr", "rebuttal_id": "HyeEUJbMsr", "sentence_index": 1, "text": "\u201cEvaluation Datasets: Did you take duplication in the crawled datasets into account? (Lopes et al. 2017 (D\u00e9j\u00e0Vu: a map of code duplicates on GitHub) suggests that this is particularly problematic for JavaScript/TypeScript)\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "BJgF7g7jFr", "rebuttal_id": "HyeEUJbMsr", "sentence_index": 2, "text": "Response: We investigated this question by running jscpd (a popular code duplication detection tool, https://github.com/kucherenko/jscpd ) on our entire data set and found that only 2.7% code is duplicated.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "BJgF7g7jFr", "rebuttal_id": "HyeEUJbMsr", "sentence_index": 3, "text": "Furthermore, most of these duplicates are intra-project.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "BJgF7g7jFr", "rebuttal_id": "HyeEUJbMsr", "sentence_index": 4, "text": "Thus, we believe that code duplication is not a severe problem in our dataset, and we will include more details about this result in any future revision.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "BJgF7g7jFr", "rebuttal_id": "HyeEUJbMsr", "sentence_index": 5, "text": "========", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "BJgF7g7jFr", "rebuttal_id": "HyeEUJbMsr", "sentence_index": 6, "text": "\u201cThe hyperparameter selection regime (and the experiments used to find them) is not described\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "BJgF7g7jFr", "rebuttal_id": "HyeEUJbMsr", "sentence_index": 7, "text": "Response: We selected hyperparameters in a standard way by tuning on a validation set as we were developing our model.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "BJgF7g7jFr", "rebuttal_id": "HyeEUJbMsr", "sentence_index": 8, "text": "We\u2019ll include more details about hyperparameters and hyperparameter selection in any future revision.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [8]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 0, "text": "Thanks for the review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 1, "text": "There are two significant inaccuracies:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 2, "text": "1. GGT does not take the view of a low-rank *approximation*. This is a central point of the paper.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 3, "text": "2. Re: iterative methods: the preconditioner is a -1/2 power of the Gram matrix, not the inverse.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3, 4, 5]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 4, "text": "More details below:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 5, "text": "@Inverse square root: We are fully aware of the distinction.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2, 3, 4]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 6, "text": "- Note that iterative solvers like conjugate gradient do not immediately apply here, as we are solving a linear system in M^{1/2}, not M.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2, 3, 4]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 7, "text": "- Krylov subspace iterative solvers suffer from a condition number dependence, incurring a hard tradeoff between iteration complexity and \\eps. [1]", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2, 3, 4]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 8, "text": "- We actually *did* try polynomial approximations to M^{-1/2} as an alternative to our proposed small-SVD step.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2, 3, 4]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 9, "text": "We saw worse approximation (the condition number dependence kicks in) and worse GPU performance (parallel computation time scales with polynomial degree).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2, 3, 4]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 10, "text": "@Full-matrix terminology: The use of \u201cfull-matrix\u201d to distinguish from \u201cdiagonal-matrix\u201d is standard, and taken directly from [2].", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 11, "text": "@Full-matrix vs. full-rank: Note that we do not consider the windowed Gram matrix to be an \u201capproximation\u201d of the \u201cfull\u201d gram matrix.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 12, "text": "The window is for the purpose of forgetting gradients from the distant past, motivated by (1) our theory, (2) the small-scale synthetic experiments, and (3) the extreme ubiquity of Adam and RMSprop, which do the same.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 13, "text": "Note that we do no approximation on the windowed Gram matrix, the fact that it is low rank is a feature.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 14, "text": "@Location of \\mu definition: Is the reviewer\u2019s suggestion simply to move this definition into the intro?", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 15, "text": "@Comparison with second-order methods", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 16, "text": ": Please refer to our response to Reviewer 1 for some additional comments.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 17, "text": "@Tweaks: We don\u2019t believe that any of the tweaks should be so controversial.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [16]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 18, "text": "- The \\eps parameters are present in *every* adaptive optimizer, for stability.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [16]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 19, "text": "The interpolation with SGD is just another take on this.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [16]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 20, "text": "- The exponential smoothing of the first moment estimator is a subtler point.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [16]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 21, "text": "As we point out in Appendix A.2, in the theory for Adam/AMSgrad [3,4], \\beta_1 *degrades* the moment estimation, yet everyone uses momentum in practice.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [16]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 22, "text": "Even if this is unconvincing, the performance gap upon removing this tweak is minor, and our empirical results hold without this tweak.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [16]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 23, "text": "We are simply offering a heuristic that we have observed to help training unconditionally, just like momentum in Adam.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [16]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 24, "text": "@Informal main theorem: By \u201cinformal\u201d we truly mean that we are suppressing the smoothness constants (L, M) for readability and space constraints. We are simply adopting the widespread practice of deferring the non-asymptotic mathematical statement to the appendix.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 25, "text": "[1] Tight complexity bounds for optimizing composite objectives. Blake E Woodworth, Nati Srebro. NIPS 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 26, "text": "[2] Adaptive subgradient methods for online learning and stochastic optimization. J Duchi, E Hazan, Y Singer. JMLR 2012.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 27, "text": "[3] Adam: A Method for Stochastic Optimization. D.Kingma,J. Ba. ICLR 2015.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJgICDN92m", "rebuttal_id": "r1e2sLyfRm", "sentence_index": 28, "text": "[4] On the Convergence of Adam and Beyond. S. Reddi, S. Kale, S. Kumar. ICLR 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJgmhEfTcH", "rebuttal_id": "rye3zaZ7or", "sentence_index": 0, "text": "We thank the reviewer for their comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJgmhEfTcH", "rebuttal_id": "rye3zaZ7or", "sentence_index": 1, "text": "Although our primary contributions are empirical, we also provided a detailed theoretical discussion in section 2, where we give a clear and simple account of why the two regimes arise.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "BJgmhEfTcH", "rebuttal_id": "rye3zaZ7or", "sentence_index": 2, "text": "Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "BJgmhEfTcH", "rebuttal_id": "rye3zaZ7or", "sentence_index": 3, "text": "We would also like to emphasize that we make a significant contribution to the debate regarding SGD and generalization.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "BJgmhEfTcH", "rebuttal_id": "rye3zaZ7or", "sentence_index": 4, "text": "While many papers have proposed that small batches may generalize better than large minibatches, it was recently pointed out by Shallue et al. that none of these experiments provide convincing evidence for this claim, because no experiment to date has compared small and large batch training under a constant step budget with a realistic learning rate decay schedule while independently tuning the learning rate at each batch size.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "BJgmhEfTcH", "rebuttal_id": "rye3zaZ7or", "sentence_index": 5, "text": "We are the first to run this experiment and conclusively establish that SGD noise does enhance generalization in popular models/datasets.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "BJgmhEfTcH", "rebuttal_id": "rye3zaZ7or", "sentence_index": 6, "text": "We believe this is an important contribution.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "BJgmhEfTcH", "rebuttal_id": "rye3zaZ7or", "sentence_index": 7, "text": "We also provide intriguing results as we vary the epoch budget, which demonstrate that the optimal learning rate which maximizes the test accuracy does not decrease as the epoch budget rises.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "BJgmhEfTcH", "rebuttal_id": "rye3zaZ7or", "sentence_index": 8, "text": "This supports the notion that SGD has an optimal \u201ctemperature\u201d which biases it towards solutions that generalize well.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "BJgmhEfTcH", "rebuttal_id": "rye3zaZ7or", "sentence_index": 9, "text": "Additional experiments in the appendix G go further and study how the optimal learning rate schedule changes as we increase the epoch budget.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "BJgzh7Schm", "rebuttal_id": "B1xXG3110Q", "sentence_index": 0, "text": "We thank the reviewer for its valuable and insightful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJgzh7Schm", "rebuttal_id": "B1xXG3110Q", "sentence_index": 1, "text": "We are reviewing our work from a theoretical point of view and will update the paper very soon to reflect this.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "BJgzh7Schm", "rebuttal_id": "B1xXG3110Q", "sentence_index": 2, "text": "Even though we have not yet proved the above, we have empirically showed that the benefit of DEBAL over plain ensemble methods consists of a better representation of uncertainty, that is paramount in active learning.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9, 10, 11, 12]]}, {"review_id": "BJgzh7Schm", "rebuttal_id": "B1xXG3110Q", "sentence_index": 3, "text": "By better we mean", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9, 10, 11, 12]]}, {"review_id": "BJgzh7Schm", "rebuttal_id": "B1xXG3110Q", "sentence_index": 4, "text": "1) more meaningful and closer to what one would expect (Fig 4 & Fig 6 (right))", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7, 8, 9, 10, 11, 12]]}, {"review_id": "BJgzh7Schm", "rebuttal_id": "B1xXG3110Q", "sentence_index": 5, "text": "2) better calibrated (Fig 6 (left)).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7, 8, 9, 10, 11, 12]]}, {"review_id": "BJgzh7Schm", "rebuttal_id": "B1xXG3110Q", "sentence_index": 6, "text": "Our initial aim was not to compare stochastic ensembles with deterministic or single MC-dropout but to correct for the mode collapse issue in estimating posteriors with MC-dropout.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9, 10, 11, 12]]}, {"review_id": "BJgzh7Schm", "rebuttal_id": "B1xXG3110Q", "sentence_index": 7, "text": "We have empirically shown that adding ensembles to this, greatly improves the MC-dropout technique and outperforms the deterministic ensembles as well.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9, 10, 11, 12]]}, {"review_id": "BJgzh7Schm", "rebuttal_id": "B1xXG3110Q", "sentence_index": 8, "text": "We had similar doubts about the benefit of adding MC-Dropout to an ensemble.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8, 9, 10, 11, 12]]}, {"review_id": "BJgzh7Schm", "rebuttal_id": "B1xXG3110Q", "sentence_index": 9, "text": "Therefore, we contrasted the performance of DEBAL against the plain ensemble method and showed empirically that DEBAL gives rise to better measures of uncertainty.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8, 9, 10, 11, 12]]}, {"review_id": "BJgzh7Schm", "rebuttal_id": "B1xXG3110Q", "sentence_index": 10, "text": "Finally, as we strive to make our assumptions hold theoretically, we agree that adding theoretical Bayesian support to our method is of great importance if we are to further improve the understanding of Bayesian deep learning.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9, 10, 11, 12]]}, {"review_id": "BJgzh7Schm", "rebuttal_id": "B1xXG3110Q", "sentence_index": 11, "text": "For your final point, although Beluch et al. (2018) showed better performance for ensembles, we have shown this in the context of a small dataset problem (i.e. the size of the final dataset acquired during AL is only a small fraction of the entire available unlabelled dataset), which we believe is more relevant to the real world cases if AL is to become a widely used method.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "BJgzh7Schm", "rebuttal_id": "B1xXG3110Q", "sentence_index": 12, "text": "As for the figures, we are aware of this and will try to make them more clear in a revised version.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [15]]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 0, "text": "Thank you very much for your time, and your constructive comments, we are looking forward to further discussions!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 1, "text": "We answer your questions and concerns in the following.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 2, "text": "> \"However, I do not understand how are the *discrete* output y is handled.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 3, "text": "For this toy problem, we represent labels y by standard one-hot encoding, and we directly regress one-hot vectors using squared loss instead of softmax.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 4, "text": "This allows us to input one-hot vectors into the inverted network to generate conditional x-samples.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 5, "text": "> \"I\u2019m not sure I understand what we are supposed to learn from the astrophysics experiments.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 6, "text": "We included this experiment to demonstrate that we are able to find multi-modal posteriors in a second real-world setting relevant to natural science.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 7, "text": "> \"INN outperforms other methods [...] over some metrics such as the error on parameters recovery (Table 1), calibration error, and does indeed have a approximate posterior which seems to correspond to the ABC solution better\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 8, "text": "We indeed consider the calibration errors (reported in Sec. 4.2 (\u201cQuantitative results\u201d) and Appendix Sec. 6) the most meaningful of these comparisons, because they directly measure the quality of the estimated posterior distributions, and INNs have a clear lead here.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 9, "text": "We will add these numbers to Table 1 to emphasize their importance.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [8]]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 10, "text": "> \"However, the real-world experiments are not necessarily the easiest to read.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 11, "text": "We understand, although we tried our best to condense the complicated nature of these applications.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [11]]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 12, "text": "For the astrophysics setting, we provide more information in the appendix, Sec. 5, and for the medical application we refer to [1] for full details.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 13, "text": "[1] Wirkert et al.: Robust near real-time estimation of physiological parameters from megapixel multispectral images with inverse monte carlo and random forest regression. International Journal of Computer Assisted Radiology and Surgery, 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 14, "text": "(https://link.springer.com/article/10.1007/s11548-016-1376-5 )", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 15, "text": "We have uploaded a revised version of the paper, thank you again for your suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 16, "text": "The changes and additions are highlighted in red font for convenience.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 17, "text": "Please also note that by adding these changes, our page count increased by half a page beyond the recommended 8 pages.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJgZjv4c37", "rebuttal_id": "rygRUCn4am", "sentence_index": 18, "text": "If this presents a problem, we can attempt shorten the paper accordingly.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 0, "text": "Thanks a lot for your review and for pointing us to the reference, we will add and discuss this work in our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 1, "text": "The referenced paper mimics the Choquet integral to fuse different neural networks such as CaffeNet, GoogLeNet, and ResNet50 that have been pre-trained for classification problems and can be viewed as ensemble method for multiple noisy classifiers.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 2, "text": "Contrary, we are interested in regression problems that have inherent non-additive effect such as automatic summarization.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 3, "text": "Furthermore, the referenced paper is much closer to the Choquet as we intent to be.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 4, "text": "As we describe in the paper, the proposed architectures are only inspired by the Choquet integral.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 5, "text": "This idea can be found in both of our architectures.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 6, "text": "In Figure 1c, u_i and in Figure 1d g_i * u_i model these meaningful intermediate values.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 7, "text": "We do not claim that we obtain any theoretical guarantees or properties of the Choquet integral.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 8, "text": "\"How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 9, "text": "As described above, the proposed approaches are inspired by the way Choquet integrals handle non-additive utility aggregations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 10, "text": "We do not claim that we obtain any theoretical guarantees or properties of the Choquet integral.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 11, "text": "Furthermore, the main idea of this work is to not learn a representation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 12, "text": "Instead, we propose to predict many meaningful intermediate values that can simply be summed to obtain a set utility.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 13, "text": "\"What is your loss or your algorithm?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 14, "text": "We describe in Section 3.3 that we use mean squared error (MSE) and mean absolute error (MAE) in our experiments.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 15, "text": "We use MSE because it is usually used in regression problems.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 16, "text": "We were also interested in the mean absolute error because minimizing this loss might be more appropriate in a task such as automatic summarization, in which we don't want to punish a model strong if it makes a few severe mistakes compared to making many small mistakes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 17, "text": "We also describe in Section 3.3. that we use Adam as optimizer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 18, "text": "\"According to the illustration, it seems that you first obtain \u201cfeatures/representations\u201d. Then the representations are fed to the four architectures you listed in figure one.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 19, "text": "This is correct.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 20, "text": "\"RNN-based approaches are with better \u201ccomplexity\u201d comparing to your sum baseline and \u201cDeepset\u201d approach.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 21, "text": "We also compare against an RNN-based approach (abbreviated with \"RNN\" in the paper).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 22, "text": "The RCN approach is the smallest modification one can make to implement our idea into a standard RNN.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 23, "text": "Hence, we think that the comparison is fair and meaningful.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 24, "text": "Furthermore, we demonstrate in the extrapolation experiments that standard RNNs tend to overfit.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 25, "text": "The simple sum baselines and deepsets perform better in this experiments.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "BJl-jwU6tB", "rebuttal_id": "SJgNfb_tor", "sentence_index": 26, "text": "Hence, a \"better\" complexity turns out to be prone to overfitting, which shows that larger models are not necessarily better.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 0, "text": "We thank the reviewer for the valuable review comments and suggestions! Please find our point-by-point response as follows.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 1, "text": "Q1: Is there only 1 adversarial party?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 2, "text": "A1: There is only one adversarial party in centralized attack.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 3, "text": "But we make sure that the total injected triggers (e.g., modified pixels) of DBA attackers is close to and even less than that of the centralized attacker.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 4, "text": "We stressed this setup in Section 3.2.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 5, "text": "That is, the ratio of the global trigger of DBA pixels to the centralized is 0.992 for LOAN, 0.964 for MNIST, 0.990 for CIFAR and 0.991 for Tiny-imagenet.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 6, "text": "Q2: What\u2019s the result for centralized attacks with the same number of scaling times as DBA, but each update includes 1/4 number of poisoning samples?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 7, "text": "A2: Following your suggestion, we conducted two sets of new experiments.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 8, "text": "1. Change the poison ratio into 1/4: We decrease the fraction of backdoored samples added per training batch into 1/4.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 9, "text": "2. Change the data size into 1/4: We divide the local dataset into 4 parts and use 1/4 dataset for each update and keep the poison ratio unchanged.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 10, "text": "We have included the results and discussion in Appendix A.4 of the revised version.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 11, "text": "Q3: If the decomposition is also useful for trigger patterns that are not necessarily regular shapes?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 12, "text": "A3: It\u2019s also useful for irregular shape triggers.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 13, "text": "1. We study the irregular pixel logo \u2018ICLR\u2019 for three image datasets.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 14, "text": "Specifically, we use \u2018ICLR\u2019 as the global trigger pattern and decompose it into \u2018I\u2019, \u2018C\u2019, \u2018L\u2019, \u2018R\u2019 for local triggers.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 15, "text": "2. We also use the physical trigger glasses (Chen et al.,2017) on Tiny-imagenet and decomposed the pattern into four parts.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 16, "text": "The results are in Appendix A.3 of our revised version.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 17, "text": "DBA is also more effective and this conclusion is consistent in different colors of glasses.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 18, "text": "Q4: Can the authors show concrete examples on how the attacks are generated? The details are especially unclear on LOAN.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 19, "text": "A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "BJl5fZS6YS", "rebuttal_id": "r1gTFFSnor", "sentence_index": 20, "text": "We have also provided more details about LOAN dataset and how we attack in Appendix A.1.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "BJloMuTnFr", "rebuttal_id": "Syem-mAisB", "sentence_index": 0, "text": "We thank the reviewer for the comments on our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJloMuTnFr", "rebuttal_id": "Syem-mAisB", "sentence_index": 1, "text": "- We have included the result concerning a noisy oracle for the F_p moment estimation problem in the paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11]]}, {"review_id": "BJloMuTnFr", "rebuttal_id": "Syem-mAisB", "sentence_index": 2, "text": "- We like the question of minimizing the number of oracle calls.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "BJloMuTnFr", "rebuttal_id": "Syem-mAisB", "sentence_index": 3, "text": "This is an interesting open problem and we intend to explore it in future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [12]]}, {"review_id": "BJlOnG0gTX", "rebuttal_id": "BJx-0jEsC7", "sentence_index": 0, "text": "Thank you for the instructive review!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJlOnG0gTX", "rebuttal_id": "BJx-0jEsC7", "sentence_index": 1, "text": "Our algorithm 1 minimizes the empirical learnable noise risk (Eq. 4), which does not assume that X_{t-1}^{(j)} follows a diagonal gaussian distribution.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "BJlOnG0gTX", "rebuttal_id": "BJx-0jEsC7", "sentence_index": 2, "text": "Originally, to justify the I^u=1/2 \\sum_l log(1+Var(X^(j)_{t-1,l})/ \\eta_{j,l}^2) term used in our experiments for estimating mutual information, we used diagonal Gaussian assumption for X_{t-1}^(j) in the experiment.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "BJlOnG0gTX", "rebuttal_id": "BJx-0jEsC7", "sentence_index": 3, "text": "In fact, a better way to justify this is to note that I^u provides an upper bound for the mutual information subject to the constraint of known variance of marginal distributions of X^(j)_{t-1}, and the upper bound is reached with the diagonal Gaussian distribution, as is proved in Appendix C in the revision.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "BJlOnG0gTX", "rebuttal_id": "BJx-0jEsC7", "sentence_index": 4, "text": "Therefore, the assumption of diagonal Gaussian assumption is dropped for the experiments in the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "BJlOnG0gTX", "rebuttal_id": "BJx-0jEsC7", "sentence_index": 5, "text": "Practitioners can choose to optimize an upper bound of the learnable noise risk for better efficiency (as is also used in the experiments in this paper), or use differentiable estimate of mutual information for better accuracy, as has also been pointed out in the paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "BJlOnG0gTX", "rebuttal_id": "BJx-0jEsC7", "sentence_index": 6, "text": "In the revision, we have also added a more detailed comparison with other methods in sections 4.2 and 4.3, showing the strength of our method.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "BJlOnG0gTX", "rebuttal_id": "BJx-0jEsC7", "sentence_index": 7, "text": "For example, in section 4.2, our method correctly identifies important causal arrows, while the four other comparison methods either have more false positives and false negatives, or completely fail to discover causal arrows.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "BJlOnG0gTX", "rebuttal_id": "BJx-0jEsC7", "sentence_index": 8, "text": "In section 4.3", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "BJlOnG0gTX", "rebuttal_id": "BJx-0jEsC7", "sentence_index": 9, "text": ", we compare with the results in previous literature.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "BJlOnG0gTX", "rebuttal_id": "BJx-0jEsC7", "sentence_index": 10, "text": "We note that although all compared methods correctly identify the causal relations, our method have the advantage that the inferred causal strength does not decay with increasing history length (we also analyzed that in the original submission).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 0, "text": "We appreciate AnonReviewer3 for encouraging comments about the importance of the proposed abductive inference and generation tasks and about the value of our proposed dataset.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_global", null]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 1, "text": "We address the main concerns individually below:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 2, "text": "Adversarially filtering using BERT and GPT gives deep learning models a disadvantage:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 3, "text": "While BERT originally achieved high performance on the originally collected dataset, several recent studies [1][2][3][4] have found the presence of annotation artifacts in crowdsourced data that inadvertently leak information about the target label.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 4, "text": "This subsequently leads to overestimation of the performance of AI systems on end tasks.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 5, "text": "Our adversarial filtering (AF) algorithm aims to address the problem of overestimation of performance.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 6, "text": "In spite of targeting GPT/BERT during AF, human performance on the AF resulting dataset is still high.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 7, "text": "The significant gap between human and BERT performance leaves scope for inventing new methods for abductive reasoning.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 8, "text": "Ensemble of BERT models:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 9, "text": "An ensemble of three BERT models achieves an accuracy of 68.9%, very close to a single model 68.6%.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 10, "text": "Average score of human:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 11, "text": "The average score of human annotations is 89.4%.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 12, "text": "This is directly comparable with BERT-Ft [Fully Connected] model\u2019s performance of 68.6% in Table 1.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 13, "text": "Re. Ground Truth:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 14, "text": "The ground truth is assigned based on whether a hypothesis was collected during the plausible (Appendix A1 Task1) or implausible (Appendix A1 Task2) phase of the data collection procedure.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 15, "text": "To measure human performance, we had three annotators select the correct hypothesis and measured human performance as the accuracy of their majority-vote.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 16, "text": "Please let us know if this answers your question. If not, could you please clarify your question?", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [9]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 17, "text": "Generative task vs classification:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 18, "text": "We completely agree.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 19, "text": "While the generative task is more general and much more interesting, the challenge of evaluating generations is significant, particularly for this task.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 20, "text": "This is due to the fact that there could be multiple distinct plausible explanations for a given pair of hypothesis.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 21, "text": "Consider the following example:", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 22, "text": "O1: Kelly and her friend wanted to take a train to the city.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 23, "text": "O2: They had to wait for another one.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 24, "text": "Plausible explanations:", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 25, "text": "1. They read the timetable incorrectly and arrived at the station just after a train had left.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 26, "text": "2. The train was full.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 27, "text": "Both explanations are plausible, and explain the observations, but automated evaluation metrics are not reliable enough to capture this phenomenon based on their reliance on surface level similarities.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 28, "text": "To simultaneously make progress on the novel abductive reasoning task and due to the ease of evaluation, we additionally introduce a discriminative version of the task.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 29, "text": "Nonetheless, we agree that in its most general form, there could be any number of observations and models should be required to generate explanatory hypotheses in natural language (alpha-NLG task).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 30, "text": "[1] Gururangan et al. Annotation artifacts in natural language inference data.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 31, "text": "[2] Poliak et al. Hypothesis only baselines in natural language inference.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 32, "text": "[3] Tsuchiya et al. Performance impact caused by hidden bias of training data for recognizing textual entailment.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJlqUqW3tH", "rebuttal_id": "H1gU-PuqiS", "sentence_index": 33, "text": "[4] Sakaguchi et al. WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 0, "text": "We would like to thank the reviewer for providing valuable and detailed feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 1, "text": "We have addressed the clarity concerns in the updated paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 2, "text": "Figure captions, metrics used in the table, etc, as mentioned in the presentation section of the review have been carefully examined and updated in the paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 3, "text": "We will reorganize the experiment section to better present the comparisons under different experimental settings.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 4, "text": "(1) Factorized Latent Variables:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 5, "text": "The factorization of latent space with respect to the modalities provides a way to differentiate observed and unobserved modalities.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 6, "text": "Therefore, VSAE is capable of handling partially-observed data where the missing modalities can be arbitrary.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 7, "text": "In addition, the embeddings are intuitively more meaningful as input to unimodal encoders is now limited to only observed modalities, eliminating the effect of missing modalities.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 8, "text": "When performing imputation/generation, however, we want to capture the dependencies between modalities.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 9, "text": "In other words, unobserved modalities should be imputed based on the information extracted from observed modalities.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 10, "text": "For experiments, we design this by conditioning decoders on all latent variables, essentially accessing information from all observed modalities.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 11, "text": "This is not in contradiction to the factorized latent variable assumption.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 12, "text": "Instead, the encoders try to embed each modalities individually, while decoders learn the dependencies between different modalities.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 13, "text": "(2) Multimodal Experiments:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [36, 37, 38, 39, 40, 41]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 14, "text": "We apologize for unclear description of experimental settings.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [36, 37, 38, 39, 40, 41]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 15, "text": "In general, we believe multi-modal data is more general than conventional image-text or video-text pairs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [36, 37, 38, 39, 40, 41]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 16, "text": "By unifying tabular data also as multi-modal (with each attribute as one modality), we show that VSAE provides us a principled way for imputation, capable of generalizing to more data families.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [36, 37, 38, 39, 40, 41]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 17, "text": "Specifically, we conducted experiments on two types of data:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [36, 37, 38, 39, 40, 41]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 18, "text": "(1) low-dimensional tabular data, and (2) high-dimensional data (pixel or text) as \"multimodal\" to better define the overall task of learning from partially-observed data.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [36, 37, 38, 39, 40, 41]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 19, "text": "Upon request, we have included more extensive experiments following [1] on MNIST/FashionMNIST, and [2] on CMU-MOSI/ICT-MMMO.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [36, 37, 38, 39, 40, 41]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 20, "text": "Results are reported in Table 10 and Table 11 (Appendix C.5).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [36, 37, 38, 39, 40, 41]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 21, "text": "As shown, VSAE consistently outperforms baseline models across the added experiments as well.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [36, 37, 38, 39, 40, 41]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 22, "text": "(3) Discussions on Comparison with Upper Bound Methods:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [32, 33, 34]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 23, "text": "Models trained with fully-observed data in theory should have better performance, thus we treat them as upper bound methods.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [32, 33, 34]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 24, "text": "However, it is very interesting to observe that in some cases, VSAE have superior performances.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [32, 33, 34]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 25, "text": "One possible explanation is that missing modalities introduces extra noise into the model as regularizer, thereby, increasing the generalization ability.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [32, 33, 34]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 26, "text": "However, detailed experiments and more discussions need to be carried out to back up this explanation.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [32, 33, 34]]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 27, "text": "[1] Wu et al. Multimodal Generative Models for Scalable Weakly-Supervised Learning, NeurIPS 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJlU6IBK9S", "rebuttal_id": "rJxyl7dOoB", "sentence_index": 28, "text": "[2] Tsai et al. Learning Factorized Multimodal Representation, ICLR 2019.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 0, "text": "We thank the reviewer for their evaluation.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 1, "text": "Please see our response at https://openreview.net/forum?id=Hkx-ii05FQ&noteId=HygFbNmL6X, where we also discuss our experimental framework.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 2, "text": "Even though we present results on two tasks, it appears the paper structure doesn\u2019t convey this clearly, and we suggest two possible ways how to update the paper in this regard.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 3, "text": "We note in this context that even though we would also like to see Cakewalk evaluated on the domains mentioned by the reviewer, these are not part of our own research agenda, and accordingly our suggestions refer to other problems in combinatorial optimization.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_none", null]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 4, "text": "Next, we address other issues raised by the reviewer.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 5, "text": "First, we\u2019d like to emphasize that the clique problem studied in the paper is far from being a toy problem.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 6, "text": "All the algorithms are evaluated on the DIMACS clique dataset which was published as part of the second DIMACS challenge which specifically focused on combinatorial optimization.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 7, "text": "Over the years, this dataset has become a standard benchmark for clique finding algorithms, and results on it are regularly published.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 8, "text": "In this respect, this dataset is an important benchmark for clique algorithms very much like CIFAR10 and CIFAR100 are for image classification methods.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 9, "text": "Notably, Cakewalk approaches the performance of the best clique finding algorithms that directly search a graph, and which are tailored to this specific task.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 10, "text": "Note that none of the tested methods were given enough samples even to recover the graph itself, as most graphs have more than 100 nodes, and we\u2019ve allowed only for 100 |V| samples in each execution.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 11, "text": "To us this seems as a rather challenging setup, not just for the algorithms we\u2019ve tested in this paper, but for any clique finding algorithm.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 12, "text": "Next, we wonder how would the reviewer correct the confounds mentioned with regard to the clique experiment.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 13, "text": "Providing a controlled experiment is always challenging, though the elements mentioned by the reviewer were specifically selected as to reduce various confounds.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 14, "text": "The main research question we try to address is whether algorithms that only rely on function evaluations can recover locally optimal solutions.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 15, "text": "Since the objective is the only source of information for such algorithms, an all-or-none kind of objective would not be very useful.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 16, "text": "Instead, the objective is designed in a manner that provides information even for partial solutions, thus allowing the tested algorithms to gradually improve the objective.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 17, "text": "In terms of the sampling distribution, as our focus is on the update step, we decided to use the simplest possible sampling distribution we can think of.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 18, "text": "In such a regime, we can attribute any performance gains to the algorithms themselves, and not to any prior knowledge that is reflected by the structure of some complex sampling distribution.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 19, "text": "Next, we agree that local optimality is a mean rather than a goal (the objective itself).", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [7, 8, 9, 10]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 20, "text": "Nonetheless, as in the problems we seek to address the global optimum cannot be found in polynomial time", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 21, "text": ", the second best approach is first to design a method that can recover locally optimal solutions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 22, "text": "Once such a method is available, repeated applications of that method can allow one to select a good solution, very much like the standard practice of repeated applications of k-means which the reviewer mentions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 23, "text": "This reasoning however is dependent on a method\u2019s capability of recovering locally optimal solutions, and therefore studying this ability makes for a worthwhile effort.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 24, "text": "Answers to the last comments:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 25, "text": "- Table 3 is indeed confusing, this is a good point. We will correct it.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 26, "text": "- Methods that apply a surrogate objective work best with AdaGrad.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 27, "text": "In this case, our data is a classical use which is explored in the AdaGrad paper uses as a motivating example.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 28, "text": "Not surprisingly, both Cakewalk and OCE work best with it.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 29, "text": "REINFORCE however is sensitive to the objective values, and it appears that Adam somewhat mitigates this problem.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 30, "text": "However, this is not as effective as applying a surrogate objective, and REINF_Z with Adam is outperformed by OCE_0.1 with AdaGrad in all measures.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 31, "text": "- Our frame of reference were algorithms that could be applied to any combinatorial problem, and which only rely on function evaluations.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [15]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 32, "text": "Control variates and reward shaping methods are mostly useful when tied to the particularities of a given objective, and thus do not fall into this category.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [15]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 33, "text": "In neural combinatorial optimization the study is focused on designing a sampling distribution that reflects some prior knowledge about a problem, and thus, we consider this line of work as orthogonal to ours.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [15]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 34, "text": "Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [15]]}, {"review_id": "BJxHwvW23X", "rebuttal_id": "Skl8N_xipQ", "sentence_index": 35, "text": "- We selected the name \u2018Cakewalk\u2019 after consulting with a few colleagues. Following a joint discussion, we concluded that this name has the best chance for increasing our work\u2019s impact.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [16]]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 0, "text": "Thank you for the review and careful reading of our paper! We\u2019re glad that you found it of interest. On revision we will fix the typos that you identified.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 1, "text": "Regarding the first point, your intuition is exactly correct and a slightly simpler discussion of this phenomenon can be found in [1].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 2, "text": "When the network is deep enough that the covariance matrix has reached its fixed point, the distribution of the outputs of the network will be independent of the inputs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 3, "text": "At this point the network becomes untrainable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 4, "text": "To reconcile this with the commonsense intuition that \u201cdeeper is better\u201d, our answer is twofold.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 5, "text": "1) As in [1] and [2] it is often possible to find configurations or architectural modifications where the covariance matrix doesn\u2019t approach its fixed point over depths often considered in machine learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 6, "text": "When this is the case one can safely increase the depth without sacrificing accuracy.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 7, "text": "2) It seems that the role of depth in performance is more subtle than standard intuition would dictate.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 8, "text": "For example, in [3] note that although the authors were able to train a 10k hidden layer network, they did not observe any improvement in accuracy.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 9, "text": "In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 10, "text": "[1] S. S. Schoenholz, J. Gilmer, S. Ganguli, J. Sohl-Dickstein.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 11, "text": "Deep Information Propagation (https://arxiv.org/abs/1611.01232)", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 12, "text": "[2] G. Yang and S. S. Schoenholz. Mean Field Residual Networks (https://arxiv.org/abs/1712.08969)", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BJxndjHwnm", "rebuttal_id": "SkxSjPODaQ", "sentence_index": 13, "text": "[3] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, J. Pennington. Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks (https://arxiv.org/abs/1806.05393)", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkeBNXOJT7", "rebuttal_id": "SyxMr6SmRQ", "sentence_index": 0, "text": "We thank the reviewer for a detailed review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkeBNXOJT7", "rebuttal_id": "SyxMr6SmRQ", "sentence_index": 1, "text": "We have implemented the suggestions for improving clarity.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "BkeBNXOJT7", "rebuttal_id": "SyxMr6SmRQ", "sentence_index": 2, "text": "Regarding our use of \u2018axioms\u2019: We follow the economics literature in using axioms as normative concepts, i.e., to denote desirable properties that a neuron importance methods.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "BkeBNXOJT7", "rebuttal_id": "SyxMr6SmRQ", "sentence_index": 3, "text": "And not the use in the mathematical literature, which is to denote statements that are self-evidently true.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "BkeBNXOJT7", "rebuttal_id": "SyxMr6SmRQ", "sentence_index": 4, "text": "We have clarified this in the submission.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 0, "text": "Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 1, "text": "Hopefully the new results in our response will better aid discussion. Your specific points are addressed below.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 2, "text": "> i) \u201cThe proposed architecture is mainly adopted from the graph attention networks (Veli\u010dkovi\u0107  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 14, 14, 14]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 3, "text": "Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18, 18, 18]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 4, "text": "We concede that the modifications to the existing models is a minor contribution.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [14, 14, 14, 14, 18, 18, 18]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 5, "text": "We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [14, 14, 14, 14, 18, 18, 18]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 6, "text": "We plan to make our implementation public to aid research in the area.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [14, 14, 14, 14, 18, 18, 18]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 7, "text": "To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_none", null]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 8, "text": "We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the paper\u2019s contribution.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_none", null]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 9, "text": "> ii) \u201cIn table 2, I don\u2019t really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 10, "text": "We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 11, "text": "This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 12, "text": "These results will be included in the new manuscript.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 13, "text": "We also feel that some of the results being \u201csignificantly worse\u201d is one of the main contributions of our paper.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 14, "text": "Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 15, "text": "This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 16, "text": "The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 17, "text": "On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 18, "text": "> iii) \u201cCould you explain why your MUTAG is now a single graph and is cast as node classification problem?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 19, "text": "The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 20, "text": "There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 21, "text": "In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 22, "text": "d1 -> hasAtom -> d1_1", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 23, "text": "d1 -> hasBond -> bond1", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 24, "text": "d1- > hasStructure -> ring_size_6-1", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 25, "text": "where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 26, "text": "There are many more types than this, and are viewable in the .owl located at [2].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 27, "text": "Nodes correspond to entities from the point of view of RDF.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 28, "text": "Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 29, "text": "This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 30, "text": "Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 31, "text": "If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 35]]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 32, "text": "[1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkemLDvAn7", "rebuttal_id": "Bkx-xQ1qpX", "sentence_index": 33, "text": "[2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 0, "text": "Many thanks for the detailed review!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 1, "text": "1/ We agree that there are many elements of our architecture that are similar to that of a convolutional network, however the network does not perform convolutions.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 2, "text": "To reflect both points, we have revised the text to:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 3, "text": "``The network does not use convolutions.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 4, "text": "Instead, the network does have pixelwise linear combinations of channels, and just like in a convolutional neural network the weights are", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 5, "text": "are shared among spatial positions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 6, "text": "Nonetheless, they are not convolutions because they provide no spatial coupling between pixels, despite how pixelwise linear combinations are sometimes called `1x1 convolutions.' '',", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 7, "text": "and we have also added a subsection comparing the compression performance of our architecture to that of a decoder with convolution layers.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 8, "text": "In a sense, what the deep decoder is doing is separating multiple roles that proper convolutional layers fill:  the DD breaks apart the spatial coupling inherent to convolutions from their channel dependence and equivariance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 9, "text": "Further, it says that the spatial coupling need not be learned or fit to data, and can be directly imposed by upsampling.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 10, "text": "2/ Yes, the upsampling analysis in Figure 5 also extends to two-dimensional images.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 11, "text": "We agree that natural images are only approximately piece-wise smooth after all, and the deep decoder only provides an approximation of natural images (albeit a very good one).", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 12, "text": "3/ We agree and have changed `batch normalization' to `channel normalization' throughout.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 13, "text": "4/ Great point; we have added the sentence ``The optimal $k$ trades off those two errors; larger noise levels require smaller values of $k$ (or some other form of regularization).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 14, "text": "If the noise is significantly larger, then the method requires either choosing $k$ smaller, or it requires another means of regularization, for example early stopping of the optimization.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 15, "text": "For example $k=64$ or $128$ performs best out of $\\{32,64,128\\}$, for a PSNR of around 20dB, while for a PSNR of about 14dB, $k=32$ performs best.''", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 16, "text": "5/ We do not mention the standard deviation, but do specify the SNR throughout (e.g., in table 1 in column identity).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 17, "text": "We have clarified this in the caption of the table.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [20]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 18, "text": "6/ It essentially produces smooth noise then.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 19, "text": "The weights learned by the deep decoder pertain to the source noise tensor.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "BkeNja15nm", "rebuttal_id": "Hkg5wOyOaX", "sentence_index": 20, "text": "We have added a corresponding figure to the jupyter notebook for reproducing Figure 6.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "BkeWgLUpnX", "rebuttal_id": "r1lLG24iR7", "sentence_index": 0, "text": "Thank you very much for the instructive and detailed review!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkeWgLUpnX", "rebuttal_id": "r1lLG24iR7", "sentence_index": 1, "text": "For the first and second comments, we appreciate the detailed example you proposed.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]}, {"review_id": "BkeWgLUpnX", "rebuttal_id": "r1lLG24iR7", "sentence_index": 2, "text": "Specifically, we agree with the 1) and 2) of your analysis.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]}, {"review_id": "BkeWgLUpnX", "rebuttal_id": "r1lLG24iR7", "sentence_index": 3, "text": "For 3), although x^(1)_{t-2} is useful for predicting x^3_{t}, due to the causal chain and the presence of independent noise in the response function Eq. (1), x^(2)_{t-1} is even more useful for predicting x^(3)_{t}. When Eq. (2) is minimized w.r.t. both f_\\theta and all \\eta, with appropriate \\lambda, it is likely that \\eta_1 will go to infinity and \\eta_2 will be finite, leading to the correct conclusion that X^(1)_{t-1} does not directly structurally cause x^(3)_t.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "BkeWgLUpnX", "rebuttal_id": "r1lLG24iR7", "sentence_index": 4, "text": "For example, in the new Appendix B.3, we show analytically and numerically that for a linear Gaussian system, the global minimum of the learnable noise risk lies on I(x^(1)_{t-2}; \\tilde{x}^(1)_{t-2})=0, i.e. \\eta_1->\\infty, for a wide range of \\lambda.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "BkeWgLUpnX", "rebuttal_id": "r1lLG24iR7", "sentence_index": 5, "text": "To study the general landscape and global minimum of the learnable noise risk, we first carefully inspect Theorem 2, and find that its original statement is not true in general.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "BkeWgLUpnX", "rebuttal_id": "r1lLG24iR7", "sentence_index": 6, "text": "We have replaced the original Theorem 2 with a detailed analysis of the loss landscape of the learnable noise risk.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "BkeWgLUpnX", "rebuttal_id": "r1lLG24iR7", "sentence_index": 7, "text": "Specifically, there are four properties that the minimum MSE (MMSE, the first term of the learnable noise risk after minimizing w.r.t. f_\\theta) must obey, as demonstrated in the new Appendix B. In particular, we prove that the MMSE based only on the uncorrupted variables that directly structurally cause x^(i)_t is the minimum among all MMSE based on any set of uncorrupted variables.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "BkeWgLUpnX", "rebuttal_id": "r1lLG24iR7", "sentence_index": 8, "text": "These properties will likely lead to nonzero mutual information for the variables that directly structurally cause x^(i)_t, at the global minimum of the learnable noise risk, as we ramp down \\lambda from infinity.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "BkeWgLUpnX", "rebuttal_id": "r1lLG24iR7", "sentence_index": 9, "text": "In a sense, the learnable noise risk behaves similarly as an L1 regularized risk.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "BkeWgLUpnX", "rebuttal_id": "r1lLG24iR7", "sentence_index": 10, "text": "Whereas L1 encourages sparsity of the parameters of the model, the mutual information term in the learnable noise risk encourages sparsity of the influence of the inputs, where the strength of sparsity inducing depends on \\lambda.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "BkeWgLUpnX", "rebuttal_id": "r1lLG24iR7", "sentence_index": 11, "text": "As also pointed out in the \u201crelated works\u201d in the revision, the learnable noise risk is invariant to model structure change (keeping the function mapping unchanged) and rescaling of inputs, while L1 or group L1 do not, making learnable noise risk suitable for causal discovery where the scale of data may span orders of magnitude and the model structure may vary.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "BkeWgLUpnX", "rebuttal_id": "r1lLG24iR7", "sentence_index": 12, "text": "For the third and fourth comment, thanks for pointing out and we have added the constraint-based methods in the related works section, and stressed that we are dealing with \u201ccausality in mean\u201d in section 3.1.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 17, 18, 19]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 0, "text": "1- There are two reasons that concept and problem embedding are performed in this work.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10, 11]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 1, "text": "Considering concept continuity is an important matter in education.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10, 11]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 2, "text": "Having concept embedding, concept continuity can be reached as is discussed in the last paragraph on page 7 and some other examples are given in table 2.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10, 11]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 3, "text": "By just having the most sophisticated concept extractor, the concept continuity cannot be retrieved.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10, 11]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 4, "text": "Furthermore, problem embedding is used by the recommender system to project the performance of students on the problems they solved onto other problems that they have not solved.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10, 11]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 5, "text": "This way, we have an idea of what problems should be recommended to them and which problems should not by having an evaluation of their ability to solve unseen problems and recommend problems in the boundary of their capacity, not way beyond, and to recommend problems in a way that covers all concepts necessary for students to learn.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10, 11]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 6, "text": "We have observed interesting patterns, e.g. similar problems are more likely to be solved correctly at the same time or wrong at the same time.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10, 11]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 7, "text": "Note that by just having the concepts of problems that are not in numerical form, performance projection may not be feasible and there is a need for using other methods like embedding.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10, 11]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 8, "text": "2- The data size being small", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 9, "text": "is just the nature of the application.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 10, "text": "Creating new problems is a creative process and is not easy, given that with the insight we have on the application, the data size seems to suffice.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 11, "text": "Furthermore, since Prob2Vec is performing well for not a relatively big data set, it would definitely do well for big data sets since the more data we have, the more precise the concept and problem embedding are.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 12, "text": "The easy-tough-to-beat method proposed by Arora et al. is the state of the art in unsupervised sentence embedding that we compared our algorithm with.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 13, "text": "Please let us know if we missed anything.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 14, "text": "Pre-training is a common practice in transfer learning (one-shot learning).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 15, "text": "The objective function does not differ from the objective function used for post training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 16, "text": "Training on only negative samples with lower training epochs than the training epochs in post training just adjusts the weights of the neural network to a better starting point.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 17, "text": "If the training epochs in pre-training is relatively smaller than the training epochs in post training, due to curse of dimensionality, the warm start for post training results in better performance for NN classifier.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 18, "text": "To make it more clear what it means to train the neural network on a pure set of negative data samples, think about batch training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 19, "text": "It's not likely, but possible, that a batch only has negative or positive samples.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 20, "text": "In the pre-training phase of our method, we intentionally used a pure set of negative samples (with fewer training epochs) to have a warm start for post training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 21, "text": "As table 3 shows, our proposed method outperforms one-shot learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "BkeXRoYqhm", "rebuttal_id": "S1etFWjga7", "sentence_index": 22, "text": "Please look at part 1 of our response to reviewer2 and part 2 of comment titled \"Response to Question on Negative Pre-Training\" below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 0, "text": "We thank the reviewer for the comments and suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 1, "text": "As the reviewer points out, the community currently lacks a strong theoretical understanding of minmax optimization, and we believe our work helps to fill this gap.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 2, "text": "We comment on the practical implications of our work below:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 3, "text": "1) While the exact form of the sufficiently bilinear condition may be somewhat unwieldy, the result gives concrete evidence that having higher bilinearity can aid convergence for certain algorithms, even for settings that are not purely bilinear.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [12]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 4, "text": "This indicates that one should pay attention to the magnitude and condition number of the off-diagonal of the Jacobian when constructing a min-max problem and choosing an algorithm to solve the problem.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 5, "text": "2) In non-convex-concave settings, HGD will converge to all types of stationary points, as the reviewer points out.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [13, 14, 14]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 6, "text": "We propose some modifications to HGD to allow it to work in non-convex settings in Appendix A, which essentially amount to explicitly determining the local curvature of the problem and running a modified algorithm, such as Hamiltonian Gradient Ascent, near undesirable critical points.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 14]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 7, "text": "This would allow us to show similar local convergence guarantees to those proven by other works in the area (see Appendix A).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 14]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 8, "text": "However, as the reviewer points out as well, the HGD analysis is also useful because it implies similar convergence results for CO, which is a practical algorithm.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 14]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 9, "text": "3) Our result for CO shows that as long as $\\gamma \\ge 4L_g/\\alpha$, then CO will converge in sufficiently bilinear settings (currently it\u2019s written as $\\gamma = 4L_g/\\alpha$ but we will change this in the final version).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 10, "text": "This indicates that increasing $\\gamma$ may speed up convergence when we are in a sufficiently bilinear region (and in particular, the algorithm may not converge if $\\gamma$ is too small and the region has a very large bilinear term).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 11, "text": "If $\\gamma$ is too large, CO will converge to stationary points that are not local min-maxes, so these two phenomena must be traded off.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 12, "text": "One could potentially detect which regime one is in by computing a few eigenvalues of the Jacobian (using a logarithmic number of Hessian-vector products) during or after training.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 13, "text": "Other comments:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [19]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 14, "text": "-We thank the reviewer for pointing out Azizian et al. 2019.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 15, "text": "This work was released concurrently to ours on Arxiv and indeed seems to have some similar findings.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 16, "text": "We will include a reference to it in our revised version.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 17, "text": "-We thank the reviewer for the comment on notation and will incorporate it into the final version.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [22, 23, 24]]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 18, "text": "References:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgIGYrAKS", "rebuttal_id": "r1ls4aeqoH", "sentence_index": 19, "text": "Azizian, Wa\u00efss, et al. \"A Tight and Unified Analysis of Extragradient for a Whole Spectrum of Differentiable Games.\" arXiv preprint arXiv:1906.05945 (2019).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 0, "text": "We thank the reviewer for their time and their clear understanding of the key aspects of the paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 1, "text": "We address the reviewer\u2019s questions in the following:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 2, "text": ">  How much does the image matter for the single-image data set?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 3, "text": "The reviewer raises an important point about the tested single images.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 4, "text": "Less crowded images could lead to many patches having no gradients (e.g. showing only the sky), leading to a failure of at least RotNet, if not also BiGAN on many samples of the augmented dataset.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 5, "text": "Our image choices were thus motivated by striving for simplicity and not further adding a pipeline that would, for example, extract only patches with sufficiently large image gradients.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 6, "text": "We are training DeepCluster now on a significantly less busy image and will report results in the coming days.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 7, "text": ">  How general is the proposed approach?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 8, "text": "We believe that this method will work well for pretext tasks that rely on learning via detecting and learning invariances, such as Exemplar [1], Colorization [2], and Noise-as-targets [3].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 9, "text": "Methods such as Context [4] and Jigsaw [5] could potentially work less well as they would potentially easily find a way to cheat given the limited amount of original data of one image.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 10, "text": "However, as the authors note in the paper cited by the reviewer, the accuracy of a pretext task does not translate to downstream task performances, so even a method that is simple on one image\u2019s patches does not necessarily fail.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 11, "text": "This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [10]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 12, "text": "> [1] found that the network architecture for self-supervised learning can matter a lot, and that by using a ResNet architecture, performance of SSL methods can be significantly improved.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 13, "text": "Indeed, the paper mentioned by the reviewer shows that the performance of various self-supervised methods for ResNets does not degrade with the depth as it does for VGG and AlexNets due to the skip-connections.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 14, "text": "However, as ResNets have not been originally used to train the methods analyzed in our paper, we have stayed in the bounds that are required for fair comparisons and only used AlexNet.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 15, "text": "We agree with the reviewer that it would be good to check if ResNets, in general, can also be trained in such a manner (e.g. could global pooling destroy the signal?), so we are running an experiment on a ResNet-18 and will report results in the upcoming days.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 16, "text": "> Does the MonoGAN exhibit stable training dynamics comparable to training WGAN on CIFAR-10, or do the training dynamics change on the single-image data set?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 17, "text": "MonoGAN trained without any exploding gradients or other problems frequently encountered by GANs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 18, "text": "As we have suggested in the paper, this might be due to the fact that image-patches from one image follow a simpler distribution than in-the-wild images of a complete dataset.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 19, "text": "\u2014", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 20, "text": "[1] A. Dosovitskiy et al. \"Discriminative unsupervised feature learning with exemplar convolutional neural networks.\" TPAMI 2015", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 21, "text": "[2] R. Zhang et al. \"Colorful image colorization.\" ECCV 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 22, "text": "[3] P. Bojanowski et al. \"Unsupervised learning by predicting noise.\" ICML 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 23, "text": "[4] D. Pathak et al. \u201cContext Encoders: Feature Learning by Inpainting\". CVPR 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgPU07CFS", "rebuttal_id": "BJx7C1bfjH", "sentence_index": 24, "text": "[5] M. Noroozi \"Unsupervised learning for visual representations by solving jigsaw puzzles.\" ECCV 2016", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 0, "text": "Thank you very much for the constructive review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 1, "text": "Summary of our response", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 2, "text": "-------------------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 3, "text": "We are certain that the data processing inequality is used correctly.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 4, "text": "As you stated, the DPI implies for any Markov chain X -> Y -> Z that I(X,Y) >= I(X,Z).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 5, "text": "Unlike suggested in the review, our model is defined in the form \\theta -> \\tilde{\\theta} -> D, as shown in Figure 1a.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 6, "text": "Following your feedback, we updated section 2.1 and 2.3 for more clarity.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 7, "text": "Detailed response", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 8, "text": "-------------------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 9, "text": "We interleave parts of the review with our detailed response for ease of reading.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 10, "text": "> [...]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 11, "text": "the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 12, "text": "As in (Cover and Thomas, 2012), which is also cited in this paper, DPI is defined on a Markov chain X -> Y -> Z and we have I(X,Y) >= I(X,Z).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 13, "text": "However, based on the definition of \\theta and \\tilde{\\theta} given in the first sentence of section 2.3, the relation between \\theta, \\tilde{\\theta} and D should be: D <- \\theta -> \\tilde{\\theta} (if it is a generative model) or D -> \\theta -> \\tilde{\\theta} (if a discriminative model).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 9]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 14, "text": "Response: We are interested in limiting the mutual information I(\\theta, D) between our learned parameters \\theta and the dataset D.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 15, "text": "However, this is hard to calculate for typical deep models.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 16, "text": "Therefore we introduce a model that forms a Markov chain \\theta -> \\tilde{\\theta} -> D, as shown in Figure 1a.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 17, "text": "Hereby, \\tilde{\\theta} is a noisy version of the parameters \\theta.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 18, "text": "Crucially, the data D is defined to be dependent only on the noise-corrupted version \\tilde{\\theta}. By choosing a convenient noise process and prior for \\theta we can easily control I(\\tilde{\\theta}, \\theta).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 19, "text": "This gives us an upper bound on the mutual information I(D, \\theta) between data and parameters, according to the DPI.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 20, "text": "We updated section 2.3 to reflect this more clearly.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 21, "text": "> I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 22, "text": "For example", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 23, "text": "- the sentence under eq. (2)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 24, "text": "- the sentence \"Because the identity of the datapoint can never be learned by ...\" What is the identity of a data point?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 25, "text": "It looks like section 2.1 wants to show the connections between eq. (2) and other popularly used inference methods.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 26, "text": "Somehow, those connections are not clear to me.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 27, "text": "Response: The aim of section 2.1 is to motivate limiting mutual information for the purpose of generalization.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2, 3, 4, 5, 6]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 28, "text": "We link generalization problems reported in the literature to the introduced information measure.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2, 3, 4, 5, 6]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 29, "text": "The information necessary to identify or distinguish between training samples is quantified by the empirical entropy, and we called it the identity of the samples.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2, 3, 4, 5, 6]]}, {"review_id": "BkgYfE5o3Q", "rebuttal_id": "rkx7sBROTX", "sentence_index": 30, "text": "We updated the section to address all of your feedback.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3, 4, 5, 6]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 0, "text": "Thank you very much for your constructive comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 1, "text": "First, with respect to baselines, we have updated the results tables to include the additional baselines mentioned, as well as runs for VAT(+EntMin) with lower numbers of labels on CIFAR-10.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 2, "text": "After updating these baselines, we note that our method still achieves state-of-the-art performance in the regime where 1000 and 2000 labels are used for training on CIFAR-10, with and without data augmentation.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 3, "text": "We have also updated the text to tone down the claims.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 4, "text": "In addition, we note that the highest performance in many of the mentioned baselines (and with VAT) are obtained with a combination of multiple approaches.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 5, "text": "When our method is compared head-to-head against the proposed method in the mentioned papers, it is competitive and sometimes outperforms them, for instance, in experiments on CIFAR-10 with 4000 labels", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 6, "text": "With augmentation:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 7, "text": "Adversarial Dropout [1] (11.32) vs ours (11.79 +/- 0.25)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 8, "text": "Without augmentation:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 9, "text": "Improved GAN + SNTG [2] (14.93) vs ours (14.34 +/- 0.17)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 10, "text": "Defining the best combination of techniques to achieve the highest performance is an interesting direction of future work; our preliminary experiments combining Mean Teacher with manifold regularization have shown some improvements and we will include the results in the final version of the paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 11, "text": "Second, with respect to novelty, we would like to re-iterate our contributions since they may not have been clear.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 12, "text": "First, while manifold regularization has been explored in (Kumar et al 2017) and (Qi et al 2018), we proposed an efficient and effective approximation of manifold regularization that is far easier to compute than the involved method in (Kumar et al 2017).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 13, "text": "Moreover, we point out issues with the standard finite difference approximation to the Jacobian regularization and propose a solution to this problem by ignoring the magnitude of the gradient and using only the direction information.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 14, "text": "Moreover, we showed manifold regularization provides significant improvements to image quality and linked it to gradient penalties used for stabilizing GAN training, which were not shown by (Qi et al 2018).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 15, "text": "We did try to use spectral normalization but did not observe any gains for semi-supervised learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 16, "text": "Finally we would like to emphasize the conceptual differences between our method and other smoothing methods like spectral normalization - such methods perform isotropic regularization, whilst ours performs anisotropic smoothing along the manifold directions of generated data-points.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "BklhkXfqn7", "rebuttal_id": "H1elMgJc0m", "sentence_index": 17, "text": "We showed through experiments using (isotropic) ambient regularization that anisotropic regularization is more beneficial in the case of semi-supervised learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 0, "text": "Thank you for the detailed review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 1, "text": "We appreciate your extremely useful pointers to existing dependency testing techniques, many of which are new to us. Before delving into them, here's our initial thoughts.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 2, "text": "First and foremost, we would like to know more details on the reasoning behind the rejection rating.", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_global", null]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 3, "text": "It seems that the criticisms are on citations to other dependency testing approaches and time-dependency of fMRI data.", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_global", null]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 4, "text": "The suggestions are invaluable and we'll gladly include them (work in progress).", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_global", null]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 5, "text": "But comparison with them is not apples to apples , so we are not sure to what extent that adds value to our work, where we already compared to the KSG estimator for MI estimation and Pearson's correlation for dependency testing.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_global", null]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 6, "text": "Comparing to other dependency testing approaches, our technique allows the use of arbitrary neural networks and directly tests MI>0, the necessary and sufficient condition of dependency, is more general and is complementary to other techniques that make stronger assumptions about the data.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_global", null]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 7, "text": "We want to point out that a simple and most widely used dependency testing technique is Pearson's correlation through test of linear correlation (sufficient condition of dependency), which we compared to and show that our technique provides complementary value on sine wave and the fMRI dataset.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_global", null]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 8, "text": "We are aware that low sample complexity dependency tests exist and can be achieved by making additional assumptions on data, and we discussed that in the conclusion section, but our technique makes less assumptions and is applicable to general datasets which may not satisfy stricter assumptions.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_global", null]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 9, "text": "We are very interested in the HSIC-based techniques which seem to be popular and we could show complementarity. But at the same time the conclusion will be the same, so we have question on what value does it add for the audience over Pearson's correlation.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_global", null]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 10, "text": "Regarding other methods for dependency testing through mutual information, after following the line of work by Barrett et al. 2017, we reached this concise summary http://ims-vilnius2018.com/content/pdf/ivc293.pdf, which explains the smoothness assumptions made to data, as well as the fact that they used an asymptotic variance which holds when a large number of samples is given (page 2 top).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [34, 35, 35]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 11, "text": "It implies that the resulting confidence intervals as well as the test results are asymptotic and not guaranteed, which puts the resulting statistical tests into question.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [34, 35, 35]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 12, "text": "We have to admit that we did not thoroughly understand this line of work because of our background, so please comment if we are wrong about that.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [34, 35, 35]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 13, "text": "Instead, our dependency test does not make assumptions about data.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [34, 35, 35]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 14, "text": "Our lower bound and its confidence interval are not asymptotic.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 34, 35, 35]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 15, "text": "Theorem 1 provides a guaranteed confidence interval for arbitrary number of samples (so do our baselines, MINE and Pearson's r).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 34, 35, 35]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 16, "text": "We compared with the KSG estimator, which also only has asymptotic confidence intervals in literature.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 34, 35, 35]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 17, "text": "In addition, the proof provided in our work is concise and is easy for readers to understand and verify.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 34, 35, 35]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 18, "text": "Regarding time dependence and test threshold, it's important and thanks for pointing it out! We think that there are two ways time affects dependency listed below.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [79, 81, 82, 82]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 19, "text": "1) First, we assume non-overlapping windows of TRs as the basis for computing number of i.i.d fMRI samples, but did not mention that in our current draft. We'll update and make it clear.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [79, 81, 82, 82]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 20, "text": "2) Second, we can see an argument on whether or not segments of fMRI signals qualify as i.i.d, although not sure if this is a problem for our MI estimation approach.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [79, 81, 82, 82]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 21, "text": "If things change very little from one second to the next, the signals could be very similar and may not really be, intuitively, independent samples and may bias result of the study.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [79, 81, 82, 82]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 22, "text": "However, which independence assumptions to use is not in scope for our paper, because our fMRI study is trying to show that dependency testing works and is complementary to Pearson's correlation, not so much on drawing neuroscience conclusions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [79, 81, 82, 82]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 23, "text": "On a side note, it may turn out that which independence assumptions to make is a deeper question that doesn't yet have a clear answer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [79, 81, 82, 82]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 24, "text": "Regarding the level of the test, our theorem 1 already provides a proof based on Hoeffding inequality.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [79, 81, 82, 82]]}, {"review_id": "BklLqYip9B", "rebuttal_id": "B1xD5RI1iB", "sentence_index": 25, "text": "This proof could be experimentally verified through computing MI on 300 test set samples and see how the estimate changes if there were >1million test set samples, repeat say 1000 times using different random seeds.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [79, 81, 82, 82]]}, {"review_id": "BklyWIdlcS", "rebuttal_id": "rklsz575oB", "sentence_index": 0, "text": "Thanks for your feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BklyWIdlcS", "rebuttal_id": "rklsz575oB", "sentence_index": 1, "text": "We reply to all reviewers jointly in our comments above.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 0, "text": "We are thankful for the valuable feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 1, "text": "The main concern of this review is the quality of the writing and experimental details.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 2, "text": "We updated the paper to clarify all the names and ensure that all terms are introduced before they are used.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 3, "text": "The two tower design was necessary since the decision whether theorem T can be rewritten using parameters P requires both pieces of information, so we need to feed them to the network.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [23]]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 4, "text": "In fact $\\omega$ does not need to predict p, but it gives extra supervision signal and therefore regularizes the prediction.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [23]]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 5, "text": "The random baseline is necessary because of the unbalanced nature of the rewrite success, this is hard to control, so we added an extra baseline that shows that our results are better than just ignoring any of the input expressions (theorem or parameter).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [28]]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 6, "text": "In addition, we have simplified the architecture described in the paper by combining the networks $\\sigma$ and $\\omega$, and included the results from this architecture.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [19, 20, 21, 22]]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 7, "text": "We have significantly improved the experimental section by further clarifying the experiments and expanding them with more supporting measurements.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17]]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 8, "text": "We have also moved the two non-baselines out of the baselines section.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [26]]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 9, "text": "Finally, thank you for the insightful questions!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 10, "text": "With our current setup, the goal is to simply perform reasoning steps in latent space without specifically proving any statements.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 11, "text": "There are several approaches to make the network predict a closed goal, for example by predicting a fixed embedding such as the zero vector.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 12, "text": "We expect that most semantic aspects of the formula could be recovered, but not superficial features as the naming of the variables should not affect the rewriteability of formulas.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 13, "text": "The question how much of the formula can be recovered is probably dependent on the theorem database, since only those aspects that manifest in different rewrite successes are expected to be recovered.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 14, "text": "We don't have much intuition on the decomposability of embeddings, but it seems like a fascinating research direction.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "Bkx-GQrhtS", "rebuttal_id": "BkgqHnp_oS", "sentence_index": 15, "text": "We are grateful for the feedback which has helped to make the paper much clearer and more readable.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkxH95gchQ", "rebuttal_id": "Hye5Y6SQR7", "sentence_index": 0, "text": "We thank the reviewer for their review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkxH95gchQ", "rebuttal_id": "Hye5Y6SQR7", "sentence_index": 1, "text": "The reviewer notes the need to emphasize how and why to use this approach.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "BkxH95gchQ", "rebuttal_id": "Hye5Y6SQR7", "sentence_index": 2, "text": "In the new revision, we have added a discussion section to make a case for this.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "BkxH95gchQ", "rebuttal_id": "Hye5Y6SQR7", "sentence_index": 3, "text": "We will publish the code to compute conductance after the blind-review phase.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "BkxH95gchQ", "rebuttal_id": "Hye5Y6SQR7", "sentence_index": 4, "text": "The reviewer also mentioned that the paper doesn\u2019t compare directly against various attribution methods.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxH95gchQ", "rebuttal_id": "Hye5Y6SQR7", "sentence_index": 5, "text": "For this, we refer the reviewer to our response for the comment by anonymous.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_unknown", null]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 0, "text": "We thank the reviewer for insightful feedback and for noting that our\u200b experiments \u200bare\u200b \u200bsolid\u200b and our setup and analyses are sound.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 1, "text": "The reviewer asks great questions, and we provide the answers below.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 2, "text": "RE: Total running time", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 3, "text": "The reviewer raises an interesting point about total training time, which includes the time to pre-train a GNN and the time to fine-tune it on a downstream task.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 4, "text": "To address this point, below, we give the results of the total training time as well as the amortized total time over different downstream tasks.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 5, "text": "We will include detailed results and a discussion in the final version of the paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 6, "text": "We note that although pre-training does take some time, it is a one-time-effort only.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 7, "text": "That is, we pre-train a GNN model only once and then reuse it many times by fine-tuning the model on any number of downstream prediction tasks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 8, "text": "Overall, we find that GNNs, once pre-trained, tend to converge much faster on downstream tasks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 9, "text": "Most importantly, we find (details below) that validation set performance converges 5-12 times more quickly when GNNs are pre-trained.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 10, "text": "We emphasize that this cannot be achieved by mere training of (non-pre-trained) GNNs longer.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 11, "text": "The following summarizes training time for chemistry and biology datasets.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 12, "text": "1) Chemistry dataset (single GPU implementation)", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 13, "text": "**", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 14, "text": "Pre-training**", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 15, "text": "\u2014 Self-supervised pre-training: 24 hours", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 16, "text": "\u2014 Supervised pre-training: 11 hours", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 17, "text": "**Fine-tuning on MUV dataset** [Time to achieve the best validation set AUC]", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 18, "text": "\u2014 From random initialization (i.e., no pre-training):", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 19, "text": "1 hour; 74.9% AUC", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 20, "text": "\u2014 From a pre-trained GNN:", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 21, "text": "5 minutes; 85.3% AUC", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 22, "text": "2) Biology dataset", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 23, "text": "**", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 24, "text": "Pre-training**", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 25, "text": "\u2014 Self-supervised pre-training:  3.8 hours", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 26, "text": "\u2014 Supervised pre-training: 2.5 hours", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 27, "text": "**Fine-tuning** [Time to achieve the best validation set AUC]", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 28, "text": "\u2014 From random initialization (i.e., no pre-training):", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 29, "text": "50 minutes; 84.8% AUC", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 30, "text": "\u2014 From a pre-trained GNN:", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 31, "text": "10 minutes; 88.8% AUC", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 32, "text": "On chemistry dataset, we see that fine-tuning a pre-trained GNN on the MUV required only 5 min.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 33, "text": "This is in sharp contrast with training a GNN from scratch, which required 12x more time, yet it gave a worse performance.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 34, "text": "We can reach similar conclusions on the biology dataset.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 35, "text": "We thus recommend using pre-trained models whenever possible as they can give better performance and can be reused for any number of downstream tasks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 36, "text": "We shall add these results and explanations to the final version of the paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 37, "text": "RE: Analysis of different pre-training strategies", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 38, "text": "Thank you for bringing up this valuable point.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 39, "text": "We agree that it is important to understand why some pre-training strategies work better over others.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 40, "text": "Our key insight backed up with extensive empirical evidence is that a combination of graph-level and node-level methods (Figure 1) is important because it allows the model to capture both local and global semantics of graphs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 41, "text": "Further, we find that our structure-based node-level methods (Context Prediction and Attribute Masking) are preferred over position-based node-level methods (Edge Prediction, Deep Graph Infomax). As future work, we plan to further investigate what graph-level and node-level methods are most useful in different domains, and understand what domain-specific knowledge has been learned by the pre-trained models.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [11]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 0, "text": "We thank for the reviewer for their comments on our work, and we share our responses below.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 1, "text": "1) We agree that we did not provide a clear definition of \"task\".", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [11, 11]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 2, "text": "In the present paper there are two tasks: classification into primary labels, and classification into secondary labels.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 11]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 3, "text": "We did not mean to imply that the classification of a specific class is a task on its own.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11, 11, 13, 14]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 4, "text": "We agree however that a clearer introduction of the terminology would be clearly helpful and we plan to add this to the final submission.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [10, 11, 11, 13, 14]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 5, "text": "2) This comment is not entirely correct and we would like to apologies for any confusion in the paper.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15, 16, 17, 18, 18, 18, 21, 22]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 6, "text": "Actually, the update of the generator depends on the improvement of the classifier for the *principal* labels on the *meta-training* data, i.e. the improvement in generalisation to unseen data.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15, 16, 17, 18, 18, 18, 21, 22]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 7, "text": "Thus, the optimal auxiliary labels are not the ground-truth labels for the principal classes, since this would make both terms in the minimisation for $\\theta_1$ (the second equation in 3.2) identical and not allow any leveraging of the meta-training data.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15, 16, 17, 18, 18, 18, 21, 22]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 8, "text": "Also, we would argue that the KL-divergence, rather then introducing noise, allows us to avoid collapsing classes which we would claim are due to dying neurons (again, there is not loss/mechanism drawing the auxiliary labels to be the same as the primary ones).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15, 16, 17, 18, 18, 18, 21, 22]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 9, "text": "These claims are supported by showing that providing random labels does not lead to any improved performance and by our experience that using hard labels does indeed improve performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16, 17, 18, 18, 18, 21, 22]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 10, "text": "3) Providing fair comparisons across a range of very different methods is not easy when other methods aim to solve a different problem.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [23, 24, 25, 26]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 11, "text": "Concerning the comparison with prototypical networks, we do agree that this is not a fair comparison and we would like to change the phrasing in the paper.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [23, 24, 25, 26]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 12, "text": "The original reason for associating this to the prototypical network was that we employ their zero-shot setup: i.e. we use a VGG network to obtain prototypes on the meta-data and then use these prototypes to define an auxiliary task on the training-data.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [23, 24, 25, 26]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 13, "text": "4) We do agree that requiring the class hierarchy is a current limitation of the work.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [27, 28, 28]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 14, "text": "While it is still general enough for solving classification tasks (we merely have to choose a fixed number of sub-classes per task, e.g. 5 without having to provide anything else), we would want to look at more general auxiliary task in future.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [27, 28, 28]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 15, "text": "One option we are considering is employing an auxiliary regression task, where the generator network would provide vectors and the corresponding loss would be simple regression.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [27, 28, 28]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 16, "text": "However, since this is the first work to use a double gradient method for auxiliary task generation, we believe that presenting results with a comparison to human auxiliary labels, which itself also requires this hierarchy, is a good starting point.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [27, 28, 28]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 17, "text": "5) We would very much like to test our approach on more complex datasets with more varied classes, and this will be part of future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [30, 31]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 18, "text": "However, we would like to repeat that our approach can work with an arbitrary hierarchy (e.g. assigning the same number of sub-classes to every class).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [30, 31]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 19, "text": "The reason why we only used 100 classes in our experiments is for allowing the comparison with human-defined classes, but in principle we could use any number of sub-classes per primary class.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [30, 31]]}, {"review_id": "Bkxpr4aq3m", "rebuttal_id": "BygTJNKtRQ", "sentence_index": 20, "text": "In the CIFAR10 dataset in which a hierarchy is not defined, we show that using 6 different hierarchies all lead to a better generalisation.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [30, 31]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 0, "text": "We thank the reviewer for their review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 1, "text": "\u201cThe paper makes use of a result from the David MacKay textbook which defines the capacity of a single layer network to memorize the labelling of $n$ inputs in $d$-dimensional space. [...] It would be great if the paper also made some attempt to consider these connections. Or at least comment on how these factors could be incorporated into a more sophisticated analysis of the capacity of a network.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22, 22, 22, 22, 22, 22, 22, 22, 30, 30]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 2, "text": "We agree with the reviewer that our analysis of capacity in section 3 does not take into account the magnitude of the weights, nor the dependence on the depth of the network.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [22, 22, 22, 22, 22, 22, 22, 22, 30, 30]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 3, "text": "Our objective in this section was to provide a empirical lower-bound on the capacity by designing a setup where we can vary the quantity of information contained in a dataset (in our case, N choose n), and evaluate empirically the effect of data augmentation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 22, 22, 22, 22, 22, 22, 22, 30, 30]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 4, "text": "In relation to section 5, we aim at seeing how much a network can remember if it is explicitly trained to remember a given set of images.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 22, 22, 22, 22, 22, 22, 22, 30, 30]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 5, "text": "We understand the limitations of MacKay's analysis, which was presented to give a rough theoretical comparison point to our empirical evaluation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 22, 22, 22, 22, 22, 22, 22, 30, 30, 32, 32, 32, 35]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 6, "text": "We will clarify this in the paper and improve the discussion along the lines discussed by the reviewer.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [22, 22, 22, 22, 22, 22, 22, 22, 30, 30, 32, 32, 32, 35]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 7, "text": "\u201cThere is a slight oxymoron in the premise of the first set of experiments.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [52, 52]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 8, "text": "The network is forced to memorize a set of positive examples relative to the negative set it sees during training. What is memorized I presume depends a lot on the negative set used for training (its diversity, closeness to the positive set and how frequently each negative example is seen during training).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [52, 52]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 9, "text": "[...] Is there a training task which would allow one to more explicitly memorize the image (some sort of reconstruction task) as opposed to an in/out classification task?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 10, "text": "In these experiments, the set of positive and negatives is fixed (when varying data augmentation and architectures).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 11, "text": "During training, we feed to the network all positives and an equal number of negatives during each epoch.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 12, "text": "The performance does indeed depend on the closeness of the positive and the negatives, but this is similar to the membership inference problem presented in section 5, where it is difficult for a network to tell apart a seen image from an unseen, very similar image.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 13, "text": "A reconstruction task would suffer the same problems: the reconstruction is only approximate so we would need to evaluate the distance between our reconstruction, positives and negatives, which also depends on the closeness between positives and negatives.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 14, "text": "Also, the reconstruction task would need to remember the values of all pixels which requires more capacity.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 15, "text": "We agree that this specific deserves a short discussion and will add it to the paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 16, "text": "\u201cThis paper is a slightly difficult read [...] because there is not one main coherent argument or goal for the paper.[...]. Yes the different sections are related but it is does not feel like they build upon each other to help form a clearer picture of memorization within neural networks.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [64, 64, 64, 64, 64, 64, 70, 70, 70, 70, 70, 70, 70, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 17, "text": "The general goal of the paper is to empirically assess memorization in neural networks, and in particular the important question of implicit memorization, which is important for privacy: does a network trained for classification remember an image, or a set of images ?", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [64, 64, 64, 64, 64, 64, 70, 70, 70, 70, 70, 70, 70, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 18, "text": "This aspect is empirically evaluated in sections 4 and 5, and section 3 is a preliminary study of the memorization capabilities for systems explicitly trained to memorize (this serves as a qualitative upper-bound for implicit memorization).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [64, 64, 64, 64, 64, 64, 70, 70, 70, 70, 70, 70, 70, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 19, "text": "We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [89, 89, 89, 89, 89, 89, 89]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 20, "text": "We decided to move it to an appendix after reading the feedback from the three reviewers.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [89, 89, 89, 89, 89, 89, 89]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 21, "text": "\u201cThe conclusions focus on the importance of section 3 and the results of the experiments performed. Do the conclusions accurately reflect the opinions of the author?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [89, 89, 89, 89, 89, 89, 89]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 22, "text": "We do not consider the conclusions of the experiments from Section 3  to be more important than those of the other sections, in fact quite the opposite.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [89, 89, 89, 89, 89, 89, 89]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 23, "text": "As mentioned above, we will move it to appendices.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [89, 89, 89, 89, 89, 89, 89]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 24, "text": "\u201c[...]In section 3 is a perturbed positive image considered a positive training image? And in the testing phase are only unperturbed versions of the positive images given to the ConvNet as input?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [97, 97, 97, 97, 97, 97, 97]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 25, "text": "When data augmentation is used, we consider that perturbed positive images are also positive images.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [97, 97, 97, 97, 97, 97, 97]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 26, "text": "In the testing phase, perturbed versions of the positive images are given to the ConvNet.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [97, 97, 97, 97, 97, 97, 97]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 27, "text": "\u201cLast paragraph page 4: \"when the accuracy gets over 60\\% and at 90\\%\". Is this training or validation accuracy?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [104, 104]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 28, "text": "We decrease the learning rate when the training accuracy reaches these thresholds.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [104, 104]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 29, "text": "We thank the reviewer for reporting typos, we will correct them in the paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [106, 107, 108, 109, 109]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 0, "text": "Overall:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 1, "text": "We thank you for your valuable suggestions in helping us avoid potential inefficiencies in our work, and suggesting ways to avoid misunderstandings.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 2, "text": "We have incorporated your comments to significantly improve our work, and hope our revised draft is able to convince you towards a favorable outcome.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 3, "text": "Concern 1: Concerns with title \u201cMeta Domain Adaptation\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 4, 5, 6, 7, 8, 9, 10, 22]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 4, "text": "\u201c\u2026unlike as advertised, the paper does not address", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 4, 5, 6, 7, 8, 9, 10, 22]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 5, "text": "\u2026 \u201c", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 4, 5, 6, 7, 8, 9, 10, 22]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 6, "text": "It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [3, 4, 5, 6, 7, 8, 9, 10, 22]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 7, "text": "We acknowledge this problem and agree with you about a possible misinterpretation.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [3, 4, 5, 6, 7, 8, 9, 10, 22]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 8, "text": "However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6, 7, 8, 9, 10, 22]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 9, "text": "We also do think that the problem setting we have proposed is an important problem that deserves attention, and has not been studied in the meta-learning paradigm.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6, 7, 8, 9, 10, 22]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 10, "text": "We are glad that you also agree that setting makes sense (\u201c... the combination \u2026 is fair\u201d).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6, 7, 8, 9, 10, 22]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 11, "text": "Overall, we think that we have made an important contribution to Meta-Learning literature, by identifying its limitation for few-shot learning under domain shift, and proposed a solution to tackle this problem.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6, 7, 8, 9, 10, 22]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 12, "text": "We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3, 4, 5, 6, 7, 8, 9, 10, 22]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 13, "text": "In fact, we have mostly changed the name from \u201cMeta Domain Adaptation\u201d to \u201cMeta Learning with Domain Adaptation\u201d, and the rest of the paper is almost identical, which we believe addresses the concerns of false advertising.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3, 4, 5, 6, 7, 8, 9, 10, 22]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 14, "text": "Concern 2: Experiments", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13, 16, 17, 18, 19, 20]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 15, "text": "Domain Adaptation Baselines + Other datasets", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13, 16, 17, 18, 19, 20]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 16, "text": "Being a new problem setting, designing appropriate baselines can be challenging.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [12, 13, 16, 17, 18, 19, 20]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 17, "text": "We considered the traditional meta-learning for few-shot learning approaches, and combined meta-learning with a popular domain adaptation baseline.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 16, 17, 18, 19, 20]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 18, "text": "We are grateful for your suggestions on the domain adaptation baselines, and fully agree that it is reasonable.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [12, 13, 16, 17, 18, 19, 20]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 19, "text": "It is something we should have done on our own.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [12, 13, 16, 17, 18, 19, 20]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 20, "text": "Accordingly, based on your suggestions, and suggestions from other reviewers, we have tried to expand the baselines substantially (specifically, we include three state of the art Domain Adaptation methods as baselines \u2013 RevGrad [1], ADDA [2] and CyCADA [3]), and our proposed methods outperform them.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13, 16, 17, 18, 19, 20]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 21, "text": "For the other dataset suggested (VisDA), for synthetic-real adaptation, it is difficult to match the training paradigm of meta-learning.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12, 13, 16, 17, 18, 19, 20]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 22, "text": "Typically, we desire several classes for meta-train, and several classes for meta-test, so that a variety of (e.g.) 5-way tasks can be crawn.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12, 13, 16, 17, 18, 19, 20]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 23, "text": "With just 12 classes, the dataset is not very suitable for such settings.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12, 13, 16, 17, 18, 19, 20]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 24, "text": "[1] Ganin, Yaroslav, et al. \"Domain-adversarial training of neural networks.\" The Journal of Machine Learning Research 17.1 (2016): 2096-2030", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 25, "text": "[2] Tzeng, Eric, et al. \"Adversarial discriminative domain adaptation.\" Computer Vision and Pattern Recognition (CVPR). Vol. 1. No. 2. 2017", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 26, "text": "[3] Hoffman, J., Tzeng, E., Park, T., Zhu, J. Y., Isola, P., Saenko, K. & Darrell, T. Cycada: Cycle-consistent adversarial domain adaptation. ICML 2018", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 27, "text": "We thank you for considering our rebuttal and updating the score.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 28, "text": "We are grateful for your time and advice, and would appreciate if we could further extend the discussion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 29, "text": "We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [31]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 30, "text": "We have identified a novel problem setting, which is closer to the real world setting, than what has been studied so far under the meta-learning paradigm.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [31]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 31, "text": "Existing solutions are not effective in this setting, restricting their use in the real world.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [31]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 32, "text": "Addressing this setting in our framework gives us a direction to improve the practical utility of meta-learning solutions for few-shot learning.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [31]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 33, "text": "Specifically, we identify that the principle of image-to-image translation is very suitable for this setting, and apply those concepts to boost the performance of few-shot learning under domain shift.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [31]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 34, "text": "As a combination of problem setting and proposed solution, we do believe we have addressed an important problem, and made a novel contribution.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [31]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 35, "text": "As regards the experiments: \u201cfairly small datasets", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [32]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 36, "text": "\u2026 feature extractor backbone\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [32]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 37, "text": "Most domain adaptation experiments use MNIST, USPS, SVHN, which are comparable in size to our Omniglot experiments.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [32]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 38, "text": "The other popular benchmark is using the Office-dataset, which also we have used (although a more recent version of a similar dataset, i.e., office-home \u2013 more suitable for meta-learning evaluation, as it has larger number of classes).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [32]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 39, "text": "See for example some of the recent domain adaptation papers [1, 2, 3].", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [32]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 40, "text": "While a feature extractor backbone network may have some influence, we would like to highlight three points.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [32]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 41, "text": "First, when networks are trained in one domain, and evaluated in another, regardless of the backbone network, it is the domain-shift that dominates the performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [32]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 42, "text": "For example, no matter how large the network is, if it is trained to recognize black and white digits, it will still struggle to recognize colored digits.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [32]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 43, "text": "Second, any benefit of a larger backbone network will likely also enhance the performance of our model.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [32]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 44, "text": "Third, we just wanted to clarify (if there was a misunderstanding), unlike domain adaptation papers, we do not use a pretrained network \u2013 we train the full network from scratch (following traditional meta-training settings).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [32]]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 45, "text": "[1] Ganin, Yaroslav, et al. \"Domain-adversarial training of neural networks.\" The Journal of Machine Learning Research 17.1 (2016): 2096-2030", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 46, "text": "[2] Tzeng, Eric, et al. \"Adversarial discriminative domain adaptation.\" Computer Vision and Pattern Recognition (CVPR). Vol. 1. No. 2. 2017", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BygGS54qnQ", "rebuttal_id": "SygmDPUFCX", "sentence_index": 47, "text": "[3] Hoffman, J., Tzeng, E., Park, T., Zhu, J. Y., Isola, P., Saenko, K. & Darrell, T. Cycada: Cycle-consistent adversarial domain adaptation. ICML 2018", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Bygh4ga53m", "rebuttal_id": "SkeJUS6_0m", "sentence_index": 0, "text": "Thank you for your review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Bygh4ga53m", "rebuttal_id": "SkeJUS6_0m", "sentence_index": 1, "text": "We agree that further investigation is needed for mutual information, and we are currently working on it.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "Bygh4ga53m", "rebuttal_id": "SkeJUS6_0m", "sentence_index": 2, "text": "As for the layer to investigate, we have presented the higher layer results because the representation regularizers showed the most improvements when applied to the higher (or even output) layer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "Bygh4ga53m", "rebuttal_id": "SkeJUS6_0m", "sentence_index": 3, "text": "We believe the representations in the lower layers are inherently less structured and therefore representation shaping can be harmful.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "Bygh4ga53m", "rebuttal_id": "SkeJUS6_0m", "sentence_index": 4, "text": "The layer dependency is further explained in the following article.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "Bygh4ga53m", "rebuttal_id": "SkeJUS6_0m", "sentence_index": 5, "text": "Daeyoung Choi and Wonjong Rhee, Utilizing class information for deep network representation shaping, AAAI 2019   (https://arxiv.org/abs/1809.09307)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "Bygh4ga53m", "rebuttal_id": "SkeJUS6_0m", "sentence_index": 6, "text": ">> The reviewer feels that for CIFAR-10 and 100, some regularizers do consistently give best or close to best networks. Could the authors comment on this?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "Bygh4ga53m", "rebuttal_id": "SkeJUS6_0m", "sentence_index": 7, "text": "Response: In general, representation regularizers showed better performance than the others.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "Bygh4ga53m", "rebuttal_id": "SkeJUS6_0m", "sentence_index": 8, "text": "Among the representation regularizers, cw-VR and L1R frequently achieved the best performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "Bygh4ga53m", "rebuttal_id": "SkeJUS6_0m", "sentence_index": 9, "text": "Nonetheless, we were not able to identify any specific task condition that makes a specific regularizer consistently best performing regularizer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 0, "text": "We would like to thank the reviewer for the time and useful feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 1, "text": "Our response is given below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 2, "text": "- The paper is mainly empirical, although the authors compute two diagnostic statistics to show the effect of the self-modulation method.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 3, "text": "It is still not clear why self-modulation stabilizes the generator towards small conditioning values.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 4, "text": "We consider self-modulation as an architectural change in the line of changes such as residual connections or gating: simple, yet widely applicable and robust.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 5, "text": "As a first step, we provide a careful empirical evaluation of its benefits.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 6, "text": "While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [5]]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 7, "text": "Similar to residual connections, gating, dropout, and many other recent advances, more fundamental understanding will happen asynchronously and should not gate its adoption and usefulness for the community.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [5]]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 8, "text": "- It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 9, "text": "It seems that the authors are not aware of this difference.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 10, "text": "We are aware of this key difference and we apply the sigmoid function to scale the output of the discriminator to the [0,1] range for the non-saturating loss.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 11, "text": "Thanks for carefully reading our manuscript and noticing this typo which we will correct.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [7]]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 12, "text": "- In addition to report the median scores, standard deviations should be reported.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 13, "text": "We omitted standard errors simply to reduce clutter.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "Byl04qxA2X", "rebuttal_id": "r1gBxvdwpm", "sentence_index": 14, "text": "The standard error of the median is within 3% in the majority of the settings and is presented in both Tables 5 and Table 6.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 0, "text": "Thank you for the detailed review.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 1, "text": "We appreciate your comments on the contributions of our work and the nature of our approach, as well as suggestion of experiments and paper writing.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_global", null]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 2, "text": "Before delving in and providing more details, we have some initial thoughts about the theoretical issues you brought up.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 3, "text": "Regarding theorem 1 and the sample complexity of MINE, we also had discussions on why we think they are comparable or not and discussed that on page 5 in our submission.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 4, "text": "The tldr is that the MINE sample complexity can not only be seen as 1) bounding best achievable MI estimation but also as 2) bounding distance from estimation to a proven MI lower bound.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 5, "text": "The former is a quite vacuous bound on generalization and would require advances in learning theory to improve, not MI estimation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 6, "text": "Our theorem 1 is trying to improve the latter to enable practical applications.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 7, "text": "Improving the former to the level of practical use is a noble goal, let us know when you have an answer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 8, "text": "Regarding \"false detection\" experiments.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 9, "text": "We really appreciate that you brought up this point.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 10, "text": "Our synthetic experiments on Gaussians rho=0.0 in Figure 1 do exactly this.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 11, "text": "Results show that MINE-f and MINE-f-ES estimates very much non-zero MI when there should have been 0 MI.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 12, "text": "MINE-f bar is not visible due to overshooting out of the chart.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 13, "text": "DEMINE approaches give estimations closer to 0.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 14, "text": "We often get questions about why our estimators give MI numbers lower than MINE and why are we claiming that our estimator is better.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 15, "text": "But in fact that's exactly because MINE gives false detection but our estimators provably don't.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 16, "text": "Hyper-parameter search example.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 17, "text": "Say we are given 3000 paired (x,y) observations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 18, "text": "First divide into 1500 train, 1500 test.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 19, "text": "Take 1500 train and run Algorithm 1 using 3-fold crossval: use 1000 for (x,y)train and 500 for (x,y)val in each run (3 runs total).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 20, "text": "Get MI estimation m1,m2,m3 over 3 folds.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 21, "text": "Compute confidence interval v using Eq.8 using the hyperparameters and 1500 as test set size.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 22, "text": "Hyperparameter search DEMINE-vr maximizes mean([m1,m2,m3])-2std([m1,m2,m3]).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 23, "text": "DEMINE-sig maximizes mean([m1,m2,m3])-v.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 23, 24, 25]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 24, "text": "Will try to make it more clear.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [22, 23, 24, 25]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 25, "text": "Regarding fMRI experiments, our focus is on demonstrating neural MI estimation and dependency test on fMRI data.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 23, 24, 25]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 26, "text": "We compare with pearson's correlation because it's another widely used technique that can perform both correlation analysis as well as significance test.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 23, 24, 25]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 27, "text": "We used a simple 1D CNN where convolution happens over the time dimension, not the spatial dimensions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 23, 24, 25]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 28, "text": "Better architectures, e.g. transformers over time + graph networks over space could improve performance, but not our focus and we leave that to future work.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 23, 24, 25]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 29, "text": "Higher-order and nonlinear covariance tests may make very appealing comparisons and we are looking into it.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 23, 24, 25]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 30, "text": "A first impression is that our technique is more general and will probably give looser bounds, but may be applicable to a wider range of problems not only ones that have specifically that type of covariance, just like DEMINE vs Pearson's correlation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 23, 24, 25]]}, {"review_id": "Byl0ceB1cH", "rebuttal_id": "rJlbnvS1oH", "sentence_index": 31, "text": "But at the same time we also have questions on how much additional insight it brings, as it's not an apples to apples comparison, so neither tight or loose estimations diminish the value of both types of approaches.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 23, 24, 25]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 0, "text": "Thank you for your encouraging comments especially with regards to novelty and thoroughness of our experiments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 1, "text": "We have addressed the minor issues you highlighted; answers to your questions are also provided below:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 2, "text": "Q: \"It would be helpful to note in the description of Table 3 what is better (higher/lower).\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 3, "text": "A: We have updated the table description (now Table 4) as suggested.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 4, "text": "Q: \"Also Table 3 seems to have standard deviations missing in Supervised DCGANs.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 5, "text": "A: The results we reported were taken from (Gulrajani et al. 2017) which did not include standard deviations for this setting.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [14]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 6, "text": "Q: \"And is there an explanation on why there isn\u2019t an improvement in the FID score of SVHN for 1000 labels?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 7, "text": "A: We re-checked our FID computation for this case and fixed a bug.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 8, "text": "We have updated the paper with updated FID scores; we note there is a high variance in the FID so while there is an improvement on average, it occasionally may not be better.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 9, "text": "Q: \"What is the first line of Table 4? Is it supposed to be combined with the second?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 10, "text": "A: It is the reference from which all 3 results (Supervised, Pi Model and Mean Teacher) were taken from; we have made this clearer in the revised paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 11, "text": "Q: \"And is the Pi model missing results or can it not be run on too few labels?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 12, "text": "A: We have updated the table with results on fewer labels reported in (Tarvainen & Valpola, 2017); results on fewer labels were not reported in (Laine & Aila 2017).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [15]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 13, "text": "Q: \"In Figure A1, why is there a dark point at one point in the inner circle? What makes the gradient super high there ?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 14, "text": "A: This is because the classifier is not smooth in this region.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 15, "text": "The classifier is probably not constrained enough.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 16, "text": "Q:\"What are the differences of the 6 pictures in Figure A7? Iterations?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18]]}, {"review_id": "Byl2kX8c2Q", "rebuttal_id": "Syl4_k19Rm", "sentence_index": 17, "text": "A: These are the results from 6 different runs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18]]}, {"review_id": "BylNfGini7", "rebuttal_id": "S1x3ypEoCX", "sentence_index": 0, "text": "Thank you for the review, and we really appreciate your suggestions!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BylNfGini7", "rebuttal_id": "S1x3ypEoCX", "sentence_index": 1, "text": "In the revision, we have added analysis in section 4.2 and section 5 on how the learned causal matrix can be used downstream, for example in RL/IL and interpretability of neural nets.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6]]}, {"review_id": "BylNfGini7", "rebuttal_id": "S1x3ypEoCX", "sentence_index": 2, "text": "In the discussion in section 5, we also analyze how the error may affect the tasks downstream.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6]]}, {"review_id": "BylNfGini7", "rebuttal_id": "S1x3ypEoCX", "sentence_index": 3, "text": "We are excited that various tasks may utilize or incorporate our algorithm, and benefit from the causal inference ability it enables.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "BylNfGini7", "rebuttal_id": "S1x3ypEoCX", "sentence_index": 4, "text": "We have also added comparison with sparse learning/feature selection methods in the \u201crelated works\u201d section.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4]]}, {"review_id": "BylNfGini7", "rebuttal_id": "S1x3ypEoCX", "sentence_index": 5, "text": "In particular, we note that L1 and group L1 regularization is dependent on the model structure change and rescaling of input variables, while our learnable noise risk is invariant to both, making it suitable for causal discovery where the scale of data may span orders of magnitude and the model structure may vary.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 0, "text": "Dear reviewer,", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 1, "text": "Thank you for your comment. Before we reply to the points you have made, we would kindly ask you to please add the references that are missing from your review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 2, "text": "Thank you for your review of our work.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 3, "text": "The following are your concerns of our work:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 4, "text": "a. Prior distributions of hyperparameters", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 16, 17, 17]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 5, "text": "b. Loss landscape plots and relation to tunability", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 5]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 6, "text": "c. Importance of search order", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 5, 12, 13, 13]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 7, "text": "d. Details of the calibration procedure", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22, 23]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 8, "text": "We address them as follows (in two parts):", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 9, "text": "a. Prior distributions of hyperparameters:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 16, 17, 17]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 10, "text": "We envisage an optimizer not merely as update equations, but as the conjunction of the update equations, the hyperparameters, and distributions of those hyperparameters.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 16, 17, 17]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 11, "text": "Those distributions should be prescribed by the designers of the optimizer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 16, 17, 17]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 12, "text": "This is crucial: For example, if we take Adam with LR between $10^1$ and $10^5$ and claim that Adam is less tunable than others, the evaluation is inherently faulty, as it doesn't capture where the mode of the distribution of LRs for which Adam is expected to work.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 16, 17, 17]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 13, "text": "These prescriptions are absent for the optimizers considered in the paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 16, 17, 17]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 14, "text": "Therefore, we define them from either mathematical reasoning (say learning rate is non-negative, $\\beta_1, \\beta_2$ in Adam are between (0, 1) and close to 1) or using the calibration step, where we determine those distributions by fitting on the configurations that yielded reasonably good results.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 16, 17, 17]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 15, "text": "We choose simple priors for their ease of estimation, though given enough computation, arbitrarily complex priors can be computed and used.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 16, 17, 17]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 16, "text": "We fail to see the explicit relationship between our work and the papers you have referenced.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 16, 17, 17]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 17, "text": "Specifically [1] only proposes that there is an optimal batch size that is dependent on the momentum parameter.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 16, 17, 17]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 18, "text": "We do not consider tuning the batch size, as we do not consider it a hyperparameter of the optimizer itself.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [13, 17, 20, 20]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 19, "text": "[2] shows that instead of using LR decay schedule, increasing batch size has a similar effects on training, but results in faster training.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [13, 17, 20, 20]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 20, "text": "[3] talks about the existence of an effective learning rate as a function of learning rate and the norm of the weights, and proposes that the optimal learning rate is inversely proportional to the weight decay parameter.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 17, 20, 20]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 21, "text": "This doesn\u2019t, however, trivially lend itself to modeling priors.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [13, 17, 20, 20]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 22, "text": "In summary, these papers show a complex interplay between the parameters giving rise to other notions, but not provide any methods to jointly model these hyperparameters.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [13, 17, 20, 20]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 23, "text": "In the absence of such knowledge, we use our calibration procedure.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 17, 20, 20]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 24, "text": "The distributions we use are justified in section 3.2 in the paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 17, 20, 20]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 25, "text": "However, we accept the fact that a more complex distribution that might model the interaction between these hyperparameters might exist, and using that to sample for an HPO would be better.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [13, 17, 20, 20]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 26, "text": "b. Loss landscape plots and relation to tunability:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 22, 23]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 27, "text": "We show in figures 1.a, 1.b, as you rightly pointed out, the landscapes of loss function of the HPO objective as a function of the hypothetical hyperparameter $\\theta$. There seems to be a misunderstanding of the purpose of figures 1.a, and 1.b.: These figures do not show what we try to measure, but they merely illustrate by example what properties we would like a tunability metric to have.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 22, 23]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 28, "text": "We describe this in the beginning of Section 2.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 22, 23]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 29, "text": "In Section 5, we explain why existing measures of tunability are unable to make the distinction between the cases in Figure 1a.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 22, 23]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 30, "text": "We would like to emphasize that the point of Figures 1a, 1b is to illustrate the necessary properties that a proposed metric for tunability - it is not our intention to create such plots for our actual experiments.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 22, 23]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 31, "text": "If you are interested in these nonetheless, a very recent publication by Asi & Duchi (2019) shows the plot of lr vs performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 22, 23]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 32, "text": "In summary, they show that the sensitivity of SGD to stepsize choices, which converges only for a small range of stepsizes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 22, 23]]}, {"review_id": "BylSI7T6qr", "rebuttal_id": "BJeO-ILxjB", "sentence_index": 33, "text": "AdamLR exhibits better robustness when tested on CIFAR10.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 22, 23]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 0, "text": "We thank AnonReviewer1 for their positive comments about the interesting-ness of our proposed abductive reasoning tasks (inference and generation) and the associated benchmark dataset.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_global", null]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 1, "text": "We address specific concerns individually below:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 2, "text": "Discussion about e-SNLI:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 3, "text": "A key distinction between e-SNLI and Abductive-NLI is that the explanations in e-SNLI serve the purpose of justifying model decisions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 4, "text": "In contrast, the goal of Abductive-NLI and Abductive-NLG is to select or generate explanatory hypotheses for given observations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 5, "text": "Indeed, analogous to e-SNLI for SNLI, Abductive-NLI can be extended to \u201ce-Abductive-NLI\u201d by providing explanations that justify the selected hypothesis.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 6, "text": "Consider the following example that BERT fails to predict correctly:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 7, "text": "O1: Chad loves Barry Bonds.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 8, "text": "H1: Chad got to meet Barry Bonds online, chatting.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 9, "text": "H2: Chad waited after a game and met Barry.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 10, "text": "O2: Chad ensured that he took a picture to remember the event.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 11, "text": "The e-Abductive-NLI task would require models to generate an explanation for selecting H2.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 12, "text": "For the above example, a possible explanation for selecting H2 could be: \u201cPeople need to be physically co-located to take a picture with someone. Meeting online does not mean two people are physically co-located\u201d.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 13, "text": "We think generating such justifications is a great next step and hope that our work will foster such interesting future research.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 14, "text": "Re. somewhat limited contribution:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 15, "text": "We appreciate the opportunity to briefly restate our contributions and to discuss its significance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 16, "text": "Abductive Commonsense Reasoning, a critical capability in human reasoning, is relatively less studied in NLP research.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 17, "text": "To support this line of research, our work introduces a dataset that focuses explicitly on this important reasoning capability.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 18, "text": "Furthermore, several recent works [1,2,3,4] have shown the presence of annotation artifacts in crowdsourced datasets -- which poses a significant challenge for dataset curation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 19, "text": "Our work makes the following contributions:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 20, "text": "i) proposes and formalizes two novel tasks of Abductive Inference and Abductive Generation,", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 21, "text": "ii) presents a new dataset in support of these tasks collected through careful crowdsourcing design and an adversarial filtering algorithm,", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 22, "text": "iii) establishes strong baselines on the task proving the difficulty of the tasks and", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 23, "text": "iv) analyses the types of commonsense reasoning that current state of the art models fall short on.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 24, "text": "Re. limited form of Abductive Reasoning:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 25, "text": "The simplifying assumptions, mentioned in the paper, allow us to i) formulate the tasks concretely and ii) curate the dataset and evaluate models viably.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 26, "text": "We show that in spite of the assumptions, our dataset presents significant challenges for current models.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 27, "text": "We totally agree that in its most general form, there should be any number of observations and models should be required to generate explanatory hypotheses in natural language (as in the alpha-NLG task).", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [12]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 28, "text": "We hope our work will lead to this future line of research.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [12]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 29, "text": "Re. the title:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 30, "text": "Thanks for the suggestion. We will update the title to reflect that this work is aimed at language-based abductive reasoning.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [13]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 31, "text": "Table 7 vs Table2:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 32, "text": "Thanks for catching that. We\u2019ve updated the paper with the fix.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14]]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 33, "text": "[1] Gururangan et al. Annotation artifacts in natural language inference data.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 34, "text": "[2] Poliak et al. Hypothesis only baselines in natural language inference.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 35, "text": "[3] Tsuchiya e. al. Performance impact caused by hidden bias of training data for recognizing textual entailment.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "ByltNYc0tr", "rebuttal_id": "rylgzEOqsB", "sentence_index": 36, "text": "[4] Sakaguchi et al. WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "ByxBaTGy67", "rebuttal_id": "HkxIqcKZa7", "sentence_index": 0, "text": "We thank the reviewer for the positive comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "ByxBaTGy67", "rebuttal_id": "HkxIqcKZa7", "sentence_index": 1, "text": "Below we address the main concerns.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "ByxBaTGy67", "rebuttal_id": "HkxIqcKZa7", "sentence_index": 2, "text": "Q: \u201dApplying the model of Hartford et al. to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation... Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 8, 10]]}, {"review_id": "ByxBaTGy67", "rebuttal_id": "HkxIqcKZa7", "sentence_index": 3, "text": "A: Our goal in performing the synthetic experiments was to quantify the expressive power that is  gained by adding our basis elements to [Hartford et al. 18].", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [8, 8, 10]]}, {"review_id": "ByxBaTGy67", "rebuttal_id": "HkxIqcKZa7", "sentence_index": 4, "text": "We felt it is an informative experiment since [Hartford et al. 18] also discuss applying their model in the jointly exchangeable setting (page 3, second column, top paragraph).", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [8, 8, 10]]}, {"review_id": "ByxBaTGy67", "rebuttal_id": "HkxIqcKZa7", "sentence_index": 5, "text": "Having said that, we agree with the reviewer that [Hartford et al. 18] probably cannot handle such tasks by construction. As we mentioned in our response to Reviewer1 we will change the wording of this section to better reflect that this is *not* a failure of Hartford et al. but merely a setting outside their scope due to a different assumption on the symmetry group of the data.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8, 8, 10]]}, {"review_id": "ByxBaTGy67", "rebuttal_id": "HkxIqcKZa7", "sentence_index": 6, "text": "If the reviewers feel strongly about this experiment, we are open to replace it with a discussion.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8, 8, 10]]}, {"review_id": "ByxBaTGy67", "rebuttal_id": "HkxIqcKZa7", "sentence_index": 7, "text": "--------------------------------------------------------------------------------------------------------------------------------", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8, 8, 10]]}, {"review_id": "ByxBaTGy67", "rebuttal_id": "HkxIqcKZa7", "sentence_index": 8, "text": "Q: \u201cSome of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al\u201918 and references in there) are missing\u201d.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "ByxBaTGy67", "rebuttal_id": "HkxIqcKZa7", "sentence_index": 9, "text": "A: We did our best to survey and compare to the most related works on the dataset collection introduced in [Yanardag & Vishwanathan 2015].", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "ByxBaTGy67", "rebuttal_id": "HkxIqcKZa7", "sentence_index": 10, "text": "These datasets contain graphs from multiple origins, where some of them consist of highly varying graph sizes (within the same dataset).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "ByxBaTGy67", "rebuttal_id": "HkxIqcKZa7", "sentence_index": 11, "text": "In any case we will make the code available as soon as possible.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [7]]}, {"review_id": "ByxBaTGy67", "rebuttal_id": "HkxIqcKZa7", "sentence_index": 12, "text": "--------------------------------------------------------------------------------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "Byxe5udgcS", "rebuttal_id": "Hkx_-lO1oB", "sentence_index": 0, "text": "Pose2Pose -- An ablation study for the P2P network can be found in Table 2, with quantitative results for each contribution.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9]]}, {"review_id": "Byxe5udgcS", "rebuttal_id": "Hkx_-lO1oB", "sentence_index": 1, "text": "We do not add a qualitative ablation study for the P2P network, since still-images (as opposed to videos) do not convey the temporal improvement in this case.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [9]]}, {"review_id": "Byxe5udgcS", "rebuttal_id": "Hkx_-lO1oB", "sentence_index": 2, "text": "Pose2Frame -- A qualitative ablation study can be found in Fig. 16.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9]]}, {"review_id": "Byxe5udgcS", "rebuttal_id": "Hkx_-lO1oB", "sentence_index": 3, "text": "As can be seen, the results justify each component used.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9]]}, {"review_id": "Byxe5udgcS", "rebuttal_id": "Hkx_-lO1oB", "sentence_index": 4, "text": "pix2pixHD -- the Pose2Frame network can be directly compared with the pix2pixHD network, since they both act as mapping functions between dense-pose representations to realistic images.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "Byxe5udgcS", "rebuttal_id": "Hkx_-lO1oB", "sentence_index": 5, "text": "A quantitative comparison can be found in Table 1, as well as a qualitative comparison in Fig. 14.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "Byxe5udgcS", "rebuttal_id": "Hkx_-lO1oB", "sentence_index": 6, "text": "As can be seen, the use of our different components described in the P2F ablation study (blending mask and regularization, object channel, two pose inputs, discriminator attention on character, etc.), results in much fewer artifacts, making the Pose2Frame network suitable for this application.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "Byxe5udgcS", "rebuttal_id": "Hkx_-lO1oB", "sentence_index": 7, "text": "Combining the Pose2Pose and pix2pixHD networks, would yield significant artifacts (as seen in Fig. 14), and is not suitable for this kind of application.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 0, "text": "1.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 1, "text": "The purpose of the counterexample is only to show that there exists some spurious solutions to GANs with general DeepSets-style discriminator for point clouds.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 2, "text": "We agree that setup we selected is destined to fail, but it was done on purpose to illustrate the presence of spurious solutions.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 3, "text": "A good generator and discriminator would definitely be a solution as well.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 4, "text": "However, solutions during optimization might not always correspond to such good solutions and can also correspond to the demonstrated spurious solutions.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 5, "text": "We found empirically that GAN with simple DeepSet-like discriminator most of the times fails to learn to generate point clouds even after converging, however, it does sometimes results in reasonable generations (although worse than proposed PC-GAN).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 6, "text": "So, we do not consider the argument to be unrealistic as we often observe the degeneracy.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 7, "text": "So the message here is that we need additional constraints for GANs with simple DeepSet-like discriminator to exclude such bad solutions and lead to a more stable training.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 8, "text": "Other architectures like RNN might work, but they are not permutation invariant, which is a desirable property for set data like point clouds.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 9, "text": "More comparisons between using RNN and DeepSets for other tasks on set data can refer to Zaheer et al., (2017).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 10, "text": "2.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 11, "text": "As we discussed in the end of Section 3, ALI and BiGan\u2019s goal is to match (z, G(z)) and (Q(X), X), which aims to infer the random noise z and enforce the latent code to follow noise  distribution (e.g. Gaussian).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 12, "text": "On the other hand, we do not enforce Q(X) to follow from Gaussian.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 13, "text": "Instead, we train the other G_theta(u) to match Q(X), which is more similar to AAE-like works (Engel et al., 2017; Kim et al., 2017; Achlioptas et al. 2017).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 14, "text": "The difference of the interpretation between PC-GAN and  those AAE-like work is also explained in the second paragraph of Sec 4.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 15, "text": "3. We followed the same protocol that we trained on ShapeNet55 and tested on ModelNet40 testing set.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 16, "text": "Please check Table 3 in the revision.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "H1e8c5rL3Q", "rebuttal_id": "HJg61YeLRX", "sentence_index": 17, "text": "PC-GAN achieves 86.9% accuracy which is better than AAE (84.5%),  3D-GAN (83.3%) and other unsupervised learning approach.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 0, "text": "(1) Reconstruction from prior during training:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [21, 22, 23, 24, 25]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 1, "text": "The crux of the proposed model is the selective proposal distribution.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [21, 22, 23, 24, 25]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 2, "text": "\"Pseudo\" sampling for unobserved modalities during training provides a way to facilitate model training process.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [21, 22, 23, 24, 25]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 3, "text": "We evaluated the model under two training settings: (I) optimize the final ELBO without conditional log-likelihood for unobserved modalities x_u; and (II) optimize the final ELBO with  conditional log-likelihood of unobserved modalities.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [21, 22, 23, 24, 25]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 4, "text": "This is realized by utilizing the \"pseudo\" sampling described before (and in the paper).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [21, 22, 23, 24, 25]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 5, "text": "The results are comparable but the added term in setting II shows benefits on some datasets.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [21, 22, 23, 24, 25]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 6, "text": "While setting I is solely based on the observed modalities, the setting II incorporates the unobserved modalities along with the observed ones.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [21, 22, 23, 24, 25]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 7, "text": "By using the complete data, the setting II describes the complete ELBO corresponding to the partially observed multimodal data (in consideration).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [21, 22, 23, 24, 25]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 8, "text": "(2) Comparison with VAEAC:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 9, "text": "In order to establish fair comparison, we used the same backbone network structures and training criteria for all baseline models and our proposed VSAE.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 10, "text": "Therefore, the implementation details differ from the original VAEAC paper.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 11, "text": "We did our best to maintain the optimization details described in all baseline papers.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 12, "text": "Experiments on VAEAC with partially-observed data are also conducted.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 13, "text": "Results show that VAEAC under this setting can achieve comparable performance on categorical datasets: 0.245(0.002) on Phishing, 0.399(0.011) on Mushroom while the errors of VSAE are 0.237(0.001) on Phishing,  0.396(0.008) on Mushroom.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 14, "text": "However, on numerical and bimodal datasets, partially trained VAEAC performs worse than VSAE :", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 15, "text": "*VSAE:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 16, "text": "0.455(0.003) on Yeast; 1.312(0.021) on Glass;0.1376(0.0002) on MNIST+MNIST; 0.1198(0.0001) on MNIST+SVHN;", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 17, "text": "*VAEAC trained partially:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 18, "text": "0.878(0.006) on Yeast; 1.846(0.037) on Glass;0.1402(0.0001) on MNIST+MNIST; 0.2126(0.0031) on MNIST+SVHN.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 19, "text": "(3) Experiments under synthetic non-MCAR masking:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 20, "text": "As mentioned by the reviewer, we conduct experiments on non-MCAR masking following state-of-the-art non-MCAR model MIWAE [2].", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 21, "text": "Same as MIWAE, we synthesize masks by defining some rules to specify the probability of a Bernoulli distribution.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 22, "text": "Please refer to Table 3 and Appendix C.4 for updated comparison results.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 23, "text": "VSAE outperforms MIWAE under all MCAR, MAR and NMAR masking mechanisms.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 24, "text": "(4) Baselines:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [31, 32, 33]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 25, "text": "All baselines considered in the paper are designed to have comparable number of parameters (same or larger than our model) to make the comparison fair.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [31, 32, 33]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 26, "text": "We have updated the baseline details in the Appendix B.3.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [31, 32, 33]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 27, "text": "Although GAN-based models show promising imputation results, they usually fail to model data distribution properly.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [31, 32, 33]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 28, "text": "Therefore, we do not consider them as our baseline models.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [31, 32, 33]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 29, "text": "It is also important to note that VSAE is not a model designed only for imputation, but a generic framework to learn from partially-observed data for both imputation and generation.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [31, 32, 33]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 30, "text": "(5) Conditional imputation:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [34]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 31, "text": "When performing imputation, we assume that the generation is not conditioned on the observed image, but only conditioned on the factorized latent variables.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [34]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 32, "text": "Input an observed image to the model, we observe a \"conditional\" distribution if we independently sample from the latent variables.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [34]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 33, "text": "See Figure.7 in updated Appendix C.2.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [34]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 34, "text": "(6) Answers to the questions:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [35]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 35, "text": "1. Please refer to point (2) for detailed explanation on comparison with VAEAC.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [36]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 36, "text": "In summary, there are multiple reasons why the performance is not identical with the original VAEAC: (I) the back-bone structures are not the same; (II) training criteria (including batch size, learning rate, etc.) are not the same; and (III)  training/validation/test split is different.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [36]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 37, "text": "We would like to emphasize that the aforementioned changes are necessary to establish fair comparison.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [36]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 38, "text": "2.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [37]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 39, "text": "We adopt the calculation from [1] where NRMSE is RMSE normalized by the standard deviation of each feature followed by an average over all imputed features.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [37]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 40, "text": "The standard deviation of ground truth features does not guarantee NRMSE < 1.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [37]]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 41, "text": "[1] Ivanov et al.Variational Autoencoder with Arbitrary Conditioning, ICLR 2019", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eOXTuD5B", "rebuttal_id": "Skl-7ddujB", "sentence_index": 42, "text": "[2] Mattei et al. MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets, ICML 2019", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eqf5OA3m", "rebuttal_id": "BJeC2_Hspm", "sentence_index": 0, "text": "We appreciate the reviewer 1 for his/her feedback on our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1eqf5OA3m", "rebuttal_id": "BJeC2_Hspm", "sentence_index": 1, "text": "The reviewer mentioned: \u201cIn general it is very unlikely that you will be able to choose every variation of out-distribution cases\u201d:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "H1eqf5OA3m", "rebuttal_id": "BJeC2_Hspm", "sentence_index": 2, "text": "Actually, for training A-CNN (Augmented CNN), we did not train it on every variation of out-distribution cases, rather, we recognize a single representative out-distribution set among the available ones according to our measurement.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "H1eqf5OA3m", "rebuttal_id": "BJeC2_Hspm", "sentence_index": 3, "text": "Then using it for training A-CNN with the aim of effectively controlling over-generalization.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "H1eqf5OA3m", "rebuttal_id": "BJeC2_Hspm", "sentence_index": 4, "text": "The reviewer mentioned: \u201c Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax\u201d:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "H1eqf5OA3m", "rebuttal_id": "BJeC2_Hspm", "sentence_index": 5, "text": "We would be appreciated if the reviewer could provide us with the references that showing using only sigmoid could control such a challenging problem of adversaries.", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [5]]}, {"review_id": "H1eqf5OA3m", "rebuttal_id": "BJeC2_Hspm", "sentence_index": 6, "text": "Please note we did not aim to devise a method that is able to reject all adversaries.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "H1eqf5OA3m", "rebuttal_id": "BJeC2_Hspm", "sentence_index": 7, "text": "Rather, we attempted to show that a CNN with less over-generalization is able to reject some of the adversaries while correctly classifies many of the remainder, particularly non-transferable attacks.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "H1eQwEeRYS", "rebuttal_id": "H1xjf2ILjr", "sentence_index": 0, "text": "We thank reviewer #2 for the useful feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1eQwEeRYS", "rebuttal_id": "H1xjf2ILjr", "sentence_index": 1, "text": "We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added theorems and more formal statements in the main text,  compressed the appendix and enhanced the description of the experimental setup.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "H1eQwEeRYS", "rebuttal_id": "H1xjf2ILjr", "sentence_index": 2, "text": "We have updated the paper and kindly ask the reviewer to take another look.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1eQwEeRYS", "rebuttal_id": "H1xjf2ILjr", "sentence_index": 3, "text": "Robustness of the curvature sampling method: we provide confidence intervals in Table 2 of our results.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "H1eQwEeRYS", "rebuttal_id": "H1xjf2ILjr", "sentence_index": 4, "text": "We learn curvatures for each of the component spaces and show learned values together with confidence intervals in Appendix E. It can be seen that these variances are in the low regime.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "H1euiaMah7", "rebuttal_id": "BJlwPu6hyE", "sentence_index": 0, "text": "We thank the reviewer for their comments and for noting correctly that our modification is quite effective, particularly regarding the large improvements on human evaluations.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1euiaMah7", "rebuttal_id": "BJlwPu6hyE", "sentence_index": 1, "text": "Our method is simpler in both conception and implementation than coverage, while requiring less parameters and being twice as likely to be chosen as better by human judges.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "H1euiaMah7", "rebuttal_id": "BJlwPu6hyE", "sentence_index": 2, "text": "We agree with the reviewer on the simplicity of our method, which we believe to be an asset.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_sentences", [8, 9, 11]]}, {"review_id": "H1euiaMah7", "rebuttal_id": "BJlwPu6hyE", "sentence_index": 3, "text": "In addition to that, we believe the Scratchpad Encoder is fundamentally interesting as a mirror to the \u2018attentive read\u2019 common in seq2seq models.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [1]]}, {"review_id": "H1euiaMah7", "rebuttal_id": "BJlwPu6hyE", "sentence_index": 4, "text": "We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [12]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 0, "text": "We thank the reviewer for the comments and suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 1, "text": "We address points individually below:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 2, "text": "1) The Higher-order Lipschitz condition is necessary for us to use the PL convergence guarantee.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 3, "text": "This condition is similar to assumptions made in convex optimization, especially where higher-order updates are involved (see eg. Agarwal et al. 2017 and Bubeck et al. 2019).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 4, "text": "If the iterates of the algorithm always have bounded norm (eg. due to constraints or regularization), then three-times differentiable functions will satisfy the Higher-order Lipschitz condition for our purposes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 5, "text": "This is because it suffices for the condition to hold for only the iterates of the algorithm ($x^{(1)},x^{(2)},...$), rather than for all of $\\mathbb{R}^d$.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 6, "text": "2) We thank the reviewer for this remark.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 7, "text": "We wanted $L_H$ to be defined for our theorem statements, but we can see how it is confusing as is. We will make it clear that $L_H$ is the smoothness parameter of $H$.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 8, "text": "3) Theorem 3.4 holds in the broadest setting out of all of these results.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 9, "text": "Theorems 3.2 and 3.3 have slightly tighter bounds for their respective settings.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 10, "text": "We will clarify this in the surrounding text.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [13]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 11, "text": "4) It is true that these results rely on the PL condition, and this is unavoidable for our current results.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [14, 15, 16, 17, 18]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 12, "text": "The novel perspective in this paper is that we consider the PL condition on a different objective, namely the squared gradient norm, rather than on the game objective $g$. This perspective allows us to prove our new bounds, although we still require some nontrivial linear algebra.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15, 16, 17, 18]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 13, "text": "The PL condition also allows us to easily prove our stochastic HGD results.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15, 16, 17, 18]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 14, "text": "5) The 1/sqrt(2) should cancel out on both sides of the guarantee in Theorem 5.2 (and eg. in equation 68).", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [19]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 15, "text": "Minor suggestions:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [20]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 16, "text": "-We appreciate the suggestion for page 5 and will make this change in the final version.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [21]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 17, "text": "We thank the reviewer for recognizing our theoretical contributions.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_sentences", [22, 23, 24]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 18, "text": "We would be happy to include some further experiments in the final version comparing HGD with other algorithms such as extragradient.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [22, 23, 24]]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 19, "text": "References:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 20, "text": "Agarwal, Naman, and Elad Hazan. \"Lower bounds for higher-order convex optimization.\" COLT 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1evEoR6YH", "rebuttal_id": "rJlZpTg5sr", "sentence_index": 21, "text": "Bubeck, S\u00e9bastien, et al. \"Near-optimal method for highly smooth convex optimization.\" COLT 2019.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 0, "text": "We thank reviewer for his insightful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 1, "text": "1.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 2, "text": "We agree with the reviewer that sinkhorn iteration is a way to obtain an upper bound on Wasserstein distance.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [6, 7, 8, 9]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 3, "text": "However, based on the original paper, they solve the Sinkhorn divergen with T iterations, later when they solve the generator based on the estimated distance, the gradient has to backpropagate through those T iterations, which is expensive and infeasible.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [6, 7, 8, 9]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 4, "text": "We also note that there is new work, IPOT (Xie et al., 2018), which can get rid of backpropagating through the T iterations as what we adopted (Bertsekas, 1985) in the paper.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8, 9]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 5, "text": "Combining PC-GAN with IPOP or other future works could be an interesting future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [6, 7, 8, 9]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 6, "text": "2.  The variance of the sandwiched estimator can be higher, but we are more concerned about bias in this work, which can be treated as a bias-variance trade-off.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 7, "text": "3. The 20:1 mixture used in practice do not directly correspond to s in theory, because the distances we compute are not scaled.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 8, "text": "For example, if the f_\\phi, the discriminator of GAN, is k-Lipschitz, the lower bound estimate should be divided by k.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 9, "text": "However, k is unknown in practice.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 10, "text": "Therefore, we just numerically did a coarse grid search and find the best mixture ratio.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 11, "text": "Also, we try different ratios as we replied to R2 above.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 12, "text": "Ratio                   D2F (Distance to Face)", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 13, "text": "Coverage", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 14, "text": "1:0", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 15, "text": "6.03E+00                        3.36E-01", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 16, "text": "40:1", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 17, "text": "6.06E+00                       3.41E-01", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 18, "text": "20:1", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 19, "text": "5.77E+00                       3.47E-01", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 20, "text": "10:1", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 21, "text": "6.85E+00                       3.56E-01", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 22, "text": "0 :1", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 23, "text": "9.19E+00                       3.67E-01", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14, 15, 16]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 24, "text": "4. We do not consider W_s to be very close from W_U. As can be seen from Figure 6, for the aeroplane examples, W_U fails to capture aeroplane tires while W_s can.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17, 18]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 25, "text": "Similarly for Chair example, W_s recovers better legs than W_U. Quantitatively, we highlight that W_s outperforms W_U consistently as shown in Table 1.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17, 18]]}, {"review_id": "H1eZXehVnX", "rebuttal_id": "r1eAuTlLCX", "sentence_index": 26, "text": "Thus, we consider both W_U and W_L is needed to generate good quality point clouds.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17, 18]]}, {"review_id": "H1g7-dMz3m", "rebuttal_id": "B1eovy5nam", "sentence_index": 0, "text": "Thank you for your thoughtful feedback!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1gcRli3YB", "rebuttal_id": "SkxzOluyor", "sentence_index": 0, "text": "1. We aimed to provide a broad variety of example applications (playing tennis, walking, fencing, dancing), while mainly focusing on the most complicated (tennis) application, for a thorough analysis of our method.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "H1gcRli3YB", "rebuttal_id": "SkxzOluyor", "sentence_index": 1, "text": "Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "H1gcRli3YB", "rebuttal_id": "SkxzOluyor", "sentence_index": 2, "text": "2. The Pose2Pose and Pose2Frame networks are trained separately.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "H1gcRli3YB", "rebuttal_id": "SkxzOluyor", "sentence_index": 3, "text": "Specifically, the P2F network is trained on the original data, and not on the output frames of the P2P network.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "H1gcRli3YB", "rebuttal_id": "SkxzOluyor", "sentence_index": 4, "text": "You are correct that some artifacts are added to the final P2F output at test time, yet they are minor due to the structural stability of the poses generated by the P2P network.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "H1gcRli3YB", "rebuttal_id": "SkxzOluyor", "sentence_index": 5, "text": "Furthermore, training the P2F network with the P2P outputs is problematic, since we do not have the ground-truth for the new pose generated by the P2P network.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "H1gcRli3YB", "rebuttal_id": "SkxzOluyor", "sentence_index": 6, "text": "3. The mask loss proposed in the review is similar to our implementation, except that we make a distinction between an inner-mask control and an outer-mask control.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21, 22]]}, {"review_id": "H1gcRli3YB", "rebuttal_id": "SkxzOluyor", "sentence_index": 7, "text": "Our mask regression losses consist of a first loss penalizing the mask from being active outside the densepose mask, and a second loss penalizing the mask from being inactive inside the densepose mask.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21, 22]]}, {"review_id": "H1gcRli3YB", "rebuttal_id": "SkxzOluyor", "sentence_index": 8, "text": "Combining them both results in the suggested loss.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21, 22]]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 0, "text": "We are thankful for the reviewer 2 to provide us with his/her feedback,", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 1, "text": "The reviewer mentioned: \"the interpolation mechanism is also too simple\":", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 2, "text": "We would like to highlight that despite the simplicity of interpolated samples, there has been demonstrated the effectiveness of using such samples on developing more regularized and generalized neural networks (Zhang et al, 2018) as well as on making them more secure", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 3, "text": "(Zhao et. al. 2018)", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 4, "text": ".", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 5, "text": "Thus, we believe that simplicity does not necessarily lead to ineffectiveness.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 6, "text": "The reviewer mentioned \u201cmany hidden assumptions on the images source or the base classier\u201d:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 7, "text": "As this statement is not clear for us, we would appreciate if the reviewer could elaborate more on it.", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [2]]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 8, "text": "The only assumption we made is on the fact that the out-distribution samples should be statistically and semantically different than the in-distribution samples.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 9, "text": "Then among such out-distribution sets, we propose a measurement for identifying the most representative one among those available.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 10, "text": "The reviewer stated \"There exists more principled approaches for *selecting* out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors\":", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 11, "text": "While there are the approaches that aim to detect out-distribution sets, they have not been designed for the selection purposes as we do.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3]]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 12, "text": "By mentioning this statement, if the reviewer means the missing of some principled approaches like ODIN in our comparisons, we would like to inform the inclusion of ODIN results in the revised version of the paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3]]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 13, "text": "Reference:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 14, "text": "- Zhao, Jake, and Kyunghyun Cho. \"Retrieval-Augmented Convolutional Neural Networks for Improved Robustness against Adversarial Examples.\" arXiv preprint arXiv:1802.09502 (2018).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 15, "text": "- Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 16, "text": "mixup: Beyond empirical risk minimization.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gMR8h6h7", "rebuttal_id": "SylJ-xIopQ", "sentence_index": 17, "text": "ICLR 2018 (arXiv preprint arXiv:1710.09412).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 0, "text": "Thank you for pointing out the other datasets in algebraic word reasoning.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 1, "text": "We\u2019ve included these in an expanded discussion of related work with discussion on how they relate to the current dataset.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 2, "text": "Please let us know if we have missed other papers.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 3, "text": "Your proposal of combining multiple extant problem sets is a good idea.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [18]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 4, "text": "We\u2019d want to ensure the combined datasets have a common format (e.g., the same unambiguous freeform text format for reasons of transferability, etc as argued in the paper), and there are interesting problem types occurring in other datasets (such as logical entailment or boolean satisfiability) that we haven\u2019t yet included.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [18]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 5, "text": "We may in the future extend the dataset to include these other problem types if the current ones become solved, and of course we solicit contributions (in the form of generation code) to the dataset.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [18]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 6, "text": "We likely could not use workbooks etc as a source for problems without significant investment, since obtaining legal permission to redistribute copyrighted problems found in these books would probably be hard and/or expensive.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [19, 20, 21, 22, 23]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 7, "text": "Having said that, it is definitely important to ensure the problems remain grounded in real-life problems (thus our small list of real-life exam questions).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [19, 20, 21, 22, 23]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 8, "text": "This was the motivation for testing trained models against \u201creal life\u201d questions occurring in school-level examinations; these questions are not intended to be a primary benchmark (with more questions and detailed grades), but rather simply a rough indication of whether training models to answer school-level questions could be achievable.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [19, 20, 21, 22, 23]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 9, "text": "On the distribution of the sampled answer (and the related question of how difficulty levels are determined), these are great questions.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 25]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 10, "text": "For some modules with two output choices (e.g., True, False), we can simply split the answers 50-50.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24, 25]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 11, "text": "But in general, the answer distribution depends on the module, with hand-tuning to ensure the (question, answer) pair is of a reasonable difficulty level as judged by humans.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24, 25]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 12, "text": "In more detail: as mentioned in the paper, we want to achieve upper bounds on the maximum probability that any single (question, answer) is sampled; thus if we sample the answer from a set of N possible answers, then to achieve a maximum probability p of a given question, the remaining choices made in generating the question must", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24, 25]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 13, "text": "be from a set of size p/N. We roughly aim to pick N (depending on p) so that conditioned on this, the question is as easy as possible; there is typically a hand-tuned sweet spot.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24, 25]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 14, "text": "On evaluating general-purpose models only, we may have phrased this badly in the paper, and have updated it.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [27, 28, 29, 30, 31]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 15, "text": "We are definitely interested in any models that learns to do mathematics and symbolic reasoning, which would include more sophisticated models tailored towards doing mathematics (one could imagine models with working memory, etc).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [27, 28, 29, 30, 31]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 16, "text": "However, we discount models that already have their mathematics knowledge inbuilt rather than learnt (for example, this includes many of the models that occur in algebraic reasoning tasks, where the model learns to map the input text to an existing equation template, that is then solved by a fixed calculator).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [27, 28, 29, 30, 31]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 17, "text": "We test DNC (differentiable neural computers) and RMC (relational memory core) models, which arguably are more specialized for doing mathematics, since they have a slot-based memory that may be appropriate for storing intermediate results.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [27, 28, 29, 30, 31]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 18, "text": "However these models obtained worse performance than the more general architectures, and we are not yet aware of models that are more tailored for doing mathematics that do not simply have their mathematics knowledge built-in and unlearnable; we hope the dataset will spur the development of new models along these lines.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [27, 28, 29, 30, 31]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 19, "text": "On the number of thinking steps, in our earlier analysis we trained up to 150k steps (compared with 500k for final performance reported in paper), and observed the following interpolation test performances by number of steps: 39% (0 steps), 46% (1 step), 48% (2), 49% (4), 50% (8), 51% (16).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [34]]}, {"review_id": "H1goRfkKnm", "rebuttal_id": "HkxKS2FxR7", "sentence_index": 20, "text": "We are re-running experiments now to confirm the final performances, which we can include in the final paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [34]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 0, "text": "Thanks for the insightful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 1, "text": "We\u2019ve tried to improve our paper based on your feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 2, "text": "Most significantly, we\u2019ve performed additional ablation studies to confirm that our modeling choices improve performance, and we provide further empirical insight on what the coreference operations do.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 3, "text": "We\u2019ve also updated the model description and the notation in Section 4 to clarify modeling mechanisms and choices.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 4, "text": "Two important additions are a high-level summary of the model, which we give at the beginning of Section 4, and a table (Table 2) that lists what each symbol represents along with its dimensions.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 5, "text": "Below we address your concerns point-by-point.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 6, "text": "The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 7, "text": "This is especially the case in a few places involving coreference:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 8, "text": "1. The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 9, "text": "2. The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 10, "text": "While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 11, "text": "======", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 12, "text": "Based on your comments, we\u2019ve performed additional ablations to measure the impact of the co-reference mechanisms.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 13, "text": "We find that removing any of them leads to a decrease in performance (Rows 2, 3, 4 of Table 5).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 14, "text": "To provide more than just this quantitative insight, we\u2019ll expand here on how KG-MRC handles coreference to better motivate the modeling choices:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 15, "text": "The construction of graph G_t from G_{t-1} uses co-reference disambiguation of nodes to prevent node duplication and to enforce temporal dependencies.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 16, "text": "We perform coreference disambiguation between location nodes of G_t and G_{t-1} via Eq. 1 (call this inter-graph coreference) and between the location nodes in the same graph Gt (call this intra-graph coreference) via Eq. 2.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 17, "text": "The inter-graph coreference yields new, intermediate representations for the nodes in G_t.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 18, "text": "These are further updated via the intra-graph coreference step.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 19, "text": "Inter-graph Co-ref: One way to think about this is that we construct a new graph G_t at every time step.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 20, "text": "Now the graph G_{t-1} might contain some location nodes which are predicted again at time step \u2018t\u2019 (e.g., in Figure 2, leaf node already existed in G_{t-1}).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 21, "text": "Instead of replacing an old node with an entirely new node at \u2018t\u2019, we take a recurrent approach and do a gated update that preserves some information stored in the node in previous time steps while adding new information unique to time step \u2018t\u2019.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 22, "text": "Intra-graph Co-ref: Inter-graph co-ref isn\u2019t enough since the MRC module makes its span predictions independently.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 23, "text": "This means that, at time step t, the model could predict the same span/location for multiple entities and add all these duplicates to the graph.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 24, "text": "Moreover, a single location might have the same surface form but be from different parts of the paragraph (e.g. \u201cleaf\u201d in the 1st and the 5th sentence of the para in figure 2).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 25, "text": "The operations in Eq. 2 resolve this by performing self-attention (i.e., the predicted locations of all entities are compared to each other).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 26, "text": "=====", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 27, "text": "Response continued from above.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 28, "text": "Why does the graph update require coreference pooling again?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 29, "text": "Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 30, "text": "=====", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 31, "text": "We agree that the coreference pooling in the graph update seems repetitive at first glance.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 32, "text": "We have further clarified the explanation given in the text and included another ablation experiment  (row 4 of Table 5) to confirm its usefulness.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 33, "text": "This step does indeed repeat Eq. 2.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 34, "text": "In a nutshell, this is necessary because, after the recurrent and residual graph updates (Eqs 3.1 - 3.3) that propagate information across edges, we may end up with different representations for location nodes corresponding to the same location.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 35, "text": "We don\u2019t want these representations to diverge from each other because of information propagation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 36, "text": "To give you more detail:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 37, "text": "The graph update step ensures information propagation between entities and location representations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 38, "text": "Specifically if the current location of entity \u201ce_t\u201d is predicted as \u201c\\lambda_t\u201d, the graph update steps ensures that both the entity and location representation gets the same update (via eq 3.2 and 3.3).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 39, "text": "This would have been sufficient if every entity had a unique location.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 40, "text": "But, multiple entities can actually exist in the same location.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 41, "text": "Let\u2019s consider this small graph below", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 42, "text": "Water - -> leaf", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 43, "text": "CO_2 --> leaf", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 44, "text": "Here both water and CO_2 exist in the same location, leaf.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 45, "text": "But let\u2019s say that the MRC model picked the \u201cleaf\u201d span from sentence 1 (of the text in Fig 2) for \u201cWater\u201d and from sentence 4 for CO_2.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 46, "text": "In reality, they refer to the same location entity \u201cleaf\u201d.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 47, "text": "Now, due to eq. 3.3, the two embeddings of leaf will get two different residual updates (one would be corresponding to Water and other would be because of CO_2).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 48, "text": "Because of the different updates, the two representations of the same entity might diverge.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 49, "text": "To remedy this, we re-use the coreference matrix \u201cU\u201d we create in eq. (2), which should already have a high attention score corresponding to the two leaf locations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 50, "text": "Thus we perform a similar operation to the intra-graph update.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 51, "text": "====", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 52, "text": "Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 53, "text": "====", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 54, "text": "The \u201cprefixes\u201d that our model reads at each time step comprise all sentences up to and including the current sentence s_t.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 55, "text": "The motivation for this modeling choice was empirical.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 56, "text": "In our preliminary experiments we evaluated alternative strategies, such as (a) only considering the current sentence s_t, and (b) considering the entire paragraph at every time step.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 57, "text": "We found that operating on prefixes performed best.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 58, "text": "This is in line with the findings of Dalvi et al., 2018, where the Pro-Global model (which uses prefixes) performs better than the Pro-Local model (which operates on single sentences).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 0, "text": "We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 1, "text": "1. Title of the paper", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 2, "text": "- We agree that the main highest-level task that we show is VQA, even though our method is more general.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 3, "text": "Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 4, "text": "Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 5, "text": "2. Description of variables", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15, 16, 17, 18]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 6, "text": "- Thanks for the feedback.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [14, 15, 16, 17, 18]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 7, "text": "Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [14, 15, 16, 17, 18]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 8, "text": "We edited the text to address variables more gently and to explain the arrow sign.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14, 15, 16, 17, 18]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 9, "text": "3. Query for the relationship module", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [19, 20, 21]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 10, "text": "- The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20, 21]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 11, "text": "When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20, 21]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 12, "text": "This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20, 21]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 13, "text": "4. CIDEr score of captioning", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22, 23, 24, 25]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 14, "text": "- That may be true to some extent.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 23, 24, 25]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 15, "text": "However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 23, 24, 25]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 16, "text": "5 and 6.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 17, "text": "Comparison with SOTA models for counting and relationship detection", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 18, "text": "- To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 19, "text": "Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 20, "text": "Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 21, "text": "This shows that additional modules help.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 22, "text": "Kim et al. (2018) which is concurrent to our work shows similar performance.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 23, "text": "For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 24, "text": "7. Table 4, accuracies are from Zhang et al. 2018", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [28, 29]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 25, "text": "- Yes, the numbers are from their paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [28, 29]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 26, "text": "One possible explanation for this could be their use of high regularization for a single model instead of ensembling.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [28, 29]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 27, "text": "Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [28, 29]]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 28, "text": "(Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 29, "text": "(Kim et al. 2018) Bilinear Attention Networks", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gUmqkh3Q", "rebuttal_id": "Sklfd2fSpX", "sentence_index": 30, "text": "(Lu et al. 2016) Visual Relationship Detection with Language Priors", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1l3mdg7qS", "rebuttal_id": "HJeT3CRDoH", "sentence_index": 0, "text": "Thank you for your feedback and additional references! To address the comments:", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1l3mdg7qS", "rebuttal_id": "HJeT3CRDoH", "sentence_index": 1, "text": "- As we explained in Thm.1, \\eta_{\\mathcal{H}} is a constant measuring how well the model family \\mathcal{H} can fit the true models from both domains.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "H1l3mdg7qS", "rebuttal_id": "HJeT3CRDoH", "sentence_index": 2, "text": "Estimating this term requires *labelled* target samples, which is usually unavailable in domain adaptation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "H1l3mdg7qS", "rebuttal_id": "HJeT3CRDoH", "sentence_index": 3, "text": "However, when we have access to a handful of labelled target data, we can certainly estimate this term and perform model selection (e.g., choosing neural network models \\mathcal{H}) better, meaning that we can find better values for alphas, and so achieve even better adaptation performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "H1l3mdg7qS", "rebuttal_id": "HJeT3CRDoH", "sentence_index": 4, "text": "- Yes, there are many methods that conduct single-source to single-target adaption in the literature.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "H1l3mdg7qS", "rebuttal_id": "HJeT3CRDoH", "sentence_index": 5, "text": "However, our main focus is *multi-source* to single-target adaptation.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "H1l3mdg7qS", "rebuttal_id": "HJeT3CRDoH", "sentence_index": 6, "text": "This is why our comparisons focus on similar methods, that also use multiple sources at the same time.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "H1l3mdg7qS", "rebuttal_id": "HJeT3CRDoH", "sentence_index": 7, "text": "They are generally more competitive than single-source methods.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "H1l3mdg7qS", "rebuttal_id": "HJeT3CRDoH", "sentence_index": 8, "text": "Following Reviewer #1's suggestions, we added one additional state-of-the-art competitor, Moment Matching for Multi-Source Domain Adaptation (M3SDA) (Peng et al., 2019), to our experiments.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_none", null]}, {"review_id": "H1l3mdg7qS", "rebuttal_id": "HJeT3CRDoH", "sentence_index": 9, "text": "Moreover, we also add the challenging Office-Home dataset as you suggested[D]; results can be found in the new Section 5.3.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12]]}, {"review_id": "H1l3mdg7qS", "rebuttal_id": "HJeT3CRDoH", "sentence_index": 10, "text": "The results with the new competitor, and on both the earlier datasets and the new one, show that our method outperforms the competition, especially on the Office-Home dataset, in which we achieve state-of-the-art performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "H1l3mdg7qS", "rebuttal_id": "HJeT3CRDoH", "sentence_index": 11, "text": "If you have any comments or concerns, feel free to leave a message here and we can discuss further. Thank you!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 0, "text": "We appreciate your constructive feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 1, "text": "Specifically, your comments about our motivation and development of our idea greatly help us to improve the quality of our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 2, "text": "If we correctly understand reviewer 2\u2019s concerns, the concerns can be divided into two folds:", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 3, "text": "1. Our suggestion to mitigate the catastrophic forgetting looks a naive combination of well-known concepts. Thus, it is more system engineering than science.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 4, "text": "2. Each component described in Figure 1 is not explained enough.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 5, "text": "Also, there is no description of the complete task.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 6, "text": "[Response for 1]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 7, "text": "As we explained at the common response, we started our research from clear open questions.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 8, "text": "Our first open question was that why other GR-based algorithms [1, 2] assume unit Gaussian priors even though they integrate classification loss into their VAE formulation.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 9, "text": "Since they do not consider the conflict between the unit Gaussian prior and discriminative loss for the latent variable z, their models generate ambiguous samples that negatively affect the performance of incremental learning, which is discussed in section 4.1 in our paper.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 10, "text": "This leads us to a more theoretical formulation for classification-regularized VAE.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 11, "text": "By introducing class conditional priors induced by the mutual information maximization, DiVA yields class-wise discriminative one mode Gaussians for latent variable z.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 12, "text": "Naturally, DiVA can conduct both class prediction and class conditional sample generation with one integrated model.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 13, "text": "The second open question was that why GR-based algorithms suffer from serious catastrophic forgetting in natural image datasets, even though generated samples are not completely noisy.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 14, "text": "We assumed that this is due to the vulnerability of neural networks [3] triggered by different distributions of pixel values between real and generated images.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 15, "text": "Thus, we defined the two domains: real domain and sample domain.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 16, "text": "To narrowing the distribution gap, we needed a solution that satisfies two conditions (also described in section 5):", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 17, "text": "1. We should translate only the style (a global pattern of a specific domain) as keeping outline patterns of given images.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 18, "text": "2. We should consider an unpaired domain translation between real and generated images because the generated images are sampled randomly.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 19, "text": "Fortunately, we were able to find an existing solution that satisfies the requirements: CycleGAN. Any other domain translators that satisfy the conditions can be used or newly studied.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 20, "text": "With the solution, we could make a breakthrough for GR-based methods.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 21, "text": "To the best of our knowledge, this is the first successful approach for a GR-based algorithm to start to resist the catastrophic forgetting problem with a natural image dataset.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 22, "text": "[Response for 2]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 23, "text": "Figure 1 is a conceptual description of our proposed model, DiVA.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 24, "text": "Each component is explained in section 4, below Equation 2, and justified in section 4.1.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 25, "text": "Also, for an easy understanding of the whole CL process with DiVA, we added another figure in Appendix E.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 26, "text": "[References]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 27, "text": "[1] van de Ven, Gido M., and Andreas S. Tolias. \"Generative replay with feedback connections as a general strategy for continual learning.\" arXiv preprint arXiv:1809.10635 (2018).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 28, "text": "[2] Mundt, Martin, et al. \"Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition.\" arXiv preprint arXiv:1905.12019 (2019).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1l4PVq4KH", "rebuttal_id": "rygR36nHjr", "sentence_index": 29, "text": "[3] Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. \"One pixel attack for fooling deep neural networks.\" IEEE Transactions on Evolutionary Computation (2019).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 0, "text": "Thank you for your comments and suggestions!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 1, "text": "Summary: our response includes (1) Clarification on math equations; (2) Analysis on diversity of additional agents; (3) Quantitative analysis for image translation.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 2, "text": "** Clarification on Mathematics in Section 3.1 **", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 3, "text": "We apologize for the confusions in Section 3.1.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 4, "text": "We have reorganized this section, as shown in our updated paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 5, "text": "For your questions:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 6, "text": "1. About equation 8, indeed there is a typo and should be a \"partial\" sign in front of the \"\\delta\" function in the numerator.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 7, "text": "Thanks for pointing this out.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 8, "text": "2. The details of derivative estimation can be found in Section 3.1 (especially equation 9 and 10 in our updated version.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 9, "text": "** Study on diversity of agents **", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 10, "text": "1. You are right.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 11, "text": "We obtained distinct \"agents\" f_i and g_i through multiple independent runs with different random seeds and different input orders of the training samples.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 12, "text": "As far as we know, there's no common quantitative metric to measure the diversity among models in NMT.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 13, "text": "But we agree with you that intuitively, more diversity among agents leads to greater improvements.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 14, "text": "2. Following your suggestions, we add a study on the diversity of agents (presented in Appendix A of the updated paper).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 15, "text": "We design three group of agents with different levels of diversity: (E1) Agents with the same network structure trained by independent runs, i.e., what we use in Section 3.3; (E2) Agents with different architectures and independent runs; (E3) Homogeneous agents of different iteration, i.e., the checkpoints obtained at different (but close) iterations from the same run.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 16, "text": "We evaluate the above three settings on IWSLT2014 De<->En dataset.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 17, "text": "The diversity of the above three settings would intuitively be (E2)>(E1)>(E3)", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 18, "text": ".", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 19, "text": "We present full results in Figure 4 (Appendix A), where the BLEU scores with Dual-5 model are:", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 20, "text": "--------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 21, "text": "E1", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 22, "text": "E2             E3", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 23, "text": "--------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 24, "text": "En -> De", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 25, "text": "35.44", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 26, "text": "35.56       34.97", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 27, "text": "De -> En", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 28, "text": "29.52", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 29, "text": "29.58       29.28", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 30, "text": "--------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 31, "text": "From the above results, we can see that diversity among agents indeed plays an important role in our method.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 32, "text": "There are, of course, many other ways to introduce more diversity, including using different optimization strategies, or training with different subsets as you suggested.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 33, "text": "All of these can potentially bring further improvements to our framework, yet are not the focus of this work.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 34, "text": "From the current studies, we show that our algorithm is able to achieve substantial improvement with a reasonable level of diversity.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 35, "text": "We leave more comprehensive studies on diversity to future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 36, "text": "Please kindly refer to Appendix A for more detailed results.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 22, 23, 24, 25, 26, 27, 28, 29, 30]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 37, "text": "** Quantitative analysis for image translation **", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 38, "text": "Thanks for your suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [10]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 39, "text": "We add two quantitative measures on image translation tasks: (1) We use the Fr\u00e9chet Inception Distance (FID score) [1], which measures the distance between generated images and real images to evaluate the painting to photos translation.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 40, "text": "(2) We use \"FCN-score\" evaluation on the cityscape dataset following [2].", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 41, "text": "The results are reported in Table 6 and Table 7 respectively.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 42, "text": "Multi-agent dual learning framework can achieve better quantitative results than the baselines.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 43, "text": "*", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 44, "text": "* Term usage of \"multi-agents\" **", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 45, "text": "Although with the same term, the \"multi-agent\" or \"agent\" in this paper has no relationship with multi-agent reinforcement learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 46, "text": "You are right in that the term \"agent\" in our context refers to \"mapping\" or \"network\".", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 47, "text": "To avoid further confusion in the discussion period, currently we decide not to change the term usage throughout the paper during rebuttal; instead, we will change the term after the acceptance/rejection decision.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [11]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 48, "text": "You can check our updated paper with clarification and new experimental results.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 49, "text": "Thanks for your time and valuable feedbacks.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 50, "text": "[1] Heusel, Martin, et al. \"Gans trained by a two time-scale update rule converge to a local nash equilibrium.\" Advances in Neural Information Processing Systems.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 51, "text": "2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1l6suNjhX", "rebuttal_id": "HJgc0z6z07", "sentence_index": 52, "text": "[2] Isola, Phillip, et al. \"Image-to-image translation with conditional adversarial networks.\" In CVPR, 2017", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 0, "text": "Thank you for your constructive comments!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 1, "text": "1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 2, "text": "2. We would like to argue that constrained optimization based formulation itself is not designed to achieve better distortion compared with regularized optimization based formulation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 3, "text": "So there is no surprise that our algorithm\u2019s distortion is not the best.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 4, "text": "On the other hand, as mentioned by the other reviewer, distortion is usually not that essential in adversarial attacks as long as it is maintained in a reasonable range.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 5, "text": "We could actually remove the distortion column, instead, we chose to include it just to show that we did not trade a lot of distortions (to make problem much easier) and thus gains speedup.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 6, "text": "From our experimental results, you can see that our proposed method achieves significant speedup while keeping the distortion around the same level as the best baselines.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 7, "text": "3. Thank you for your suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 8, "text": "We have further added success rate vs queries plot (for black-box case) and loss vs iterations plot (for white-box case) in the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 9, "text": "As you can see, in terms of number of iterations / queries, our method still outperforms the other baselines by a large margin.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 10, "text": "4. Thank you for your suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [16]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 11, "text": "We have further added experiments on ResNet V2 model and averaging over 500 correctly classified pictures to strengthen our result.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 12, "text": "Again, this additional experiments show that our method outperforms the other baselines for both white-box attack and black-box attack.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 13, "text": "5. Regarding poor time complexity in practice, first, as you mentioned, adversarial training currently is quite slow due to the slow adversarial attack steps.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21, 22]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 14, "text": "Better time complexity of adversarial attack could significantly speed up adversarial training algorithms.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21, 22]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 15, "text": "Second, it is worth noting that the running time complexity of adversarial attack also highly depends on the input size.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21, 22]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 16, "text": "For example, if you attack a CIFAR-10 classifier or an MNIST classifier, it could take only seconds per attack even for the slowest algorithm since the input size is only 32 by 32 (or 28 by 28).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21, 22]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 17, "text": "However, if you attack a ImageNet classifier or even higher dimensional data classifier, it could take significantly longer time (minutes).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21, 22]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 18, "text": "That is why reducing the runtime of adversarial attack is very important.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20, 21, 22]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 19, "text": "6. We apologize for this confusion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 20, "text": "Regarding \u201cgradient-based\u201d / \u201coptimization based\u201d methods and coordinate-wise black-box attacks, we have changed our description to avoid confusion.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "H1lALLCRhX", "rebuttal_id": "BJxWfM-c0X", "sentence_index": 21, "text": "Thank you for pointing it out.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 0, "text": "(1) Multimodal setting:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 1, "text": "We apologize for not describing experimental settings clearly.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [11]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 2, "text": "In general, we believe multi-modal data is more general than simply image-text or video-text pair.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 3, "text": "By unifying tabular data also as multi-modal data (with each attribute as one modality), we show that VASE provides us a principled way for imputation and is capable of generalizing to more data families.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 4, "text": "We update additional multimodal dataset experiments in the point (3) below.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 5, "text": "(2) Prediction and Representation learning:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 6, "text": "We consider conducting these experiments during the rebuttal but none of the paper's code has been released by the authors.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [14]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 7, "text": "We agree deep latent variable models explicitly model the data distribution and provide a natural way for representation learning, but in our paper we evaluate the model from the perspective of imputation and generation.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 8, "text": "(3) Additional experiments:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 9, "text": "We updated additional imputation experiments on multimodal datasets (see in Appendix C.5) : CMU-MOSI/ICT-MMMO (Tsai et al. 2019), FashionMNIST/MNIST (Wu et al. 2018).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 10, "text": "Each dataset contains two or three modalities.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [12]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 11, "text": "VSAE outperforms other baselines on multimodal datasets under partially-observed setting.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [12]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 12, "text": "(4) Require mask during training:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 13, "text": "In our experiments, the binary mask is always fully-observed as is the nature of partially-observed data.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 14, "text": "A mask simply indicates which  modalities are observed and which are not.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 15, "text": "We agree that it is very interesting to design a model with partially-observed or even unobserved mask.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 16, "text": "However, it is beyond the scope of this work and we will consider it in future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [15]]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 17, "text": "[1] Wu et al. Multimodal Generative Models for Scalable Weakly-Supervised Learning, NeurIPS 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1ly2z2RFS", "rebuttal_id": "HJlxtt_Osr", "sentence_index": 18, "text": "[2] Tsai et al. Learning Factorized Multimodal Representation, ICLR 2019.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 0, "text": "We thank the reviewer for the valuable feedback!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 1, "text": "The suggestion comments were very helpful and led to a clear improvement of our manuscript.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 2, "text": "We reply to the answers and comments in the order they were raised:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 3, "text": "(1) While indeed we need more samples of weight matrices than e.g. for applying VI for BNNs for due to the input dependency, we do not believe this makes our method unscalable to real world scenarios.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 4, "text": "Note, that input dependent samples are also needed in the variational training of VAEs (where the number of hidden variables is of course much smaller than the number of weight parameters in our setting).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 5, "text": "While we present the training algorithm naively in an online version for clearness in Algorithm 1, in practice mini-batching can be done efficiently, due to the availability of batched linear algebra operations, at least in the framework we use (PyTorch), e.g. torch.bmm, broadcasting semantics, etc.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 6, "text": "For convolution layers, we can simply use a different type of mixing distribution, e.g. a fully-factorized multivariate normal instead of matrix-variate normal.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 7, "text": "(2) Thank you very much for the pointer to VIB!", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 8, "text": "We have added a section in the updated manuscript to compare the objective of CDNs with that used in VIB and VI for Bayesian neural networks (see new Section 4).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 9, "text": "Furthermore, while we  always used 1 sample during training in the original submission (which indeed makes the CDN an instance of VIB) we now added experiments using 10 samples (see Section 6.4) in an experimental analysis of the different objectives.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 10, "text": "The results show that the CDN objective produces superior results compared to VI and VIB.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 11, "text": "(3) Of course! We have moved the test accuracy (which previously was only given in the Appendix and thus hard to find) to the legends of the plots to make it more easily accessible.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 12, "text": "CDNs give better uncertainty estimates while still having similar predictive power compared to the baselines.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 13, "text": "(4) Thank you for the great suggestion.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 14, "text": "We performed the following 2 experiments for the revised version: First, we picked a weight of a CDN trained on a toy regression experiment (with heteroscedastic  noise) at random and visualized its conditional distributions given different values of x. We found that the means and variances vary for different x.  Furthermore, we picked a weight of a CDN trained on a toy classification dataset (created by sampling x ~ 1/2*N(-3, 1) + 1/2*N(3, 1), and assign y=0 if x comes from the first Gaussian and y=1, otherwise) at random and visualized its marginal distributions.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [15]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 15, "text": "We found that CDNs indeed capable of learning multimodal weight distribution and to learn input specific mixing distributions.. We detail this in Appendix G.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [15]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 16, "text": "(5) We found that the regularization term has a significant impact on the quality of the prediction and the uncertainty estimate (we found that the uncertainty estimates are worse with small \\lambda).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17, 18, 19, 20, 21]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 17, "text": "It makes sure that the variance of \\theta is not shrinking too much, i.e. encouraging the mixing distribution to be close to the prior implies it should have similar variance to the prior (which was chosen to be large).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17, 18, 19, 20, 21]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 18, "text": "Naturally, the coefficient \\lambda controls this behavior: as \\lambda increases the validation accuracy is decreasing while the uncertainty is increasing (and vice versa).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17, 18, 19, 20, 21]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 19, "text": "This gives rise to the selection heuristic for \\lambda we applied: pick the highest \\lambda that still gives high accuracy on the validation set (e.g. > 0.97 in MNIST).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17, 18, 19, 20, 21]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 20, "text": "We found that this works very well in the experiments we did (on OOD and adversarial examples).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17, 18, 19, 20, 21]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 21, "text": "Furthermore, indeed CDNs are rather designed to capture the (heteroscedastic) aleatoric uncertainty.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17, 18, 19, 20, 21]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 22, "text": "We have revised the toy experiments to better account for that.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 17, 18, 19, 20, 21]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 23, "text": "However, curiously, CDNs also work well in tasks that are usually shown as prime examples of epistemic uncertainty, e.g. OOD classification and adversarial attack.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17, 18, 19, 20, 21]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 24, "text": "(6) Thank you for this feedback.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [23, 24, 25, 26, 27]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 25, "text": "You are right! We have revised the baseline experiments with Bayesian models so that they either use \\lambda = 1 or the settings that the original authors recommended, i.e. we only tune \\tau in KFLA and set \\tau = 0.01 in noisy-KFAC as these are the settings suggested in their respective publications.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [23, 24, 25, 26, 27]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 26, "text": "Note, that the conclusions keep unchanged.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [23, 24, 25, 26, 27]]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 27, "text": "References:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 28, "text": "[1] Louizos, Christos, and Max Welling. \"Structured and efficient variational deep learning with matrix gaussian posteriors.\" International Conference on Machine Learning. 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1lZP6Jchm", "rebuttal_id": "rJxslWKrCX", "sentence_index": 29, "text": "[2] Kingma, Diederik P., Tim Salimans, and Max Welling. \"Variational dropout and the local reparameterization trick.\" Advances in Neural Information Processing Systems. 2015", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 0, "text": "We thank Reviewer 2 for the constructive feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 1, "text": "Here is our point-to-point response to the comments and questions raised in the review:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 2, "text": "1. \u201cIt is unclear to me whether the \"efficient method for SN in convolutional nets\" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides. There is no direct comparison of performance.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 3, "text": "We do not claim that our method is more efficient than Miyato et al.\u2019s method, which uses the spectral norm of the convolution kernel matrix to approximate the spectral norm of the convolution operation.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 4, "text": "In fact, our proposed method is computationally more expensive than their approximate scheme because each power iteration in our method requires a conv/deconv operation rather than a simple division used by Miyato et al.\u2019s.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 5, "text": "We introduce our new spectral normalization scheme for convolutional layers because there exist examples where the true spectral norm of a convolution operation can be arbitrarily larger than Miyato et al.\u2019s approximation.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 6, "text": "Therefore, Miyato et al.\u2019s normalization scheme is not guaranteed to control the spectral norm of convolutional layers which is critical for controlling a DNN\u2019s generalization performance (please see our generalization bounds in Section 3).", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 7, "text": "To further support our argument, we performed additional experiments demonstrating how our proposed method better controls the spectral norm of convolution layers, resulting in better generalization and test performance.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 8, "text": "The results are presented in Appendix A.1.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 9, "text": "Furthermore, we run several experiments to show that our method is not significantly slower than Miyato et al.\u2019s method, and we report the results in Appendix A.1, Table 3.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 10, "text": "2. \u201cFig. 3 needs more explanation. The horizontal axes are unlabelled, and \"margin normalization\" is confusing\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 11, "text": "We relabel the axes and add a more thorough explanation in the caption.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 12, "text": "We note that the text explaining Figure 3 mentions how the margin normalization is performed (paragraph 3 in section 5.1): the margin normalization factor is exactly the capacity norm \\Phi described in Theorems 1-4.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 13, "text": "We clarify that we divide the obtained margins by the values of \\Phi estimated on the dataset.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 14, "text": "3. \u201cThe epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 15, "text": "Yes, the epsilons are chosen to be different depending on whether we are looking at norm_inf attacks or norm_2 attacks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 16, "text": "This is because the two norms can behave very differently in adversarial attack experiments.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 17, "text": "For example, a norm_inf attack of 0.5 implies that all pixels can be changed by 0.5.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 18, "text": "On the other hand, a norm_2 attack of 0.5 means the overall Euclidean norm of perturbation across all pixels is bounded by 0.5, resulting in a much less powerful attack.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 19, "text": "Based on this comment, we update the plots with the same attack-norm to have the same scale.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 20, "text": "4. \"Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15, 16, 17]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 21, "text": "However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15, 16, 17]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 22, "text": "We redo the visualization in Figure 6 to make the gains provided by SN clearer.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14, 15, 16, 17]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 23, "text": "We see that using SN can improve the test performance by over 12% for some FGM, PGM, and WRM cases.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14, 15, 16, 17]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 24, "text": "5. \"The baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 25, "text": "It is thus unclear whether the advantage can be maintained after applying these standard regularisers.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 26, "text": "We did not originally discuss weight decay, dropout, and batch normalization as none of these methods were motivated by the theory we introduced in section 3.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 27, "text": "However, due to the reviewers\u2019 concern in the updated draft we compare spectrally-normalized networks to networks with the same architecture except with weight decay, dropout, or batch norm in Appendix A.2.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "H1x3aUom2X", "rebuttal_id": "H1eRd3dyCQ", "sentence_index": 28, "text": "In our experiments, the SN-regularized network still performs better in terms of test accuracy.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "H1xdKxiDp7", "rebuttal_id": "HkxDvh89A7", "sentence_index": 0, "text": "Thanks for your valuable review! We've added an experiment section in the new revision, showing how BN helps convergence in the training process.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 0, "text": "Thank you for your review!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 1, "text": "> Pros:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 2, "text": "> 1.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 3, "text": "The method used a latent dynamics model, which avoids reconstruction of the future images during inference.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 4, "text": "> 2. The learned actor-critic model replaced online planning, where actions can be evaluated in a more efficient manner.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 5, "text": "> 3. The model achieved better performances in challenging control tasks compared to previous latent space planning methods, such as PlaNet.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 6, "text": "This is an accurate summary.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [4, 5, 6, 7]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 7, "text": "We would like to highlight two additional points.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 8, "text": "First, the improved performance is attributed to a novel actor-critic algorithm that uses analytic multi-step gradients of predicted state-values (not Q-values).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 9, "text": "Second, in addition to outperforming previous latent space planning methods, the proposed algorithm also outperforms the model-free D4PG algorithm, the previous state-of-the-art on this benchmark suite.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 10, "text": "> 1.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 11, "text": "The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 12, "text": "In contrast to PlaNet, the difference is that this work learns an actor-critic model in place of online planning with the cross entropy method.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 13, "text": "However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 14, "text": "Dreamer is a novel algorithm that belongs to the family of actor critic methods.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 15, "text": "At a high level, previous approaches can be grouped into those using Reinforce gradients with V baselines (A3C, PPO, ACER) and those using deterministic or reparameterization gradients of learned Q functions (DDPG, SAC, MVE, STEVE).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 16, "text": "In comparison, Dreamer uses reparameterization gradients of V functions by backpropagating the value estimates through the latent dynamics.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 17, "text": "Specifically, while Reinforce estimators typically learn V functions, these are only used to reduce the variance of the gradient estimate rather than directly maximizing them with respect to the actor.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 18, "text": "Actor-critic algorithms that use analytic gradients of Q critics differ from Dreamer in two ways.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 19, "text": "First, they learn a Q function rather than just a V function.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 20, "text": "Second, the actor only maximizes the Q value predicted for the current time step rather than maximizing multi-step value estimates.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 21, "text": "While MVE and STEVE learn dynamics models (from proprioceptive inputs), the dynamics are not directly used to update the policy.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 22, "text": "Instead, they only serve for computing multi-step Q targets for learning the Q critic.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 23, "text": "Thus, no gradients are backpropagated through the dynamics model for learning the actor or critic.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 24, "text": "Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 25, "text": "> 2.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 26, "text": "Since the actor-critic model is the novel component in this model (propagating gradients through the learned dynamics), I would like to see additional analysis and baseline comparisons of this method to previous actor-critic policy learning methods, such as DDPG and SAC training on the (fixed) latent state representations, and recent work of MVE or STEVE that use the learned dynamics to accelerate policy learning with multi-step updates.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 27, "text": "As summarized above, Dreamer differs from previous actor-critic algorithms not just by using latent dynamics but also by using analytic multi-step gradients of a V function rather than one-step gradients Q function.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [12]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 28, "text": "This renders Dreamer conceptually distinct from DDPG, SAC, MVE, and STEVE.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [12]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 29, "text": "We have run experiments with MVE in the latent space of the same dynamics model and tuned the learning rate for actor and Q function.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 30, "text": "We did not find an improvement over Dreamer (MVE worked worse across tasks) in these experiments, possibly because it only updates the Q function at the initial state of the imagination rollout.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 31, "text": "Note that with a model, Q values can be computed by combining the dynamics with a value function, so learning Q is not necessary anymore.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 32, "text": "Since using V in Dreamer outperforms the state-of-the-art D4PG agent and is simpler than the Q function in DDPG and MVE and substantially simpler than STEVE (ensemble of models) and SAC (two Q functions, one V function), we argue for this design choice.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 33, "text": "> 3. [...] It'd be interesting to see how optimization of the actions would lead to better state representations by propagating gradients from the actor-critic model to the world model.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 34, "text": "We have run these experiments and it prevented learning completely.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 35, "text": "Using gradients of the action or value models to shape the dynamics allows them to \"cheat\".", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 36, "text": "Specifically, the actions maximize value estimates; using these to update the dynamics results in overly optimistic dynamics.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 37, "text": "The values maximize Bellman consistency; using these to update the dynamics can encourage collapse of the latent space.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 38, "text": "As a result, we suggest the perspective of viewing the dynamics as a fixed MDP during imagination training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 39, "text": "We will add a discussion of this to the paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "H1xSQUW2tS", "rebuttal_id": "HJezKUjisr", "sentence_index": 40, "text": "If we addressed your concerns satisfactorily, we would be happy if you would consider updating your score.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 0, "text": "We very much appreciate your valuable comments, efforts and times on our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 1, "text": "Our responses for all your questions are provided below. Our major revisions in the new draft are colored by red.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 2, "text": "Q1. Updated proof.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 3, "text": "To address your concerns, we provided more detailed explanations of our proof arguments in the revised draft (see Appendix F).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2]]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 4, "text": "We also re-organized our proof completely for better understanding.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2]]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 5, "text": "Q2. Relation to Tandem approach.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 6, "text": "As you pointed out, our method is somewhat related to Tandem approach [1] in that both post-process a generative model on top of hidden features extracted by DNNs.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 7, "text": "However, the main purpose of Tandem is not for handling noisy labels.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 8, "text": "In particular, the Tandem approaches utilize the EM algorithm that should be highly influenced by outliers, while our method is specialized to be robust against them.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 9, "text": "We clarified this in Section 2 of the revised draft.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4]]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 10, "text": "[1]  Hermansky, H., Ellis, D.P. and Sharma, S., Tandem connectionist feature extraction for conventional HMM systems. In IEEE ICASSP, 2000.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 11, "text": "Thanks a lot,", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 12, "text": "Authors", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 13, "text": "Dear AnonReviewer1,", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 14, "text": "We hope that you found our rebuttal/revision for you and other reviewers in common.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 15, "text": "If you have any remaining questions/concerns, please do not hesitate to let us know and we would be happy to answer.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 16, "text": "Thank you very much,", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJe1O6BchX", "rebuttal_id": "B1elymcSCX", "sentence_index": 17, "text": "Authors", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJeleGmwnQ", "rebuttal_id": "Byl64ga_C7", "sentence_index": 0, "text": "Thanks for your review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJeleGmwnQ", "rebuttal_id": "Byl64ga_C7", "sentence_index": 1, "text": "The ION theorem is an important part of explaining sparsity, dead units, and rank as well, but perhaps our writing was not clear enough.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [2, 3, 4, 5]]}, {"review_id": "HJeleGmwnQ", "rebuttal_id": "Byl64ga_C7", "sentence_index": 2, "text": "We will work on the writing in the future version of this work.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [2, 3, 4, 5]]}, {"review_id": "HJeleGmwnQ", "rebuttal_id": "Byl64ga_C7", "sentence_index": 3, "text": "As for the mutual information related comment (#2), the results that you have mentioned are well known from information bottleneck paper or from the following information invariance paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "HJeleGmwnQ", "rebuttal_id": "Byl64ga_C7", "sentence_index": 4, "text": "Alessandro Achille and Stefano Soatto.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "HJeleGmwnQ", "rebuttal_id": "Byl64ga_C7", "sentence_index": 5, "text": "Emergence of invariance and disentangling in deep representations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "HJeleGmwnQ", "rebuttal_id": "Byl64ga_C7", "sentence_index": 6, "text": "Journal of Machine Learning Research. 2018", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 0, "text": "We thank the reviewer for their time and welcome feedback, which we are incorporating into the revised version.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 1, "text": "R: - \"important features for the new task should be in similar locations ...\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 2, "text": "- \"the locations for important features should be comparatively stable ...\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 3, "text": "A:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 4, "text": "-- Continual learning typically assumes a degree of similarity among the tasks.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 5, "text": "If tasks are completely different from each other, then most continual learning frameworks will somehow struggle.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 6, "text": "For example, the standard Split MNIST benchmark is in line with this \u201clocations of important features\u201d assumption.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 7, "text": "Having said that, we acknowledge that more agility to, at least, discover that early on would be beneficial.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 8, "text": "More importantly, a normalization strategy on top of our attention map would help enhance its invariance properties, potentially leading to a more robust treatment of the locations of important features.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 9, "text": "In page 4 in the revised version (footnote 3), we have clarified this and notified its potential for future work.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 10, "text": "-- Thank you for the suggestion regarding the fixed attention map.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 11, "text": "We tried an experiment using the fixed attention map as a baseline, and as expected it performs significantly worse than ours.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 12, "text": "We have added that to the revised version (see p.6 and Appendix A).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 13, "text": "R: - FSM vs. Classification performance", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18, 19, 20, 21]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 14, "text": "A:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 15, "text": "-- It is true that evaluating the FSM is not necessarily the same as the classification results, which is precisely the reason why we show both in our results.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17, 18, 19, 20, 21]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 16, "text": "As specified in page 2, \u201cHere we propose a new measure ...\u201d - our point in this regard is to propose another (different) manner via which catastrophic forgetting can be estimated, which is not the same as the classification accuracy.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19, 20, 21]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 17, "text": "The goal is that (as we know and agree they are two different measures that might agree or disagree in their judgments on catastrophic forgetting) both can be used to inspect the degree of catastrophic forgetting.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19, 20, 21]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 18, "text": "We have further clarified that in Section 6.2 in the experiments by stressing that the obtained FSM results \u201calong with the classification results\u201d denote the significance of the whole framework in addressing catastrophic forgetting.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19, 20, 21]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 19, "text": "It is definitely a good idea to analyse the correlation between changes in classification accuracy and in FSM values, thank you. We will rigorously investigate this in future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [17, 18, 19, 20, 21]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 20, "text": "R: - Statistical significance", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 21, "text": "A:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 22, "text": "-- Thank you.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 23, "text": "We have added the statistical significance results to the revised version.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 24, "text": "Since we were concerned that adding this information to the plots would make them harder to read", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 25, "text": ",  statistical significance of the the average accuracy and FSM results obtained after completing the last two tasks from each dataset, i.e. the corresponding values of the last two tasks of all the plots in Figures 1, 2, 3 and 4, are now displayed in the tables in Appendix A.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 26, "text": "Checking cases where the learner incorrectly classifies the image in the second time step is sound and will be inspected in future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [25, 26]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 27, "text": "R: - Clarity", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [27, 28, 29, 30, 31]]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 28, "text": "A:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJeXDu9h2X", "rebuttal_id": "HkeYvOCVTm", "sentence_index": 29, "text": "-- We have fixed the typos in the revised version, thank you: i) The first sentence of the third paragraph in Section 4 now reads: \u201cFor input images of ..., the averaged weight of evidence matrix  is referred to as $\\text{WE}_{\\bm{i}}(\\bm{x}) \\in \\RR^{\\bm{r} \\times \\bm{c}}$.\u201d  ii) In page 6: \u201cThe size of the surrounding square \u2026 is 16 $\\times$ 16 pixels.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [27, 28, 29, 30, 31]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 0, "text": "We thank the reviewer for the reading and suggestions of our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 1, "text": "Q1: The exact difference between the proposed method and the ES baseline is not as clear as it could be.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 2, "text": "We agree and apologize for the lack of clarity in some parts of our paper.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 3, "text": "We renamed all the models based on the original papers and their properties.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 4, "text": "We refer the reviewer to general response for further details of each baseline algorithms.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [7]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 5, "text": "We also improved clarity in the revised version.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 6, "text": "Q2: The second point is that the proposed approach seems to modify a few things from the ES baseline.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 7, "text": "We thank the reviewer for the insightful suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [10]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 8, "text": "In the latest version, to test the efficacy of each submodule of NGE, the baselines now include the algorithm with the inclusion of the pruning step, and the algorithms with AF and without AF using MLP.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 9, "text": "More specifically, the baselines are named:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 10, "text": "1. ESS-Sims", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 11, "text": "It is the baseline algorithm without the use of AF, as use by (Sims, 1994), (Cheney, 2014) and (Taylor, 2017).", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 12, "text": "2. ESS-Sims-AF", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 13, "text": "The modern variant of ESS-Sims with the inclusion of AF.", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 14, "text": "3. ESS-GM-UC", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 15, "text": "The modern variant of ESS-Sims with the inclusion of AF and graph mutation with uncertainty (pruning).", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 16, "text": "For this baseline, we included the pruning module on top of ESS-Sims-AF.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [10]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 17, "text": "Similar to the original baselines available, we performed a grid search of hyperparameters and plot the average performance of the best set of hyperparameters.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 18, "text": "|        NGE         | ESS-Sims  |  ESS-Sims-AF  | ESS-GM-UC | ESS-BodyShare |  RGS", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 19, "text": "fish        |", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 20, "text": "**70.21**", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 21, "text": "|", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 22, "text": "38.32      |", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 23, "text": "51.24         |", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 24, "text": "54.40", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 25, "text": "|", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 26, "text": "54.97         |", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 27, "text": "20.96", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 28, "text": "Walker  |", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 29, "text": "**4157.9**  |", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 30, "text": "1804.4    |", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 31, "text": "2486.9        |", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 32, "text": "2458.19     |", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 33, "text": "2185.1       |", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 34, "text": "1777.3", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 37, "text": ".", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 40, "text": "On the other hand, the use of AF can greatly affect the performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 41, "text": "The previous approach ESS-Sims can only get 38.32 / 1804 average final reward for fish and walker, respectively.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 42, "text": "The performance of walker is even very close to random graph search with no evolution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "HJg1qgpZTm", "rebuttal_id": "S1x7SdplAX", "sentence_index": 43, "text": "With the help of AF, the performance increases from 38.32 to 51.24 and 1804.4 to 2486.9, respectively.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 0, "text": "We would like to thank the reviewer for the time and useful feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 1, "text": "Our response is given below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 2, "text": "- Interpretation of self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 3, "text": "Overall, self-modulation appears to yield the most consistent improvement for the deeper ResNet architecture, than the shallower, more poorly performing, SNDC architecture.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 4, "text": "Self-modulation doesn\u2019t help in the SNDC/Spectral Norm setting on the Bedroom data, where the SNDC architecture appears to perform very poorly compared to ResNet.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 5, "text": "For the other three datasets, self-modulation helps in this setting though.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 6, "text": "- The ablation study shows that the impact is highest when modulation is applied to the last layer (if only one layer is modulated).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 7, "text": "It seems modulation on layer 4 comes in as a close second.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 8, "text": "I am curious about why that might be.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 9, "text": "Figure 4 in the Appendix contains the equivalent of Figure 2(c) for all datasets.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 10, "text": "Considering all datasets: (1) Adding self-modulation to all layers performs best.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 11, "text": "(2) In terms of median performance, adding it to the layer farthest from the input is the most effective.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 12, "text": "We believe that the apparent significance of layer 4 in Figure 2(c) is statistical noise.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 13, "text": "- I would like to see some more interpretation on why this method works.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 14, "text": "We consider self-modulation as an architectural change in the line of changes such as residual connections or gating: simple, yet widely applicable and robust.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 15, "text": "As a first step, we provide a careful empirical evaluation of its benefits.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 16, "text": "While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [14]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 17, "text": "Similar to residual connections, gating, dropout, and many other recent advances, more fundamental understanding will happen asynchronously and should not gate its adoption and usefulness for the community.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [14]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 18, "text": "- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 19, "text": "A 10% change in FID is visually noticeable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 20, "text": "However, we note that FID rewards both improvements in sample quality (precision) and mode coverage (recall), as discussed in Sec 5 of [1].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 21, "text": "While we can easily assess the former by visual inspection, the latter is extremely challenging.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 22, "text": "Therefore, an improvement in FID may not always be easily visible, but may indicate a better generative model of the data.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 23, "text": "[1] https://arxiv.org/abs/1806.00035", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 24, "text": "- Overall, the idea is simple, the explanation is clear and experimentation is extensive. I would like to see more commentary on why this method might have long-term impact (or not).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 25, "text": "We view this contribution as a simple yet generic architecture modification which leads to performance improvements.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "HJgjezT1Tm", "rebuttal_id": "Bkgu7f_Dp7", "sentence_index": 26, "text": "Similarly to residual connections, we would like to see it used in GAN generator architectures, and more generally in decoder architectures in the long term.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [16]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 0, "text": "Thanks for your valuable comments and feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 1, "text": "R: \"The authors need to be clear about their contribution. Is the paper only about replacing the traditional parametric functions of influence and probability with  deep neural networks? \"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 2, "text": "A: Yes but not only.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 3, "text": "We propose to go beyond the classical markov hypothesis of cascade models that states that any infected node owns the same transmission probabilities whatever from whom comes the propagated content.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 4, "text": "We indeed do this by replacing the traditional parametric functions with  deep neural networks, which enables to consider recurrent latent states for infected nodes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 5, "text": "This allows us to embed the past in node states and hence to output different future diffusion distributions regarding the past trajectory of the propagated content, which is our main contribution (a cascade model with neural network was already proposed for instance in (bourigault et al., 2016) but without past inclusion).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 6, "text": "While existing works on cascade models learn parameters by inferring the direct infector of every infected node (i.e., estimating $P(I_i|D_{\\leq i})$), we need to infer the whole past trajectory to compute node states (i.e., considering $P(I_i|D,I_{<i})$), which is greatly more difficult but the proposed learning approach allowed us to efficiently deal with it.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 7, "text": "R: \"The experimental sections look rather mechanical. I would have put some results on the learned embedding. Or some demonstration of the embedded history or probability to intuitively convey the idea and how it works. This could have made the paper much stronger.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 8, "text": "A: To give more clues about the good behavior of the algorithm, we added results about the accuracy of the sampled trajectories on the artificial datasets (for which we have the ground truth on who infected whom).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 9, "text": "We report the rate of good infector choices (i.e., the rate of I_i that equal the ground truth) for our approach and the others.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 10, "text": "Results show that our approach actually performs better infector choices than CTIC which does not consider the history of the diffusion in its infection probabilities.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 11, "text": "The use of our recurrent architecture helps the process to distinguish some different diffusion contexts from the past.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 12, "text": "We also added a second artificial dataset to further analyze the behavior of the approaches.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 13, "text": "R: \"It was nice that the paper iterated and reviewed the possible inference and learning ways. There is one more way. Similar to [1] one can use MCMC with importance sampling on auxiliary variables to infer the hidden diffusion given the observed cascades in continuous-time independent cascade model.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 14, "text": "A: Thanks for the proposal and the reference that we added in the paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 15, "text": "The full computation of the posterior distributions could indeed be avoided by using an importance sampling MCMC procedure with auxiliary variables", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 16, "text": "(such as done in [1] in the context of diffusion source detection), but in our context we think that the increased computation efficiency would be at the cost of a very higher variance in the learning process, due to the strong intrication of latent and observed variables.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 17, "text": "In [1], the problem is easier: they do not have to perform optimization on the diffusion parameters (since relying on a diffusion model learned a priori), the problem is to sample hidden infection times to estimate likelihoods and then identifying the most probable source of diffusion.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 18, "text": "R: \"The paper can benefit from a proofreading.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11, 11, 11]]}, {"review_id": "HJgQO5qAnQ", "rebuttal_id": "B1gDcrvdCm", "sentence_index": 19, "text": "A: Thanks, we indeed corrected serveral typos like this in the new version of the paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10, 11, 11, 11]]}, {"review_id": "HJl5cVvoYS", "rebuttal_id": "H1g9CATusr", "sentence_index": 0, "text": "We thank the reviewer for the great feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJl5cVvoYS", "rebuttal_id": "H1g9CATusr", "sentence_index": 1, "text": "We have simplified the architecture described in the paper by combining the networks $\\sigma$ and $\\omega$, and included the results from this architecture as well, producing a more robust architecture that performs better for multiple rewrite steps (while keeping the original, more complicated solution as one of the baselines).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "HJl5cVvoYS", "rebuttal_id": "H1g9CATusr", "sentence_index": 2, "text": "As suggested, we have added further analysis of failure cases.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7]]}, {"review_id": "HJl5cVvoYS", "rebuttal_id": "H1g9CATusr", "sentence_index": 3, "text": "We also corrected the typos and clarified the definitions of True, Pred (One Step) and Pred (Multi Step) variants.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "HJl5cVvoYS", "rebuttal_id": "H1g9CATusr", "sentence_index": 4, "text": "We are very grateful for the review that helped to improve the paper significantly.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 0, "text": "Thank you very much for the comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 1, "text": "We believe that this response can help the Reviewer to be more convinced about the validness of our experiments; in particular, the validness of our retraining methodology.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 2, "text": "Q1. The paper is not very self-contained, and I have to constantly go back to [1] and [2] in order to read through the paper.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 3, "text": "In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 4, "text": "Based on the Reviewer\u2019s comments, we added more description about the schemes we adopted from [1] and [2] in Appendix A.1 and A.2 of the revised manuscript.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 5, "text": "Q2. The input and output types of each block in Figure 1. should be clearly stated, and the figures are almost useless because the captions contain very little information.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6, 9]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 6, "text": "We tried to add more information to the figures in the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6, 9]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 7, "text": "First, in Figure 1, we added the more mathematically precise description of input and output of each block to show how the exact weight representation is changed at each process.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6, 9]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 8, "text": "We also added additional explanation for 'D' of Figure 2 in its caption.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6, 9]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 9, "text": "For the Figure 4, we added the description of the underlined numbers.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6, 9]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 10, "text": "Q3.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 11, "text": "Optimizing compression rates should be done on the training set with a separate development set.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 12, "text": "The test set should not used before the best compression scheme is selected.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 13, "text": "Both the results on the development set and on the test set should be reported for the validity of the experiments.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 14, "text": "Thanks for pointing this out.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 15, "text": "We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 16, "text": "We agree that optimizing compression rates should not use the test set before the best compression scheme is selected.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 17, "text": "In fact, in case of PTB and Wikitext-2 corpus, we already used the provided validation set and measured the test PPW only once after training (Table 2) in the original manuscript.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 18, "text": "From the Table 2, we can see that our proposed scheme maintains the accuracy of the uncompressed baseline network.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 19, "text": "On the other hand, the CIFAR-10 dataset does not include a separate validation set, so we had to use the test set in the retraining process.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 20, "text": "To avoid using the test set in the retraining process as the Reviewer pointed out, we randomly selected 5K validation images among the original 50K training images in CIFAR-10 dataset, and applied our scheme.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 21, "text": "Then, we observed the training and validation accuracy at each training epoch, and measured the test accuracy once after training.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 22, "text": "The accuracy results are as shown in the following table.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 23, "text": "Note the compression rates are the same as the data in Table 3 in the original manuscript.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 24, "text": "----------------------------------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 25, "text": "Compression scheme   Validation Error (%)    Test Error (%)", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 26, "text": "------------------------------  ---------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 27, "text": "---------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 28, "text": "Baseline", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 29, "text": "11.5                         12.2", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 30, "text": "Pruning [1]                         11.4                         12.2", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 31, "text": "VWM (Ours)", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 32, "text": "11.4                         12.4", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 33, "text": "----------------------------------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 34, "text": "The test accuracy in the above table is about 1 % less than the accuracy which we reported in the originally submitted manuscript because the number of training data was decreased as part of the data set is used as a validation set.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 35, "text": "However, the results show that our proposed method does not make the network be overfitted to test data as the Reviewer doubted because the difference between the accuracy for validation set and test set are consistent with the values from the previous works.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 36, "text": "Note that even the uncompressed baseline network exhibits similar accuracy difference between the validation error and the test error compared with the compressed networks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 37, "text": "Therefore, we believe that our proposed compression method does not suffer from the concerned overfitting problem regardless of the types of neural networks or dataset.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 38, "text": "Reference", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 39, "text": "[1] Dongsoo Lee, Daehyun Ahn, Taesu Kim, Pierce I. Chuang, and Jae-Joon Kim. Viterbi-based pruning for sparse matrix with fixed and high index compression ratio. International Conference on Learning Representations (ICLR), 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJlAUngO2X", "rebuttal_id": "rygt-_dF0X", "sentence_index": 40, "text": "[2] Chen Xu, Jianqiang Yao, Zouchen Lin, Wenwu Qu, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. International Conference on Learning Representations (ICLR), 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 0, "text": "We thank the reviewer for their review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 1, "text": "We address the different remarks below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 2, "text": "\u201cIt is unclear whether the data augmentation techniques is applied only at training time or also at test time. In other words: at test time, do you present the original images only or transformed images too?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 3, "text": "We apply the data augmentation both at training and test time.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 4, "text": "\u201cIn section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [28]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 5, "text": "We agree that the full distribution of the softmax layer provides more information, but there is no straightforward way to extend the Kolmogorov-Smirnov distance to multi-dimensional distributions, beyond the two- and three-dimensional cases.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [28]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 6, "text": "We focus on confidence as a proxy to the loss, and we assume that the loss is the quantity that should be the most different between training and testing, as the optimization phase explicitly minimizes the loss on the training set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [28]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 7, "text": "Moreover, early experiments showed that using the outputs of intermediate layers provide no improvement for membership inference (on preliminary CIFAR-10 experiments, we obtained respectively 67.7 accuracy with the output layer and 66.5 when using all layers).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [28]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 8, "text": "\u201cSection 5 is somewhat less clear than the previous sections. The authors should more clearly define what the private, public and evaluation sets are, right from the beginning. The purpose of the public set is explained only in section 5.2.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [29, 30, 31]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 9, "text": "We will update this section to make it clearer.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [29, 30, 31]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 10, "text": "\u201cThe experimental results of section 5.2 are somewhat disappointing. Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [32, 33]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 11, "text": "Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [34]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 12, "text": "This seems to be too low to be of practical use.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [35]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 13, "text": "This might be because the Bayes and MAT attacks are too simplistic.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [36]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 14, "text": "Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [37]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 15, "text": "We agree that better performance could be obtained by running the initial model for more epochs, but our goal is to stay close to the standard training of Imagenet models, i.e. 90 epochs with an initial learning rate of 0.1, divided by 10 every 30 epochs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [32, 33, 34, 35, 36, 37]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 16, "text": "We emphasize that the last line of Table 3 corresponds to the most difficult setup, where the network has been trained with a strong data-augmentation, and we only use the intermediate layers of the network (which amounts to less than 62% of the parameters for e.g. Resnet101), this is why the performance is significantly impacted.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [32, 33, 34, 35, 36, 37]]}, {"review_id": "HJlO-N9psQ", "rebuttal_id": "H1gBJFp-AQ", "sentence_index": 17, "text": "We experimented with more sophisticated models, and it did not bring any improvement (see shadow models in appendix E, e.g. the performance before the softmax layer is 58.2 for Resnet101 and 60.8 for our method).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [32, 33, 34, 35, 36, 37]]}, {"review_id": "HJlWUOjV37", "rebuttal_id": "H1x8HiTh1E", "sentence_index": 0, "text": "We are aware of the related work you mention.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [2, 2, 2, 2, 2, 7]]}, {"review_id": "HJlWUOjV37", "rebuttal_id": "H1x8HiTh1E", "sentence_index": 1, "text": "Please note that unfortunately the \u201cSemantically Conditioned LSTM\u2026\u201d is not directly comparable because, as they state in their paper, \u201cthe generator is further conditioned on a control vector d, a 1-hot representation of the dialogue act (DA) type and its slot-value pairs\u201d.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "HJlWUOjV37", "rebuttal_id": "H1x8HiTh1E", "sentence_index": 2, "text": "Our goal is to work with arbitrarily complex questions that map to correspondingly arbitrarily complex logical forms and not a very restricted set of logical forms that could be represented in a one-hot fashion.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "HJlWUOjV37", "rebuttal_id": "H1x8HiTh1E", "sentence_index": 3, "text": "Please do note that we ran 2 sets of human evaluations (Adequacy and Fluency), as is standard in Machine translation in order to deal with the evaluation bias problem you describe - we took this into account when conducting experiments and will make it more clear in a revised version.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "HJlWUOjV37", "rebuttal_id": "H1x8HiTh1E", "sentence_index": 4, "text": "We also observe significant improvements in both human evaluations, suggesting that the improvement comes from our method and not from evaluation bias.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "HJlWUOjV37", "rebuttal_id": "H1x8HiTh1E", "sentence_index": 5, "text": "Our dataset only contains a single logical form for each question and vice-versa, making it impossible to evaluate quantitative metrics (bleu, rouge, meteor) in the multi-reference setting you describe.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "HJlWUOjV37", "rebuttal_id": "H1x8HiTh1E", "sentence_index": 6, "text": "Please also note that metrics like bleu and rouge have been commonly used in a non multi-reference setting by significant work in the natural language processing community.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "HJlWUOjV37", "rebuttal_id": "H1x8HiTh1E", "sentence_index": 7, "text": "We thank the reviewer for their comments and will take them into account in a revised version.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_global", null]}, {"review_id": "HJly3bK2h7", "rebuttal_id": "BJgraHZmRm", "sentence_index": 0, "text": "The reviewer feels that the proposed model is too simple, and suggests comparing against more complex models, suggesting a few in particular.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "HJly3bK2h7", "rebuttal_id": "BJgraHZmRm", "sentence_index": 1, "text": "Our response is twofold:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJly3bK2h7", "rebuttal_id": "BJgraHZmRm", "sentence_index": 2, "text": "- The accuracy of the proposed simple model exceeds the accuracy of far more complex models by a wide margin, and this consistently over a range of networks (all commonly used networks in this literature), against a range of baselines (all either commonly used baselines, or methods known as achieving state-of-the-art accuracies), and on two important tasks (link prediction and multi-label classification).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "HJly3bK2h7", "rebuttal_id": "BJgraHZmRm", "sentence_index": 3, "text": "We do not agree that its simplicity reduces its merit, we think it rather contributes to its merit.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "HJly3bK2h7", "rebuttal_id": "BJgraHZmRm", "sentence_index": 4, "text": "- We thank the reviewer for suggesting additional comparisons with specific more complex models, although we feel that calling these methods 'stronger' requires some clarification or support.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "HJly3bK2h7", "rebuttal_id": "BJgraHZmRm", "sentence_index": 5, "text": "Of the suggested methods, graph convolutional and message passing neural networks need attributed graphs as inputs, and are thus not applicable.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "HJly3bK2h7", "rebuttal_id": "BJgraHZmRm", "sentence_index": 6, "text": "We have in the meantime been able to include struc2vec in the evaluation, again showing superiority of CNE by a wide margin -- showing that it is maybe more complex but certainly not 'stronger' as in 'more accurate'.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "HJly3bK2h7", "rebuttal_id": "BJgraHZmRm", "sentence_index": 7, "text": "The paper will be updated very soon to include these results.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "HJly3bK2h7", "rebuttal_id": "BJgraHZmRm", "sentence_index": 8, "text": "Perhaps the reviewer is incredulous regarding this large increase in performance a method as 'simple' as CNE achieves w.r.t. the state-of-the-art.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "HJly3bK2h7", "rebuttal_id": "BJgraHZmRm", "sentence_index": 9, "text": "We believe that this is due to the conceptual advance made in CNE.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "HJly3bK2h7", "rebuttal_id": "BJgraHZmRm", "sentence_index": 10, "text": "In our opinion a conceptual advance that achieves a strong boost in accuracy without increasing complexity, is at least as valuable as a method that achieves the same boost in accuracy while also increasing complexity.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "HJly3bK2h7", "rebuttal_id": "BJgraHZmRm", "sentence_index": 11, "text": "Also note that all code is provided, and we invite the reviewer to replicate our experiments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 0, "text": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 1, "text": "Here we respond to your specific comments.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 2, "text": "\"What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. \"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 3, "text": ">>> In few-shot learning, episodic paradigm proposed by Matching Networks [1] is widely adopted by current researchers (we follow the same setting to make a fair comparison).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 4, "text": "In each episode, a small subset of N-way K-shot Q-query examples is sampled from the training set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 5, "text": "Typically, for 1-shot experiments, N=5, K=1, Q=15 and for 5-shot experiments, N=5, K=5, Q=15", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 6, "text": ".", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 7, "text": "Thus, the number of training examples are Nx(K+Q) (80 for 1-shot and 100 for 5-shot)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 8, "text": ".", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 9, "text": "Constructing label propagation matrix W involves both support and query examples (80 or 100).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 10, "text": "So the dimension of W is either 80x80 or 100x100.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 11, "text": "Running label propagation on such small matrix is quite efficient.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 12, "text": "\"It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 13, "text": ">>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [6]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 14, "text": "After we get the per-example feature representation f_{\\varphi}(x_i) for x_i, we feed it into the graph construction module g_{\\phi}. The output of this module is a one-dimensional scalar.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 15, "text": "f and g are learned in an end-to-end way in our approach.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 16, "text": "\"solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization \"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 17, "text": ">>> We want to answer this question from two aspects.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 18, "text": "On one hand, few-shot learning assumes that training examples in each class are quite small (only 1 or 5).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 19, "text": "In this situation, Eq (3) and the closed-form version can be efficiently solved, since the dimension of S is only 80x80 or 100x100.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 20, "text": "On the other hand, there is plenty of prior work on the scalability and efficiency of label propagation, such as [2], [3], [4], which can extend our work to large-scale data.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 21, "text": "On miniImagenet, we performed iterative optimization and got 53.05/68.75 for 1-shot/5-shot experiments with only 10 steps.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 22, "text": "This is slightly worse than closed-form version (53.75/69.43), because of the inaccurate computation and unstable gradients caused by multiple step iterations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 23, "text": "[1] Vinyals, Oriol, et al. \"Matching networks for one shot learning.\" NIPS. 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 24, "text": "[2] Liang, De-Ming, and Yu-Feng Li. \"Lightweight Label Propagation for Large-Scale Network Data.\" IJCAI. 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 25, "text": "[3] Fujiwara, Yasuhiro, and Go Irie. \"Efficient label propagation.\" ICML. 2014.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJx6UQbfhX", "rebuttal_id": "r1xOHPbuAm", "sentence_index": 26, "text": "[4] Weston, Jason. \"Large-Scale Semi-Supervised Learning.\"", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 0, "text": "With all our due respect to Reviewer #3\u2019s valuable time and effort in reviewing our manuscript, we must admit that we are a bit upset by this last late review, due to the apparent lack of understanding before placing comments, and several factual errors that make the current comments at least poorly grounded.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_global", null]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 1, "text": "We understand that the idea of \u201cmodel falsification as model comparison\u201d might not be trivial to understand for people primarily from practical deep learning backgrounds.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_global", null]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 2, "text": "The idea is deeply rooted in a successful series of studies from image perceptual assessment research: a basic introduction can be found in (Wang & Simoncelli (2008)).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_global", null]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 3, "text": "We notice that Reviewer #2 also kindly points out another interdisciplinary foundation of MAD in software differential testing.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_global", null]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 4, "text": "We hope Reviewer #3 can carefully read the below explanation, and reconsider the rating to a more serious and appropriate one.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 5, "text": "Q1:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 6, "text": "One of the main advantages is that it can select a sample set from an arbitrarily large unlabeled images.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 7, "text": "However, to compare different classifiers, the proposed algorithm still needs humans to annotate the selected dataset, which is very expensive compared with traditional methods.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 8, "text": "Response:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 9, "text": "Our method is very efficient in terms of human annotation budget compared with traditional methods, which is one of the main claims we elaborated in our paper.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 10, "text": "We are disappointed that this major important point was not well understood.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 11, "text": "In fact, MAD provides the very first and efficient solution (in the context of image classification) to exploit a large-scale image set under the constraint of the very limited budget for human labeling.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 12, "text": "We have noticed that the other two reviewers agree with us and appreciate this point.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 13, "text": "For example, quote Reviewer #2: \u201cBecause of the efficacy of such \"worst-case\" comparison, the needed set size is very small and thus minimizes the human annotation workload\u201d.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 14, "text": "To evaluate the relative performance of two ImageNet classifiers, traditional evaluation methods compute accuracy on a fixed test set.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 15, "text": "For ImageNet validation set, human annotations for 50,000 images need to be provided.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 16, "text": "This number is large in terms of human labeling effort, but is extremely small compared to the set of all natural images (the natural image manifold).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 17, "text": "As also mentioned by the reviewer, annotation for each image is a 1000-class classification task, which makes the labeling task more difficult compared to a binary classification problem.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 18, "text": "In contrast, rather than comparing fixed test sets which are typically small, the proposed MAD adaptively samples a test set from an arbitrarily large corpus of unlabeled images so as to maximize the discrepancies between the classifiers, measured by the distance over WordNet hierarchy.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 19, "text": "Human labeling is only required on the resulting small and model-dependent image sets, which contains only k=30 images (for each pair of classifiers) on the ImageNet experiment as reported in our paper.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 20, "text": "Our experiments show that the MAD ranking stabilizes at around k>15 (see figure 5) and successfully tracks the recent progress in image classification .", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 21, "text": "For comparing 11 classifiers, the total labeled images needed are 1,650 (see page 6): it is obviously smaller than 50,000 and leaves much room to compare more classifiers (before it reaches 50, 000).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 22, "text": "In conclusion, our method is apparently much more efficient in terms of human annotation budget compared with traditional methods.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 23, "text": "In addition, despite the fact that the selected set by MAD is small (as a way of maximizing the efficiency of human labeling), it provides the strongest examples to let classifiers compete with one another.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 24, "text": "Quote Reviewer #2: \u201cThe proposed MAD competition distinguishes classifiers by finding their respective counterexamples. It is therefore an \"error spotting\" mechanism\u201d.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 25, "text": "Their respective strengths, weaknesses as well as biases can be most easily revealed (see figures in the appendix), which sheds light on potential ways to improve the classifiers or combine them into a better one.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 26, "text": "Those gains are way beyond the scope of collecting random image samples.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 27, "text": "Q2: Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 28, "text": "Response:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 29, "text": "We agree with the reviewer that k is a critical parameter in MAD.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 30, "text": "We want to however draw the reviewer\u2019s attention to the ablation study and figure 5, if they were accidentally missed in the first reading.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 31, "text": "Based on them, we cannot concur with the statement \u201cif k is relatively small the method seems very sensitive to selected examples\u201d.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 32, "text": "When we apply MAD to compare imageNet classifiers, we find that the MAD ranking stabilizes very quickly when around k>15.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 33, "text": "We would like to also emphasize that despite the small size of labeled images, MAD successfully tracks the steady progress in image classification, as verified by a reasonable Spearman rank-order correlation coefficient (SRCC) of 0.89 between the accuracy rank on ImageNet validation set and the MAD rank on our test set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 34, "text": "As also pointed out by Review #2, the selected top-k images provide the strongest examples to let classifiers compete with one another.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 35, "text": "Through this process, their respective strengths, weaknesses as well as biases can be most easily revealed (see figures in the appendix).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 36, "text": "Q3: The authors invite five volunteer graduate students to annotate the selected example.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 37, "text": "However, for many categories, it\u2019s nor easy for normal people to distinguish.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 38, "text": "So the experiments in this paper is also not convincing.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 39, "text": "Response:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 40, "text": "As veterans in performing subjective studies, we understand and agree with the reviewer that querying ground truth labels for a 200-class classification problem is difficult. That is exactly why we have carefully designed our subjective experiment.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 41, "text": "Given an image x, which is associated with two classifiers f_i and f_j , we pick two binary questions for human annotators: \u201cDoes x contain an f_i(x)?\u201d and \u201cDoes x contain an f_j (x)?\u201d.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 42, "text": "For each question, we follow  the original ImageNet instructions and include the definition of f_i(x) (or f_j(x))  with a link to a corresponding Wikipedia page.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 43, "text": "We also show several example images of f_i(x) (or f_j(x)) sampled from the ImageNet validation set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 44, "text": "Moreover, if more than three of our five human annotators find difficulty in labeling x, it is discarded and replaced.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 45, "text": "When both answers to the two binary questions are false (corresponding to Case III), we cease to source the ground-truth label of x for reasons mentioned by the reviewer, and treat x as a strong counterexample for both f_i and f_j.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "HJxHV7JPjB", "rebuttal_id": "BJgM_eowiS", "sentence_index": 46, "text": "Based on the above, we cannot concur with the judgement \u201cthe experiments in this paper is (are) also not convincing\u201d.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 0, "text": "We would like to thank the reviewer for their feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 1, "text": "We address each comment below individually with appropriate headings.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 2, "text": "- Summary", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 3, "text": "We would like to point out that the reviewer in the summary incorrectly described that our approach uses the \"triplet loss as a convex relaxation of the ordinal embedding problem\".", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_error", null]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 4, "text": "Using the triplet loss as a proxy does not make the problem convex.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_error", null]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 5, "text": "- The relation between data distribution and hardness of ordinal embedding", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 6, "text": "Ordinal embedding is NP-hard independent of the data distribution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 7, "text": "The paper \u201cLandscape of non-convex quadratic feasibility\u201d (Bower et al. 2018) can shed more light on this.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 8, "text": "The equation (1) in this paper rephrases the ordinal embedding problem as a homogeneous quadratic feasibility problem.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 9, "text": "The constraint matrices of the problem (P_i in the paper), which correspond to the triplet inequalities, are all indefinite which makes the whole optimization NP-hard.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 10, "text": "Moreover, many of our experiments in this paper feature the uniform distribution, which does not satisfy any nice structural assumptions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 11, "text": "- Using a convex solver", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 12, "text": "As we pointed out earlier, using the triplet loss does not make the optimization problem convex and hence using a convex solver would not be possible here.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 13, "text": "- \u201cEquations (3) and (4):  isn't this the same as using the hinge loss to bound the zero-one loss?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "HJxl4O3AYB", "rebuttal_id": "SJgNYnkcsS", "sentence_index": 14, "text": "Yes, that is true.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "HJxwToa927", "rebuttal_id": "SJeVPpHQA7", "sentence_index": 0, "text": "We thank the reviewer for a detailed review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJxwToa927", "rebuttal_id": "SJeVPpHQA7", "sentence_index": 1, "text": "We agree with the reviewer that a uniqueness result based on the axioms is desirable, but we don\u2019t have it.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "HJxwToa927", "rebuttal_id": "SJeVPpHQA7", "sentence_index": 2, "text": "While we\u2019re able to show that the paths at the input and at the hidden layer must be coupled (i.e. non-oblivious), we just don\u2019t understand the space of non-oblivious methods that well.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "HJxwToa927", "rebuttal_id": "SJeVPpHQA7", "sentence_index": 3, "text": "Mathematically, we don\u2019t have a handle on how the path at the hidden layer can vary as the network below the hidden layer is changed.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "HJxwToa927", "rebuttal_id": "SJeVPpHQA7", "sentence_index": 4, "text": "Partition consistency is the only axiom about the network below the hidden layer, but it is not applicable to all networks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "HJxwToa927", "rebuttal_id": "SJeVPpHQA7", "sentence_index": 5, "text": "We probably need another axiom to prove uniqueness.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "HJxwToa927", "rebuttal_id": "SJeVPpHQA7", "sentence_index": 6, "text": "Another key observation made by the reviewer is the interpretability vs importance of neurons.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "HJxwToa927", "rebuttal_id": "SJeVPpHQA7", "sentence_index": 7, "text": "While those are not the same, we demonstrate that conductance can give us some insights about the network (Sections 5.1 and 6.1).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "HJxwv99GqS", "rebuttal_id": "SyxcJXRjjr", "sentence_index": 0, "text": "We thank the reviewer for the comments on our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HJxwv99GqS", "rebuttal_id": "SyxcJXRjjr", "sentence_index": 1, "text": "Designing more efficient streaming algorithms with machine learning techniques is a relatively new research topic and we have included more related work in our updated version of the manuscript (highlighted in the blue color).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10]]}, {"review_id": "Hkg0R50bpQ", "rebuttal_id": "SkxQaAFh6m", "sentence_index": 0, "text": "Thank you for your careful consideration and feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Hkg0R50bpQ", "rebuttal_id": "SkxQaAFh6m", "sentence_index": 1, "text": "Following your request, we updated the paper to include mean learning curves for different models in Figure 6 in Appendix C. Our models converge faster than DNC.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13]]}, {"review_id": "Hkg0R50bpQ", "rebuttal_id": "SkxQaAFh6m", "sentence_index": 2, "text": "Some of them (especially DNC-MD) also have significantly lower variance than DNC.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 0, "text": "Thank you for the comments!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 1, "text": "To reviewer\u2019s concerns:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 2, "text": "- First of all, the state of interest does not have to be the object state.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 3, "text": "It can be the state of the robot, for example, the state of actuators.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 4, "text": "Maximizing the mutual information between two sets of actuator states can help the agent to learn to control itself.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 5, "text": "We did a new experiment in navigation environments, where train the agent to maximize the mutual information between its left wheel states and its right wheel states.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 6, "text": "The agent learns to run in a straight line instead of in random directions.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 7, "text": "The video showing experiment results is available at https://youtu.be/l5KaYJWWu70?t=134", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 8, "text": "- Although we evaluated our method in robotic manipulation tasks, it does not mean it won\u2019t work for other tasks.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 9, "text": "We added additional experiments in a new navigation task, see the video at https://youtu.be/l5KaYJWWu70?t=104", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 10, "text": "We consider our algorithm as a general-purpose skill learning algorithm in the sense that it guides the agent to learn any skills to control the states of interests.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 11, "text": "The states of interest could be any states, such as the robot states, the object states, or the states of the environment.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 12, "text": "- The state of interest is specified by the user with little domain knowledge.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 13, "text": "However, when there is no clear divide from the user, the agent can learn from different combinations of the states of interest and the context states.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 14, "text": "In the end, the user can choose skills from the learned skill sets that are useful for the task at hand.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 15, "text": "- The combination of our method and DIAYN enables DIAYN to learn manipulation skills efficiently, while DIAYN alone did not learn.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 16, "text": "Furthermore, compared to MISC, the combined method enjoys the benefits brought by DIAYN, such as learning combinable motion primitive with skill-conditioned policy for hierarchical reinforcement learning [1].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 17, "text": "Reference:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Hkgcm01WqB", "rebuttal_id": "HJeVtBG5sB", "sentence_index": 18, "text": "[1] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SJx63jRqFm.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HkgKMIsTFr", "rebuttal_id": "ryePhK-giS", "sentence_index": 0, "text": "1. I feel the approach to implicitly assume that the classifiers to be compared are already \"reasonably accurate\"; since if not, both classifiers might be easily falsified by certain trivial examples, making the \"disagreed examples\" not as meaningful. If that is true, I would suggest the authors to make this hidden assumption clearer in the paper", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "HkgKMIsTFr", "rebuttal_id": "ryePhK-giS", "sentence_index": 1, "text": "Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "HkgKMIsTFr", "rebuttal_id": "ryePhK-giS", "sentence_index": 2, "text": "2. The idea shows clear liaison to the \"differential testing\" concept in software engineering besides the cited work of perceptual quality assessment.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HkgKMIsTFr", "rebuttal_id": "ryePhK-giS", "sentence_index": 3, "text": "The idea has a cross-disciplinary nature and is fairly interesting to me.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HkgKMIsTFr", "rebuttal_id": "ryePhK-giS", "sentence_index": 4, "text": "I can see the paper to be of interest to a quite broad audience and can motivate many subsequent works.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HkgKMIsTFr", "rebuttal_id": "ryePhK-giS", "sentence_index": 5, "text": "Response: Thanks for recognizing the strengths of the paper. We will add the appropriate references regarding the \"differential testing\" concept in software engineering.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HkgKMIsTFr", "rebuttal_id": "ryePhK-giS", "sentence_index": 6, "text": "3. One minor comment: for images in \"Case III\", the authors considered them \"contribute little to performance comparison between the two classifiers\" and therefore did not source labels for them.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HkgKMIsTFr", "rebuttal_id": "ryePhK-giS", "sentence_index": 7, "text": "However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HkgKMIsTFr", "rebuttal_id": "ryePhK-giS", "sentence_index": 8, "text": "Response: Thanks for pointing it out.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HkgKMIsTFr", "rebuttal_id": "ryePhK-giS", "sentence_index": 9, "text": "We agree with the reviewer that images falling into Case III can be used to distinguish the associated two classifiers using the proposed semantic tree distance.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HkgKMIsTFr", "rebuttal_id": "ryePhK-giS", "sentence_index": 10, "text": "We will revise the writing to make it more rigorous.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HkgKMIsTFr", "rebuttal_id": "ryePhK-giS", "sentence_index": 11, "text": "In our current subjective assessment environment, we choose to stop labeling images in Case III because it is difficult for humans to select one among 200 classes, especially when they are unfamiliar with the class ontology.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 0, "text": "Thank you for your careful review and useful comments!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 1, "text": "Overall, in response to your review and that of referee 3 we will include a more intuitive discussion of our results in the next revision of our text.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 2, "text": "To reply to your other specific comments,", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 3, "text": "1) The intuition for batchnorm can be put in a more general setting.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 4, "text": "If a function f: X -> Y tends to spread out small clusters in the input space almost evenly in the output space, then one can expect that its gradients will be large typically.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 5, "text": "In our case, a batchnorm network can be understood as a function that sends a batch of inputs to a batch of outputs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 6, "text": "In the appendix, we showed that the correlation between two different batches tend to a constant value independent of the input batches.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 7, "text": "No matter how close two input batches are, the output batches will have the same \u201cdistance\u201d from each other -- small movements in the input space leads to large movements in the output space.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 8, "text": "Thus we can expect the gradients to be large as well.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 9, "text": "We have added a new figure to the Appendix to further support this intuition.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 10, "text": "In it, we pass through a linear batchnorm network 2 minibatches.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 11, "text": "Both minibatches contain points on the same circle and 1 point off the circle that is unique to each minibatch.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 12, "text": "While the circle in each minibatch will remain an ellipse as they are propagated through the network, the angle between the planes spanned by them increasingly becomes chaotic with depth.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 13, "text": "3) As observed in [1] and [2], depthwise convergence to covariance fixed points is bad for training, and the best networks are either moderately deep or initialized such that the depthwise convergence rate to the fixed point is as slow as possible.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 14, "text": "We observe that deep networks whose activation statistics resemble a non-BSB1 fixed point typically feature worse gradient explosion than BSB1 networks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 15, "text": "This seems to be because the nonlinearities that induce these fixed points increase rapidly (for example, polynomials with high degrees), so that the corresponding derivatives are also large, causing gradient explosion.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 16, "text": "(The reason that rapidly increasing nonlinearities don\u2019t converge to BSB1 fixed points is that, after a spontaneous symmetry-breaking, begins a \u201cwinner-take-all\u201d covariance dynamics, in which the activations of a few examples in the batch suddenly dominates those of the others in the batch, and this dominance persists across each layer.)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 17, "text": "4) We were a bit confused by what was meant by \u201cpractice\u201d here.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 18, "text": "We have thoroughly verified that for realistic input distributions (MNIST and CIFAR10) and common initialization strategies (weights that are randomly distributed) our theory makes accurate prediction.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 19, "text": "Moreover, we have shown that these predictions can be connected to practice in the sense that they predict whether or not the network can be trained.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 20, "text": "Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 21, "text": "If this did not properly address your question, please feel free to let us to know and we will improve this response!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 22, "text": "[1] S. S. Schoenholz, J. Gilmer, S. Ganguli, J. Sohl-Dickstein.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 23, "text": "Deep Information Propagation (https://arxiv.org/abs/1611.01232)", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HkgmPcrZpX", "rebuttal_id": "BJlt5Odvam", "sentence_index": 24, "text": "[2] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, J. Pennington. Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks (https://arxiv.org/abs/1806.05393)", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HkljIvR3tr", "rebuttal_id": "r1eb20l9sB", "sentence_index": 0, "text": "We thank the reviewer for the comments and suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HkljIvR3tr", "rebuttal_id": "r1eb20l9sB", "sentence_index": 1, "text": "Our paper indeed focuses on theoretical results, but we believe the theory has some practical implications as well.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [1, 7, 11, 12, 16, 17]]}, {"review_id": "HkljIvR3tr", "rebuttal_id": "r1eb20l9sB", "sentence_index": 2, "text": "In particular, while the exact form of the sufficiently bilinear condition may be somewhat unwieldy, the result gives concrete evidence that having higher bilinearity can aid convergence for certain algorithms, even for settings that are not purely bilinear.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [1, 7, 11, 12, 16, 17]]}, {"review_id": "HkljIvR3tr", "rebuttal_id": "r1eb20l9sB", "sentence_index": 3, "text": "This indicates that one should pay attention to the magnitude and condition number of the off-diagonal of the Jacobian when constructing a min-max problem and choosing an algorithm to solve the problem.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [1, 7, 11, 12, 16, 17]]}, {"review_id": "HklmZ1xqhm", "rebuttal_id": "BJeJx-H7RQ", "sentence_index": 0, "text": "We would like to thank Reviewer 1 for their review and constructive suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HklmZ1xqhm", "rebuttal_id": "BJeJx-H7RQ", "sentence_index": 1, "text": "Our responses inline:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklmZ1xqhm", "rebuttal_id": "BJeJx-H7RQ", "sentence_index": 2, "text": ">Can you elaborate more on why BatchNorm statistics are computed across all devices as opposed to per-device? Was this crucial for best performance?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22]]}, {"review_id": "HklmZ1xqhm", "rebuttal_id": "BJeJx-H7RQ", "sentence_index": 3, "text": "-The primary reason is to ensure that training is invariant to the per-device batch size.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22]]}, {"review_id": "HklmZ1xqhm", "rebuttal_id": "BJeJx-H7RQ", "sentence_index": 4, "text": "When scaling from resolution 128x128 to 256x256, we increase the number of devices but maintain the same overall batch size, reducing the per-device batch size.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22]]}, {"review_id": "HklmZ1xqhm", "rebuttal_id": "BJeJx-H7RQ", "sentence_index": 5, "text": "Cross-replica BatchNorm ensures that the smaller per-device batch size does not affect training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22]]}, {"review_id": "HklmZ1xqhm", "rebuttal_id": "BJeJx-H7RQ", "sentence_index": 6, "text": "Switching to per-device BatchNorm at 128x128 results in a performance drop, albeit not a crippling one: for a model which would otherwise get an IS of 92 and an FID of 9.5, switching to per-device BatchNorm results in an IS of 78 and FID of 13.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22]]}, {"review_id": "HklmZ1xqhm", "rebuttal_id": "BJeJx-H7RQ", "sentence_index": 7, "text": ">It is not clear if provided analysis for large-scale GANs apply for small-medium sized GANs.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "HklmZ1xqhm", "rebuttal_id": "BJeJx-H7RQ", "sentence_index": 8, "text": "Providing such analysis would be also helpful for the community.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "HklmZ1xqhm", "rebuttal_id": "BJeJx-H7RQ", "sentence_index": 9, "text": "-The goal of this work is to explore GANs at large scale; the exploration of small or medium scale models would indeed be interesting for another study.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "HklmZ1xqhm", "rebuttal_id": "BJeJx-H7RQ", "sentence_index": 10, "text": "Having said that, we do evaluate BigGAN on conditional CIFAR-10 (mentioned briefly in Appendix C.2) and obtain an IS of 9.22 and an FID of 14.73 without truncation, which to our knowledge are better than any published results.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "HklmZ1xqhm", "rebuttal_id": "BJeJx-H7RQ", "sentence_index": 11, "text": ">How do you see the impact of the suggested techniques on tackling harder data-modalities for GANs, e.g. text or sequential data in general?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [25]]}, {"review_id": "HklmZ1xqhm", "rebuttal_id": "BJeJx-H7RQ", "sentence_index": 12, "text": "-Any of the proposed techniques could be applied to standard GANs for text or other sequential data in principle, but we have not experimented with these applications ourselves.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [25]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 0, "text": "Thank you for your review and valuable comments!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 1, "text": "Summary: our response includes: (1) Clarification on language translation baselines; (2) Discussion on image translation evaluation; (3) Reference and clarification.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 2, "text": "** Language Translation Baselines **", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 3, "text": "1. For the baseline models reported:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 4, "text": "1.1) We use the transformer model with \"transformer_big\" setting [1], which is a strong baseline that outperforms almost all previously popular NMT models based on CNN [2] and LSTM [3].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 5, "text": "Transformer is the state-of-the-art NMT architecture.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 6, "text": "Our numbers of the baseline transformer model match the results reported in [1].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 7, "text": "1.2) In addition to the standard baseline models, we also compare our method against all the relevant algorithms including knowledge distillation (KD) and back translation (BT).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 8, "text": "1.3) As can be seen in many well-known and recent NMT works ([4], [5])", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 9, "text": ", it is a common practice to use transformer as the robust baseline model.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 10, "text": "Furthermore, it is also shown from these works that it is hard to improve over the transformer baseline, and 0.5-1 BLEU score improvement is already considered substantial.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 11, "text": "2. We further add newly obtained results on the WMT18 challenge.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 12, "text": "We compare our method with both the champion translation system MS-Marian (WMT18 En->De challenge champion).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 13, "text": "Our method achieves the state-of-the-art result on this task.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 14, "text": "---------------------------------------------------------------------------", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 15, "text": "WMT En->De", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 16, "text": "2016", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 17, "text": "2017", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 18, "text": "2018", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 19, "text": "---------------------------------------------------------------------------", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 20, "text": "MS-Marian (ensemble)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 21, "text": "39.6", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 22, "text": "31.9          48.3", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 23, "text": "Ours (single)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 24, "text": "40.68", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 25, "text": "33.47       48.89", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 26, "text": "Ours (ensemble)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 27, "text": "41.23", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 28, "text": "34.01       49.61", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 29, "text": "---------------------------------------------------------------------------", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 30, "text": "Please refer to Section 3.4 \"Study on generality of the algorithm\" for more details and Table 4 for full results in our updated paper.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 31, "text": "** Image Translation Evaluation **", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 32, "text": "For image-to-image translation tasks, we further add two quantitative measures: (1) We use the Fr\u00e9chet Inception Distance (FID) [6], which measures the distance between generated images and real images to evaluate the painting to photos translation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 33, "text": "(2) We use \"FCN-score\" evaluation on the cityscape dataset following [7].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 34, "text": "The results are reported in Table 6 and Table 7 respectively.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 35, "text": "Multi-agent dual learning framework can achieve better quantitative results than the baselines.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 36, "text": "We are not sure what you meant by \u201cHow does their ensemble method compare to just their single-agent dual method?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 37, "text": "\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 38, "text": ".", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 39, "text": "The standard CycleGAN model (baseline) already leverages both primal and dual mappings, which is equivalent to our \u201cDual-1\u201d model in NMT experiments, i.e., the dual method with only one pair of agents f_0 and g_0.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [6]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 40, "text": "Our model involves two additional pairs of agents (f_1 and g_1, f_2 and g_2) during training.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [6]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 41, "text": "Unlike ensemble learning, only one agent (f_0 for forward direction, or g_0 for backward direction) is used during inference.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [6]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 42, "text": "** Reference **", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 43, "text": "Thanks for pointing a reference paper \"Multi-Column Deep Neural Networks for Image Classification\" (briefly, MCDNN) and we have added reference to it (Section 4).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 44, "text": "Although MCDNN also uses multiple agents (i.e., several columns of deep neural networks), it differs from our model in two aspects: (1) Our work leverages the duality of a pair of dual tasks while this paper does not; (2) In an MCDNN framework, during the training phase, all the columns are updated by winner-take-all rule; and during inference, all columns work like an ensemble model through weighted average.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 45, "text": "In comparison, we only update one primal and one dual agent during training, and use one agent for inference.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 46, "text": "** Clarity **", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 47, "text": "Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 48, "text": "Please kindly refer to first paragraph in Section 3.3.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 49, "text": "You may check our updated paper with clarification and new experimental results.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 50, "text": "Thanks for your time and feedbacks.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 51, "text": "[1] Vaswani, Ashish, et al. \"Attention is all you need.\" In NIPS. 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 52, "text": "[2] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional Sequence to Sequence Learning. In Proc. of ICML, 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 53, "text": "[3] Wu, Yonghui, et al. \"Google's neural machine translation system: Bridging the gap between human and machine translation.\" arXiv preprint arXiv:1609.08144 (2016).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 54, "text": "[4] Chen, Mia Xu, et al. \"The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.\" In Proc. of the ACL, 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 55, "text": "[5] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proc. of NAACL, 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 56, "text": "[6] Heusel, Martin, et al. \"Gans trained by a two time-scale update rule converge to a local nash equilibrium.\" In NIPS, 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 57, "text": "[7] Isola, Phillip, et al. \"Image-to-image translation with conditional adversarial networks.\" In CVPR, 2017", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 58, "text": "Dear AnonReviewer1,", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 59, "text": "Before the final decision concludes, do you have further questions regarding our rebuttal and updated paper?", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_none", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 60, "text": "Our paper revision includes reorganization of the introduction to our framework (Section 3.1), the additional experiments on WMT18 English->German translation challenge (Section 3.4), the additional study on diversity of agents (Appendix A), and quantitative evaluation on image-to-image translations (Section 4.3 and 4.4) following your suggestions.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_none", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 61, "text": "In particular, we would like to highlight that:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 62, "text": "(1) The calibration of BLEU score: We would like to point out that our improvement over the previous state-of-the-art baselines is substantial.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_none", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 63, "text": "For example, on the WMT2014 En->De translation task, the performance of the transformer baseline is 28.4 BLEU score [1] (our baseline matches this performance).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_none", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 64, "text": "The improvement over this baseline is 0.61 in [2], 0.8 in [3] (1.3 BLEU improvement over the re-implemented 27.9 baseline in [3]) and 0.9 in [4], while ours is 1.65 BLEU score.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_none", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 65, "text": "(2) The baselines: As we explained in the previous response, we are using the state-of-the-art transformer as our backbone model, and comparing against all the relevant algorithms including KD, BT and the traditional 2-agent dual learning (Dual-1).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_none", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 66, "text": "Moreover, we also show on WMT18 En->De challenge that our method can further improve the state-of-the-art model trained with extensive resources (Section 3.4 of our updated paper).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_none", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 67, "text": "We hope our rebuttal and paper revision could address your concerns.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_none", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 68, "text": "We welcome further discussion and are willing to answer any further questions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_none", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 69, "text": "[1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 70, "text": "[2] He, Tianyu, et al. \"Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation\". Advances in Neural Information Processing Systems. 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 71, "text": "[3] Shaw, Peter, Jakob Uszkoreit, and Ashish Vaswani. \"Self-Attention with Relative Position Representations.\" In Proc. of NAACL, 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 72, "text": "[4] Anonymous. Universal transformers. In Submitted to International Conference on Learning Representations, 2019.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 73, "text": "URL https://openreview.net/forum?id=HyzdRiR9Y7.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HklXxIdqn7", "rebuttal_id": "rJxe6Bpf07", "sentence_index": 74, "text": "Under review as a conference paper at ICLR 2019", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 0, "text": "Thank your very much for your review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 1, "text": "We have updated the manuscript with more details in the derivation of the first order approximation of KL divergence.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_none", null]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 2, "text": "1) Elaborated derivation of Eq. 10", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 3, "text": "Q1: We have added one more line to explain the derivation.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 4, "text": "Basically a baseline is subtracted, and GAE is introduced.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 5, "text": "2) Gradient update on \\phi from KL divergence", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 6, "text": "The gradients w.r.t. \\phi from the KL divergence is stopped for variance reduction with acceptable bias, which we prove with MuProp [1].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 7, "text": "Details could be found in Appendix C.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 8, "text": "Q3: Rather than [2], we employ MuProp to reduce variance in our development of NADPEx.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 9, "text": "Thank your for your suggestion.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 10, "text": "Q4: Yes \\theta and \\phi are jointly and simultaneously optimized at Eq. 12, though the gradients w.r.t. \\phi from the KL divergence is stopped.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 11, "text": "Q7: Due to the stop-gradient manipulation in the KL divergence, gradients w.r.t. \\phi remains the same as in stated in last subsection.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [21, 22, 23]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 12, "text": "3) Mean policy in the KL divergence", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 13, "text": "What motivates the mean policy is not variance reduction, but the idea that dropout policy had better to be close to each other.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 14, "text": "As intuitively \\phi is controlling the distance between dropout policies, it would further remedy the little bias mentioned above.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 15, "text": "However, the computation complexity for \"close to each other\" would be O(N^2), with N being the number of dropout policies in this batch.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 16, "text": "We employ mean policy to make it linear. And it could be regarded as an integration on a Gaussian approximation of the Monte Carlo estimate according to [3].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 17, "text": "Details could be found in Appendix C.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 18, "text": "Q2: No the mean policy is not used due to the likelihood ratio trick.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 19, "text": "And the approximation of using mean policy is discussed in [3], with a sound deduction.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11, 12]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 20, "text": "Q3: Mean policy is not motivated by variance reduction, which is addressed as introduced above.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 21, "text": "Thank you for your suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 22, "text": "Q5: In the updated version, we have explicitly pointed out that the gradients w.r.t. \\phi from KL divergence is stopped. Thanks for this suggestion.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 23, "text": "Hope our response addresses your concerns!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 24, "text": "[1] Gu et al., \"MuProp: Unbiased Backpropagation for Stochastic Neural Networks\", ICLR 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 25, "text": "[2] Titsias et al., \"Local Expectation Gradients for Black Box Variational Inference\", NIPS 2015.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HkxrqOb6nm", "rebuttal_id": "B1xcdgFE67", "sentence_index": 26, "text": "[3] Wang et al., \"Fast dropout training\", ICML 2013.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HyecCk_TFH", "rebuttal_id": "BkexWBXKor", "sentence_index": 0, "text": "Thank you for your comments and suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HyecCk_TFH", "rebuttal_id": "BkexWBXKor", "sentence_index": 1, "text": "We will address the issues you mentioned.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HyecCk_TFH", "rebuttal_id": "BkexWBXKor", "sentence_index": 2, "text": "1.\tThank you for the insightful suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "HyecCk_TFH", "rebuttal_id": "BkexWBXKor", "sentence_index": 3, "text": "We now have added related work about video compositional methods in section 2.3 in the second version of the paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10, 10, 10]]}, {"review_id": "HyecCk_TFH", "rebuttal_id": "BkexWBXKor", "sentence_index": 4, "text": "2.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "HyecCk_TFH", "rebuttal_id": "BkexWBXKor", "sentence_index": 5, "text": "In the original version of the paper, all experiments are conducted on trimmed video classification datasets.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "HyecCk_TFH", "rebuttal_id": "BkexWBXKor", "sentence_index": 6, "text": "Although most papers in this field only report results on the trimmed video datasets, we do agree that more complicate cases should be tested.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "HyecCk_TFH", "rebuttal_id": "BkexWBXKor", "sentence_index": 7, "text": "Additionally, we evaluated our V4D for untrimmed video classification on ActivityNet v1.3, which contains videos of 5 to 10 minutes and typically large time lapses of the videos are not related with any activity of interest.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "HyecCk_TFH", "rebuttal_id": "BkexWBXKor", "sentence_index": 8, "text": "The very competitive result is reported in the appendix of the second version of paper, which demonstrated the generalization and robustness of our V4D.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "HyecCk_TFH", "rebuttal_id": "BkexWBXKor", "sentence_index": 9, "text": "In fact, unlike previous video compositional methods, even when local events are not well aligned or misclassified, long-term modelling with 4D convolution and video-level aggregation with global average pooling are very likely to correct the partial error.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "HyecCk_TFH", "rebuttal_id": "BkexWBXKor", "sentence_index": 10, "text": "3.About complexity, in the original version of the paper, we have reported parameters and FLOPs of V4D and compared it with other baseline methods in Table 2.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [20]]}, {"review_id": "HyecCk_TFH", "rebuttal_id": "BkexWBXKor", "sentence_index": 11, "text": "4. We have already corrected the typo in title in the second version of the paper. Yet it seems that we are not able to modify the title on OpenReview. Thank you for pointing it out.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [24]]}, {"review_id": "HyecCk_TFH", "rebuttal_id": "BkexWBXKor", "sentence_index": 12, "text": "Hopefully our rebuttal could stress your concerns. If there are still any possible issues, please don\u2019t hesitate to tell us and we will response as soon as possible.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 0, "text": "First of all, thank you for taking your time to review our paper and providing feedback. We have judiciously taken the comments of the reviewers,  and apologize for the late response due to additional experiments and modifications of the paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 1, "text": "Remark 1. Expression and detail", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 2, "text": "A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [3]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 3, "text": "Remark 2.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 4, "text": "What is \"Selection Network\"?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 5, "text": "A : It is a module that estimates the confidence of the softmax output according to the inputs of the classification network.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 6, "text": "The selection network is trained with sigmoid and binary cross-entropy in a supervised manner.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 7, "text": "And the threshold is not 0.5 but high because selection network is learned with many \u20191\u2019 labels with close to 100 % training accuracy.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 8, "text": "The selection network has advantages in out of class unlabeled data.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 9, "text": "Since softmax output is a relative value, the softmax output can be high for some out of class unlabeled data.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 10, "text": "In our original paper (in table 10), there already exist results of softmax output for in or out of class unlabeled data with 0.9999 thresholds.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 11, "text": "Further, we experimented with the same threshold in table 4 of the new version and the results have shown that out of class unlabeled data are added even with an extremely small threshold such as 0.99999 (epsilon = 10^-5).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 11, 12, 13, 14, 15]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 12, "text": "Remark 3. \"As the base classifier is different for various baselines, it is hard to compare the methods.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [25, 26]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 13, "text": "A : SST has a network structure similar to other papers.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [25, 26]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 14, "text": "The difference of structure was that the selection network is added and Gaussian noise and the mean only batch norm are not used.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [25, 26]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 15, "text": "As mentioned in the paper (4. Experiments), our supervised learning performs slightly better than conventional SSL algorithms because of different settings such as learning rate and Gaussian noise on the input layer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [25, 26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 16, "text": "(When SST uses Gaussian noise, ours are also degraded.)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [25, 26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 17, "text": "Remark 4.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 18, "text": "Experiments Detail ( data setting, threshold, number of iterations, animal vs nonanimal)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 19, "text": "A :", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 20, "text": "==> Data setting", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 21, "text": "The purpose of experiments is to show that the SST algorithm is comparable to the conventional SSL algorithms.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 22, "text": "Therefore, we experimented with the popular setting.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 23, "text": "We have added a detailed description on the data setting to Section 6.3 of the supplementary material.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 24, "text": "==> Iterations & Threshold", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 25, "text": "We have missed out on a detailed description of how to set up some hyper-parameters.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 26, "text": "We set parameters as follows.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 27, "text": "The number of training iteration and thresholding epsilon are very important parameters in our algorithm and have a considerable correlation with each other.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 28, "text": "In the first experiment, the iteration number remains fixed and the growth rate of epsilon is adjusted so that the validation accuracy saturates near the settled iteration number.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 29, "text": "While the validation accuracy is evaluated using the cross-validation, we set the number of training iteration to be 100 so that the model is trained enough until it saturates.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 30, "text": "Epsilon is increased in log-scale and begins at a very small value (10^(\u22125)) where no data is added.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 31, "text": "The growth rate of epsilon is determined according to when the validation accuracy saturates.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 32, "text": "The stopping criterion is that the accuracy of the current iteration reaches the average accuracy of the previous 20 steps.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 33, "text": "If the stopping iteration is much less than 100 times, the epsilon growth rate should be reduced so that the data is added more slowly.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 34, "text": "If the stopping iteration significantly exceeds 100 iterations, the epsilon growth rate should be increased so that the data is added more easily.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 35, "text": "We allow 5 iterations as a deviation from 100 iterations and the growth rate of epsilon is left unchanged in this interval.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 36, "text": "(In previous versions, the growth ratio of epsilon for CIFAR-10 was applied to SVHN and CIFAR-100.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 37, "text": "However, since the epsilon growth rate is different for each dataset, as the reviewer mentioned, we have performed the cross-validation for SVHN and CIFAR-100 and modified our results.) As a result, the epsilon is gradually increased in log-scale by 10 times every 33 iterations in CIFAR-10 and SVHN.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 38, "text": "In the case of CIFAR-100, the epsilon is increased by 10 times in log-scale every 27 iterations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 39, "text": "In the second experiment, we leave the epsilon fixed and simply train the model until the stopping criterion is satisfied.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 40, "text": "Other details are the same as those of the first experiment.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 41, "text": "(In previous versions, the training iterations of fixed mode had been fixed.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 42, "text": "Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 43, "text": "=", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 44, "text": "=> Animal vs non-animal", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 45, "text": "The citation of that part is obscure and has been modified.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 46, "text": "We experimented similar to the [1] and they categorized according to the animal.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 47, "text": "Our approach is similar but not identical.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 48, "text": "Their unlabeled data came from only in 4 classes, however, we selected unlabeled data in all classes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 49, "text": "[1] Odena, Augustus, et al. \"Realistic Evaluation of Semi-Supervised Learning Algorithms.\" (2018)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27, 28, 29, 30, 31]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 50, "text": "Some Questions and comments", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [33]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 51, "text": "Remark 5. \"The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [34]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 52, "text": "A : To the best of our knowledge, the main purpose of transfer learning is to improve the performance on the target domain by effectively utilizing the knowledge of the source domain.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [34]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 53, "text": "However, in our case, there is no separated source and target domains.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [34]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 54, "text": "We focus on the single classification task.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [34]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 55, "text": "We think that the goal of our method and that of transfer learning are quite different.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [34]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 56, "text": "Remark 6. \"What do you mean in section 3.3 by \"if one class dominates the dataset, the model tends to overfit\"?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [37]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 57, "text": "A \" We have modified that expression and we wanted to address that \"if one class dominates the dataset, the performances are degraded by the imbalanced distribution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [37]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 58, "text": "(Analysing the classification of imbalanced data-sets with multiple classes: Binarization techniques and ad-hoc approaches, 2013)\"", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [37]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 59, "text": "Remark 7. \"Figure 3: wouldn\u2019t the plot of accuracy vs amount of data be more suitable here?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [39]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 60, "text": "A : I agree that your suggestion is more suitable for the figure.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [39]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 61, "text": "However, it is difficult to show the figure you want because the number of selected samples is different every time.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [39]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 62, "text": "Remark 8. \"Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [40]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 63, "text": "A : The performance depends on the initial points, therefore sometimes the performance is not good. Since the inputs are the x and y coordinate values, it can be very easy to add to the training set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [40]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 64, "text": "(ex.. class 1 : (-1, 0), (1, 0) , class 2 : (0.5, -0.5), (1.5, -0.5) , then decision boundary could be (:, -0.25) then class 2 unlabeled data (0, 0.5) is classified as class 1 and can have a very high selection score.)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [40]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 65, "text": "Remark 9. Can you explain the sentence \"To prevent data being added suddenly, no data was added until 5 iterations\"?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [41]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 66, "text": "A : In fixed mode, we ensemble the selection scores, which makes the prediction more consistent.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [41]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 67, "text": "Also, for a more reliable selection score, we do not add unlabeled data to the new training set and train with labeled data only for 5 iterations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [41]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 68, "text": "Remark 10. \"How was it possible to improve the performance in the experiment of section 4.2 with 100% of irrelevant classes?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [42]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 69, "text": "A : We suspect that this performance improvement is due to re-initializing learning rate.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [42]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 70, "text": "After constructing a new training dataset, we retrain our model with the learning rate of the initial value.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [42]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 71, "text": "In decay mode (Figure 2, Figure 3 (a) and (b) of the original manuscript), the accuracy is slightly increased and gets saturated while unlabeled data is not being added.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [42]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 72, "text": "However, the accuracy begins to increase or decrease relatively more after adding selected data to the new training dataset.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [42]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 73, "text": "In fixed mode (Figure 3 (c) and (d) of the original manuscript), the improvement with the 100% of irrelevant classes seems to be due to re-initializing learning rate.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [42]]}, {"review_id": "HyeHzlJ537", "rebuttal_id": "HygpTo-FR7", "sentence_index": 74, "text": "However, SST algorithm with other ratios of out-of-class samples results in performance improvement compared to the 100% because out-of-class samples are not selected.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [42]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 0, "text": "We thank reviewer 2 for the detailed feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 1, "text": "We are glad that the reviewer found the VAE-GAN model to be a natural extension for the problem and that our work provides a good baseline for future work.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [16]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 2, "text": "We address the individual questions below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 3, "text": "We changed Section 3.1 to explain that the posterior dependence on pairs of adjacent frames is to have temporally local latent variables that capture the ambiguity for only that transition, a sensible choice when using i.i.d. Gaussian priors.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 4, "text": "Another choice is to use temporally correlated latent variables, which would require a stronger prior (e.g. as in Denton & Fergus (2018)).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 5, "text": "For simplicity, we opted for the former.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 6, "text": "The blurriness in a VAE can indeed be attributable to a weak inference model.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 7, "text": "Note that our VAE variant and both SVG variants are able to predict sharp robot arms in the BAIR dataset, but often blur out the small objects being pushed.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 8, "text": "We tried recurrent posteriors and learned priors with our models, and the results were similar.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 9, "text": "We are now running additional experiments with a deeper encoder and with more filters.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 10, "text": "Although in principle a strong inference model could produce sharper images, an alternative approach is to use better losses, which is the approach we chose in this work.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 11, "text": "It is an interesting suggestion to experiment with the effect of the hyperparameters on the trade-off between realism and diversity.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 12, "text": "We are currently running experiments for various weightings of the KL loss and the adversarial loss, and we plan to include results that illustrate the trade-offs based on these hyperparameters.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 13, "text": "We also plan to include results on the trade-offs between accuracy and realism.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 14, "text": "In fact, a recent result [1] proves that this is a fundamental trade-off for all problems with inherent ambiguity.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 15, "text": "The statement that \u201cGANs prioritize matching joint distributions of pixels over per-pixel reconstruction\" is a criticism of per-pixel losses, and not of VAEs in general. We clarified in the introduction that VAEs can indeed model joint distributions of pixels.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "Hyen_JS9nX", "rebuttal_id": "rygvTCLQRX", "sentence_index": 16, "text": "[1] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Conference on Vision and Pattern Recognition (CVPR), 2018. https://arxiv.org/abs/1711.06077", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_global", null]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 0, "text": "We thank Reviewer 1 (R1) for their review and for asking interesting questions that helped us to understand where our paper may have been unclear.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 1, "text": "In our response below we will try our best to better explain our motivation for building and using SQOOP, as well as address R1\u2019s other questions and concerns.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 2, "text": "A key concern that R1 expressed in their review is that we perform our study on the new SQOOP dataset, instead of using an available one (for example CLEVR or Abstract Scenes VQA).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 3, "text": "Though we appreciate the concern (it has spurred us to rethink and rephrase how we justify SQOOP) we still believe that the SQOOP dataset is the best choice for precisely testing our ideas.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 4, "text": "We kindly invite R1 to consider the following arguments in favor of doing so:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 5, "text": "The goal of our study was to perform a thorough investigation of systematic generalization of language understanding models.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 6, "text": "To that end, we wanted a setup that is as simple as possible, while still being challenging by testing the ability to extend the relational reasoning learned to unseen combinations of seen words.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 7, "text": "We therefore choose to focus on simplest relational questions of the form XRY, as they also allow us to factor out challenges of discrete optimization in choosing the right module layout (required for Stochastic N2NMN).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 8, "text": "The simplicity is also useful because most models get to 100% accuracy on the training set of SQOOP, which allowed us to put aside any remaining optimization challenges and just focus our study on systematic generalization.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 9, "text": "In contrast, we find that the popular CLEVR dataset does not satisfy our requirements and if we did modify it sufficiently, we believe that it would only differ from SQOOP in the actual rendering and would not affect our conclusions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 10, "text": "Though visually more complex, CLEVR has only 3 object types: cylinder, sphere and cube.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 11, "text": "Therefore, it would only allow for 3x4x3=36 different XRY relational questions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 12, "text": "This is arguably not enough to sufficiently represent real world situations, and would definitely hinder our experiments.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 13, "text": "Specifically, we would not be able to sufficiently vary the difficulty of our generalization challenge when allowing 1,2,4,8 or 18 possible right hand-side objects in the questions (we clarify why splits with lower #rhs/lhs are more difficult than those with higher #rhs/lhs later in this response).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 14, "text": "Hence, we did not find the original CLEVR readily appropriate for our study.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 15, "text": "We could, in theory, introduce new object types to CLEVR and rerender a new dataset in 3D using Blender (the renderer that was used to create CLEVR) with different lighting conditions and partial occlusions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 16, "text": "Though enticing, we believe that such a 3D version of SQOOP would lead to exactly same conclusions, because the vision required to recognize the objects in the scene would still be rather trivial.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 17, "text": "The Ying and Yang dataset is clearly a valuable resource (and we thank the reviewer for the pointer), but we do not think it is readily suitable for the kind of study that we aim to perform.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 18, "text": "The dataset, to the best of our understanding, uses crowd-sourced questions (as the questions are taken from Abstract VQA dataset, whose captions were entered by a human, according to the original VQA paper https://arxiv.org/pdf/1505.00468v6.pdf).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 19, "text": "Using crowd-sourced questions would not allow us to control our experiments at the level of precision that we wanted to achieve (e.g. we would not know the ground-truth layouts, it would be harder to construct splits of varying difficulty, etc.).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 20, "text": "As well, Abstract VQA contains only 50k scenes, and from our experience with SQOOP we know that this number would be not sufficient to rule out overfitting to training images as a factor.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 21, "text": "We thank R1 for their constructive suggestion to consider NMNs that form a DAG.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 22, "text": "We are currently investigating a chain-structured NMN with shortcuts from the output of the stem to each of the modules, and we will soon report these additional results in the upcoming revision of the paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 23, "text": "We hope that these results, combined with further qualitative investigations we are conducting, will answer the legitimate question of R1 as to why Chain-NMN performs so much worse than Tree-NMN.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [21]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 24, "text": "We acknowledge that the text of the paper can be improved to explain better why splits with lower #rhs/lhs are generally harder than those with higher #rhs/lhs, and we thank R1 for pointing this", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [22]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 25, "text": "out", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [22]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 26, "text": ".", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [22]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 27, "text": "Our reasoning is that lower #rhs/lhs are harder because the training admits more spurious solutions in them.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 28, "text": "In such spurious regimes models adapt to the specific lhs-rhs combinations from the training and can not generalize to unseen lhs-rhs combinations (i.e. generalizing from questions about \u201cA\u201d in relation with \u201cB\u201d to \u201cA\u201d in relation to \u201cD\u201d (as in #rhs/lhs=1) is more difficult than generalizing from questions about \u201cA\u201d in relation to \u201cB\u201d and \u201cC\u201d to the same \u201cA\u201d in relation to \u201cD\u201d (as in #rhs/lhs=2).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 29, "text": "We will update the paper to be more explicit in explaining these considerations.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [22]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 30, "text": "We would like to conclude our response by replying to the higher-level concern of R1 that the findings of our study may not \u201cgeneralize to other more complex datasets where the network layout NMN might be more complex, the number of modules and type of modules might also be more\u201d.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 31, "text": "While we fully agree that more complex datasets with more complex questions would bring new challenges, these are ones we purposely put aside (such as the general unavailability of ground-truth layouts for vanilla NMN, the need to consider an exponentially large set of possible layouts for Stochastic N2NMN, etc.) We believe that it is highly valuable for the research community to know what happens in the simple ideal case of SQOOP, where we can precisely test our specific generalization criterion.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 32, "text": "This knowledge (e.g. the superiority of trees to chains, the sensitivity of layout induction to initialization, the emergence of spurious parameterization in end-to-end learning), will guide researchers in choosing, designing and troubleshooting their models, as they now know what to expect modulo the optimization challenges that they may face.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 33, "text": "The field of language understanding with deep learning is not easily amenable to mathematical theoretical investigations and, with that in mind, rigorous minimalistic studies like ours are arguably very important.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 34, "text": "To some extent, they play the role of the former: they inform researcher intuition and lay a solid foundation for scientific dialogue.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 35, "text": "We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 36, "text": "We believe that the total of our results makes a complete conference paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 37, "text": "All that said, we would welcome specific suggestions of additional experiments that we could carry out in order to better validate our claims.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Hyere82c2m", "rebuttal_id": "H1xbP6B8TX", "sentence_index": 38, "text": "We hope that this response has clarified to R1 what our paper was insufficiently clear about. A new revision with additional experiments and fixed typos will soon be uploaded to OpenReview, and we hope that R1 takes this response and the changes that we will make to the paper into account.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 0, "text": "Glad to know that you like our paper!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 1, "text": "1) Difference from parameter noise except for memory consumption:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 2, "text": "As stated in Section 3.3, we believe NADPEx is a generalization of parameter noise, with not only flexible memory consumption but also lower variance in gradients.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 3, "text": "This theory is examined in Section 4.2, where NADPEx shows faster convergence and lower variance in performance with different random seeds.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 4, "text": "Besides, comparing with [1], our work provides a theoretical modeling for the idea \"a hierarchy of stochasticity for exploration\".", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 5, "text": "We model the NADPEx policy as a joint distribution of dropout random variables and actions, such that it could be combined seamlessly with existing on-policy policy gradient methods.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 6, "text": "One example is the policy space constraint stated in Section 3.2.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 7, "text": "We also provide another distribution i.e. Bernoulli distribution for stochasticity at high level, for which we derive gradient alignment and policy space constraint, as well as empirical results.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 8, "text": "As a minor point, in [1], the stochasticity at the high level i.e. the variance of parameter noise, is adjusted in a heuristic manner.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 9, "text": "NADPEx, in contrast, aligns the stochasticity throughout the hierarchy with end-to-end gradient update.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 10, "text": "2) Other good side effects:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 11, "text": "The robustness of the NADPEx policy is orthogonal to our current work, but will be an interesting direction for the future.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [9]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 12, "text": "Currently we only have some preliminary results.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 13, "text": "For example, it is more robust to adversarial neural attacks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 14, "text": "In the future we will investigate how robust NADPEx policies could be when the environment is perturbed, e.g. agents are dragged slightly by humans as in [2, 3].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 15, "text": "That temporally consistent exploration is fairly important for physical robots is one of our motivations for this whole project.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 16, "text": "In the next step we will look for simulator environments with more authentic actuators to see how NADPEx could help solve that.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [11]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 17, "text": "Our ultimate goal is to find a safer and more efficient way for on-policy exploration on physical robots.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 18, "text": "We believe the application of NADPEx to off-policy exploration is straightforward.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 19, "text": "However, as stated in Section 1, off-policy methods benefit from stronger flexibility for experience sampler.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [12]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 20, "text": "This makes the gradient alignment and policy space constraint not as important as in the on-policy methods.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [12]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 21, "text": "As off-policy methods have the potential to be much more data-efficient, we will compare in the future how NADPEx performs comparing with auto-correlated noise in [4] and separate sampler in [5].", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [12]]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 22, "text": "[1] Plappert et al., \"Parameter Space Noise for Exploration\", ICLR 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 23, "text": "[2] Tassa et al., \"Synthesis and stabilization of complex behaviors through online trajectory optimization\", IROS 2012.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 24, "text": "[3] Clavera et al., \"Learning to Adapt: Meta-Learning for Model-based Control\", arXiv 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 25, "text": "[4] Lillicrap et al., \"Continuous control with deep reinforcement learning\", ICLR 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HyeuvtzF2X", "rebuttal_id": "S1g0_Au4pm", "sentence_index": 26, "text": "[5] Xu et al., \"Learning to explore via meta-policy gradient\", ICML 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 0, "text": "Thank you for your thoughtful review. We will address your concerns in turn.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 1, "text": "Q1: The idea is very related to Yeh et al.\u2019s work which is not mentioned at all.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 2, "text": "A1: The entire first paragraph of our related work section is focused on Yeh et al.\u2019s work.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 3, "text": "As we explained in the paragraph, there is a major theoretical flaw in their method.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 4, "text": "Yeh et al. (2017) use the discriminator loss of a trained GAN as an indicator of how realistic their restoration is.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 5, "text": "However, Goodfellow et al. (2014) already prove that the discriminator is unable to identify how realistic an input is after several steps of training, if the GAN has enough capacity.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 6, "text": "Ideally the generator will have all the information of the data distribution while the discriminator will have none.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 7, "text": "That is why we use the generator of a trained GAN as an implicit probability density model in our method.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 8, "text": "Another difference between their work and ours is that they only focus on image inpainting problem, while our method applies to various image restoration problems.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 9, "text": "Q2: Total variation regularization can also handle different degradations.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 10, "text": "A2: We think you underestimate the difficulty of those restoration problems.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [8, 9, 10, 11, 12]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 11, "text": "Please check the degraded images in Table 3.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [8, 9, 10, 11, 12]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 12, "text": "These images are damaged so badly that TV cannot recover any meaningful thing.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [8, 9, 10, 11, 12]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 13, "text": "As a handcrafted prior, TV performs much worse than our data-driven baseline method in these tasks.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [8, 9, 10, 11, 12]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 14, "text": "Q3: Does the proposed method learn the image inpainting mask as well?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 15, "text": "What are the parameters of the degradation in the applications?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 16, "text": "A3: The image inpainting mask is known and fixed.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 17, "text": "We use four different kinds of degradation to test the generality of our method.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 18, "text": "The first three kinds of degradation are 7\u00d7 downsampling, making a 14\u00d714 square hole in the center of the image, and adding Gaussian white noise with a standard deviation of 1.0, respectively.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "HygHWpHH2m", "rebuttal_id": "HyghIbr50m", "sentence_index": 19, "text": "The last kind of degradation is a composition of a series of degradation in order, which are (a) adding linear motion blur by at most 14 pixels in any direction, (b) 4\u00d7 downsampling, (c) adding uniform noise between -0.05 and 0.05, (d) randomly removing 10% of the pixels.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 0, "text": "Thanks for your feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 1, "text": "We discuss each comment in the following:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 2, "text": "- The experiments are not large scale", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 3, "text": "We respectfully disagree with the reviewer's main comment that the experiments are not large scale.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 4, "text": "One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 5, "text": "Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 6, "text": "Sure, this is not the scale of 80 million tiny images; but one wouldn\u2019t ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 7, "text": "Representation learning, the topic of this conference, has many facets.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 8, "text": "Learning representations from \u201cbig data\u201d (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 9, "text": "Both are valuable in different circumstances.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 10, "text": "- No substantiate insight with respect to NP-hard problems", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 11, "text": "We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 12, "text": "We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 13, "text": "To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 14, "text": "These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 15, "text": "Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 16, "text": "Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 17, "text": "Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 18, "text": "So powerful non-convex solvers might be of a significant advantage over convex relaxations.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 19, "text": "Our paper simply shows ONE example for this.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 20, "text": "- It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 21, "text": "It would not be possible to set the input dimension the same as the embedding dimension.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 22, "text": "Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 23, "text": "The size of the embedding dimension can be too low to achieve this.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 24, "text": "One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 25, "text": "However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 26, "text": "- Methods, where items have no representation, are questionable", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 27, "text": "Items having no representation is a caveat of the data available rather than that of the method.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 28, "text": "The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 29, "text": "- How to generalize to unseen items", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 30, "text": "First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 31, "text": "We believe that in our case, generalization is realizable.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 32, "text": "One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 33, "text": "The network can be trained with extra batches of triplets which involves the new items.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 34, "text": "- The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "HygLj-cG9B", "rebuttal_id": "SJgYkJgqsS", "sentence_index": 35, "text": "We don\u2019t really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this.", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [5]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 0, "text": "Q: \"I had hard time to understand latent canonicalization...\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_error", null]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 1, "text": "A: \"latent canonicalization\" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_error", null]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 2, "text": "Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 3, "text": "A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 4, "text": "In those cases full access to factors of variation is available as this is used to generate the data.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 5, "text": ", The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 6, "text": "We are not focusing on a setting where the source domain has no labels.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 7, "text": "That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 8, "text": "Q: How can the proposed method be generalized to non-image data?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 9, "text": "The experiments were only done on simple image datasets.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 10, "text": "I am wondering this method can be applied to other complex datasets whose latent factors are unknown.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 11, "text": "A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 12, "text": "In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 13, "text": "Critically, these two sources of data need not be identical!", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 14, "text": "This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 15, "text": "Q: I do not understand this: \"to fit well the method overfitting rate\" in Section 3.3.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 16, "text": "Thank you for pointing this sentence out!", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 17, "text": "It is indeed unclear.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 18, "text": "All we were trying to say is that each baseline\u2019s training duration was chosen independently to prevent overfitting.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "Hyguk1hhKB", "rebuttal_id": "BygujjyNiB", "sentence_index": 19, "text": "We have updated the draft to be more clear.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 0, "text": "We appreciate your time and effort of reviewing our paper, and thank you for the insightful and constructive comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 1, "text": "For simplicity of the main paper, we moved all the detailed proofs to the Appendix.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 2, "text": "More specifically, the proofs for Theorem 1, Lemma 1, Theorem 2, Corollary 1, and Theorem 3 are given in Appendix A, C, D, E, and F, respectively.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 3, "text": "Thanks a lot for pointing out the smoothness conditions for reparameterization; we have carefully revised our paper to remove the misleading statements and to make it clearer when our method (and also the reparameterization trick, Rep) is applicable.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [11, 12, 13, 14, 15, 16, 17]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 4, "text": "For your comments wrt discrete random variables (RVs), unfortunately, we haven\u2019t found a principled way to back-propagate gradient through discrete internal RVs (like in multi-layer sigmoid belief networks).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [11, 12, 13, 14, 15, 16, 17]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 5, "text": "However, as stated in the last paragraph of Sec. 7.2, we presented in Appendix B.4 a procedure to assist our methods in handling discrete internal RVs.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 12, 13, 14, 15, 16, 17]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 6, "text": "We believe that procedure could be useful for the inference of models like the multi-layer sigmoid belief networks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13, 14, 15, 16, 17]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 7, "text": "As for the conditional independency, it is actually removed after marginalizing out additional continuous RVs (which could be non-reparameterizable RVs like Gamma).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13, 14, 15, 16, 17]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 8, "text": "Also note that one can strengthen the aforementioned procedure by inserting more additional continuous internal RVs into the inference model to enlarge its (marginal) description power.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13, 14, 15, 16, 17]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 9, "text": "The notations are chosen for harmony and also to keep consistency with the main literature.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 10, "text": "For example, one can add another expectation wrt the true data distribution q(x) to the ELBO in Eq. (1), that is, E_{q(x)} [ELBO] = E_{q(x) q(z|x)} [log p(x,z) \u2013 log q(z|x)]  \\propto  - KL[q(x)q(z|x) || p(x,z)].", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 11, "text": "For dropout, since the dropout rate is a tunable hyperparameter that need not be learned (thus no back-propagation is required)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [25]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 12, "text": ", one can use Rep to construct the q distribution you defined.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [25]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 13, "text": "If we understand correctly, in that case we cannot demonstrate our advantages.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [25]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 14, "text": "Currently, the proposed method cannot be directly applied to multi-layer sigmoid belief networks (without the procedure in Appendix B.4).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [27]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 15, "text": "We have made an explicit statement of this in the revised manuscript.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [27]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 16, "text": "Thank you for pointing this out.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 17, "text": "However, it\u2019s believed that Rep cannot be applied to Gamma distributions [1,2].", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [30]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 18, "text": "We have revised our statement to \u201cThere are situations for which Rep is not readily applicable, e.g., where the components of y may be discrete or nonnegative Gamma distributed\u201d.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [30]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 19, "text": "[1] F. Ruiz, M. Titsias, and D. Blei. The generalized reparameterization gradient. In NIPS, pp. 460\u2013468, 2016.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 20, "text": "[2] C. Naesseth, F. Ruiz, S. Linderman, and D. Blei. Rejection sampling variational inference. arXiv:1610.05683, 2016.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 21, "text": "Yes, Lemma 1 shows that our deep GO will reduce to Rep when Rep is applicable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [32, 33, 34, 35]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 22, "text": "We are not sure whether you were asking about the difference in Fig. 1 or Fig. 2.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [32, 33, 34, 35]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 23, "text": "So, two responses are given below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [32, 33, 34, 35]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 24, "text": "(A) In Fig. 1, the difference comes from the definition of node y^(i).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [32, 33, 34, 35]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 25, "text": "For deterministic deep neural networks, node y^(i) is the activated value after an activation function, where deterministic chain rule can be readily applied; while for deep GO gradient, node y^(i) might be the sample of a non-reparameterizable RV, where deterministic chain rule is not applicable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [32, 33, 34, 35]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 26, "text": "Please also refer to the main contribution (ii) of our response to Reviewer 2.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [32, 33, 34, 35]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 27, "text": "(B) If you were interested in the difference in Fig. 2 (a)(b), the reasons include (1) the standard Rep cannot be applied to Gamma RVs; (2) both GRep and RSVI are designed to approximately reparametrize Gamma RVs; (3) GO generalizes Rep to non-reparameterizable RVs; or in other words, GO is identical to the exact Rep for Gamma RVs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [32, 33, 34, 35]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 28, "text": "Yes, the sticking approach was implicitly adopted for all the compared methods when it is applicable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [37]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 29, "text": "We have made a clear statement in the revised paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [37]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 30, "text": "Since stochastic computation graph (SCG) is based on REINFORCE and our method is based on GO, the comparison between SCG and our method is (roughly speaking) identical to that between REINFORCE and GO.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [37]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 31, "text": "That is, SCG is more generally applicable but with higher variance; the proposed method has less generalizability but with much lower variance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [37]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 32, "text": "We have added the following discussion into Related Work.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [37]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 33, "text": "\u201c\u2026as the Rep gradient (Grathwohl et al., 2017).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [37]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 34, "text": "SCG (Schulman et al., 2015) utilizes the generalizability of REINFORCE to construct widely-applicable stochastic computation graphs.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [37]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 35, "text": "However, REINFORCE is known to have high variance, especially for high-dimensional problems, where the proposed methods are preferable when applicable (Schulman et al., 2015).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [37]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 36, "text": "Stochastic back-propagation\u2026\u201d", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [37]]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 37, "text": "Thank you for pointing out these fundamental conditions, which we have added into the revised manuscript.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "HygWY_1c2X", "rebuttal_id": "S1eUAxbKa7", "sentence_index": 38, "text": "We hope your concerns have been addressed. If not, further discussion would be welcomed.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 0, "text": "We think your suggestions are very meaningful. We respond to them one by one:", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 1, "text": "1. We will explain anti-aliasing in our updated paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 2, "text": "Roughly, anti-aliasing is helpful for signal reconstruction.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 3, "text": "However, we can\u2019t provide a strict treatment of how anti-aliasing relates to classification.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 4, "text": "But we have intuitions: first, we believe reconstruction relates to classification (see our next response); second, frequency components are orthogonal.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 5, "text": "Aliasing means different components are mixed again.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 6, "text": "This may mislead the next layers for processing.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 7, "text": "2. To our knowledge, researchers haven\u2019t fully understood the whole process of image classification until now.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 10]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 8, "text": "Thus, we can\u2019t provide a strict treatment of how reconstruction optimality relates to classification optimality.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 10]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 9, "text": "But we have intuitions and empirical evidence of their relation: convolution layers are used to transform a signal which makes it easier to be classified.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 10]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 10, "text": "So if we accept that the feature extracted by previous convolution layers is useful, then it is best to keep it as much as possible for the current pooling layer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 10]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 11, "text": "In this way, it is reasonable to assume that reconstruction optimality is consistent with classification optimality.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 10]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 12, "text": "On the other hand, it is difficult to directly define classification optimality for an intermediate layer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 10]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 13, "text": "Moreover, several works, such as [1] have shown that using self reconstruction loss as an auxiliary is helpful for classification.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 10]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 14, "text": "3.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13, 14, 15, 16, 17]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 15, "text": "Please refer to our general response.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13, 14, 15, 16, 17]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 16, "text": "With suitable settings, the shift consistency of F-pooling is much better.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15, 16, 17]]}, {"review_id": "Hyl73mFK9H", "rebuttal_id": "BJxED1cGsB", "sentence_index": 17, "text": "[1] Semi-Supervised Learning with Ladder Networks, NIPS2015", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HylcynXA2X", "rebuttal_id": "S1giTvK6am", "sentence_index": 0, "text": "We thank the reviewer for their encouraging words.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HylcynXA2X", "rebuttal_id": "S1giTvK6am", "sentence_index": 1, "text": "\"There are better baselines for MNIST to MNIST-M\": yes, the presented method might not outperform all other methods on all other datasets. Still, we hope to convince the reviewer that the results on the other datasets are worth being considered.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "HylcynXA2X", "rebuttal_id": "S1giTvK6am", "sentence_index": 2, "text": "\"Office is not suitable unless one wants to be in a few-datasample regime or work with data with noisy labels\": we would like to point that this regime is quite realistic in Bioimage informatics (noisy, with few samples per class).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "HylcynXA2X", "rebuttal_id": "S1giTvK6am", "sentence_index": 3, "text": "\"The results on Cell are not convincing\": as our goal is multi-domain learning on this dataset, the relevant performance indicator is the average risk over all domains.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "HylcynXA2X", "rebuttal_id": "S1giTvK6am", "sentence_index": 4, "text": "Table 2 details what happens in various categories of cases (on classes with/without labelled samples).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "HylcynXA2X", "rebuttal_id": "S1giTvK6am", "sentence_index": 5, "text": "Despite the (well-known) degradation of the results on labeled classes when one also considers unlabelled classes, the bottom line is that -- regarding the average risk -- our method outperforms the baseline.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "HylcynXA2X", "rebuttal_id": "S1giTvK6am", "sentence_index": 6, "text": "Importantly, MuLANN results on Cell are significantly better than the baseline on all rows which involve unlabeled classes (rows  3, 6, 9, 13), while remaining not significantly different to the baseline on 6/9 of the other rows.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "HylcynXA2X", "rebuttal_id": "S1giTvK6am", "sentence_index": 7, "text": "\"Comparison with other methods did not take into account a variety of hyperparameters\".", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "HylcynXA2X", "rebuttal_id": "S1giTvK6am", "sentence_index": 8, "text": "The reviewer is right.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "HylcynXA2X", "rebuttal_id": "S1giTvK6am", "sentence_index": 9, "text": "Complementary experiments have thus been performed, and tables 1, 2 updated.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9]]}, {"review_id": "HylcynXA2X", "rebuttal_id": "S1giTvK6am", "sentence_index": 10, "text": "We investigated the impact of varying the learning rate, the weight lambda on the discriminator loss, the weight dzeta of the known-unknown discrimination loss, the learning rate schedule, lambda schedule as well as using different learning rates for pre-trained layers versus from scratch layers (see Table 5 for more detailed information).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "HylcynXA2X", "rebuttal_id": "S1giTvK6am", "sentence_index": 11, "text": "These results show a moderate sensitivity of MuLANN, MADA and DANN wrt hyper-parameters and confirm that MuLANN outperforms both MADA and DANN (detailed results available here https://drive.google.com/file/d/1NjtMKF53qmnx4_Jyvh-ofxb0WjzcDvow/view?usp=sharing).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "Hylj7vZsh7", "rebuttal_id": "HJxewogcRX", "sentence_index": 0, "text": "Thank you for your encouraging comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Hylj7vZsh7", "rebuttal_id": "HJxewogcRX", "sentence_index": 1, "text": "We agree with your suggestions and we will revise our paper accordingly.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [6, 7, 8, 9, 10, 11, 12, 14, 14, 14, 17, 18, 19, 20, 21]]}, {"review_id": "Hylj7vZsh7", "rebuttal_id": "HJxewogcRX", "sentence_index": 2, "text": "We will also comment on the gap between our analysis and AGZ in the introduction to make it clearer, and discuss potential future work (e.g., considering approximation errors due to MCTS and the value function) in the conclusion.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_global", null]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 0, "text": "Thank you for the detailed comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 1, "text": "Our goal is to develop a quantitative understanding of AlphaGo Zero (AGZ), moving beyond the intuitive justification for the algorithms in the original work.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 2, "text": "We believe that a rigorous mathematical analysis is crucial to provide a solid foundation for understanding AGZ and similar algorithms.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 3, "text": "This requires developing (i) a precise mathematical model, (ii) a quantitative performance bound within the model.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 4, "text": "Our work takes an important step in this direction by modeling AGZ\u2019s self-play and its supervised learning algorithm accurately.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 5, "text": "In particular, we use the turn-based game model to capture the self-play aspect.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 6, "text": "We develop a quantitative bound in terms of cross-entropy loss in supervised learning, which is the \u201cmetric\u201d of choice in AGZ.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 7, "text": "While the cross-entropy loss seems intuitive, using it as a quantitative performance measure requires careful thought.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 8, "text": "For example, in Appendix F (page 19, 2nd paragraph), we discussed a scenario where this intuition is incorrect under a careless measure.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 9, "text": "That is, seemingly \u201cobvious\u201d algorithms can fail in the absence of a rigorous mathematical proof.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 10, "text": "We agree that there is a gap between AGZ and our model.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 11, "text": "As mentioned in our paper, MCTS converges to the optimal policy for both classical MDPs and stochastic games.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 12, "text": "Hence in this paper, we model the AGZ\u2019s MCTS policy by the optimal policy, and mainly focus on the other two key ingredients of AGZ, self-play and supervised learning.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 13, "text": "It will be interesting to study how the error between MCTS and the optimal policy affects the iterative algorithm.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 14, "text": "This is a research direction we think is worth pursuing in the future.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 15, "text": "We also agree with the reviewer that some of our statements might be too strong.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [10, 16]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 16, "text": "We will revise accordingly. Instead of ``immediate justification``, we believe this work does provide a first-step, formal framework towards a better theoretical understanding.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [10, 16]]}, {"review_id": "HyljDMze6m", "rebuttal_id": "S1xqVig50Q", "sentence_index": 17, "text": "We will also revise the title, perhaps to ``applying AGZ`` so as to make the connection to MDP more clear in our paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [10, 16]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 0, "text": "Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 1, "text": "A: Yes we did and interestingly, it didn't show improvement.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 2, "text": "The results are reported in Appendix A, Table A1 in the row titled: \"Ours + classifier after\" and discussed in Section 4.2.4.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 3, "text": "Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 4, "text": "Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 5, "text": "A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 6, "text": "So latents are canonicalized in both possible orderings.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 7, "text": "We discuss this point at the bottom of page 4 after equation 6 and will further clarify.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 8, "text": "Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 9, "text": "There might be other constructions that are more efficient and less restrictive.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 10, "text": "A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 11, "text": "While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 12, "text": "First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 13, "text": "However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 14, "text": "Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 15, "text": "A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 16, "text": "Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 17, "text": "If the above mentioned assumption indeed holds, the font should be the only change in the set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 18, "text": "We order the samples according to that axis and plot them in a row from left to right.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "HylTsRJxqS", "rebuttal_id": "H1lrdi14jH", "sentence_index": 19, "text": "We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 0, "text": "We thank you for the constructive feedback and discuss some of your comments below.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 1, "text": "R3: \"it would be valuable to compare the performance of curiosity-based learning versus learning based on well-defined extrinsic rewards\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 2, "text": "=> We would like to highlight that evaluating success of pure curiosity-driven exploration (no extrinsic rewards for training) by measuring the extrinsic score of game is just a proxy to evaluate exploration.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 3, "text": "Our results show that exploration via curiosity has striking correlation with game scores.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 4, "text": "But we expect that when environments have a well-defined (and well-shaped!) extrinsic reward, a policy trained using that extrinsic reward should outperform the policy trained with only curiosity especially when the performance is measured by the extrinsic return.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 5, "text": "There are, however, examples, such as the Bowling Atari game, where a policy trained with only curiosity does *better* than a policy trained with extrinsic rewards.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 6, "text": "The purely curious agent learns to play the game better than agents trained to maximize the (clipped) extrinsic reward directly.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 7, "text": "We think this is because the agent gets attracted to the difficult-to-predict flashing of the scoreboard occurring after the strikes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 8, "text": "We expect such examples to come from environments with misleading or poorly-shaped extrinsic rewards.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 9, "text": "R3: \"...it would be beneficial for the authors to explore reasons that random features perform well in reinforcement learning.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 10, "text": "=> In the paper, Section 2.1, we discuss that random features have advantages that they are they are stable, compact, and tend to include most relevant information about the observation.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [12]]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 11, "text": "However, in our opinion, a more interesting question is not why random features perform so well, but rather why the feature learning methods perform so poorly (relative to this baseline).", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12]]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 12, "text": "Learning the features introduces non-stationarity that confounds the effects of learning the dynamics.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "HylyhLLF3X", "rebuttal_id": "r1xr8BYm0m", "sentence_index": 13, "text": "We believe that if methods are developed to address this non-stationarity, or environments that are more visually complex are used, then the benefits of the learning the features will become more noticeable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 0, "text": "Thank you for your positive review, we address your reasonable concern for the applications of the proposed method in below:", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 1, "text": "Q1:  It would be nice to see a better case made for spherical convolutions within the experimental section.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 2, "text": "A1", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_none", null]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 3, "text": ": We believe the best case is the non-rigid shape classification and retrieval.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 4, "text": "Our method achieves a state-of-the-art classification and retrieval performance on Shrec\u201911.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 5, "text": "The good performance on non-rigid shape is mainly contributed by the use of bijective spherical parameterization method, which obtains the input spherical image without topological information losses.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 6, "text": "When using spherical projection method to represent 3D shape, there will be information loss if the object is non-convex.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 7, "text": "The lossy input affect the performance of rigid shape analysis to some extent.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 8, "text": "Q2: The experiments on SHREC17 show all three spherical methods under-performing other approaches.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 9, "text": "It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 10, "text": "Is there a task that this representation significantly outperforms other spherical methods and non-spherical method?", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 11, "text": "A2: The experiments on SHREC17 does show all three spherical methods slightly under-perform some other state-of-the art approaches, We believe this is mainly due to the information losses introduced in the spherical projection process which retains only the convex portion geometric information.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 12, "text": "Future improvements can be added by using:  (1) less lossy input in spherical projection methods (e.g. on top of SEF, and EDF, we can also add other statistic information such as the minimum distance of intersection, mean distance of intersections or standard deviation of the intersection and so on to reduce the information losses); (2) extend the spherical parameterization method on to the general 3D shapes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 13, "text": "Currently, the spherical parameterization method only works for genus-0 closed object.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 14, "text": "The 3D models presented on ModelNet and Shrec\u201917 are of arbitrary genus which prevents us from using spherical parameterization method.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 15, "text": "Generalization of spherical parameterization methods to objects with arbitrary topology will be one of the future work.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 16, "text": "Compared to non-spherical method, spherical image is one of the most compact representation for 3D shape analysis, the spherical convolution methods rely on no data augmentation (for Type I and Type II) or reduced data augmentation (for Type III, only SO(2) rotation augmentation is required).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 17, "text": "Other non-spherical method (such as volumetric or multi-view based methods)  can only be generalized into unknown orientations using SO(3) rotation augmentations, their representation of 3D shapes are either too sparse (voxel model) or too redundant (multi-view projections).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 18, "text": "Compared to SO(3) spherical convolution method (Cohen et al. 2018), our network is computationally efficient (in terms of network model), but we have to admit that this is at the price of required data augmentation", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 19, "text": ".", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_none", null]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 20, "text": "Another advantage is that our network allows local filters and local-to-global multi-level spherical features extraction.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 21, "text": "The other two spherical convolution methods (Cohen et al 2018 and Esteves et al 2018) use a lat-lon grid and conduct convolution in the Fourier space, which has a common disadvantage: the Fourier transform does not support local spherical filters.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 22, "text": "Another disadvantage is the use of lat-lon grid which introduces unevenness of the perception field (due to the high resolution near the poles, and low resolution at the equator).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 23, "text": "Experimentally, we compared the three spherical convolution methods in Table 3 using Shrec\u201917 perturbed shape retrieval experiment.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 24, "text": "Cohen et al 2018 and ours obtain similar performances because both of the methods used anisotropic filters, the former achieves rotational invariant using SO(3) rotations for filters, while ours achieves rotational invariant using alt-az rotation of filter and SO(2) rotation augmentation of input shapes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 25, "text": "As expected, anisotropic filter perform  better than the isotropic filter proposed in Esteves et al 2018 which limits the model capacity.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 26, "text": "Q3:  Is there a specific useful application where spherical methods in general outperform other approaches?", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 27, "text": "A3: As mentioned in Cohen et al 2018, perhaps the most exciting future application of the Spherical CNN is in omnidirectional vision.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [6]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 28, "text": "Although very little omnidirectional image data is currently available in public repositories, the increasing prevalence of omnidirectional sensors in drones, robots, and autonomous cars makes this a very compelling application of our work.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "HylYyr91T7", "rebuttal_id": "BkgnVCCK6Q", "sentence_index": 29, "text": "Omnidirectional vision is a better application to show the strength of the spherical convolution method.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 0, "text": "Thank you for your thoughtful review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 1, "text": "We have been hard at work to perform additional experiments to compare with other state of the art methods on a broader dataset.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 2, "text": "We summarize the changes here and will upload a revised the manuscript with complete quantitative evaluations before the revision deadline.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 3, "text": "Q1: More extensive evaluations", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 15]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 4, "text": "In response to your comments, we have trained and tested our method on the dataset provided in [1], using the F1 score rather than Chamfer Distance in accordance with the recommendations in that work.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 15]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 5, "text": "This dataset contains more than 4 times as many classes as our original dataset.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 15]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 6, "text": "We find that HOF is competitive with the performance of various state of the art methods reported in [1], showing the highest average F1 score out of all methods compared in [1].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 15]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 7, "text": "See Section 4.1.2 for added discussion, and the Appendix Sections A7 and A8 for complete class performance breakdowns for both our original experiments as well as the new comparison with [1].", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 15]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 8, "text": "We hope this extended comparison provides a more convincing experimental evaluation of HOF.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [13, 15]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 9, "text": "Q2: Technical description and justification", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 10, "text": "In the paper, we make the observation that codeword based approaches are equivalent to learning the biases of a fixed network, whereas the fast-weights-based HOF approach learns all of the weights.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 11, "text": "Therefore, we conclude that HOF is mathematically at least as general as codeword based architectures.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 12, "text": "We further show, with experiments, that the coding provided by HOF is more efficient than codeword-based approaches in terms of number of parameters in the decoder.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 13, "text": "There is similar evidence in the literature which suggests that fast-weights based approaches can be more efficient than static networks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 14, "text": "However at this point, similar to our paper, the evidence is empirical and a theoretical justification of this phenomenon is missing.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 15, "text": "In response to your comments, in our concluding remarks, we mention this lack of theoretical analysis and note it as an important direction for future research", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 16, "text": "Q3: Architecture of encoder/mapping network", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 17, "text": "In addition to experiments on a new dataset, we have performed new evaluations of variants of HOF on our original dataset to demonstrate that HOF performs competitively even when we change the encoder architecture, decoder depth, decoder activation function, or input sampling for the decoder network.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 18, "text": "For example, using Resnet18 as the encoder network gives almost identical performance in terms of average chamfer distance on our original test set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 19, "text": "The complete quantitative results of these comparisons will be included in an updated PDF before the end of the discussion period.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 20, "text": "Q4: Number of mapping layers", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 21, "text": "Our original results reported in Table 1 compare two different mapping function architectures.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 22, "text": "HOF-1 has one hidden layer with 1024 units, HOF-3 has 3 hidden layers with 128 units each.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 23, "text": "We have updated the text to clarify this distinction.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 24, "text": "In response to your comments, we have also conducted an additional experiment with a mapping network with 6 hidden layers with 128 units each; the test performance of this architecture is almost identical to that of HOF-3 (1.2485 average Chamfer distance with 6 layers compared with 1.247 average CD for 3 layers).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 25, "text": "Q5: Vanishing/exploding gradients", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 26, "text": "In all of our experiments, we address the problem of vanishing/exploding gradients by dividing by the square root of the in-degree of each neuron (as in [2]).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 27, "text": "Using the same initialization in the encoder network, we find that training a mapping function with 6 hidden layers (\"HOF-6\") trained easily with no modifications to our training code.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 28, "text": "Another advantage of HOF over deeper, fixed decoder architectures is that it admits extremely shallow decoders, which require less careful tuning of hyperparameters such as initialization scaling and normalization compared with deeper networks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 29, "text": "For this work, our goal was not necessarily to find the optimal architecture for the decoder, but rather to demonstrate that the usage of the higher-order function paradigm allows for a much smaller decoder architecture than LVC methods.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 30, "text": "Thank you for bringing the additional literature to our attention.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [16, 17, 18, 19]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 31, "text": "We have included it in our discussion of related work in a revised manuscript.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 17, 18, 19]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 32, "text": "We have also updated the text to more clearly explain the path-based evaluation and its motivation.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 17, 18, 19]]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 33, "text": "We hope that these additional experiments better demonstrate the effectiveness of HOF as a competitive, parameter-efficient 3d reconstruction paradigm.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 34, "text": "Thank you again for your feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 35, "text": "[1] M. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, and T. Brox, \u201cWhat do single-view 3d reconstruction networks learn?,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3405\u2013 3414, 2019.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HyxflLtAYB", "rebuttal_id": "r1gIbEt9jr", "sentence_index": 36, "text": "[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 0, "text": "Thank you for your thorough assessment and helpful comments! To answer your two questions:", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 1, "text": "1) Upper bound", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [59]]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 2, "text": "You are right that it would be ideal to optimize the target loss L_T(h, f_T) directly.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [59]]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 3, "text": "However, this is not possible because we do not have labelled target data (i.e., f_T is unknown).", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [59]]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 4, "text": "Minimizing an upper bound is arguably the only viable option *with theoretical generalization guarantee*. It is a common practice in the domain adaptation community (Mansour et al., 2009a, 2009b; Ben-David et al. 2007, 2010; Cortes and Mohri, 2011), and it is essentially the key idea of PAC learning and generalization analysis (Gy\u00f6rfi et al., 2006; Sch\u00f6lkopf et al., 2002; Vapnik, 2013).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [59]]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 5, "text": "Besides, our method *directly* optimizes the upper bound without resorting to heuristics, unlike prior methods.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [59]]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 6, "text": "Ref:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 7, "text": "- Gy\u00f6rfi, L., Kohler, M., Krzyzak, A. and Walk, H., 2006. A distribution-free theory of nonparametric regression. Springer Science & Business Media.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 8, "text": "- Sch\u00f6lkopf, B., Smola, A.J. and Bach, F., 2002. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 9, "text": "- Vapnik, V., 2013. The nature of statistical learning theory. Springer science & business media.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 10, "text": "2) More experiment", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 11, "text": "Thank you for pointing out these datasets.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [62, 63]]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 12, "text": "We have conducted further experiments on the Office-Home dataset,  using the ResNet50 as the backbone architecture and changing the classification head to 65 classes.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [62, 63]]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 13, "text": "As we can see in the new Section 5.3, our method achieve state-of-the-art performance and outperform all alternatives", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [62, 63]]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 14, "text": "; these results are statistically significant.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [62, 63]]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 15, "text": "In addition, we have added one more competitive baseline (M3SDA) from the DomainNet paper you mentioned, using their public code with a few necessary adjustments for each dataset (e.g., network architecture, etc).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [62, 63]]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 16, "text": "We ensure that all methods use the same backbone architecture for a fair comparison.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [62, 63]]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 17, "text": "Again, our method outperform M3SDA in all datasets.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [62, 63]]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 18, "text": "If you have any other comments or concerns, we are happy to provide further feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Hyxp9P_RuH", "rebuttal_id": "BkeUECAvsr", "sentence_index": 19, "text": "Thank you!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 0, "text": "We thank the reviewer for their comprehensive review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 1, "text": "We updated the paper with better results over more tasks, either matching or outperforming the human baseline in terms of final accuracy, and outperforming the model-free baseline in all cases.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 2, "text": "We also included results over multiple runs of all experiments, showing the minimum, maximum and mean accuracy.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 3, "text": "1. While it is true that the manually-tuned baseline we provided is simple, it is a standard practice in the field to adjust the learning rate during training and keep the rest of the hyperparameters constant.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 4, "text": "Adjusting all of them requires significantly more effort and is infeasible in many cases.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 5, "text": "2. Due to time constraints, we have not benchmarked our method against more hyperparameter-tuning baselines yet.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 6, "text": "We agree that it would be a very valuable comparison and leave that for future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [15]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 7, "text": "Nevertheless, please note that the human baselines we use for Transformer have been tuned by researchers using auto-tuners among other tools.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 8, "text": "3. [1] successfully use PPO with an LSTM policy on a challenging, partially-observable environment.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 9, "text": "It is equally principled to use a Transformer policy, since both would operate on the same sequence of observations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 10, "text": "The SimPLe algorithm runs PPO on an MDP approximated by a powerful model that handles stochasticity well, which is also a valid approach.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 11, "text": "4. We updated the paper with a justification of our action discretization scheme.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 12, "text": "Such a discretization has a number of benefits, including multi-modality, which cannot be achieved using a parameterized Gaussian policy.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 13, "text": "[2] show that discretization of the action space improves the average performance, stability and robustness to hyperparameters of reinforcement learning agents on a range of continuous control tasks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 14, "text": "5. While we have not included such transfer experiments in our current work, we do believe that a model trained on enough architectures and tasks will generalize to new ones.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 15, "text": "For instance, in the updated version of the paper, we show that the learned policy employs similar learning rate and weight decay rate adjustment schemes across very different tasks.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 16, "text": "Substantiating this claim in the general case will likely require a large-scale study, which we plan to perform in the future.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [18]]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 17, "text": "[1] OpenAI et al. \u201cLearning Dexterous In-Hand Manipulation\u201d, arXiv preprint arXiv:1808.00177 (2018)", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Hyxs5tB0FS", "rebuttal_id": "HJlRC3djiB", "sentence_index": 18, "text": "[2] Tang et al. \u201cDiscretizing Continuous Action Space for On-Policy Optimization\u201d, arXiv preprint 1901.10500 (2019)", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "HyxsH-ORFH", "rebuttal_id": "BkxWkgY2sB", "sentence_index": 0, "text": "Thank you for your comments!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 0, "text": "Thank you for the fruitful comments!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 1, "text": "We addressed your main concern and updated Section 1 of the paper to better situate it in the existing literature.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 2, "text": ">> Would it be possible to train the vq-wav2vec model jointly with BERT, i.e. as one model? [...] Similarly to the above question, would there be a way to incorporate the BERT principles directly into an end-to-end model, e.g. by randomly masking some of the continuous input speech?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 3, "text": "The focus of this paper is a quantization approach for audio.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 4, "text": "Replacing the two-step training process by an adaptation of BERT to continuous data (using a wav2vec/CPC-like objective function instead of the cross entropy) is an interesting direction for future work (and we amended the future work section accordingly).", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 5, "text": "However, our current paper is a proof of concept that a pre-training scheme based on masked inputs (BERT) can improve over previous methods in the speech domain.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 6, "text": ">> What exactly does \"mode collapse\" refer to in this context?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [28]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 7, "text": "In several configurations (especially for one and two groups) considerably less codewords than theoretically possible are used.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [28]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 8, "text": "We loosely refer to mode collapse as the phenomenon when very few codewords per group are used (cf. Appendix A).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [28]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 9, "text": "We updated the paper to also refer to the appendix where we outline the number of codewords that the model uses.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [28]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 10, "text": "We observed that in the \u201cfew group regime\u201d (G=1...4), only a few of the available centroids per group are used and refer to this phenomenon as mode collapse \u2014 for BERT training, this is actually favorable e.g. in the G=2, V=320 setting as it yields a codebook of acceptable size for NLP model training (13.5k/23k).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [28]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 11, "text": "Mode collapse could potentially be circumvented by strategies like embedding re-initialization used in classical k-means and this is an interesting avenue for future work.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [28]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 12, "text": ">> [...] BERT is required on top of the vq-wav2vec discrete symbols.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 13, "text": "Is it possible that the output acoustic model is simply better-matched to continuous rather than discrete input (direct vq-wav2vec gives discrete while BERT gives continuous)? Would it make sense to train the wav2vec acoustic model on top of the vqvae codebook entries (e) instead of directly on the symbols?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 14, "text": "We actually did what you suggest: when we train acoustic models on top of vq-wav2vec, we input the dense embedding vectors corresponding to the discrete codewords.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 15, "text": "On the other hand, we also trained an NLP sequence to sequence (Section 6.3) which takes the quantized audio codes as input and then generates the transcriptions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 16, "text": "This gives reasonable accuracy and suggests that the discrete codes by themselves, and without the learned continuous representations, are useful.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 17, "text": "We clarified this in the updated version of the paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 18, "text": "We believe the reason the dense embeddings for the discrete codewords work less well", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 19, "text": "is because they do not encode as much detailed context information as a representation built by wav2vec or BERT.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "r1e9Ipmo_r", "rebuttal_id": "ryllakFnsS", "sentence_index": 20, "text": "The information in the codebook is ultimately less detailed than a context vector specific to the current input sequence.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 0, "text": "We thank the reviewer for the comments and feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 1, "text": "We will also include the suggested experiment that shows the plug-and-play nature of PMN.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 2, "text": "1. Residual modules", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 3, "text": "- Residual modules are small neural networks (e.g., an MLP for Mvqa, Sec. 3.4, (4)) that a task module may use when other lower level modules are incapable of providing a solution to a given query.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 4, "text": "For example, consider the question \u201cis this person going to be happy?\u201d on an image of a person opening a present.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 5, "text": "Lower level modules of Mvqa may not be sufficient to solve the question.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 6, "text": "Therefore, Mvqa would make use of its residual module, which would essentially learn to \u201cpick up\u201d all queries that lower level modules cannot answer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 7, "text": "2. Effect of fine-tuning", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 8, "text": "- While it might be beneficial to fine-tune the modules for a specific parent task we want each module to be an expert for their own task as it facilitates a plug-and-play architecture.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 9, "text": "Fine-tuning may push the modules towards blindly improving parent module\u2019s performance but (i) badly affect interpretability of inputs and outputs; and (ii) may also reduce the lower module\u2019s performance on its own task.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 10, "text": "Most importantly, it would not scale with the number of tasks, as for each task the agent would need to keep several fine-tuned modules of the lower tasks in memory.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 11, "text": "3. Feeding in the ground-truth", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 12, "text": "- Thanks for this great suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 13, "text": "We performed an experiment where we evaluate the benefits that the VQA model may achieve by using ground-truth captions instead of captions generated by the caption module.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 14, "text": "Our preliminary experiments show a gain of about 2.0% which is a relatively high gain for VQA.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "r1eaptK5hm", "rebuttal_id": "SJlY63MSpX", "sentence_index": 15, "text": "This points to important properties of the PMN allowing human-in-the-loop type of continual learning, where a human teacher can pinpoint flaws in the reasoning process and potentially help the model to fix them.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 0, "text": "Thank you for your detailed review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 1, "text": "On releasing a structured (parsed) form of the dataset: we agree that examining performance on structured input is a very useful exploration direction, that can give insight into what effect parsing has on ease of training.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 2, "text": "We feel, however, that there\u2019s no single canonical choice for the structure that may be suitable for all types of networks (e.g., tree networks, graph networks, etc), or different levels of structure that aid the network to different amounts, from completely unstructured to tree-like structures that essentially determine the required order of calculation.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [10]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 3, "text": "For example, in the question type of \u201cmultiple function composition\u201d, one could have a structure that lists the functions, and also the desired composition order; or one could actually have a tree structure with the functions already embedded in the correct composition order (which we suspect would be quite easy to learn models on).", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [10]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 4, "text": "In lieu of this, we hope the released dataset source code will allow researchers to easily tailor the dataset to their specific problems and models.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 5, "text": "We have rewritten the section describing the neural models, with clearer terminology, and the differences between the different models made much more explicit.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 6, "text": "Thank you for pointing this out, and please let us know if any parts are still unclear.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [10]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 7, "text": "The \u201cattentional LSTM\u201d model is just the standard encoder/decoder+attention architecture prevalent in neural machine translation as introduced in \u201cNeural machine translation by jointly learning to align and translate\u201d (Bahdanau et al).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 8, "text": "However, we confusingly used the terms \u201cparser\u201d instead of \u201cencoder\u201d, and we have fixed the description.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 9, "text": "On running the decoding LSTM for a few steps before outputting the answer: we found that it was one of the few (relatively simple) architectural changes to the standard recurrent encoder/decoder setup that significantly helped performance (thus the performance on the standard architecture can be taken to be slightly worse than the numbers reported in the paper for the architecture with \u201cthinking steps\u201d), but we also realize that it is not a widespread architectural change. (Possibly the need for this is less in standard machine translation tasks.) Since your review, we have also ran experiments using the published architecture introduced in \u201cAdaptive Computation Time for Recurrent Neural Networks\u201d (Graves).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 10, "text": "This architecture has an adaptive number of \u201cthinking\u201d steps at every timestep dependent on the input, learnt via gradient descent.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 11, "text": "More specifically we investigated the use of this for both the recurrent encoder and decoder (replacing the single fixed number of \u201cthinking\u201d steps at the start of the decoder).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 12, "text": "After some tuning, its test performance was still around 3% worse than the same architecture without adaptive computation time.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 13, "text": "We\u2019ve updated the paper to mention this.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 14, "text": "Please refer to the updated PDF of the paper to see these changes.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "r1eF1-xjh7", "rebuttal_id": "BJeVJhFxRX", "sentence_index": 15, "text": "We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 0, "text": "(1) Prior Network:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 1, "text": "During training phase, we sample from prior network to generate \"pseudo\" observations for unobserved modalities.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 2, "text": "The pseudo observations are then used to estimate the conditional likelihood for such modalities (E_x_j in the ELBO).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 3, "text": "Practically, we follow a two-stage method in our implementation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 4, "text": "At each iteration, the first stage imputes unobserved modalities (with latent code sampled from approximate posterior for observed modalities, and prior for unobserved modalities), followed by the second stage to estimate ELBO based on the imputation and backpropagate corresponding gradients.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 5, "text": "(2) Conditioning on Ground-Truth Mask:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 6, "text": "We conduct experiments with decoder p(x|z, m) conditioned on the original mask in training set, and observe comparable performance and convergence time.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 7, "text": "The mask distribution might be easier to learn as compared to data distribution (since the mask is fully-observed)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 8, "text": ".", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 9, "text": "However, we argue that jointly learning the mask distribution and data distribution provides us an opportunity to further analyze the missing mechanism and potentially can facilitate other down-stream tasks.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 10, "text": "(3) Image Inpainting:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 11, "text": "We appreciate the reviewer's suggestion on evaluate the effectiveness of our model on image inpainting task.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [11]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 12, "text": "However, with our current setup, an encoder is trained for each modality respectively, making it difficult to scale to inpainting task, if we treat each pixel as an individual modality.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [11]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 13, "text": "Nevertheless, we believe this is an interesting extension.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [11]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 14, "text": "The backbone models and mathematical formulations can be very similar, if not the same.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "r1gk4ck25B", "rebuttal_id": "ryxqjgdujS", "sentence_index": 15, "text": "A potential solution could be to employ patch level encoders to reduce the total number of encoders needed.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 0, "text": "We thank reviewer 1 for the detailed feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 1, "text": "In this response, we clarify the accuracy-realism trade-off, revise the accuracy metrics, indicate reruns and new experiments, and address the individual questions.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 2, "text": "We updated Section 4.4 to indicate that it is to be expected that, although our SAVP model improves on diversity and realism, it also performs worse in accuracy compared to pure VAE models (both our own ablation and SVG).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 11, 13]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 3, "text": "A recent result [1] proves that there is a fundamental tradeoff between accuracy and realism, for all problems with inherent ambiguity.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 11, 13]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 4, "text": "In fact, a recent challenge held at ECCV 2018 in such a problem [2] evaluates all algorithms on both of these axes, as neither adequately captures performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 11, 13]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 5, "text": "Although the SVG generator is simpler than ours, ours is just a simple variation from Ebert et al. (2017).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 11, 13]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 6, "text": "Since proposing a strong generator architecture is not the goal of this paper,", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 11, 13]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 7, "text": "any video generator (including the one from Denton & Fergus (2018)) could be used with our losses.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 11, 13]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 8, "text": "We added this clarification to Section 3.4.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 11, 13]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 9, "text": "Instead, we provide a systematic analysis of the effect of the loss function on this task (which could be applied to any generator).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 11, 13]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 10, "text": "It's also worth noting that with a simpler feed-forward posterior and a unit Gaussian prior, our VAE ablation and SVG achieve similar performance on various metrics.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 11, 13]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 11, "text": "We added Section 3.5 to point out the differences between the VAE component of our model and prior work.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 12, "text": "We have included a revised plot in Figure 14 (note that this temporary plot will be incorporated into Figure 6), where we use the official implementation of SSIM and replace the VGG metric with the LPIPS metric (Zhang et al., 2018).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 13, "text": "LPIPS linearly calibrates AlexNet feature space to better match human perceptual similarity judgements.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 16, 17, 18]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 14, "text": "Aside for the first two predicted frames, our VAE ablation and the SVG model both achieve similar SSIM and LPIPS.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 16, 17, 18]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 15, "text": "After examining the KTH results further, we realized that our results are likely weaker than they should have been, because we did not use the same preprocessing as prior work.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 16, 17, 18]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 16, "text": "The experiments from our original submission cropped the videos into a square before resizing, and thus discarded information from the sides of the video.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 16, 17, 18]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 17, "text": "We are currently rerunning the KTH experiments and we plan to update the results in the paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [14, 16, 17, 18]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 18, "text": "We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 16, 17, 18]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 19, "text": "Although the combination of VAEs and GANs have been explored recently for conditional image generation (Zhang et al. 2018), the video prediction task is substantially different, with unique challenges, due to spatiotemporal relationships and inherent compounding uncertainty of the future.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 20, "text": "Furthermore, while the individual components have indeed been known for video prediction, their combination is novel and not present in prior work, and we demonstrate that this produces state-of-the-art results in terms of diversity and realism.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 21, "text": "In addition, this work provides a detailed comparison of the effect of the losses on the various metrics.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 22, "text": "Furthermore, we are currently running experiments for various weightings of the KL loss and the adversarial loss, and we plan to include additional results that illustrate the trade-offs based on these hyperparameters.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 23, "text": "Although MoCoGAN performs well for videos with a single frame-centered actor, it struggles with multiple simultaneously moving entities.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 24, "text": "The authors of MoCoGAN also mentioned in personal correspondence that the conditional version (i.e. video prediction) was significantly harder to train.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 25, "text": "We noticed the same in earlier iterations of our model.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 26, "text": "In our case, we found that the model would degenerate to static videos or videos with a cyclic flickering artifact, which are issues that aren't a problem in conditional image generation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 27, "text": "We added details to Section 3.4 describing the importance of a few components, such as spectral normalization and not conditioning the discriminator in the ground-truth context frames.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 28, "text": "The purpose of adding adversarial losses to a pure VAE is to improve on blurry predictions where the latent variables alone cannot capture the uncertainty of the data.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10, 14, 14, 16, 17, 18]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 29, "text": "However, that is typically not the case of synthetic datasets.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 14, 14, 16, 17, 18]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 30, "text": "In early experiments, we trained our pure VAE model on the stochastic shape movement dataset from Babaeizadeh et al. (2018), and our pure VAE was able to model the dataset without any blur and with perfect separation of the possible futures.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 14, 14, 16, 17, 18]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 31, "text": "We agree that plausibility is indeed important, and that's what our human subject studies try to capture.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 21]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 32, "text": "Since we provide predictions of the whole sequence to the human evaluator, we are not only evaluating for image realism but also for plausibility of the dynamics.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 21]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 33, "text": "Unlike the VAE models that implausibly erase the small objects that are being pushed in the BAIR dataset, our SAVP model moves those objects in a more plausible way.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 21]]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 34, "text": "[1] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Conference on Vision and Pattern Recognition (CVPR), 2018. https://arxiv.org/abs/1711.06077", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_global", null]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 35, "text": "[2] Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. 2018 PIRM Challenge on Perceptual Image Super-resolution. In Perceptual Image Restoration and Manipulation (PIRM) workshop at ECCV 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_global", null]}, {"review_id": "r1gzmeTDnm", "rebuttal_id": "rJexhWRXCm", "sentence_index": 36, "text": "https://arxiv.org/abs/1809.07517", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_global", null]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 0, "text": "We thank the reviewer for their helpful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 1, "text": "Please could the reviewer clarify why they felt our work muddies the debate regarding large-batch training?", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [4]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 2, "text": "We demonstrate that one can initially increase the batch size with no loss in test accuracy by simultaneously increasing the learning rate.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 3, "text": "However for very large batch sizes the test accuracy degrades under both constant epoch and constant step budgets.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 4, "text": "We agree that some of our observations under constant epoch budgets in sections 2 and 3 have been made in previous work.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 5, "text": "However there are also several important differences:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 6, "text": "1.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 7, "text": "Our paper is the first to relate the two regimes of SGD to the popular analogy between SGD and stochastic differential equations (SDEs).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 8, "text": "As we show in sections 4 and 5, this perspective is crucial to understanding the influence of batch size and learning rate on test accuracy.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 9, "text": "A common criticism of this analogy is that SGD noise is not Gaussian when the batch size is small.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 10, "text": "To our knowledge, we are the first to show that the analogy between SGD and SDEs holds for non-Gaussian short-tailed noise (appendix B).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 11, "text": "2. Zhang et al. argued that Momentum only helps in the large batch limit.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 12, "text": "However, their analysis is based on the noisy quadratic model, which cannot explain the results we observed on the test set in sections 4 and 5.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 13, "text": "These experiments clearly demonstrate that, unlike the SDE perspective, the noisy quadratic model is not an appropriate model for predicting test set performance in deep learning.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 14, "text": "Their work also does not clarify the assumptions under which linear scaling of the learning rate should arise.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 15, "text": "3. Our empirical results in section 3 are similar to Shallue et al., however their work argues that there is no reliable relationship between learning rate and batch size.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 16, "text": "We draw a very different conclusion: the learning rate usually obeys linear scaling, but linear scaling only holds theoretically when the assumptions we specify are satisfied.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 17, "text": "Linear scaling may not hold in cases where these assumptions break down (e.g., language modelling).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 18, "text": "4. The observation that the test accuracy is independent of batch size in the noise dominated regime is a natural consequence of the SDE analogy, since any two training runs which integrate the same SDE should sample final parameters from the same probability distribution.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 19, "text": "We will clarify this in the updated text.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 20, "text": "Two reviewers complained that it was difficult to tell from the text which contributions are novel and which also appear in previous works.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 21, "text": "We apologise for this.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_global", null]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 22, "text": "It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_global", null]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 23, "text": "Turning to our generalization experiments in sections 4 and 5.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 24, "text": "It is true that a number of papers in recent years have claimed that SGD noise enhances generalization.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 25, "text": "However Shallue et al. recently argued no previous work had provided convincing empirical evidence for this claim.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 26, "text": "Indeed in their abstract, they state \u2018We find no evidence that larger batch sizes degrade out-of-sample performance\u2019.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 27, "text": "In another recent paper, Zhang et al. argued that optimization in deep learning is well described by a noisy quadratic model which predicts that increasing the batch size should always enhance performance under constant step budgets.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 28, "text": "Crucially, to establish that SGD noise enhances generalization, one must show that small batch sizes generalize better than large batch sizes under constant step budgets, with realistic learning rate decay schedules, and one must independently tune the learning rate at each batch size.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 29, "text": "In section 4, we are the first authors to perform this experiment and confirm that the final test accuracy of SGD does degrade for very large batch sizes under both constant epoch and constant step budgets, contradicting the claims of both Shallue et al and Zhang et al.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 30, "text": "Furthermore, we show in section 5 that the optimal SGD temperature which maximizes the test accuracy is almost independent of the epoch budget.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 31, "text": "These results provide the first convincing empirical evidence that SGD noise does enhance generalization in well-tuned networks with learning rate decay schedules.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1l1CEFwKr", "rebuttal_id": "ryltuFZ7sr", "sentence_index": 32, "text": "We believe this is an important contribution.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 0, "text": "We thank the reviewer for the detailed comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 1, "text": "\u201c1.[...] without sufficient justification the assertion in the paper that the capacity of the network is well approximated by the number of parameters does not seem correct.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 24, 26]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 2, "text": "Also, the claim in Fig. 1 that the transition from \u2018\u2019high capacity\u2019\u2019 to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand (*)\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 24, 26]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 3, "text": "We agree raw parameter count is not a fine estimate of the capacity of the network.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [24, 24, 26]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 4, "text": "However, an information-theoretic argument shows that an upper-bound of the capacity is the raw parameter count times the size of the representation (i.e. 32 bits for float32, this argument is close to that of [A]).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [24, 24, 26]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 5, "text": "Experimentally, we show that networks with no data-augmentation (figure 1 - purple curve) stop fitting perfectly when the parameters get within 1/10 of the quantity of information in the learning set, thus we think that raw parameter count is a good first-order approximation up to that factor.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [24, 24, 26]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 6, "text": "\u201c2. Sec. 3.3, [...] capacity alone cannot explain why VGG converges faster than Resnet-18 [...]\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [28, 28]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 7, "text": "We observe that the rate of memorization depends on the architecture and the optimization algorithm, but predicting or explaining this rate is beyond the scope of this paper.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [28, 28]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 8, "text": "\u201c(*) 3. Scenario discussed in Sec. 4 seems somewhat impractical. [...] one might also need to figure out if it is neither train nor val\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [31, 31]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 9, "text": "In section 4, we do not train a classifier to distinguish between a training and a validation set.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [31, 31]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 10, "text": "Rather, we use a readily-available classifier (trained for e.g. image recognition) for a completely different purpose than what it was trained for, i.e. to distinguish datasets of images (section 4.1) or detect if a set of images comes from a given set (section 4.2).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [31, 31]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 11, "text": "Section 4.2 shows how to use the K-S test to detect leakage, but the same test could tell if the m-set comes from neither the train nor the validation sets.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [31, 31]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 12, "text": "\u201c4. Fig. 3 (right): It is not clear why the fact that the classifier is able to predict which dataset the image \u2018m\u2019 corresponds to is useful or practical, as this seems to be a property of the set \u2018m\u2019 rather than the property of the trained classification model (f_\\theta). Please clarify. [...]\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [33, 33, 33, 36, 37, 37]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 13, "text": "Being able to tell from the classifier output (using e.g. the confidence) if a set of images comes from the training or the validation set is a good indicator of how much the network has memorized these images.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [33, 33, 33, 36, 37, 37]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 14, "text": "In our opinion, the most important outcome of the experiment of section 4.1 (figure 3, right) is to determine how many samples are needed to reliably discriminate the training set from the validation set (this corresponds to the solid curves), which is related to how much the model has memorized images from the training set.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [33, 33, 33, 36, 37, 37]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 15, "text": "\u201c6. Does [section 5] use the classifier for predicting the dataset, or is the approach reported in the section, the MAT approach?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [42]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 16, "text": "The baseline approaches make use of the loss of the model (which is not the same as the confidence).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [42]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 17, "text": "The proposed approach uses the lower layers of the original model, and upper layers learnt on a separate, public set (this is the \u201cpartial-layers\u201d setting).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [42]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 18, "text": "Table 3 reports the results of the Bayes method on top of this network with upper layers retrained, as the MAT usually gives similar results on this task (for instance, Table 4 reports 60.8% performance with Softmax + Flip, Crop on Resnet101 for the Bayes method, and the MAT gets 61.14%).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [42]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 19, "text": "\u201c7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images\u2019\u2019 -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen\u2019\u2019 images which it labels as the negative class, thus the negative class is also seen by the memorization model.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 20, "text": "(*)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 21, "text": "\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 22, "text": "We feed our model an equal number of positives and negatives (chosen randomly) at each epoch.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 23, "text": "For n < 10K, after 300 epochs the model has seen at most 3M negatives out of 15M, and yet still generalizes to the unseen negatives.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 24, "text": "\u201c1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization\u2019\u2019.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [47]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 25, "text": "In addition, the paper would also need to show that such a model does not generalize to a validation set of images. [...]\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [47, 48]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 26, "text": "With the downstream application of sections 4 and 5, we are interested in \u201cmemorization\u201d in the sense of any classifier that can tell apart images marked as \u201cpositives\u201d from images marked as \u201cnegatives\u201d.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [47, 48]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 27, "text": "This notion is somewhat different from", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [47, 48]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 28, "text": "memorization as defined in other papers, where it is related to having a good training accuracy and a a validation accuracy close to random guessing.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [47, 48]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 29, "text": "With the setup used in section 3, there is no good notion of validation: our model is expected to predict \u201c0\u201d on held-out data.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [47, 48]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 30, "text": "\u201c2. Figure 3: [the term CDF] is confusing\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [50]]}, {"review_id": "r1l7E6X22m", "rebuttal_id": "Hyls7CabCQ", "sentence_index": 31, "text": "We will update the caption to make it less ambiguous.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [50]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 0, "text": "Thank you for your insightful comments. We have incorporated your suggestions into the revised version of the paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 1, "text": "Q: Relationship to optimizers with adaptive learning rates, and comparison between Adam and Adam-APO.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 14]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 2, "text": "While Adam and Adagrad are often described as having \u201cadaptive learning rates,\u201d they still have a global learning rate that is just as critical to tune as for SGD.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [10, 14]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 3, "text": "In our experiments, we consider tuning the learning rate for RMSprop, which also maintains adaptive learning rates for each parameter, and is closely related to Adam/Adagrad.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [10, 14]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 4, "text": "Adam is essentially RMSprop with momentum; APO can be applied to Adam by applying momentum on top of the updates computed by APO.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [10, 14]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 5, "text": "To address your question about Adam, we added experiments for tuning the global learning rate of Adam with APO in appendix Section G, Figure 14, where Adam-APO achieves better performance than Adam with a fixed global learning rate, and achieves comparable performance as Adam with a manual schedule.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 6, "text": "Q: Comparison with population-based training (PBT)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 7, "text": "We have added a comparison between APO and PBT in appendix Section H, Figure 15.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 8, "text": "For population-based training, one must carefully select many hyperparameters, including the size of the population, the perturbation strategy (e.g., randomly perturb the learning rate by multiplying it by 1.2 or 0.8), the exploration interval (e.g., the number of training iterations to run before exploiting other members of the population).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 9, "text": "We used PBT and APO to tune the learning rate of RMSprop while training a ResNet34 model on CIFAR-10.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 10, "text": "For PBT, we used a population of size 4, and chose to exploit/explore after each epoch of training.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 11, "text": "We tried multiple exploration strategies, and found that it was critical to set the probability of resampling a learning rate from an underlying distribution to be 0; otherwise, the learning rates could jump from small to large values, and yield unstable training.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 12, "text": "In contrast, APO only requires a simple grid search over lambda, and all other hyperparameters can be kept at their default settings.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 13, "text": "We found that  APO substantially outperformed PBT, achieving a lower final training loss and equal test accuracy in much less wall-clock time; this shows the advantage of gradient-based methods for tuning learning rates, such as APO, compared to evolutionary methods based on random perturbations such as PBT.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 14, "text": "Q: The convergence results appear to rely on strong convexity of the loss.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 15, "text": "How is this a reasonable assumption?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 16, "text": "Note that we assume strong convexity of the loss as a function of the output units, not as a function of the weights.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 17, "text": "Hence, our assumption is fairly realistic in the neural net setting.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 18, "text": "The loss function on top of the network output is usually defined as a simple convex function; for instance, in regression, a common choice of loss function is the quadratic loss (i.e, the squared distance between the network output and the true label), which is strongly convex.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 19, "text": "In fact, even without assuming that the loss function is strongly convex and that the output manifold is dense, we are still able to show a fast convergence rate.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 20, "text": "In the updated version of the paper, we show that our algorithm with an oracle converges to stationary point globally with a fast rate, which provides insight into why APO works well.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 21, "text": "Q: In your experiments, you set the learning rate to be really low. What happens if you set it to be arbitrarily high? Can you algorithm recover good learning rates?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 22, "text": "APO is robust to the initial learning rate of the base optimizer, using the default meta learning rate suggested in our updated paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 23, "text": "We have added a section to the appendix in which we include RMSprop-APO experiments on Rosenbrock, MNIST, and CIFAR-10 to show that the training loss, test accuracy, and learning rate trajectories are nearly identical when starting with initial learning rates {1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7}, spanning 5 orders of magnitude.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18]]}, {"review_id": "r1labrqphm", "rebuttal_id": "rkxxb72KCX", "sentence_index": 24, "text": "Note that 1e-2 is quite a large initial learning rate for RMSprop.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18]]}, {"review_id": "r1lORJDq3m", "rebuttal_id": "H1gQCvdvTm", "sentence_index": 0, "text": "Thank you for your review and very useful comments! We\u2019re happy you found our manuscript interesting.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1lORJDq3m", "rebuttal_id": "H1gQCvdvTm", "sentence_index": 1, "text": "To address your comments:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1lORJDq3m", "rebuttal_id": "H1gQCvdvTm", "sentence_index": 2, "text": "1) Thank you for pointing out that we had not defined the delta.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "r1lORJDq3m", "rebuttal_id": "H1gQCvdvTm", "sentence_index": 3, "text": "Here delta is the Kronecker delta defined so that \\delta_{a,b} = 1 if a = b and 0 if a != b. In the context of the variance of the multivariate normal distribution, the delta function indicates that the different neurons in each layer have zero covariance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "r1lORJDq3m", "rebuttal_id": "H1gQCvdvTm", "sentence_index": 4, "text": "We\u2019ll add an explicit discussion of this fact to the manuscript.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [8]]}, {"review_id": "r1lORJDq3m", "rebuttal_id": "H1gQCvdvTm", "sentence_index": 5, "text": "2) Thanks for pointing this out, we\u2019ll correct it in the next revision.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [9]]}, {"review_id": "r1lORJDq3m", "rebuttal_id": "H1gQCvdvTm", "sentence_index": 6, "text": "3) It is true that the extent to which randomized weights describe trained networks is unclear.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "r1lORJDq3m", "rebuttal_id": "H1gQCvdvTm", "sentence_index": 7, "text": "However, it is true that most commonly used weight initialization schemes are random.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "r1lORJDq3m", "rebuttal_id": "H1gQCvdvTm", "sentence_index": 8, "text": "For example, He initialization [1] and Xavier initialization [2] strategies are both special cases of the setup considered here.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "r1lORJDq3m", "rebuttal_id": "H1gQCvdvTm", "sentence_index": 9, "text": "We therefore view our theory as a theory of neural networks at initialization.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "r1lORJDq3m", "rebuttal_id": "H1gQCvdvTm", "sentence_index": 10, "text": "(There are, however, initialization schemes that are not random and that are not described by our theory).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "r1lORJDq3m", "rebuttal_id": "H1gQCvdvTm", "sentence_index": 11, "text": "[1] K. He, X. Zhang, S. Ren, J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. (http://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html)", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1lORJDq3m", "rebuttal_id": "H1gQCvdvTm", "sentence_index": 12, "text": "[2] X. Glorot, Y. Bengio, Y. W. Teh, M. Titterington. Understanding the difficulty of training deep feedforward neural networks. (http://proceedings.mlr.press/v9/glorot10a.html)", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 0, "text": "Thank you for your review!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 1, "text": "> You have the phrase \"allowing to imagine thousands of trajectories in parallel\". I would like some elaboration on this. I think you have ideas of what is happening in the latent space that I am not following.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 2, "text": "The latent states are defined as 330 dimensional activation vectors with 300 deterministic and 30 sampled components.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 3, "text": "We can predict imagined trajectories for thousands of initial states in parallel since they fit into the memory of the GPU at once.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 4, "text": "Specifically, Dreamer predicts imagined trajectory of length 20 from each of the 50x50=2500 latent states for the current training batch.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 5, "text": "Performing the same amount of imagination steps with a dynamics model that generates images during inference would be challenging.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 6, "text": "For example, we could only fit up to 500 trajectories of length 10 into GPU memory with the SV2P model (Babaeizadeh et al. 2017).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 7, "text": "Besides the memory constraints for predicting multiple trajectories in parallel, predictions in the latent space are often an order of magnitude faster than in pixel space.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 8, "text": "> I find the learning in the latent space the important part and there are things like how much simulation is done in the latent learning not clearly spelled out. How does the effort compare to the 1E9 steps of the base line your refer to?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 9, "text": "We will include more details in the final version.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [4]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 10, "text": "Dreamer was run for 2e6 environment steps (20 hours) compared to D4PG that was run for 1e9 environment steps (24 hours).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 11, "text": "Both algorithms used a single GPU each.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 12, "text": "As outlined in our previous answer, Dreamer performs 10 billion imagination steps throughout training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 13, "text": "Please note that imagination steps are often considered free for robotic learning, because the bottleneck is the time of physical interaction with the real world.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 14, "text": "> [...] understanding what structures get learned in latent space, are the in fact compact, diverse?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 15, "text": "The amount of information in the latent representation is upper bounded by the KL divergence loss.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 16, "text": "We observed a typical KL divergence of 15 bits per time step,", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 17, "text": "compared to the 64 x 64 x 3 x 8 bits of the corresponding images.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 18, "text": "This bounds the compression ratio to at least 1 : 6500.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 19, "text": "We make no claims regarding diversity.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 20, "text": "However, since the behaviors are learned purely in latent space, there must be a sufficient amount of diversity to solve the presented tasks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 21, "text": "We agree that exploring the semantics of the latent space is an interesting orthogonal direction for future work.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 22, "text": "> Perhaps there is room for memory/memories in the latent space?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 23, "text": "It would be interesting to combine Dreamer with external memory modules.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 24, "text": "Gregor et al. (2019) provide a comparison of such modules.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "r1lsMBCdFB", "rebuttal_id": "HylXRrsijS", "sentence_index": 25, "text": "However, this would better be addressed in a separate work to keep the paper focused on the main contribution of learning behaviors by latent imagination.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [7]]}, {"review_id": "r1lTmBYHtB", "rebuttal_id": "H1e6lQ9Mjr", "sentence_index": 0, "text": "Thanks for your comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1lTmBYHtB", "rebuttal_id": "H1e6lQ9Mjr", "sentence_index": 1, "text": "We are sorry to say that we miss some previous works, especially the ECCV one. And we will provide more literature review in our updated paper.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "r1lTmBYHtB", "rebuttal_id": "H1e6lQ9Mjr", "sentence_index": 2, "text": "We admit that the computation process of F-pooling and the ECCV method is the same.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "r1lTmBYHtB", "rebuttal_id": "H1e6lQ9Mjr", "sentence_index": 3, "text": "However, we defend the novelty of our work.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "r1lTmBYHtB", "rebuttal_id": "H1e6lQ9Mjr", "sentence_index": 4, "text": "The values of our work are not how the output of F-pooling is computed.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "r1lTmBYHtB", "rebuttal_id": "H1e6lQ9Mjr", "sentence_index": 5, "text": "Instead, the values are the strict definition of shift-equivalence and the theoretical properties of F-pooling.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "r1lTmBYHtB", "rebuttal_id": "H1e6lQ9Mjr", "sentence_index": 6, "text": "In previous works, they even don\u2019t give an operable definition shift-equivalence when down sampling involved.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "r1lTmBYHtB", "rebuttal_id": "H1e6lQ9Mjr", "sentence_index": 7, "text": "Please refer to our general response for more of F-pooling\u2019s values.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "r1lTmBYHtB", "rebuttal_id": "H1e6lQ9Mjr", "sentence_index": 8, "text": "Moreover, we discuss some practical problems of F-pooling.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "r1lTmBYHtB", "rebuttal_id": "H1e6lQ9Mjr", "sentence_index": 9, "text": "Such as how to deal with the imaginary part and the zero-padding of convolutions.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "r1lTmBYHtB", "rebuttal_id": "H1e6lQ9Mjr", "sentence_index": 10, "text": "With suitable settings, the shift consistency of F-pooling is much better.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 0, "text": "Thank you for your insightful feedback and suggestions! To address your comments:", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 1, "text": "- Following your suggestion, we have added one experiment on a synthetic regression task in Appendix C. Here, we show that our method can learn meaningful models for target domains, and also learn the source domain weights in a way that selects only relevant source domains for training.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 2, "text": "More importantly, we can learn the model and the domain weights simultaneously, unlike many existing works that use two-stage learning (learn the weights then the model).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 3, "text": "- We also followed your suggestion and show the domain weights of MDMN on the Amazon dataset for comparison (see the new Section 5.4).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 4, "text": "As you mentioned, our weights are theoretically justified while theirs are only heuristically computed.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 5, "text": "We see that the weights provided by MDMN are not very stable, changing from one source domain to another drastically during training.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 6, "text": "This instability makes their weights difficult to interpret.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 7, "text": "- W.r.t. the naive method of using (fixed) coefficients.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 8, "text": "Note that whichever domain discrepancy is used, it has to depend on the data representation.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 9, "text": "For complicated models like deep neural networks, the feature representations (hidden layers) are changing constantly during training and arguably there is no \"right\" way to compute *fixed* coefficients/weights", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 10, "text": "based on ever-changing representations", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 11, "text": ".", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 12, "text": "Computing the coefficients directly from the images is extremely difficult, if not impossible, because calculating the domain discrepancy using such high-dimensional data is not feasible.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 13, "text": "Note that MDMN DOES use W1-distance to compute domain weights, and our comparison in Section 5.4 shows that it is not very stable, as mentioned above.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 14, "text": "To summarize, there is no easy way to compute meaningful fixed coefficients and our method is indeed suitable for dynamic representations during neural network training.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 15, "text": "We hope our explanation and additional results address and resolve your concerns. If you have any other comments, we are happy to have further discussions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1lvRFUliS", "rebuttal_id": "HklAo1JujB", "sentence_index": 16, "text": "Thank you!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 0, "text": "> I found the paper interesting to read and well written.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 1, "text": "The main contribution (backpropagating analytic gradients through imagined trajectories?) could potentially be highlighted more but otherwise the paper was clear.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 2, "text": "Thank you.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 3, "text": "Correct, our main contribution is to learn long-horizon behaviors by propagating analytic value gradients through imagined trajectories.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 4, "text": "Moreover, we show that this yields a scalable algorithm that solves control tasks of higher difficulty than was previously possible using model-based agents.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 5, "text": "> I wonder if the authors ever looked at how much the size of the latent vector determines the performance of the system? Is there an optional latent vector size across domains or is that optimal size task dependent?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 6, "text": "For our experiments, we used the same hyper parameters across all tasks, including the state size.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 7, "text": "We conducted an additional experiment where we trained Dreamer with latent states of 100, 200, 300, 400, 500 deterministic units and 10, 20, 30, 50, 100 stochastic units.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 8, "text": "We find that all sizes equal to or larger than the 200 and 30 used in our main experiments yield very similar performance, while smaller sizes result in suboptimal scores on some of the tasks, hinting at insufficient model capacity.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 9, "text": "> Additionally, how much variance is there in the imagined trajectories from a certain starting state? In other words, are the endpoints of most imagined trajectories similar or very different?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 10, "text": "We have not studied this quantitatively.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 11, "text": "Qualitatively, we find more diversity in the stochastic multi-step predictions near states that are more challenging to predict.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 12, "text": "For example, this includes collisions with the ground, the unstable equilibrium of an upright balanced pendulum that could either rotate left or right, or cheetah balancing on its front feet which might flip over on its back or fall back on its feet.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 13, "text": "> There is actually not too much for me to critique and I would suggest this paper should be accepted.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "r1lYlete9S", "rebuttal_id": "BklFDSojir", "sentence_index": 14, "text": "Thank you.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_sentences", [7]]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 0, "text": "We hope that the reviewer will change his opinion once we clarify the goal of our paper and explain how it relates to prior work, as we believe we are fundamentally on the same page.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 1, "text": "We are well aware of SIFT, HOG, the results of Olshausen and Field on learning image filters from a few example images (some of us are sufficiently old to have implemented all such methods from scratch as grad students!) and no annotations, as well as Mallat\u2019s Scattering nets [1].", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 9, 10, 11, 16]]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 2, "text": "In fact, we discuss and evaluate Oyallon\u2019s 2017 implementation [2] of this at page 5 and table 2 in the paper.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 9, 10, 11, 16]]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 3, "text": "However, the existence of these methods does not detract from the message of this paper.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 9, 10, 11, 16]]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 4, "text": "Our goal is to provide \u201ccritical analysis\u201d of current self-supervision methods because these *specific* tools are now very heavily researched.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 9, 10, 11, 16]]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 5, "text": "Our paper sends a cautionary message: current self-supervised learning techniques cannot improve on what can be obtained from a single image plus transformations for early layers in a network, and only improves in a limited manner for deeper layers, despite ingesting millions of images (which is touted as their key advantage).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 9, 10, 11, 16]]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 6, "text": "In particular, the claims are not limited to the first few layers as we show that one image recovers two thirds of the performance of deeper layers as well.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 9, 10, 11, 16]]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 7, "text": "This message, which is a partially negative result, stands on its own, regardless of whether good low-level features can be obtained in some other ways (e.g. manually) and, we hope the reviewer will agree, should be known by the community.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 9, 10, 11, 16]]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 8, "text": "Nevertheless, we also agree with the reviewer that it is interesting to put these findings in a broader context, so we are happy to expand the discussion of prior feature learning/design work further.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [5, 9, 10, 11, 16]]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 9, "text": "However, please note that none of this literature makes our specific findings on the limits of self-supervision obvious.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 9, 10, 11, 16]]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 10, "text": "Furthermore, although this is a little besides the point, in the paper we do show in Table 2 that scattering transforms works as well as conv1, but that from conv2 onwards self-supervision on a single image does better, so even the claim that handcrafted features are equivalent to the first few layers in deep networks is not proven.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 9, 10, 11, 16]]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 11, "text": "Also, the fact that Olshausens\u2019s filters resemble conv1 does not mean that they are equivalent to conv1 in recognition performance.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 9, 10, 11, 16]]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 12, "text": "\u2014", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 13, "text": "[1] J. Bruna and S. Mallat. \"Invariant scattering convolution networks.\" TPAMI 2013", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1x7498kcS", "rebuttal_id": "Hkl_Uybzor", "sentence_index": 14, "text": "[2] E. Oyallon, et al. \"Scaling the scattering transform: Deep hybrid networks.\" ICCV 2017", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 0, "text": "Overall", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 1, "text": "We thank you for your kind comments, in acknowledging that the work is well motivated, and the problem is an important one, currently not studied under the meta-learning paradigm.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 2, "text": "We have made substantial improvements based on your suggestions, and other reviewers\u2019 comments, and hopefully we are able to address most of your concerns.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 3, "text": "Concern 1: Reproducibility", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15, 16, 17]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 4, "text": "Our code is built on top of existing code (Prototypical Networks and Image-to-Image Translation from CycleGAN).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15, 16, 17]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 5, "text": "Thus, we adopt the same hyperparameters and architectures as the prior work, and as a result our work is fairly easy to reproduce.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15, 16, 17]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 6, "text": "We will of course release the code.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15, 16, 17]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 7, "text": "As suggested, we have utilized the appendices to give detailed information about the experimental setup.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14, 15, 16, 17]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 8, "text": "Concern 2: Size of unlabelled test set, data-split information", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18, 19, 20]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 9, "text": "We did provide some details on the first version (in the appendix).", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [18, 19, 20]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 10, "text": "In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18, 19, 20]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 11, "text": "Concern 3: Jointly learning vs Freezing GAN", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 12, "text": "Training in joint manner can be very tricky, and may often cause stability issues.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 13, "text": "You are right in your suggestion, that it is similar to first styling, and then applying meta-learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 14, "text": "Having said that, this is a common strategy in several state of the art domain adaptation techniques, where the GAN-based domain adaptation and task-specific classifier are trained in multiple steps.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 15, "text": "For example, see training protocol in [1,2,3, etc.].", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 16, "text": "[1] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alyosha Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 17, "text": "[2] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 4, 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1x9Ih1c2Q", "rebuttal_id": "BkehyvUFR7", "sentence_index": 18, "text": "[3] Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., & Krishnan, D. Unsupervised pixel-level domain adaptation with generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 0, "text": "We thank the reviewer for the valuable feedback!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 1, "text": "The suggestion comments were very helpful and led to a clear improvement of our manuscript.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 2, "text": "We reply to the answers and comments in the order they were raised:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 3, "text": "(1) If one uses the same matrix-variate normal distribution that we use for p(\\theta | x) as approximate posterior p(\\theta) of a BNN in conjunction with the ELBO objective, one arrives at a BNN proposed by Louizos and Welling (2016) [1], i.e. the Variational Matrix Gaussian (VMG).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 4, "text": "We found that VMG\u2019s results (obtained from their original code https://github.com/AMLab-Amsterdam/SEVDL_MGP) are not as good as that for the CDN, as shown in Figure 8 in the appendix.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 5, "text": "This is further discussed in the newly added section 6.4.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 6, "text": "(2) Thank you for this valuable suggestion!", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16, 17, 18]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 7, "text": "We have added a new section (Sec. 4) to discuss the differences between the objective used for CDN, when performing variational inference for BNNs, and in the variational information bottleneck (VIB) framework.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 8, "text": "Furthermore, we present an experimental investigation of these different objectives (Sec. 6.4).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 9, "text": "We found that the CDN objective leads to superior results, especially in the adversarial examples experiment.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 10, "text": "(3) We observed that as \\lambda increases, in the validation set, the uncertainty is increasing, while the accuracy is decreasing.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 11, "text": "So", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 12, "text": ", a simple heuristic that we use is to choose the highest \\lambda that allow high validation accuracy (e.g. > 0.97 on MNIST).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 13, "text": "We found that this heuristic works very well in our experiments (the results have updated to reflect on this heuristic).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 14, "text": "We have made this procedure clear in the revised manuscript.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 15, "text": "Detailed comments about experiments:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 16, "text": "(a) Thanks for catching this. Indeed this was due to a bug in the toy regression experiment which we have fixed now.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [22, 23]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 17, "text": "(b) We have revised the baselines so that they either use \\lambda = 1 or the settings that the original authors recommended.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [24, 25, 26]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 18, "text": "We detail this in Appendix F.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [24, 25, 26]]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 19, "text": "References:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1xXo7UF3m", "rebuttal_id": "ryg0dWFrC7", "sentence_index": 20, "text": "[1] Louizos, Christos, and Max Welling. \"Structured and efficient variational deep learning with matrix gaussian posteriors.\" International Conference on Machine Learning. 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 0, "text": "Thank you very much for your encouraging review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 1, "text": "> I read the paper and understand it, for the most part.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 2, "text": "The idea is to interpret some regularization techniques as a form of noisy bottleneck, where the mutual information between learned parameters and the data is limited through the injection of noise.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [1]]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 3, "text": "While the paper is a pleasant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 4, "text": "Perhaps other referee will have a clearer opinion.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 5, "text": "The main contribution of our paper is indeed to establish a connection between variational inference and regularization by observing that Gaussian mean field introduces an upper bound on the mutual information between data and model parameters.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 6, "text": "Reinterpreting mean field as point estimation in a noisy model allows us to quantify observed regularizing effects.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 7, "text": "We show links to existing regularization strategies and validate the usefulness for regularization in targeted experiments.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 8, "text": "While the focus of our present work lies on establishing links between existing directions of research, we believe that our information-theoretic perspective on regularization opens up plenty of avenues for future work, both in supervised and unsupervised learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 9, "text": "For example, we are interested in improving extraction of unsupervised representations by controlling the amount of extracted information.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 10, "text": "In particular, we aim to mitigate latent collapse, a problem reported for example in language generation [1] and autoregressive image generation [2], which is currently mitigated with ad-hoc strategies such as KL annealing.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 11, "text": "Intuitively, if all information can be stored in the model itself, there is little incentive to use a per-sample latent.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 12, "text": "This is also known as the information preference problem, as briefly discussed at the end of section 2.1.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 13, "text": "Therefore, limiting mutual information of the data with the model might offer a robust mitigation strategy.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 14, "text": "Additionally, we believe that the approach can lead to improved representations through disentanglement, as done by beta-VAE [3].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 15, "text": "Our formal connection to beta-VAE derived in Appendix C offers a promising information-theoretic perspective on their empirical results.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 16, "text": "More generally, we want to explore non-MAP inference on noise-injected models as this would allow for using highly expressive variational distributions while enjoying the information-theoretic guarantees of simpler approximate distributions, as motivated in section 3.3.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 17, "text": "Since these directions are rather orthogonal, we think that sharing our theoretical framework with the community in an independent piece of work is the most effective way of communicating our ideas.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_error", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 18, "text": "> I'd be interested to hear if the authors see a connection between their formalism and the one of Reference prior in Bayesian inference (Bernardo et al https://arxiv.org/pdf/0904.0156)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 19, "text": "Reference priors are opposite to our work in the sense that they maximize the amount of information data provides about the parameters, while we aim to find models to limit it.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 20, "text": "Also, see [4] for the relation of Fisher information to generalization.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 21, "text": "References", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 22, "text": "[1] Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R. & Bengio, S. (2015). Generating sentences from a continuous space.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 23, "text": "arXiv preprint arXiv:1511.06349.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 24, "text": "[2] Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A. A., Visin, F., Vazquez, D. & Courville, A. (2016). Pixelvae: A latent variable model for natural images. arXiv preprint arXiv:1611.05013.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 25, "text": "[3] Burgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G. & Lerchner, A. (2018). Understanding disentangling in beta-VAE.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 26, "text": "arXiv preprint arXiv:1804.03599.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 27, "text": "[4] Ly, A., Marsman, M., Verhagen, J., Grasman, R. P. & Wagenmakers, E. J. (2017). A tutorial on Fisher information.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1xY4JK62Q", "rebuttal_id": "ryxS38C_am", "sentence_index": 28, "text": "Journal of Mathematical Psychology, 80, 40-55, page 30", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 0, "text": "Regarding comment 1: Thanks for recognizing the merit of our idea.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 1, "text": "As also mentioned by Reviewer #2, the proposed MAD implicitly assumes that classifiers in the competition are reasonably accurate.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 2, "text": "Otherwise, the selected counterexamples may be less meaningful.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 3, "text": "We will make this assumption explicit in the revised manuscript.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 4, "text": "From this perspective (and other reasons mentioned in the discussion section), MAD should be viewed as complementary to, rather than a replacement for, the conventional accuracy comparison for image classification.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 5, "text": "When two classifiers perform at a reasonable level and achieve very close accuracy numbers (e.g., VGG16BN and ResNet34 on ImageNet validation set), MAD provides the most efficient way of differentiating the two models by maximizing their discrepancies over a large-scale image set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 6, "text": "We want to emphasize that MAD is especially useful on image classification tasks where most cutting-edge classifiers achieve very close performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 7, "text": "In these situations, the MAD competition ranking, which is obtained by evaluating on corner examples searched from web-scale unlabeled dataset, is more convincing than something like 1% accuracy advantage on the validation set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 8, "text": "For problem domains where there are few sufficiently accurate models, we may still apply the underlying principle behind MAD to create adaptive test sets such that the strengths and weaknesses of the models are most easily revealed.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 9, "text": "In those scenarios, we conjecture that we need increase k to a reasonably larger number, thus at the cost of efficiency.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 10, "text": "Regarding comment 2: Thanks for the comment.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [5]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 11, "text": "As long as two (or multiple) models differ (even in slightly different ways), MAD provides the highly efficient way of spotting such differences by exploring a large-scale unlabelled dataset.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 12, "text": "However, these differences are less likely to be revealed using a fixed and small test set (i.e., they will probably have the same accuracy numbers as models to be compared are very similar and are biased in similar ways).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 13, "text": "For the extreme case that two models are exactly the same (i.e, they are biased in identical ways and make identical prediction errors), both MAD and traditional accuracy-based methods will draw the same conclusion - the two models have the same performance.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 14, "text": "Accuracy-based evaluation methods arrive at this conclusion by comparing model predictions with ground truth labels and outputting the same accuracy numbers.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 15, "text": "In contrast, MAD arrives at the same conclusion without any human labeling since the set S for subjective testing is empty.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 16, "text": "So in this extreme case, both MAD and accuracy fail to compare those two models.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 17, "text": "In summary, to reliably compare the relative performance of computational models, all evaluation methodologies (including MAD) rely on the assumption that the models to be compared should be diverse to a certain extent, and the proposed MAD makes this assumption more explicit.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 18, "text": "In fact, MAD makes the best use of model discrepancies (even if models are biased in very similar but not identical ways) to rank the model performance.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 19, "text": "As a matter of fact, based on our experiments, we find that state-of-the-art ImageNet classifiers do have their own biases. (See figure 8 in the appendix.)", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 20, "text": "Regarding comment 3: Thanks for the excellent question.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 21, "text": "We believe that the parameter k is task-dependent.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 22, "text": "For problem domains where there are reasonably accurate models (e.g., imageNet classification in our example), we may obtain a stable ranking with a relative small k (e.g., k=15 in the imageNet classification example).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 23, "text": "For problem domains where there are no good models, we may increase k to the limit of human labeling budget in order to obtain reasonable performance comparison.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 24, "text": "Regarding comment 4: Thanks for the comment.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 25, "text": "In our current setting, we restrict the dataset D to the domain of interest that contain natural images of mainly 200 classes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 26, "text": "However, as the construction of D is noisy and coarse, D contains plenty of open-set images, which do not belong to any class of interest.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 27, "text": "Since we do not perform any manually data screening at this stage, some of the open-set images may even be selected to construct the dataset S.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 28, "text": "This means that although the selected open-set image is out of the domain of interest, the associated two classifiers make different, high-confident (with threshold set to 0.8), but incorrect predictions (Case III).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 29, "text": "As a result, we consider it as a strong counterexample of the two classifiers.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 30, "text": "Note that this situation rarely happens, at least in our experiments because the competing classifiers tend to give open-set images low-confidence, and therefore are automatically filtered out.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 31, "text": "We agree with the reviewer that selecting images that contain only one salient object requires a lot of human effort.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "r1xYhv5J5H", "rebuttal_id": "BJgfCUWlsB", "sentence_index": 32, "text": "So we did not eliminate Case I. It turns out that keeping Case I does not seem to affect the comparison and analysis of competing models.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 0, "text": "Thank you very much for your encouraging review and helpful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 1, "text": "We will make revisions to address the several points you have raised in your review.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 2, "text": "Below we first address the main concerns.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 3, "text": "Q1: \u201cAlt-az\u201d rotation is not a group.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16, 17, 18, 19]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 4, "text": "A1: Thank you for pointing this out.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [15, 16, 17, 18, 19]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 5, "text": "You are correct.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [15, 16, 17, 18, 19]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 6, "text": "The Alt-az rotation, according to our definition, is not a group.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16, 17, 18, 19]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 7, "text": "SO(3)  is a group which can be parametrized by a 3-sphere .", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16, 17, 18, 19]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 8, "text": "But when we reduce one parameter from it, it is not a group anymore mathematically; the composition of two alt-az rotations becomes a general rotation in SO(3).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16, 17, 18, 19]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 9, "text": "In the new revision, we will use the term alt-az rotation in \u201cquotient SO(3)/SO(2)\u201d  instead of alt-az rotation group.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [15, 16, 17, 18, 19]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 10, "text": "Moreover, the quotient SO(3)/SO(2) is isomorphic to $S^2$ and to avoid the ill-definition on the two poles (the two degenerate points), we will add a constraint to the alt-az rotation, i.e. $\\phi=0, if \\theta=0 or \\theta=\\pi$. This is because, when the altitude rotation is zero or PI, the azimuth rotation is meaningless in a alt-az rotation and is therefore fixed as zero.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15, 16, 17, 18, 19]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 11, "text": "If $\\theta=0 or \\theta=pi, and \u201c\\phi \\neq 0$, this rotation belongs to the azimuthal rotation in SO(2) group.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15, 16, 17, 18, 19]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 12, "text": "(Q2) Equivariance property of the Alt-az convolution", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 13, "text": "We think we can still have the equivariance property but only for single alt-az rotation.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 14, "text": "Notice the definition of alt-az convolution do not use any composite rotation.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 15, "text": "Here is our tentative proof:", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 16, "text": "Under the definition of alt-azimuth anisotropic convolution and using the unitary property (5) of rotation operators, we have (assume the number of channels K=1 for simplicity, assume Q and R be both alt-az rotations):", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 17, "text": "************************************************", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_none", null]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 18, "text": "\\begin{equation}", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 19, "text": "\\begin{aligned}", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 20, "text": "& (h \\star D_{Q} f) (R)", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 21, "text": "\\\\", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 22, "text": "&", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 23, "text": "= \\int_{S^2}(D_Rh)(\\hat{u})f(Q^{-1}\\hat{u})ds(\\hat{u}) \\\\", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 24, "text": "&", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 25, "text": "=\\int_{S^2}h(R^{-1}\\hat{u})f(Q^{-1}\\hat{u})ds(\\hat{u}) \\\\", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 26, "text": "&", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 27, "text": "=\\int_{S^2}h(R^{-1}Q\\hat{u})f(\\hat{u})ds(\\hat{u}) \\\\", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 28, "text": "&", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 29, "text": "=\\int_{S^2}h((Q^{-1}R)^{-1}\\hat{u})f(\\hat{u})ds(\\hat{u}) \\\\", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 30, "text": "&", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 31, "text": "=(h \\star f)(Q^{-1}R) = D_{Q}( h \\star f)(R)", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 32, "text": "\\end{aligned}", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 33, "text": "\\end{equation}", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 34, "text": "**************************************************", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_none", null]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 35, "text": "This means that for a single alt-az rotation of input spherical image; the output of a convolution layer will rotate in the same way.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 36, "text": "Although the property doesn\u2019t hold if one performs multiple alt-az rotations to the input spherical image, it is still valuable because we assume the different SO(3) orientation of an input 3D shape is from a composite of an azimuthal rotation and an alt-az rotation, the azimuthal rotation is treated by data augmentation and the single alt-az rotation is treated by the network equivariance and invariance.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 37, "text": "Q3: alt-az convolution is not well defined on the south pole", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 38, "text": "A3: Yes, we agree that our original definition of alt-az convolution is not well defined on both north and south poles.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 39, "text": "Therefore, in the new revision, we will add the constraints to the definition of alt-az rotation and make it one-to-one corresponds to the set points on $S^2$. See A1.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [22, 23, 24, 25, 26, 27, 28]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 40, "text": "Q4: The paragraph motivating the alt-az convolution on page 4 is not very clear.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35, 36, 37]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 41, "text": "A4: Thanks for the comments, as you suggested, we will rewrite this paragraph in the new version, and acknowledge the importance and effectiveness of the recent work on the group equivariance and rotation invariant networks.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [35, 36, 37]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 42, "text": "Q5: It would be nice to see a more direct comparison between the three definitions of spherical convolution (general SO3, isotropic S2, and anisotropic S2)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 45, 46]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 43, "text": "A5: The two related papers (Cohen et al 2018 for general SO3 and Esteves, and Esteves et al 2018 for isotropic S2) both use lat-lon grid and Fourier domain convolution, while ours uses a icosahedron-sphere grid and direct spherical domain convolution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 45, 46]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 44, "text": "The use of different sampling in the input spherical image, and the use of filters are totally different.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 45, 46]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 45, "text": "We think a direct comparison should be done in one of the following ways : (a) perform the three types of spherical convolution all using icosahedron-sphere grid and then convolve in the spherical domain.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 45, 46]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 46, "text": "(b) perform the three types of spherical convolution all using lat-lon grid and convolve in the Fourier domain.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 45, 46]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 47, "text": "For the first type of direct comparison, to implement isotropic spherical convolution (Type II), we should make the geodesic disc filter share an identical weight along the angular direction.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 45, 46]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 48, "text": "To implement a general SO(3) spherical convolution, we should add a rotation degree of freedom into our disc filter.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 45, 46]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 49, "text": "We are conducting this experiment and if the time and paper page limit are allowed, we will report the comparison result in the revised version.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 45, 46]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 50, "text": "Otherwise, we will put it into our future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [44, 45, 46]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 51, "text": "For the second type of direct comparison, we need to conduct alt-az spherical convolution in the Fourier domain, this is possible by determining the spherical harmonic coefficient, $<g_0, Y_l^m> $ for the alt-az convolution in terms of the spherical harmonic coefficient of input spherical signal $f$ and the filter $h$. This comparison needs re-designing of our network and we can not finish it within the rebuttal period, we\u2019ll leave it for future work.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 45, 46]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 52, "text": "Q6: Initially, I was a bit puzzled about why SO(3) augmentation seems to reduce accuracy in table 1.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [47, 48, 49]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 53, "text": "I think this is because SO(3) augmentation actually makes the classification problem harder if the input is initially aligned.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [47, 48, 49]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 54, "text": "Some more explanation / discussion would be good.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [47, 48, 49]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 55, "text": "A6: Theoretically, our method will be rotation invariant with only AZ rotation, it will be full rotation invariant with SO(2) rotation augmentation about an arbitrary axis .", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [47, 48, 49]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 56, "text": "In table I, we believe the reason alt-az augmentation performs better because it contains more training data.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [47, 48, 49]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 57, "text": "SO(3) augmentation underperforms the AZ augmentation because several random SO(3) rotation augmentation might not be able to cover all the relative rotation wrt the filter's orientation (see appendix).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [47, 48, 49]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 58, "text": "Q7: It would be nice to explain the spherical parameterization in more detail.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [50]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 59, "text": "Is this operation itself rotation equivariant?", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [50]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 60, "text": "A7: Due to the page limit of the conference paper, we could not explain the spherical parameterization method in detail.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [50]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 61, "text": "This operation is theoretically rotation equivariant.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [50]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 62, "text": "Spherical parameterization establishes a map that transforms the points of a closed surface into the points on the unit sphere.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [50]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 63, "text": "A good spherical mapping for a closed surface should satisfy the following properties:bijective mapping and least distortion.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [50]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 64, "text": "Bijective mapping is the most important but most difficult in this process which implies that the resulting map is one-to-one, fold-free, and therefore feature preserving (information lossless).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [50]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 65, "text": "Least distortion seeks a good sampling rate such that interesting features of the model receive enough real estate on the sphere in order to be accurately sampled.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [50]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 66, "text": "We achieved the bijective mapping by adapting a coarse-to-fine strategy with minor modifications (See http://hhoppe.com/proj/sphereparam/).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [50]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 67, "text": "The minimizing of the map distortion is obtained using the authalic parameterization proposed in Sinha et al 2016.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [50]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 68, "text": "This process is rotation equivariant because the initial bijective mapping is depends on the object orientation and the authalic remeshing does not change the orientation of the spherical embeddings.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [50]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 69, "text": "Spherical parameterization is a good way to retain geometric and topological information of original shapes (compared to the spherical projection method), but currently it works only for genus-0 closed object, extending it to 3d shapes with arbitrary topology is still an unsolved problem, that is why we could not adapt this method for dataset such as ModelNET and Shrec\u201917, they contain 3D objects with arbitrary topology.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [50]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 70, "text": "(Q8) Other typos and minor issues", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]]}, {"review_id": "rJeeoDOinQ", "rebuttal_id": "HyV6CRt6m", "sentence_index": 71, "text": "We will correct all the typos and other minor issues in the revised paper, thank you again for the detailed review. WE really appreciate your help.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 0, "text": "Q1: In Theorem 4.3, the result holds for any $k$ and $M$. The authors claim that if we take a limit of $M \\to \\infty$ with fixed $k$, the practical dynamics converges to the discrete-time mean-field limit, in Section 4.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 9]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 1, "text": "However, to state the result of Theorem 4.3, $k$ should be bigger than $M c_\\eta$ from the dentition of $\\tilde{\\rho}_k^M$, as shown under the equation (4).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 2, "text": "How do we take a limit of $M \\to\\infty$ ? Does k also go $\\infty$?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 3, "text": "A1: Thanks for pointing this out.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [9, 9, 11, 12]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 4, "text": "The result of this theorem holds uniformly for any $k$ (not a fixed $k$).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 9, 11, 12]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 5, "text": "Besides, we do not require $k$ bigger than $M c_\\eta$ in the definition of $\\tilde{\\rho}_k^M$. When $k$ is no more than $M c_\\eta$, $\\tilde{\\rho}_k^M$ and $\\rho_k^M$ are stochastic processes with same distribution and thus the Wasserstein distance between them is 0.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 9, 11, 12]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 6, "text": "And for any $k$ is greater than $M c_\\eta$, we have the uniform bound (w.r.t. $k$) as stated in the theorem 4.3.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 9, 11, 12]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 7, "text": "We are sorry for not stating this clearly in the theorem and we have revisited the present of the theorem. We will fix this issue in the next revision.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [9, 9, 11, 12]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 8, "text": "We also point out that, as our system is complicated, in taking the limit of $M\\to\\infty$, we need to ensure that the number of iteration we run is larger than $Mc_\\eta$. To be specific, the asymptotic convergence would be", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 9, 11, 12]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 9, "text": "$$\\lim_{k,M \\to\\infty, \\eta \\to 0^+} \\mathbb{D}_{\\text{BL}} (\\rho_k, \\rho^*)=0$$", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 9, 11, 12]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 10, "text": "where the joint limit of k and M requires that $k\\eta\\to\\infty$; $\\exp(C\\alpha^{2}k\\eta)\\eta^{2}=o(1)$; $(k\\eta)/(Mc)=q(1+o(1))$ with $q>1$. Here if $q \\leq 1$, we degenerate to Langevin. But when $q>1$ (intuitively that means, when $M$ is large, the number of iterations we run is larger), our dynamics is different from Langevin, which is what we do in the practice.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 9, 11, 12]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 11, "text": "Also, we would like to remark that this seemingly strange things is in fact the \u2018artifact\u2019 caused by the using of Langevin dynamics at beginning to obtain the $M$ initial samples when we designed the practical implementation of the proposed methods.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 9, 11, 12]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 12, "text": "However, it is not really necessary to use Langevin dynamics to get $M$ initial samples, as we can simply using some other initialization distribution and get the $M$ initial samples from that distribution (and by this setting, our dynamics is simply the second phases in Eq (3)).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 9, 11, 12]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 13, "text": "All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 9, 11, 12]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 14, "text": "Q2: Regarding other minor comments", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "rJeF90yCKS", "rebuttal_id": "rJlvwS37sS", "sentence_index": 15, "text": "A2: Thanks for your notification! We will polish our paper and rewrite the corresponding part in the next revision.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 0, "text": "1. Thank you for your suggestion, we have addressed this in the revision as you suggested. This is indeed a good motivation.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 1, "text": "2. Thank you for your suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 2, "text": "We have further added experiments using even stronger query limit (previously 500000, now 50000) for the additional experiments on ResNet V2 model in the supplemental material. (We did not choose to use smaller epsilon because first, we already used a quite standard choice of epsilon, second, as you said, going for extremely small distortion does not really mean anything in adversarial context.) As you can see, in this even harder setting our proposed algorithm still maintain a performance lead over other baselines.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 3, "text": "Also, we have revised the statement in the abstract as you suggested.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 4, "text": "3. You are right, it is a quite weak attack and we have removed it from the table (just mention it in the text).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 11]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 5, "text": "4. Yes, we could just remove the distortion column in our result.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 6, "text": "We choose to include it because we do not want others to think that we actually trade a lot of distortions (to make problem easy) for speedup in runtime.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 7, "text": "5. We have added further empirical evidence to show that in the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14, 15, 16, 17]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 8, "text": "From an intuitive perspective, using lambda>1 is essentially a \u201crelax and tighten\u201d step by first relax the constraint to make the problem easier, and then tighten it back to the real constraint.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15, 16, 17]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 9, "text": "The \u201crelax and tighten\u201d idea has been widely used in constrained optimization, and we adapted this idea to Frank-Wolfe algorithm to make it even faster.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15, 16, 17]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 10, "text": "6. As mentioned in an anonymous comment, there is one paper which proposed a similar but different zeroth-order non-convex FW algorithm as well as convergence rate analysis ahead of us.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 11, "text": "We were not aware of this paper when we prepared our ICLR submission, since it was posted only ten days before the ICLR deadline.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20, 20]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 12, "text": "We have cited this paper and modify our claim correspondingly in the revision.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20, 20]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 13, "text": "Nevertheless, it does not affect the main contribution of our paper: a novel Frank-Wolfe based adversarial attack framework for both white-box and black-box attacks, which is much more efficient than existing white-box/black-box adversarial attacks in both query complexity and runtime.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 14, "text": "7. Thank you for your suggestion and we have explicitly written down the update for a better comparison in the supplemental materials (Section A) in the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [19, 20, 20]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 15, "text": "8. It means it is invariant to an affine transformation of the constraint set, i.e., if we choose to re-parameterize of the constraint with some linear or affine transformation M, the original and the new optimization problem will looks the same to the Frank-Wolfe algorithm.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 16, "text": "Please refer to [Jaggi (2013)], [Lacoste-Julien (2016)] for more details.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 17, "text": "9. In white-box setting, we perform grid search / binary search for parameter epsilon (or c for CW) for all algorithms.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [23]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 18, "text": "This will lead to better/ closer distortions for all methods.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [23]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 19, "text": "In black-box setting, we care more about query complexity and thus did not perform the grid search/binary search steps to avoid extra queries in finding the best epsilon/lambda.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [23]]}, {"review_id": "rJefA1rv3X", "rebuttal_id": "HyxXl7Z5RX", "sentence_index": 20, "text": "10. Thank you for pointing these typos out, we have addressed it in the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "rJgbJ0v52m", "rebuttal_id": "SkgCXcb16X", "sentence_index": 0, "text": "We thank the reviewer for the detailed review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJgbJ0v52m", "rebuttal_id": "SkgCXcb16X", "sentence_index": 1, "text": "Below we address the main concerns.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJgbJ0v52m", "rebuttal_id": "SkgCXcb16X", "sentence_index": 2, "text": "--------------------------------------------------------------------------------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "rJgbJ0v52m", "rebuttal_id": "SkgCXcb16X", "sentence_index": 3, "text": "Q:\u201dso", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "rJgbJ0v52m", "rebuttal_id": "SkgCXcb16X", "sentence_index": 4, "text": "framing the synthetic experiments as, \"here are some simple functions for which we would need the additional parameters that we define\" makes sense; but arguing that Hartford et al. \"fail approximating rather simple functions\" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "rJgbJ0v52m", "rebuttal_id": "SkgCXcb16X", "sentence_index": 5, "text": "A: We agree with the reviewer and will change our wording accordingly.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "rJgbJ0v52m", "rebuttal_id": "SkgCXcb16X", "sentence_index": 6, "text": "--------------------------------------------------------------------------------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "rJgbJ0v52m", "rebuttal_id": "SkgCXcb16X", "sentence_index": 7, "text": "Q:\u201dI would have liked to see more discussion about why the two results differ to give readers intuition about where the extra flexibility comes from\u201d.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "rJgbJ0v52m", "rebuttal_id": "SkgCXcb16X", "sentence_index": 8, "text": "\u201cthe two results coincide for the exchangeable case\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "rJgbJ0v52m", "rebuttal_id": "SkgCXcb16X", "sentence_index": 9, "text": "A: We agree with the reviewer that such a discussion will be helpful to the reader. We will add such a discussion (in addition to the short discussion at the end of Appendix 1).", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [5]]}, {"review_id": "rJgbJ0v52m", "rebuttal_id": "SkgCXcb16X", "sentence_index": 10, "text": "--------------------------------------------------------------------------------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "rJgbJ0v52m", "rebuttal_id": "SkgCXcb16X", "sentence_index": 11, "text": "Q: Comparison to popular graph convolution methods (GCN [kipf et al. 2016] / GraphSAGE [Hamilton et al. 2017] / Monti et al [2017] / etc.).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rJgbJ0v52m", "rebuttal_id": "SkgCXcb16X", "sentence_index": 12, "text": "A: As discussed in our response to Reviewer 2, We will add a theoretical result that shows that our model is at least as powerful in terms of universality as [Kipf & Welling ICLR 2017].", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 0, "text": "Thanks for the review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 1, "text": "@Wall-clock: We don\u2019t quite understand the question.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 2, "text": "As mentioned in the response to Reviewer 3, our NLP example does answer the natural question about end-to-end gains. Is the reviewer only concerned with the location of the plots?", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 3, "text": "- Another note: to perform a full wall-clock comparison with algorithms that have different per-iteration costs, one must disentangle and retune various hyperparameter choices, most notably the learning rate schedule.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 4, "text": "Thus we decided to feature the per-iteration comparison in the main paper, as it is the cleanest one.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 5, "text": "@L-BFGS: On a high level, we agree that GGT develops a similar window-based approximation to the gradient Gram matrix as L-BFGS does to the approximated Hessian.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 6, "text": "While adaptive methods have proven effective in practice, quasi-Newton algorithms are not in general regarded as competitive for deep learning (despite recent efforts [1,2]), and that\u2019s why it is not compared to in the vast majority of deep learning papers.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 7, "text": "- Quasi-Newton methods are suited for deterministic problems, while stochasticity is crucial in deep learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 8, "text": "This is because they try to approximate the Hessian by finite differences, which seems unstable with stochastic gradients in practice.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 9, "text": "- Direct second-order methods require significant modifications to converge in the non-convex setting (see [3,4]).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 10, "text": "Even these have not been observed to work well in deep learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 11, "text": "- One reason for the practical success of AdaGrad-like algorithms we believe is the difference of  -1/2 vs. -1 power on the Gram matrix, which seems to change the training dynamics dramatically.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 12, "text": "With the gradient Gram matrix and a -1 power, meaningful end-to-end advances have only been claimed for niche tasks other than classification.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 13, "text": "[1] Stochastic L-BFGS: Improved Convergence Rates and Practical Acceleration Strategies.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 14, "text": "R. Zhao and W. Haskell and V. Tan. arXiv, 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 15, "text": "[2] A Stochastic Quasi-Newton Method for Large-Scale Optimization. R. Byrd, S. Hansen, and J. Nocedal, and Y. Singer SIAM Journal on Optimization, 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 16, "text": "[3] Accelerated methods for nonconvex optimization.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 17, "text": "Y. Carmon, J. Duchi, O. Hinder, A. Sidford. SIAM Journal on Optimization, 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 18, "text": "[4] Finding approximate local minima faster than gradient descent.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJggFt49nX", "rebuttal_id": "B1lP_IyMRm", "sentence_index": 19, "text": "N. Agarwal, Z. Allen-Zhu, B. Bullins, E. Hazan, and T. Ma. STOC 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 0, "text": "Thank you for your valuable feedback!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 1, "text": "After carefully reading it, we plan to modify the manuscript as discussed below (the planned changes are shown by **", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 2, "text": "\u2026 *", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 3, "text": "*", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 4, "text": ")", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 5, "text": ".", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 6, "text": "We would appreciate it if you could let us know whether the proposed changes address your concerns, or whether we have misinterpreted your comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 7, "text": "\u2022", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 8, "text": "You have raised an interesting question about how the accuracy of the GAN impacts the accuracy of the proposed method.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 9, "text": "In order to address this, we have developed analytical estimates for the error in the point estimates computed using the proposed approach and show that these are intimately tied to error in computing the point estimates for the prior using the GAN.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 10, "text": "We have also demonstrated that as the generator and the discriminator of the GAN become more expressive this error tends to zero, and the exact point estimates, for both the prior and the posterior, are recovered. ** In the revised manuscript, we will include this mathematical analysis in the Appendix and refer to it in the main text. **", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [9]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 11, "text": "\u2022", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 12, "text": "We note that our method of inferring the desired image from the measured image is an unsupervised method; for training we only need a set of desired images  to construct the prior.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 13, "text": "We are not aware of any other unsupervised learning approach for solving these types of problems with quantified uncertainty.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [6]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 14, "text": "In that regard, the calculation of point-wise variance (our metric of uncertainty) is possible only using our approach, and therefore a direct comparison is not possible, since other supervised methods (explained below) cannot work in this setting where only set of desired images are available. ** We will clarify this unique aspect in the revised version of the manuscript. **", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 15, "text": "\u2022\tThere has been some work on computing the uncertainty in an inferred image within a supervised learning framework where pairs of measured and desired images are used for training the network [1, 2].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 16, "text": "In these articles the authors have used methods like Bayesian dropout and variational autoencoder to compute uncertainty in the inferred images.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 17, "text": "*", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 18, "text": "* We will refer to these works in the revised version to better orient reader.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [8]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 19, "text": "*", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 20, "text": "*", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 21, "text": "[1]. A. Kendall and Y. Gal, \u201cWhat Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 22, "text": "\u201d, NIPS (2017).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 23, "text": "[2]. Kohl, S.A., Romera-Paredes, B., Meyer, C., Fauw, J.D., Ledsam, J.R., Maier-Hein, K.H., Eslami, S.M., Rezende, D.J., & Ronneberger, O. \u201cA Probabilistic U-Net for Segmentation of Ambiguous Images\u201d, NeurIPS (2018).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 24, "text": "Thank you for your valuable feedback! After carefully reviewing it, we have modified manuscript as discussed below (description of changes is enclosed within ** ... **).", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 25, "text": "\"Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 26, "text": "In numerical experiments, there is no comparison with major competitors besides random sampling in the active learning setup.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 27, "text": "Hence, the effectiveness and advantage of the proposed methods are not clear.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 28, "text": "We have addressed this by responding to the specific questions below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 29, "text": "\"- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc. \"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 30, "text": "We are not aware of any other methods for computing uncertainty in recovered images that have been used to drive an active learning task in image inpainting.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [8]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 31, "text": "While methods based on dropout (\\cite{Kendall2017a, Kendall2019}) or variational inference (\\cite{Kohl2018a}) could be extended to accomplish this, this has not been done thus far.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [8]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 32, "text": "**We have added this comment in Section 3.1**", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 33, "text": "Another big difference between the methods mentioned above and our approach is that while they require image pairs (true and corrupted images) for training, our approach only requires uncorrupted images.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 34, "text": "Thus while our algorithm relies on unsupervised learning, the other algorithms fall under the category of supervised learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 35, "text": "*", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 36, "text": "*We have also clarified this within the \"Our Contributions\" Section**", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 37, "text": "\"- How does the estimation accuracy of GAN relate to the estimation accuracy of the proposed method? Showing a quantitative description would be nice.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 38, "text": "We thank the reviewer for raising this important question.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [9]]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 39, "text": "*", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "rJghfrNnKS", "rebuttal_id": "r1lA7pumoH", "sentence_index": 40, "text": "*We have addressed it thoroughly in Appendix A. We have provided a proof that demonstrates the weak convergence of the posterior density calculated using our method to the true posterior density as the number of weights in the discriminator and generator components of the GAN is increased.**", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9]]}, {"review_id": "rJgwCU_82Q", "rebuttal_id": "rygdEjy1CX", "sentence_index": 0, "text": "We thank our third reviewer for his comment.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJgwCU_82Q", "rebuttal_id": "rygdEjy1CX", "sentence_index": 1, "text": "We do understand your concern about the significant increase in computational time.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "rJgwCU_82Q", "rebuttal_id": "rygdEjy1CX", "sentence_index": 2, "text": "However, we believe that in the context of active learning, the main problem is not related to computational power, rather to the scarcity of data.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "rJgwCU_82Q", "rebuttal_id": "rygdEjy1CX", "sentence_index": 3, "text": "Therefore, a better way of making the most out of little data is critical.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "rJgwCU_82Q", "rebuttal_id": "rygdEjy1CX", "sentence_index": 4, "text": "For example, a 10 \\% increase for only 300 samples acquired, could make a huge difference in a critical field where active learning is most valuable.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "rJgwCU_82Q", "rebuttal_id": "rygdEjy1CX", "sentence_index": 5, "text": "We believe that this is exactly what we manage to achieve with our method and this comes as a result of a better representation of uncertainty during AL.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "rJgwCU_82Q", "rebuttal_id": "rygdEjy1CX", "sentence_index": 6, "text": "Furthermore,  Beluch et al. (2018) showed that going beyond 3 networks in their deterministic ensemble method does not add any significant improvements in terms of performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "rJgwCU_82Q", "rebuttal_id": "rygdEjy1CX", "sentence_index": 7, "text": "Therefore we use 3 stochastic ensembles for our method.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "rJgwCU_82Q", "rebuttal_id": "rygdEjy1CX", "sentence_index": 8, "text": "As for the novelty of this method, although it seems more like an engineering solution, we believe that it makes a significant contribution in the field of deep active learning.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 11]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 0, "text": "We thank for the reviewer for their comments on our work, and we share our responses below.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_none", null]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 1, "text": "1. Novelty: To the best of our knowledge, this is the first paper presenting a simple solution to generating useful auxiliary tasks in a self-supervised manner.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 2, "text": "The idea indeed was inspired by other works in auxiliary learning, but only to the extent that we also use auxiliary tasks to improve performance of a principal task.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 3, "text": "The method is not a heuristic; it is theoretically motivated by use of the double gradient, and inspired by the success of this in meta learning (e.g. MAML [1]).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 4, "text": "If the reviewer thinks our method is an incremental contribution or similar to previous algorithms, please list the specific references.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 5, "text": "2.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 6, "text": "The theoretical insight in this paper comes from the recent advancements in using a double gradient, such as in MAML [1], or understanding what makes a good auxiliary data sampler [2].", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 7, "text": "The inner gradient is based on the standard auxiliary learning loss as proposed in other works, whereas the outer gradient uses this inner gradient to actually learn the auxiliary tasks.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 8, "text": "The use of an outer gradient for auxiliary learning is our key novelty, and has not been used in any works before.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 9, "text": "3. Feature distributions of training and meta-training data (target and auxiliary data in your language) are actually not identical.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [11]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 10, "text": "The \"learning to generalise\" success from our method is due to closing the *existing* distribution shift in these two datasets. If the distributions are identical, then we wouldn't have any improved generalisation from our method.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [11]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 11, "text": "4. Both CIFAR10 and CIFAR100 are the subsets from 80 million tiny images dataset [3].", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 12, "text": "As described in the website and paper, all images are collected from the internet and partially labelled by humans, and thus indeed present a real-world setup rather than a synthetic setup.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 13, "text": "Further, we show that if a harder test set with a more variety exists (CIFAR10.1v6), out method could provide even better generalisation (Figure 4).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 14, "text": "Thus, we hope the reviewer could better explain why you think our algorithm could fail in real-world scenarios.", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [12]]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 15, "text": "[1] Finn et al. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks ICML, 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 16, "text": "[2] Zhang et al. Fine-Grained Visual Categorization using Meta-Learning Optimization with Sample Selection of Auxiliary Data, ECCV 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJl-0ufypm", "rebuttal_id": "HkesnzFKCQ", "sentence_index": 17, "text": "[3] http://people.csail.mit.edu/torralba/tinyimages/", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 0, "text": "We thank the reviewer for the thoughtful review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 1, "text": "Responses:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 2, "text": "- We clarified the unclear parts, and will upload the revised version asap. (Also see below for specific responses.)", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_global", null]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 3, "text": "- It is unclear to us if the reviewer thinks the computational complexity is high, or the mathematical complexity.", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [7]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 4, "text": "With regards to mathematical complexity, we believe the model is actually rather simple (see also other reviews).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 5, "text": "Thus, we assume computational complexity is meant.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 6, "text": "Computational complexity is discussed in detail in the manuscript though, and is certainly not higher than competing methods (in part thanks to the low mathematical complexity of the model).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 7, "text": "See also next point.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 8, "text": "- The datasets we used are as large as the datasets used in other related work in the area.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 9, "text": "To demonstrate CNE's superior scalability, we included another network with around 200.000 nodes and around 1.000.000 edges (http://snap.stanford.edu/data/loc-Gowalla.html), run on a basic single CPU laptop.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 10, "text": "Again, CNE outperforms all other methods in accuracy by a wide margin, and is substantially faster as well.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 11, "text": "The results are included in the revised manuscript.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 12, "text": "Detailed comments:", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 13, "text": "- \"In the introduction, \"it is in general impossible to find an embedding in R^d such that ...\", [...]?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 14, "text": "We apologize for having been a bit brief here, we will clarify this in the revision (uploaded asap).", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [10]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 15, "text": "We meant to say that in network embedding methods that aim to model first-order proximity (where proximity in the embedding space implies a higher probability of being linked), this is a requirement (otherwise, proximity of v and v' would imply they are likely to be linked).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 16, "text": "Thus our argument only applies to such first-order proximity methods.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 17, "text": "Methods that aim to model second-order proximities (where proximity in the embedding space implies a greater overlap between the sets of adjacent nodes), however, are similarly vulnerable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 18, "text": "For example, there can be a 50% overlap (which is highly significant in sparse networks) between the neighborhoods of nodes A and B, as well as between the neighborhoods of nodes B and C, but zero overlap between the neighborhoods of nodes A and C.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 19, "text": "This would mean that nodes A and B need to be embedded close to each other, nodes B and C as well, but nodes A and C distant from each other.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 20, "text": "The triangle inequality makes this hard.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 21, "text": "Finally, these are but examples of how a Euclidean embedding on its own lacks representational power.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 22, "text": "We believe that our empirical results also demonstrate this without having to refer to easy-to-identify problematic situations for pure embedding-based methods.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 23, "text": "- \"In Equation (2), How is P_ij defined exactly [...]?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 24, "text": "They are not parameters: they are numbers between 0 and 1 representing the prior probability of a link between nodes i and j (i.e. prior to seeing the embedding).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 25, "text": "These numbers are such that the prior knowledge of the types described are satisfied in expectation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 26, "text": "In other words, they are implied and can be computed automatically and highly efficiently based on prior work, after one has decided on which prior knowledge to use.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 27, "text": "For details about how P_ij are fitted given such prior knowledge constraints, and how they can be represented efficiently, we have to refer to Adriaens et al. (2017) and van Leeuwen et al. (2016).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 28, "text": "We have however summarized the relevant aspects: the fact that all probabilities P_ij, although there are n^2 of them, can be represented using much fewer parameters, and the fact that they can be fitted highly efficiently (in our experiments, even on the largest networks, this always took only a tiny proportion of the total computation time).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 29, "text": "In the new version to be uploaded soon, this will be further clarified.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [11]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 30, "text": "- \"In Equation (6), the posterior distribution should be P(X|G) [...]?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 31, "text": "No, the equation is correct as stated.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 32, "text": "Footnote 2 warned the reader about this, as we know it is unusual.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 33, "text": "The posterior is a distribution for the network, such that finding the best embedding is indeed a maximum likelihood problem (not a maximum a posteriori problem), even though the likelihood function is computed as a posterior given a prior for the network and a conditional for the embedding given the network.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 34, "text": "We suspect that it is this unusual aspect of the CNE formulation that makes it original with respect to the state-of-the-art.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [12, 12]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 35, "text": "- \"In Table 2 and 3, how are [...]?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 36, "text": "Equation (6) defines the posterior of the network given the embedding, which we maximize w.r.t. the embedding.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 37, "text": "It explicitly depends on the prior probabilities P_ij, which are computed based on the prior knowledge about the degrees or about the block density structure of the adjacency matrix.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "rJldfOy-27", "rebuttal_id": "ryezm5-70X", "sentence_index": 38, "text": "Thus, this information is brought into the model by considering the posterior distribution for the network, where the prior models the degrees of the nodes, or the block structure.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 0, "text": "Thank you for your time and effort of reviewing our paper. Please see our response below.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 1, "text": "Our main contributions include:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 2, "text": "(i) For single-layer random variables (RVs), we propose a unified gradient named GO by exploiting the integration-by-parts idea, which is applicable to continuous/discrete RVs.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 3, "text": "In the special case of single-layer continuous RVs where GO recovers Implicit Rep or pathwise gradients, we consider it\u2019s our contribution to provide a principled explanation (via integration-by-parts) why Implicit Rep and pathwise gradients have low Monte Carlo variance; or in other words, we prove that their implicit differentiation originates from integration-by-parts.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 4, "text": "(ii) For multi-layer RVs, our main contribution is the discovery that with GO (or in other words, the introduced variable-nabla), one can back-propagate gradient information through a nested combination of nonlinear functions and general RVs (including non-reparameterizable continuous RVs, back-propagating through which is challenging).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 5, "text": "Another interpretation of this contribution is that GO enables generalizing the deterministic chain rule to a statistical version.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 6, "text": "Here, we refer to deterministic chain rule as back-propagating gradient through deterministic functions (like neural networks) or reparameterizable RVs (like Gaussian).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 7, "text": "By contrast, statistical chain rule is referred to as back-propagating gradient through more general RVs (including non-reparameterizable ones).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 8, "text": "Of course, statistical chain rule recovers deterministic chain rule for deterministic functions and reparameterizable RVs, because GO recovers the standard Rep.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 9, "text": "(iii) Another 2 minor contributions include Lemma 1 and Corollary 1.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 10, "text": "In Lemma 1, we explicitly prove that our deep GO gradient contains the standard Rep as a special case, in general beyond Gaussian.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 11, "text": "Note neither Implicit Rep nor pathwise gradients can recover Rep in general, because a neural-network-parameterized reparameterization usually leads to a nontrivial CDF.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 12, "text": "In Corollary 1, we reveal the fact that the proposed method degrades into the classical back-propagation algorithm under specific settings.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 13, "text": "Finally, we believe it is interesting to create a consistent architecture, which unifies (a) a GO gradient which contains many popular gradients as special cases, and (b) a more general statistical chain rule developed based on GO which recovers the well-known deterministic chain rule under specific cases.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 14, "text": "For your comments not addressed above, please see our additional response below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 15, "text": "(1) We have made clearer the relationships among the standard Rep, Implicit Rep/pathwise, and our GO in the revised manuscript.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9, 10]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 16, "text": "In the revised paper we have explicitly pointed out that the experiments from (Figurnov et al. 2018, Jankowiack & Obermeyer,2018) additionally support our GO in the special case of single-layer continuous RVs.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 9, 10]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 17, "text": "(2) Please refer to our main contributions summarized above, where other contributions, beyond GO for discrete RVs, are clarified.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 18, "text": "(3) Please refer to our main contributions (ii)-(iii).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 17, 20, 22, 23, 24]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 19, "text": "As stated in our paper, many works tried to solve the problem of stochastic/statistical back-propagation.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 17, 20, 22, 23, 24]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 20, "text": "We consider our contributions in Secs. 4 and 5 as one step toward that final goal.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 17, 20, 22, 23, 24]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 21, "text": "Please note that what\u2019s done in Secs. 4 and 5 is not straight-forward and has not been reported before.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17, 17, 20, 22, 23, 24]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 22, "text": "Since stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015) focuses mainly on reparameterizable RVs, deterministic chain rule as mentioned in main contribution (ii) can be readily applied.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17, 17, 20, 22, 23, 24]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 23, "text": "By contrast, we target towards more general situations in Secs. 4 and 5 where deterministic chain rule might not be applicable, such as for non-parameterizable (continuous) RVs.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17, 17, 20, 22, 23, 24]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 24, "text": "We prove that one can utilize our GO to sequentially back-propagate gradient though non-parameterizable continuous RVs, namely the statistical chain rule mentioned in main contribution (ii).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17, 17, 20, 22, 23, 24]]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 25, "text": "We have revised the last paragraph of the Introduction to make a more explicit summation of our main contributions, as mentioned above.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "rJlsvSSchm", "rebuttal_id": "ryerVgbtT7", "sentence_index": 26, "text": "We hope your concerns have been addressed. If not, further discussion would be welcomed.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxGlK6Ttr", "rebuttal_id": "BJeoaTpujH", "sentence_index": 0, "text": "We thank the reviewer for the constructive feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxGlK6Ttr", "rebuttal_id": "BJeoaTpujH", "sentence_index": 1, "text": "The use of a fixed embedding space $L$ and a separate space $L^\\prime$ was useful as it naturally prevents the collapse of embeddings.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "rJxGlK6Ttr", "rebuttal_id": "BJeoaTpujH", "sentence_index": 2, "text": "However this could be counteracted by stopping the gradient at the right place in the simplified architecture  which was suggested in the original paper and is now described in the updated paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "rJxGlK6Ttr", "rebuttal_id": "BJeoaTpujH", "sentence_index": 3, "text": "As suggested, we have added further analysis of failure cases, and describe strategies for negative mining from these examples.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "rJxGlK6Ttr", "rebuttal_id": "BJeoaTpujH", "sentence_index": 4, "text": "In addition, we have included a brief description of the graph neural network architecture used in Paliwal et al (2019).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10]]}, {"review_id": "rJxGlK6Ttr", "rebuttal_id": "BJeoaTpujH", "sentence_index": 5, "text": "We also include further details on the construction of training set.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12]]}, {"review_id": "rJxGlK6Ttr", "rebuttal_id": "BJeoaTpujH", "sentence_index": 6, "text": "Training a decoder to predict the results of rewrites from the latent space is an interesting idea, but is technically challenging and we felt it was out of scope for this paper.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_global", null]}, {"review_id": "rJxGlK6Ttr", "rebuttal_id": "BJeoaTpujH", "sentence_index": 7, "text": "We managed to counteract the noisiness of predicted embedding by training on noisy embeddings which trains the network to be robust to random changes and improves the prediction of multi-step rewrites significantly.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "rJxGlK6Ttr", "rebuttal_id": "BJeoaTpujH", "sentence_index": 8, "text": "We are grateful for the suggestions that contributed significantly to improving the quality of the paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 0, "text": "We thank the reviewer for their helpful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 1, "text": "We agree that our most surprising results are for SGD under constant step budgets or unlimited epoch budgets.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 2, "text": "However the behaviour of SGD under constant epoch budgets has generated a lot of debate in the literature in recent years, and we felt it was important to address this simple case first.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 3, "text": "We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 4, "text": "1. Ma, Bassily and Belkin also introduced the notion of two regimes, however their theory holds for convex losses in the interpolating regime.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 5, "text": "We will discuss their contribution explicitly in the updated text.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 6, "text": "Our discussion in section 2 clarifies why the two regimes arise in practical deep learning models for which these conditions may not hold.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 7, "text": "2.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 8, "text": "Our paper is the first to relate the two regimes of SGD to the popular analogy between SGD and stochastic differential equations (SDEs).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 9, "text": "As we show in later sections, this perspective is crucial to understanding the influence of batch size and learning rate on test accuracy.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 10, "text": "A common criticism of this analogy is that SGD noise is not Gaussian when the batch size is small.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 11, "text": "To our knowledge, we are the first to show that the analogy between SGD and SDEs holds for non-Gaussian short-tailed noise (appendix B).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 12, "text": "3. We clarify the differences to some other recent papers in our reply to reviewer 1.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 13, "text": "Two reviewers complained that it was difficult to tell from the text which contributions are novel and which also appear in previous works.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 14, "text": "We apologise for this.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 15, "text": "It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4, 5, 6, 7, 8, 9, 10, 11]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 16, "text": "Turning to our generalization experiments in sections 4 and 5.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4, 5, 6, 7, 8, 9, 10, 11]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 17, "text": "We agree that many authors have proposed that SGD noise enhances generalization.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 18, "text": "Most notably, Keskar et al. argued that large minibatches perform worse than small minibatches on the test set, even when both achieve similar performance on the training set.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 19, "text": "However their experiments do not provide convincing evidence for this claim, because they tuned the learning rate with small batches and then used the same learning rate value with large batches.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 20, "text": "A convincing experiment should independently tune the learning rate at all batch sizes under a constant step budget, and it should use a realistic learning rate decay schedule.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 21, "text": "Indeed, Shallue et al. recently argued that no existing paper has provided convincing evidence that small batch sizes generalize better than large batch sizes under constant step budgets, and they state in their abstract \u2018We find no evidence that larger batch sizes degrade out-of-sample performance\u2019.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 22, "text": "Meanwhile, Zhang et al. argued that optimization in deep learning is well described by a noisy quadratic model which predicts that increasing the batch size should always enhance performance under constant step budgets.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 23, "text": "To our knowledge, our experimental results in section 4 are the first to provide convincing evidence that very large minibatches do perform worse than small batch sizes on the test set, even under constant step budgets and when the learning rate is independently tuned.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 24, "text": "We believe this is an important contribution.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 25, "text": "Meanwhile, our results in section 5 suggest that SGD has an optimal temperature early in training which promotes generalization and is independent of the epoch budget.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 26, "text": "In response to the reviewer\u2019s specific comments:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 27, "text": "1) Looking at Figure 1c, while the optimal learning rate at 8k with Momentum is 4, the error bars at this batch size range from 4 to 32.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 15, 16, 17]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 28, "text": "These error bars can be very large in the curvature regime, precisely because the optimal learning rate is close to instability.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 15, 16, 17]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 29, "text": "2) Yes, Momentum will help under constant step budgets if the batch size is large, since it enables us to achieve larger effective learning rates which are beneficial for generalization.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 30, "text": "We will add additional experiments to the text to clarify this.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18]]}, {"review_id": "rJxkq6waYr", "rebuttal_id": "HkemB2WXiB", "sentence_index": 31, "text": "3) We will clarify the meaning of warm up, epoch budget and step budget as requested.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [19]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 0, "text": "We sincerely appreciate your constructive comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 1, "text": "We respond to your main concerns below:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 2, "text": "1. The Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 3, "text": "- To demonstrate that strong generalization performance of Meta-Dropout is not the effect of using larger number of model parameters, we doubled the number of channels for the base model and report its performances (MAML(x2)).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 4, "text": "Models\t\t   #param.\tOmni-1shot\tOmni-5shot\tmimg-1shot\tmimg-5shot", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 5, "text": "MAML\t\t   x1\t        \t95.23+-0.17\t98.38+-0.07\t49.58+-0.65\t64.55+-0.52", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 6, "text": "MAML(x2)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 7, "text": "x4", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 8, "text": "94.96+-0.16\t98.36+-0.08", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 9, "text": "48.19+-0.64", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 10, "text": "65.84+-0.52", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 11, "text": "Meta-SGD         x2", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 12, "text": "96.16+-0.14", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 13, "text": "98.54+-0.07", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 14, "text": "48.30+-0.64", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 15, "text": "65.55+-0.56", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 16, "text": "Meta-dropout  x2", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 17, "text": "96.63+-0.13\t98.73+-0.06", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 18, "text": "51.93+-0.67", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 19, "text": "67.42+-0.52", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 20, "text": "The number of parameters of MAML(chx2) is four times of that of MAML, while Meta-dropout is only doubled.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 21, "text": "Nonetheless, MAML(chx2) does not improve on MAML, demonstrating that the effectiveness of meta-dropout does not simply come from using larger number of parameters.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 22, "text": "Meta-SGD also doubles the number of parameters in the base MAML model, but is significantly outperformed by Meta-dropout.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 23, "text": "We want to emphasize that Deterministic meta-dropout is also one of our models, and that its good performance does not hurt our claim on the effectiveness of the multiplicative noise.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 24, "text": "This is because meta-dropout consists of two parts: meta-learned deterministic multiplicative perturbation and random noise.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 25, "text": "Thus the deterministic meta-dropout still \u201clearns to perturb\u201d, although not random, and is actually a core component of meta-dropout (See Table 3 in the revision).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxKsYxycS", "rebuttal_id": "rJgYVv-PoB", "sentence_index": 26, "text": "Please also see our response to the Reviewer #3, comment #4.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 0, "text": "Thank you for your fruitful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 1, "text": ">> 1.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6, 7, 7, 9, 10]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 2, "text": "[...]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6, 7, 7, 9, 10]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 3, "text": "One observation from the submission is that the token set may need to be very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive [...] I think some more motivation or exploration (what kind of information did BERT learn) is needed to understand why that is the case.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6, 7, 7, 9, 10]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 4, "text": "Our BERT vocabulary sizes (13.5k for the gumbel version and 23k for the k-means version) compare favorably to the setups commonly used in NLP where vocabularies are double or triple of our sizes.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 7, 9, 10]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 5, "text": "We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [6, 7, 7, 9, 10]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 6, "text": "Here we focus on a new quantization method evaluated via downstream performance in phone and speech recognition settings by employing models that worked well (and were extensively tuned) in NLP contexts.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 7, 9, 10]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 7, "text": ">> 2.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 8, "text": "A more economical approach is to use BERT-trained model as initialization for acoustic model training, which is the classical way how RBMs pre-training were used in ASR.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 9, "text": "Yes, this is an interesting avenue for future work!", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 10, "text": "We did not follow this direction due to two motivations: first, our aim is to contribute a new quantization scheme for audio data that is trained to predict the context in a self-supervised way.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 11, "text": "Second, we wanted to show that good performance can be achieved with discretized audio on actual speech tasks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 12, "text": ">> 3. One concern I have with discrete representation is how robust they are wrt different dataset.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 13, "text": "We agree that an ablation study on robustness of the embeddings across different datasets would be very interesting.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 14, "text": "Here we are mostly focusing on relatively clean data (WSJ, TIMIT, Librispeech) following the original wav2vec paper but we would be interested in exploring robustness in the future.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 15, "text": "However we note that representations transfer at least well across datasets from the \u201cclean speech\u201d domain: vq-wav2vec and BERT is only trained on Librispeech and never tuned on TIMIT/WSJ.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 16, "text": ">> 4.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 17, "text": "Another curious question is whether the features would still provide as much improvement when a stronger ASR system than AutoSeg (e.g., Lattice-free MMI) is used.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 18, "text": "The original wav2vec paper (Schneider et al., 2019) reports better results than LF-MMI on the WSJ benchmark, however, the two setups are not strictly comparable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 19, "text": "In some sense, the LF-MMI result has an edge because it is based on a phoneme-based ASR system which is typically stronger than the character-based ASR system used with wav2vec.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "rJxLRp2s9H", "rebuttal_id": "ByeQLeYhsH", "sentence_index": 20, "text": "We agree that evaluation on stronger baselines is an important future direction though.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [16]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 0, "text": "Dear Reviewer:", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 1, "text": "Thank you for your valuable comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 2, "text": "We have addressed typos in the revision accordingly.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 3, "text": "And please find our response as follows.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 4, "text": "-  Can you be more specific about the gains in training versus inference time?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 5, "text": "We would like to emphasize that the our goal is to speed up the inference time for softmax, so we do not include any comparisons in terms of training time.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 6, "text": "According to our experiments, most speedup can be achieved in few epochs (given all other layers are pre-trained) so that the training time increase is not significant compared to the original one.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 7, "text": "- You motivate some of the work by the fact that the experts have overlapping outputs. Maybe in section 3.7 you can address how often that occurs as well?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 8, "text": "Thanks for the suggestion.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 9, "text": "We demonstrate that ambiguous words are often overlapped between clusters as illustrated in Figure 3(b).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 10, "text": "We added one more Figure in Appendix B, Figure (b), to demonstrate the distribution of overlapping.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 11, "text": "- It wasn't clear how the sparsity percentage on page 3 was defined?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 12, "text": "Sorry for the possible confusion.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 13, "text": "The sparsity in page 3 means the percentage of pruned words.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 14, "text": "We have added more clarifications in the revised version.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 15, "text": "- Can you motivate why you are not using perplexity in section 3.2?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 16, "text": "We use top-k accuracy, instead of perplexity, because approximating top-k is required for most inference tasks in practice (see [1]).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 17, "text": "Perplexity captures the normalized log-likelihood of all possible words, while top-k accuracy is a better measure for inference speedup for top-k retrieval.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 18, "text": "For example, in some extreme cases, if a word only has a very small probability which makes it unpredictable at all (i.e. couldn\u2019t be retrieved by top-k for any reasonably small k)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 19, "text": ", it could still have a huge impact in terms of perplexity, but has a much smaller impact on top-k accuracy, which seems more reasonable given the goal of top-k retrieval.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "rJxPUFHc3m", "rebuttal_id": "HJxF95J5aX", "sentence_index": 20, "text": "[1] Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS), NIPS 2014", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 0, "text": "We would like to thank reviewer #1 for the constructive critics.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 1, "text": "We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 2, "text": "We have updated the paper pdf and kindly ask the reviewer to take another look.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 3, "text": "THEORY:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 4, "text": "Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 5, "text": "Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 6, "text": "Theorem 1 formally states when k-addition is defined for spherical spaces.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 7, "text": "Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 8, "text": "Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 9, "text": "It also gives the derivative of the distance function w.r.t. curvature around 0.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 10, "text": "Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 11, "text": "Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 12, "text": "This is a desirable property for Riemannian averaging.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 13, "text": "Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 14, "text": "LITERATURE:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18, 19, 20, 21, 22, 23, 24]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 15, "text": "A more careful review of the literature has been incorporated into the introduction section.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17, 18, 19, 20, 21, 22, 23, 24]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 16, "text": "We have also incorporated the recent works [1,2] to appear soon at Neurips\u201919 (their text became available very recently and after the ICLR submission deadline).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17, 18, 19, 20, 21, 22, 23, 24]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 17, "text": "WRITING:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [25, 26, 27, 28, 29, 30]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 18, "text": "We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [25, 26, 27, 28, 29, 30]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 19, "text": "We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [25, 26, 27, 28, 29, 30]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 20, "text": "MOTIVATION:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 21, "text": "We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 22, "text": "This is in accordance with the previous related work on non-Euclidean embeddings.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 23, "text": "Moreover, we have detailed some limitations of Euclidean spaces in Appendix B.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 24, "text": "EXPERIMENTAL SETTINGS & HYPERPARAMETERS:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11, 12]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 25, "text": "Are added to section 4 and appendix E", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10, 11, 12]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 26, "text": "EXPERIMENTAL RESULTS:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11, 12]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 27, "text": "Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10, 11, 12]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 28, "text": "We believe further experimental investigations are needed to better train non-Euclidean graph neural network models.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10, 11, 12]]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 29, "text": "[1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips\u201919", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 30, "text": "[2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips\u201919", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 31, "text": "We would love to hear your feedback on our substantially improved paper (based on your suggestions).", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxr_fmNFS", "rebuttal_id": "ByetjhULsH", "sentence_index": 32, "text": "Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 0, "text": "Thank you very much for your time, and your constructive comments, we are looking forward to further discussions!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 1, "text": "We answer your questions and concerns in the following.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 2, "text": "> \"The advantage of INN is not crystal clear to me versus other generative methods such as GAN and VAE.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 3, "text": "It is indeed possible to adapt other network types to the task of predicting conditional posteriors.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 4, "text": "We are currently setting up experiments for detailed analysis of the respective advantages and disadvantages and will report about these results in a future paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [3]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 5, "text": "In the present paper, we focus on demonstrating that high-quality posteriors can actually be learned using bi-directional training as facilitated by INNs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 6, "text": "Concerning the comments/questions:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 7, "text": "1.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 8, "text": "> \"could the authors elaborate on the comparison against cGAN\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 9, "text": "cGAN generators are at an inherent disadvantage relative to INNs, because they never see ground-truth pairs (x,y) directly -- they are only informed about them indirectly via discriminator gradients.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 10, "text": "This it not a problem for simple relationships, e.g. between images x and attributes y, and cGANs work very well there.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 11, "text": "However, it makes learning of complicated forward processes much harder and may cause the resulting posteriors to be inaccurate.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 12, "text": "Moreover, INNs are forced to embed every training point x somewhere in the latent space, whereas cGAN generators may fail to allocate latent space for some x, because this is never explicitly penalized.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 13, "text": "This can lead to mode collapse and insufficient diversity.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 14, "text": "> \"Can cGAN be used to estimate the density of X (posterior or not)?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 15, "text": "cGANs can in principle do this by choosing a generator architecture with tractable Jacobian (using e.g. coupling layers or autoregressive flow), but we are not aware of published results about this possibility.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 16, "text": "2.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 17, "text": "> \"For the bidirectional training, did the ratios of the losses (L_z, L_y, L_x) have to be changed, or the iterations of forward/backward trainings have to be changed (e.g., 1 forward, 1 backward vs. 2 forward, 1 backward)?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 18, "text": "Yes, the weights of the losses are considered as hyperparameters, because the magnitude of MMD-based losses depends on the chosen kernel function.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 19, "text": "Hyperparameter optimization suggested an up-weighting of MMD-based losses by a factor of 5, to give them approximately equal impact as the supervised loss.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 20, "text": "For the iterations, we accumulated gradients over one forward and one inverse network execution before each parameter update.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 21, "text": "We also tried alternating parameter updates after each forward and backward pass, which resulted in equal accuracy, but was a bit slower.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 22, "text": "We did not experiment with other ratios than 1:1.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 23, "text": "3.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 24, "text": "> \"Is this to effectively increase the intermediate network dimensions?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 25, "text": "This is precisely the reason: It improves the representational power of the INN, as mentioned in Sec. 3.2 and discussed in our response to reviewer 1.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 26, "text": "At present, we find this is only necessary for the toy problem in Fig. 2.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 27, "text": "> \"It seems that there needs some way to enforce them to be zero to ensure that the propagation happens only among the entries belonging to the variables of interests (x, y and z).\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 28, "text": "This is correct.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 29, "text": "We explicitly prevent information from being hidden in the padding dimensions in the following way:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 30, "text": "A squared loss ensures that the amplitudes are close to zero.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 31, "text": "In an additional inverse training pass, we overwrite the padding dimensions with noise of the same amplitude, and minimize their effect via a reconstruction loss.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 32, "text": "We will add this to the relevant paragraph in the paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [17]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 33, "text": "4.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 34, "text": "> \"I am curious if this model could succeed on higher dimensional data\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 35, "text": "Works such as [1, 2, 3] (also cited in our paper) have shown that the coupling layer architecture in general works well with images.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 36, "text": "These works use maximum likelihood training, i.e. exploit the tractable Jacobians to maximize the likelihood of the data embedding in latent space.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 37, "text": "To scale-up our approach, we may need to replace MMD loss with maximum likelihood as well, and first experiments with this show promising results, see", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 38, "text": "https://i.imgur.com/ft09Pk9.png .", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 39, "text": "[1] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv:1605.08803, 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 40, "text": "[2] Diederik P Kingma and Prafulla Dhariwal.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 41, "text": "Glow: Generative flow with invertible 1x1 convolutions. arXiv:1807.03039, 2018", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 42, "text": "[3] Schirrmeister, Robin Tibor, et al. \"Generative Reversible Networks.\" arXiv:1806.01610, 2018", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 43, "text": "We have uploaded a revised version of the paper, thank you again for your suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 44, "text": "The changes and additions are highlighted in red font for convenience.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 45, "text": "Please also note that by adding these changes, our page count increased by half a page beyond the recommended 8 pages.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxS7urch7", "rebuttal_id": "SylKBy6Eam", "sentence_index": 46, "text": "If this presents a problem, we can attempt shorten the paper accordingly.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 0, "text": "We thank the reviewer for the suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 1, "text": "Q1: Robot design were explored in (Sims, 1994) etc. The novelty of the paper is fairly incremental.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 2, "text": "We respectfully disagree and believe our contributions are significant.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 3, "text": "We note that only NGE among all the baselines has the ability to optimize both the graph G and the controller parameters.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 4, "text": "Graph neural network formulation is KEY here, enabling it to perform this efficient policy transfer.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 5, "text": "To the best of our knowledge, the traditional methods (such as (Sims, 1994)) require re-optimizing parameters of the controllers from scratch for each different topologies, which is computationally demanding and breaks the joint-optimization.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 6, "text": "To further showcase our work with respect to prior art, we added (Sims, 1994) as an additional baseline in the latest revision.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 7, "text": "We refer the reviewer to the general response for details.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 8, "text": "NGE has about 2x performance of (Sims, 1994) in both fish and walker environments.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 9, "text": "Moreover, we argue the videos of (Sims, 1994) might be confusing as it mixes the results of policy evolution from human-designed robots and structure evolution.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 10, "text": "Q2: Can it be applied to more complex morphologies?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 11, "text": "Humanoid etc.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 12, "text": "maybe?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 13, "text": "NGE can be applied to evolve humanoids, however, there are two major difficulties in doing that in practice.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 14, "text": "1. Training humanoid controllers is of orders of magnitude more difficult than training cheetah (Schulman, 2017).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 15, "text": "2. To evolve realistic humanoid structure (e.g. hands, symmetrical limbs), one would need to have more realistic environments that better reflect tasks and complexity in the real world.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 16, "text": "However, we agree that this is a very interesting direction for the future.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 17, "text": "Q3: Comparison to more baseline, for example models with no message passing.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 18, "text": "We thank the reviewer for pointing out the baseline of no message passing in GNN, which we named as ESS-BodyShare.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 19, "text": "In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 20, "text": "In general, NGE has significant improvement both quantitatively and qualitatively.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 21, "text": "We refer the reviewer to the general response for further information.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 22, "text": "Specifically for ESS-BodyShare baseline:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 23, "text": "|", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 24, "text": "NGE     | ESS-BodyShare", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 25, "text": "fish         |  70.21   |  54.97", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 26, "text": "(78.3% of NGE)", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 27, "text": "Walker   |", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 28, "text": "4157.9 |   2185.1 (52.5% of NGE)", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 29, "text": "In environment where global information is needed (for example, walker with multiple rigid body contact), the performance is jeopardized. But in an easier environment, message passing is less needed.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 30, "text": "Q4: Clarification of Figure-4 (Section-4.2)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 25, 26, 27]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 31, "text": "Our aim was to show that in the case where the human-engineered topology needs to be preserved, it is better to co-evolve the attributes and controllers with NGE rather than only training the controllers (controllers are trained from scratch for both NGE and baselines).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24, 25, 26, 27]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 32, "text": "The x-axis was scaled according to the number of updates.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24, 25, 26, 27]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 33, "text": "We apologize for the lack of clarity.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [24, 25, 26, 27]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 34, "text": "We revised the x-axis from \u201cgenerations\u201d to parameter \u201cupdates\u201d in the latest revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [24, 25, 26, 27]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 35, "text": "In the latest revision, we also included the curve where the topologies are allowed to be changed, which leads to better performance, but does not necessarily preserve the initial structure.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [24, 25, 26, 27]]}, {"review_id": "rJxUPOqhhX", "rebuttal_id": "BkeBMdpg07", "sentence_index": 36, "text": "Schulman, 2017. \"Proximal policy optimization algorithms.\" arXiv preprint arXiv:1707.06347 (2017).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 0, "text": "Thank you for your constructive comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 1, "text": "We are glad that you found our experiments extensive and that our approach provides significant improvements.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_global", null]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 2, "text": "In response to your comment that \"similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts\" we would like to take this opportunity to clarify the novelty of our approach.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 3, "text": "First, with regards to (Mescheder et al 2018), our method is not simply the application of existing gradient penalties (GPs) in the context of semi-supervised learning.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 4, "text": "Our approach is conceptually different since the regularizer proposed by (Mescheder et al 2018) is an (isotropic) ambient regularizer in the input space, whereas the regularizer we used performs (anisotropic) smoothing on the manifold parametrized by the latent generative model.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 5, "text": "We believe we are the first to show the benefits of anisotropic Jacobian regularizers in the context of semi-supervised learning.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 6, "text": "Moreover, an important contribution of our work is the efficient computation of such gradient penalties in the context of semi-supervised learning.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 7, "text": "Current application of such penalties uses the exact Jacobian which is especially computationally expensive in the case of semi-supervised learning as it is now a tensor (one matrix per class in the case of Improved GAN), which quickly becomes intractable with large numbers of classes.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 8, "text": "We proposed and demonstrated the effectiveness of an efficient (non-obvious) approximation of the Jacobian-based regularizer which significantly accelerates training.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 9, "text": "We provide responses to further questions/comments below:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 10, "text": "Q: \"A comparison with Graph Convolutional Network based techniques seems appropriate (e.g. Kipf and Welling 2017).\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 11, "text": "A: Methods such as (Kipf and Welling 2017) are designed for semi-supervised learning on graphs; here a key challenge is in defining the structure (edges and edge weights) of the graph.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 12, "text": "Defining the graph structure is not trivial for the image datasets commonly used as benchmarks.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 13, "text": "In this light, one of the advantages of our approach is that the manifold (graph structure) is implicitly learned by the GAN, thus avoiding the need to explicitly define it.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 14, "text": "That said, it is an interesting direction for future work and we thank the reviewer for the suggestion.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [12]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 15, "text": "Q: \"How do the FID/Inception improvements compare to (Mescheder et al 2018)?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 16, "text": "A: We cannot directly compare our image generation scores with those reported in (Merscheder et al 2018) as we used different GAN architectures; for reference, they reported an Inception score of 6.2.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [13]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 17, "text": "We have updated the paper with Inception/FID scores from the ambient regularizer on CIFAR-10 (Table 4), which is an approximation of the proposed regularizer in (Merscheder et al. 2018) using stochastic finite differences.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 18, "text": "As mentioned earlier, it is not practical to compare the non-approximated regularizer due to the substantial increase in computational complexity in the semi-supervised GAN setting.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [13]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 19, "text": "We observe that ambient regularization gives better image generation scores; however it does not perform as well on semi-supervised learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 20, "text": "This tradeoff between image generation and semi-supervised learning performance was previously reported in (Salimans et al., 2016) \"Improved Methods for Training GANs\".", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 21, "text": "Q: \"It would be interesting to discuss why the FID score for SVHN gets worse in presence of 1000 labels.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 22, "text": "A: We re-checked our FID computation for this case and fixed a bug.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 23, "text": "We have updated the paper with updated FID scores; we note there is a high variance in the FID so while there is an improvement on average, it occasionally may not be better.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 24, "text": "Q: \"Although there is a clear improvement in FID scores for Cifar10. It would be informative to show the generated images w/ and w/o manifold regularization.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 25, "text": "A: We have included generated images with and without manifold regularization in the Appendix (Figure A5 for CIFAR-10, Figure A6 for SVHN) - these show clear improvements as well.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 26, "text": "Q: \"More analysis should be provided on why (Kumar et al 2017) perform so well on SVHN.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 27, "text": "A: We note that although on average the method of (Kumar et al 2017) performs better on SVHN, the standard deviation is also much higher than many other methods (including ours) on both SVHN and CIFAR-10 indicating that it is not as robust.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 28, "text": "Q: \"It should be stated that bold values in tables do not represent best results (as it is usually the case) but rather results for the proposed approach.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18]]}, {"review_id": "rkepvb7c2Q", "rebuttal_id": "H1g061y90Q", "sentence_index": 29, "text": "A: We have revised the tables such that bold values represent the best results for clarity.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18]]}, {"review_id": "rkeqvIaJor", "rebuttal_id": "B1lphCIxoB", "sentence_index": 0, "text": "We agree with most of the comments.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_global", null]}, {"review_id": "rkeucgq15r", "rebuttal_id": "r1gNsdHnjH", "sentence_index": 0, "text": "Thank you for your appreciation of our work!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rkeUDe_i37", "rebuttal_id": "BJlxyt63kN", "sentence_index": 0, "text": "Your interpretation of section 3 is exactly right.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 10, 12, 13]]}, {"review_id": "rkeUDe_i37", "rebuttal_id": "BJlxyt63kN", "sentence_index": 1, "text": "Thank you for suggesting additional experiments to better understand the behavior of the scratchpad component.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "rkeUDe_i37", "rebuttal_id": "BJlxyt63kN", "sentence_index": 2, "text": "We would like to note that beyond the gains across all evaluated quantitative metrics (bleu, rouge, meteor), our method shows substantial gains on human evaluations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "rkeUDe_i37", "rebuttal_id": "BJlxyt63kN", "sentence_index": 3, "text": "In future work we propose to use our method to generate a large dataset and evaluate its performance.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [8]]}, {"review_id": "rkeUDe_i37", "rebuttal_id": "BJlxyt63kN", "sentence_index": 4, "text": "We don\u2019t claim to be the first to generate questions from logical form, but the experiments within show that our approach is superior to standard approaches in the literature.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 0, "text": "Thank you to Reviewer 1 for noting the clarity of our presentation and reproducibility.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 1, "text": "We also appreciate the constructive criticism and thought that went into your review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 2, "text": "We spent a considerable amount of time trying to fulfill the reviewer\u2019s request to match state of the art (SOTA) on PTB.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 3, "text": "To get SOTA on PTB, we need massive architectures, which considerable computing power and experimentation at the extreme limit of what is achievable for our team.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 4, "text": "Still, we pursued two directions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 5, "text": "First, we tried to reimplement an architecture similar to  Melis et al. 2017.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 6, "text": "However, they did not publish their code, hyperparameters, or weights, requiring re-implementing and re-training from scratch.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 7, "text": "We tried this path, but soon realized we would not be done in time (especially with a hyperparameter search).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 8, "text": "We then tried to weave neuromodulation and differentiable plasticity into the architecture and code base of Merity et al., ICLR 2018 (also tied for SOTA).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 9, "text": "However, while they could simply leverage existing PyTorch implementations of LSTMs (written in extremely fast C++), we had to re-implement LSTMs \u201cby hand\u201d (i.e. as a series of connected layers) in PyTorch to introduce plasticity and neuromodulation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 10, "text": "As a result, our networks thus ran considerably slower, by more than 10x (not because our method is intrinsically slower, but just for lack of engineering optimizations on our bespoke Python implementations; we confirmed this by observing that a similar \u201chand-built\u201d reimplementation of simple, non-plastic LSTMs ran similarly slower, while producing results identical to Merity et al.).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 11, "text": "These experiments are thus unfortunately still running.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 12, "text": "For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 13, "text": "That said, we still believe the results in the current paper demonstrate the benefits of our techniques on a sizable model, and thus it would benefit the community to allow people to know about, and build upon, these new methods and results.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 14, "text": "The purpose of the present paper is to introduce a novel technique and show that it can produce an advantage in realistic settings, which we believe our PTB task confirms.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 15, "text": "Our claim is that, all other things being equal (especially the number of parameters), a neuromodulated plastic LSTM outperformed a standard LSTM on this particular benchmark task.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 16, "text": "We do **not** want to claim that our results are anywhere near SOTA.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 17, "text": "We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 18, "text": "Additionally, philosophically, If SOTA results are the bar for all papers to be accepted into conferences like ICLR, then those venues will be the exclusive domain of those with either the computation or time (i.e. large-scale resources) to dedicate to such results.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 19, "text": "In that case, many cutting edge ideas will by necessity be excluded from the discussion, as will many research groups.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 20, "text": "Moreover, insisting on papers to be SOTA to be accepted also likely encourages p-hacking and shoddy science to game the results (even if unintentionally), reducing the quality of science our community tries to build on.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 15, 16]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 21, "text": "Re: \"Parameters of the model\": All trainable parameters of the Hebbian synapses (alpha and w in Equation 1, plus the neuromodulation parameters) are included in this parameter count.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 22, "text": "To equalize the number of parameters across architectures, we reduce the number of hidden units in the plastic models in comparison to the non-plastic baseline.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 23, "text": "We have clarified this in the text.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 24, "text": "Re: \"Attention\": Non-trainable, homogenous plasticity can indeed be compared to a form of attention, i.e. \u201cattending to the recent past\u201d in the words of Ba et al. 2016.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 25, "text": "However, differentiable plasticity allows for the plasticity of each connection to be trained; as a result, different connections play different roles and it is not at all clear that the analogy with attention remains relevant (see e.g. the clever mechanisms automatically implemented by the trained plasticity connections in the image completion experiment of the Differentiable Plasticity paper, Miconi et al. 2018, sections 4.3 and S.3, which can hardly be described as simply \u201cattention\u201d)", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "rkg2K06S2m", "rebuttal_id": "HyxE2PPcRm", "sentence_index": 26, "text": "Re: \"Style (font)\": We used the template and do not see the discrepancy. Can you clarify? We are happy to fix it.", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [20]]}, {"review_id": "rkgfiCmT5r", "rebuttal_id": "ryln6geqiS", "sentence_index": 0, "text": "We thank the reviewer for the feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rkgfiCmT5r", "rebuttal_id": "ryln6geqiS", "sentence_index": 1, "text": "In the following, we address the comments individually:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rkgfiCmT5r", "rebuttal_id": "ryln6geqiS", "sentence_index": 2, "text": "-  Comparison with conventional triplet methods using images and their corresponding RGB images", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 14]]}, {"review_id": "rkgfiCmT5r", "rebuttal_id": "ryln6geqiS", "sentence_index": 3, "text": "We did not consider comparisons with conventional triplet approaches: the message of our paper was not to demonstrate the utility of ordinal embedding approaches over conventional (representation-based) triplet approaches.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 14]]}, {"review_id": "rkgfiCmT5r", "rebuttal_id": "ryln6geqiS", "sentence_index": 4, "text": "It was rather to show that when input representations are NOT available, we provide a scalable approach to solve the ordinal embedding problem.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 14]]}, {"review_id": "rkgfiCmT5r", "rebuttal_id": "ryln6geqiS", "sentence_index": 5, "text": "We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 14]]}, {"review_id": "rkgfiCmT5r", "rebuttal_id": "ryln6geqiS", "sentence_index": 6, "text": "-  More synthetic experiments comparing the various ordinal embedding approaches", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "rkgfiCmT5r", "rebuttal_id": "ryln6geqiS", "sentence_index": 7, "text": "There is a large literature that compares existing ordinal embedding approaches, and in order to not overload the figures, we had decided to just compare against the most popular traditional algorithms. But we can definitely add more comparisons in the revision of the paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "rkgfiCmT5r", "rebuttal_id": "ryln6geqiS", "sentence_index": 8, "text": "-", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "rkgfiCmT5r", "rebuttal_id": "ryln6geqiS", "sentence_index": 9, "text": "The \u201cclaim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18]]}, {"review_id": "rkgfiCmT5r", "rebuttal_id": "ryln6geqiS", "sentence_index": 10, "text": "We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18]]}, {"review_id": "rkgfiCmT5r", "rebuttal_id": "ryln6geqiS", "sentence_index": 11, "text": "We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 0, "text": "Thank you for your comments and suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 1, "text": "We will address the issues you mentioned.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 2, "text": "1.\tWe have added more details about sampling strategy to section 3.1 in the new version, with mathematical definition and dimensionality explicitly described.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 3, "text": "2.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 4, "text": "We did not argue the computation cost of 3D kernels in section 3.2.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 5, "text": "Instead, we argued that 3D kernels usually are not large enough to cover the holistic video so that Max Pooling operations are applied in most 3D CNNs to enlarge the receptive field.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 6, "text": "Yet this causes the loss of detailed information.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 7, "text": "But indeed, in order to preserve the details and increase the receptive field, simply enlarge the 3D kernels to cover the holistic video will bring enormous computation cost.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 8, "text": "Considering a video of size UxTxHxW, where U is number of action units, and T,H,W means temporal length, height and width of each action unit.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 9, "text": "In order to model the interaction of 1st frame and the (kT+1)th frame, a 3D kernel of at least (kT+1) x k x k has to be applied, which brings linear increasing of computation cost.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 10, "text": "Yet with our 4D kernels, a simple k x k x k x k will cover the interaction from the 1st frame to the (kT+1)th frame, because 4D convolution can go beyond space and time, making the long range interaction possible.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 11, "text": "For parameters, 4D kernels are k times larger than 3D kernels.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 12, "text": "So in order to reduce the parameters, we apply k x k x 1 x 1 kernels in most experiments, as mentioned in section 3.2 and section 4.2.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 13, "text": "We also propose Residual 4D Blocks to ease the optimization and preserve short-term details.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 14, "text": "3.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 15, "text": "Yes, Mini-Kinetics and Kinetics contain videos about 10 seconds.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 16, "text": "For Something-Something-v1, they select one frame from every 12 frames so that the original video should be around 430 frames to 860 frames, which are of about half or one minute.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 17, "text": "We agree that these are still too short to be called videos.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 18, "text": "Yet here we call our method \u201cvideo-level\u201d mainly to stress that our V4D models the holistic duration instead of a certain part.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 19, "text": "Additionally, we evaluated our V4D for untrimmed video classification on ActivityNet v1.3, which contains videos of 5 to 10 minutes and typically large time lapses of the videos are not related with any activity of interest.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 20, "text": "The very competitive result is now reported in the appendix of the second version of paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "rkgoA2UCKB", "rebuttal_id": "ByeBzTMtiS", "sentence_index": 21, "text": "Hopefully our rebuttal could stress your concerns. If there are still any possible issues, please don\u2019t hesitate to tell us and we will response as soon as possible.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 0, "text": "Thank you for your time and effort of reviewing our paper. Please see our response below.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 1, "text": "\\kappa is an assistant notation to remove the ambiguity of the two \\gammas in G_{\\gamma}^{q_{\\gamma} (y)}. \\kappa stands for the parameter/variable of which the gradient information is needed.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 2, "text": "For example,", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 3, "text": "(i) g_{\\kappa}^{q_{\\gamma}(y)} = frac{-1}{q_{\\gamma}(y)} \\nabla_{\\kappa} Q_{\\gamma}(y)}, where \\kappa is \\gamma, as in Theorem 1;", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 4, "text": "(ii) g_{\\kappa}^{q_{\\gamma}(y|\\lambda)} = frac{-1}{q_{\\gamma}(y|\\lambda)} \\nabla_{\\kappa} Q_{\\gamma}(y |\\lambda), where \\kappa could be \\gamma or \\lambda.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 5, "text": "Eqs. (7) and (8) are the foundations GO is built on, but they are not our GO.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 6, "text": "GO is defined in Eq. (9) of Theorem 1.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 7, "text": "For Eq. (9), yes, y_{-v} is selected from one sample y in the experiments.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 8, "text": "But GO is not the local expectation gradient (Titsias & Lazaro-Gredilla, 2015), because GO uses different information (the derivative of the CDF and the difference of the expected function).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 9, "text": "As pointed out in the last paragraph of Sec. 3, when y_v has finite support and the computational cost is acceptable, one could use the local idea from Titsias & Lazaro-Gredilla(2015) for lower variance, namely analytically evaluate a part of expectations in Eq. (9).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 10, "text": "For a detailed example, please refer to Appendix I.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 11, "text": "The main difference between the local expectation gradient and the proposed GO is that the latter is applicable to where the former might not be applicable, such as where y_v has infinite support or the computational cost for the local expectation is prohibitive.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 12, "text": "Please note our GO is defined in Eq. (9).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 13, "text": "As pointed out in the last paragraph of Sec. 3, calculating Dy[f(y)] (requiring V+1 f evaluations) could be computationally expensive.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 14, "text": "We also stated there, \u201cfor f(y) often used in practice special properties hold that can be exploited for ef\ufb01cient parallel computing\u201d.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 15, "text": "We took the VAE experiment in Sec 7.2 as an example and gave in Appendix I its detailed analysis/implementation, in which you might be interested.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 16, "text": "More specifically, the two bullets after Table 4, should be able to address your question on fast speed.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 17, "text": "Also, as noted in the penultimate paragraph of Sec. 7.2, less parameters (without neural-network-parameterized control variant) could be another reason for GO\u2019s efficiency.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 18, "text": "As for computation complexity, since different random variables (RVs) have different variable-nabla (as shown in Table 3 in Appendix), GO has different computation complexity for different RVs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 19, "text": "After choosing a specific RV, one should be able to obtain GO\u2019s computation complexity straightforwardly.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 20, "text": "For quantitative evaluation, the running time for each experiment has been given in the corresponding Appendix.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 21, "text": "Please check there if interested.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_unknown", null]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 22, "text": "Thank you for pointing out the concern on multi-sample-based REINFORCE.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 23, "text": "We have added another curve labeled REINFORCE2 to the one-dimensional NB experiments (see Fig. 8 for complete results), where the number 2 means using 2 samples to estimate the REINFORCE gradient.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 24, "text": "In this case, REINFORCE2 uses 2 samples and 2 f evaluations in each iteration, whereas GO uses 1 sample and 2 f evaluations.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 25, "text": "As expected, REINFORCE2 still exhibits higher variance than GO even in this simple one-dimensional setting.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 26, "text": "Multi-sample-based REINFORCE for other experiments is believed unnecessary, because (i) the variance of REINFORCE is well-known to increase with dimensionality; (ii) after all, if multi-sample-based REINFORCE works well in practice, why we need variance-reduction techniques?", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 27, "text": "Please refer to Sec. 7.2 and Appendix I, the author released code from Grathwohl(2017) (github.com/duvenaud/relax) were run to obtain the results of REBAR and RELAX.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_unknown", null]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 28, "text": "We adopted the same hyperparameter settings therein for our GO.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 29, "text": "So, we do not think the hyperparameter settings favor our GO in the reported experiments.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 30, "text": "Please refer to the first paragraph of Sec. 7.2, \u201cSince the statistical back-propagation in Theorem 3 cannot handle discrete internal variables, we focus on the single-latent-layer settings (1 layer of 200 Bernoulli random variables).\u201d", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 31, "text": "If you are interested, as stated in the last paragraph of Sec 7.2, we presented in Appendix B.4 a procedure to assist our methods in handling discrete internal RVs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 32, "text": "We believe that procedure might be useful for the inference of models with discrete internal RVs (like the multi-layer discrete VAE).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 33, "text": "Please refer to the last paragraph of Appendix I, where we explained this misunderstanding in detail.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 34, "text": "In short, GO does not suffer more from overfitting; one reason is GO can provide higher validation ELBO.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 35, "text": "Actually, we believe it is GO\u2019s efficiency that causes this misunderstanding.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "rklz9YLKh7", "rebuttal_id": "ryxx8ZbFa7", "sentence_index": 36, "text": "We hope your concerns have been addressed. If not, further discussion would be welcomed.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 0, "text": "Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 1, "text": "Hopefully the new results in our response will better aid discussion. Your specific points are addressed below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 2, "text": "> \u201c...the additions proposed are small modifications to existing algorithms", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 3, "text": "We concede that the modifications to the existing models is a minor contribution.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 4, "text": "We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 5, "text": "We plan to make our code public to aid research in the area.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 6, "text": "To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 7, "text": "This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 8, "text": "> \u201c...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017).\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 9, "text": "Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesn\u2019t support relationship types.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 10, "text": "Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 11, "text": "In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 12, "text": "In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 13, "text": "During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 14, "text": "This leads us to conclude that vanilla GAT would not perform well on the RDF tasks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 15, "text": "As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 16, "text": "As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 17, "text": "We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 18, "text": "> \u201c...the results achieved in the experiments are very small improvements compared to the baseline of RGCN", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 19, "text": "\u2026\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 20, "text": "We agree that any improvements compared to RGCN are marginal.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 21, "text": "In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 22, "text": "The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 23, "text": "We also see value in reporting these negative results.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 24, "text": "It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 25, "text": "The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 26, "text": "On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 27, "text": "> \u201c...often these small variations in results can be compensated with better baselines training", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 28, "text": "\u2026\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 29, "text": "We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 30, "text": "To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 31, "text": "In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 32, "text": "On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3]]}, {"review_id": "rkx6mDnd37", "rebuttal_id": "ryxiFneiam", "sentence_index": 33, "text": "This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 0, "text": "We thank the reviewer for the insightful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 1, "text": "We address the questions in the following:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 2, "text": "- How many images did you have in the experiment?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 3, "text": "We had 7500 images in total.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 4, "text": "We had 3 concept classes, and 2500 images for each concept.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 5, "text": "We will mention the total number in the main text.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [3]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 6, "text": "- The proposed network is not deep, but shallow", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 7, "text": "We agree that a clear distinction line between shallow and deep networks does not exist.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 8, "text": "So we will make a note on that issue.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [4]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 9, "text": "- More experiments on the number of layers", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 10, "text": "We had experimented with fewer layers.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 11, "text": "We realized that in this case the width of the network should be increased to compensate for the representation power of the network.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 12, "text": "As we already had an extensive set of experiments, we decided not to report that.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 13, "text": "As the proposed architecture already performs well to solve the ordinal embedding problem, we found it unnecessary to try deeper networks.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [5]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 14, "text": "- \"I don't see a clear conclusion of how to pick the width of hidden layers, maybe a better representation could be used.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 15, "text": "There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 16, "text": "We also generated line plots (multiple curves in one plot) and 3D mesh plots to show the dependency.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 17, "text": "In the end, we found the heat-map more informative.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 18, "text": "In the revision, we will add the other plots to support the claim.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 19, "text": "- \"I don't see a discussion about the downsides of the method\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 20, "text": "One of the drawbacks is that our method needs GPUs, while the more traditional algorithms run on CPUs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 21, "text": "This can be of disadvantage if non-machine learning experts want to use our method.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 22, "text": "However, this is the case for most recent ML methods based on neural nets.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 23, "text": "The number of required triplets is theoretically lower bounded by nd log n, and this is also being confirmed by our experiments (our algorithm, as well as our competitors, break down when they get fewer triplets).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 24, "text": "Therefore, in a setting with passive triplet answers, and without extra information, it is impossible to overcome this problem.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 25, "text": "- \"in section 4.4", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 26, "text": "when comparing the proposed approach with another method why not use more complex datasets (like those used in section 4.3)\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 27, "text": "Independent of the dataset complexity, provided with enough triplet answers, all methods can yield less than 5% triplet error.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 9]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 28, "text": "However, the computation time is significantly lower for our proposed method.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 9]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 29, "text": "Due to the iterative nature of all algorithms, the computation time does not depend on the data distribution, but on the number of input points.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 9]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 30, "text": "Thus, a simple uniform dataset could serve to show our intention in this section.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 9]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 31, "text": "- \"in section 4.3, there is no guarantee that the intersection between the training set and the test set is empty.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 32, "text": "Yes, in theory that is true, but in practice this is negligible: the total number of possible triplets is about 10^9. So the likelihood that two sets of size 1000 intersect is close to 0.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [11]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 33, "text": "- \"in section 4.3", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 34, "text": "how is the reconstruction built (Figure 3b)?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 35, "text": "Figure 3b is the exact output of the ordinal embedding in two dimensions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 36, "text": "The colors are the initial labels of the input items.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 37, "text": "There are two or three labels assigned to demonstrate the quality of reconstruction.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 38, "text": "Note that the ordinal embedding output is unique only up to isometric transforms.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "rkxBU7f0qH", "rebuttal_id": "rklIgxx5oH", "sentence_index": 39, "text": "In other words, every valid output is still valid with rotation, scaling and translation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 0, "text": "Thank you for the useful feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 1, "text": "We\u2019ve updated our paper to take it into account -- we\u2019ve updated the model description and the notation in Section 4 to clarify our method.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 2, "text": "Two important additions are a high-level summary of the model, which we give at the beginning of Section 4, and a table (Table 2) that lists what each symbol represents along with its dimensions.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 3, "text": "We also made several updates that address your specific questions.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 4, "text": "1. Are e_{i,t} and lambda_{i,t} vectors?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 15, 15, 18]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 5, "text": "Scalars?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 15, 15, 18]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 6, "text": "Abstract node notations? It is not clear in the model section.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 15, 15, 18]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 7, "text": "Also, it took me a long time to figure out that \u2018i\u2019 is used to index each entity (it is mentioned later).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 15, 15, 18]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 8, "text": "The entity and location embeddings  e_{i,t} and lambda_{i,t} are d-dimensional vectors, although we also overload the symbols to refer to abstract nodes in the model\u2019s knowledge graphs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 15, 15, 18]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 9, "text": "In the updated manuscript we state both these facts explicitly and state much earlier that \u2018i\u2019 is the index for entities.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 15, 15, 18]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 10, "text": "2. The paper says v_i (initial representation of each entity) is obtained by looking at the contextualized representations (LSTM outputs) of entity mention in the context.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 11, "text": "What happens if there are multiple mentions in the text? Which one does it look at?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 12, "text": "When there are multiple mentions of entity i, the initial representation v_i is formed by summing the representations of each mention.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 13, "text": "We have updated the paper to clarify this (Sec 4.1).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 14, "text": "3. For the LSTM in the graph update, why does it have only one input? Shouldn\u2019t it have two inputs, one for previous hidden state and the other for input?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [21]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 15, "text": "Good point! We\u2019ve improved the notation used to describe the model in Section 4.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [21]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 16, "text": "The update equation now shows clearly that the LSTM takes in the concatenation of two node inputs (entity and location embeddings) along with the previous hidden state.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 17, "text": "4. Regarding Recipe experiments, the paper says it reaches a better performance than the baseline using just 10k examples out of 60k.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22, 23]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 18, "text": "This is great, but could you also report the number when the full dataset is used?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22, 23]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 19, "text": "We\u2019ve completed an experiment on the full Recipes dataset and updated the paper to describe the result (this experiment did not finish in time for the initial submission).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [22, 23]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 20, "text": "The model\u2019s F1 score improves from 51.64 on the partial data to 54.27 on the full data, surpassing the previous state of the art by a more significant margin.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [22, 23]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 21, "text": "5. What does it mean that in training time the model \u201cupdates\u201d the location node representation with the encoding of the correct span. Do you mean you use the encoding instead?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 22, "text": "We meant that we perform teacher-forcing to train the model.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 23, "text": "During training, we extract the context encodings for the groundtruth span and use these in downstream operations  to obtain the node representations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 24, "text": "At test time, we use the MRC module\u2019s predicted span rather than the groundtruth.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 25, "text": "6. For ProPara task 2, what threshold did you choose to obtain the P/R/F1 score?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [25, 26]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 26, "text": "Is it the threshold that maximizes F1?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [25, 26]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 27, "text": "For ProPara task 2, our model was optimized for micro averaged F1 on the development set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [25, 26]]}, {"review_id": "rkxUMYW6hX", "rebuttal_id": "H1e6Tr4X0Q", "sentence_index": 28, "text": "Tandon et al. (2018) were kind enough to provide us with their evaluation script.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [25, 26]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 0, "text": "We thank reviewer 3 for the detailed feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 1, "text": "We are glad that the reviewer found the extensive evaluation appropriate, and that our model behaves well for the realistic and diversity measures.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [3, 6, 23]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 2, "text": "We now address all the individual questions.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 3, "text": "We added Section 3.5 to point out the differences between the VAE component of our model and the SV2P and SVG models from prior work.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 4, "text": "In Section 3.4, we clarified what frames the discriminator takes, and in Section 4.3 we added a description of the deterministic version of our model.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 5, "text": "In Section A.1.1, we provided a better description of how frames are predicted at each time step.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 6, "text": "In Section 3.5 and A.1.2, we clarified that the latent variables are sampled at every time step.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [22]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 7, "text": "We updated Section 4.4 to indicate that it is to be expected that although our SAVP model improves on diversity and realism, it also performs worse in accuracy compared to pure VAE models (both our own ablation and SVG from Denton & Fergus (2018)).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13, 14, 23, 24]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 8, "text": "A recent result [1] proves that there is a fundamental tradeoff between accuracy and realism, for all problems with inherent ambiguity.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 9, "text": "In fact, a recent challenge held at ECCV 2018 in such a problem [2] evaluates all algorithms on both of these axes, as neither adequately captures performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 10, "text": "Note that proposing a generator architecture is not the goal of this paper.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 14, 20]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 11, "text": "Instead, we provide a systematic analysis of the effect of the loss function on this task (which could be applied to any generator).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 20]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 12, "text": "We use a warping-based generator, from prior work (Ebert et al. 2017), and include a comparison to SVG for completeness.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 20]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 13, "text": "In the updated draft, we clarify in Section 3.4 that the warping component assumes that videos can be described as transformation of pixels, but that any generator (including the one from Denton & Fergus (2018)) could be used with our losses.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13, 14, 20]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 14, "text": "Since evaluating generator architectures is not the emphasis of this paper, we did not test the importance of the warping component nor test on videos where this hypothesis is less suitable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 20]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 15, "text": "We have included a revised plot in Figure 14 at the end of the Appendix (note that this temporary plot will be incorporated to Figure 6), where we use the official implementation of SSIM and replace the VGG metric with the Learned Perceptual Image Patch Similarity (LPIPS) metric (Zhang et al., 2018).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13, 14, 20]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 16, "text": "LPIPS linearly calibrates  AlexNet feature space to better match human perceptual similarity judgements.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 20]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 17, "text": "Aside for the first two predicted frames, our VAE ablation and the SVG model both achieve similar SSIM and LPIPS performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14, 20]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 18, "text": "[1] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Conference on Vision and Pattern Recognition (CVPR), 2018. https://arxiv.org/abs/1711.06077", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_global", null]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 19, "text": "[2] Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. 2018 PIRM Challenge on Perceptual Image Super-resolution. In Perceptual Image Restoration and Manipulation (PIRM) workshop at ECCV 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_global", null]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 20, "text": "https://arxiv.org/abs/1809.07517", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_global", null]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 21, "text": "We have included a revised plot in Figure 15 at the end of the Appendix (which will be incorporated to Figure 7) that fixes the KTH dataset preprocessing.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 22, "text": "Our VAE-only model now achieves substantially higher accuracy and diversity than SVG (Denton & Fergus, 2018).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 10, 11]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 23, "text": "As before, the GAN-only model mode-collapses and generates samples that lack diversity.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 10, 11]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 24, "text": "Our SAVP method, which incorporates the variational loss, improves both sample diversity and similarities, compared to the GAN-only model.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 10, 11]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 25, "text": "Our SAVP model also achieves higher accuracy than SVG.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 10, 11]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 26, "text": "The experiments from our original submission (1) cropped the videos into a square before resizing, and thus discarded information from the sides of the video, and (2) did not filter out the empty frames, and thus our models were trained on uninformative frames.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 10, 11]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 27, "text": "We fixed those issues to match the preprocessing used by Denton & Fergus (2018).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 10, 11]]}, {"review_id": "rkxwCK8chQ", "rebuttal_id": "BkgZKhUX07", "sentence_index": 28, "text": "In addition, we have also included experiments where we condition on only 2 frames instead of 10 frames, in order to test on a setting with more stochasticity.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 10, 11, 20, 20]]}, {"review_id": "rye5LcLAYH", "rebuttal_id": "ByghS3dssB", "sentence_index": 0, "text": "We thank the reviewer for the effort, however we believe there is a mis-understanding.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rye5LcLAYH", "rebuttal_id": "ByghS3dssB", "sentence_index": 1, "text": "As for the synthetic curves experiment, we updated the paper with a justification.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [1]]}, {"review_id": "rye5LcLAYH", "rebuttal_id": "ByghS3dssB", "sentence_index": 2, "text": "This task, while simple, showcases the ability of Transformer to model a distribution over curves of similar shape to real training curves with varying speeds of convergence.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [1]]}, {"review_id": "rye5LcLAYH", "rebuttal_id": "ByghS3dssB", "sentence_index": 3, "text": "It has been designed so it is easy to quantify the diversity of generated curves and the fit between the distribution generated by the model and the real one.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [1]]}, {"review_id": "rye5LcLAYH", "rebuttal_id": "ByghS3dssB", "sentence_index": 4, "text": "Furthermore, we included two additional tasks, attesting to the ability of Transformer to model a wide range of distributions over training curves.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [1]]}, {"review_id": "rye5LcLAYH", "rebuttal_id": "ByghS3dssB", "sentence_index": 5, "text": "We also updated the citation of the paper you mentioned with an arxiv URL.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2]]}, {"review_id": "rye5LcLAYH", "rebuttal_id": "ByghS3dssB", "sentence_index": 6, "text": "We still believe that while focusing on the synthetic task the reviewer might have missed the main point of the paper, namely that time-series forecasting with Transformer works really well, at least in the context of modeling deep learning dynamics.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_global", null]}, {"review_id": "rye5LcLAYH", "rebuttal_id": "ByghS3dssB", "sentence_index": 7, "text": "The general problem has been studied in the community for many decades and we believe that we made significant progress, so we kindly encourage the reviewer to reconsider their assessment of our contributions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 0, "text": "Thank you Reviewer 2 for your positive appraisal of our results and presentation.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 1, "text": "As documented below, we do our best to address your questions, which have helped us improve the paper.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 2, "text": "Re: \"The citation to Zaremba et al. in Table 1 made it seem like the perplexity result on that line of the table was directly from Zaremba et al's paper. I'd recommend removing the citation from that line to avoid confusion.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 3, "text": "We have added \u201csimilar to\u201d in order to emphasize that we adapted and re-ran their architecture (we still use some of their code, which we believe might warrant citation; we are happy to drop it altogether if it is found confusing).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 4, "text": "Re: \"One thing I would have loved to see from this paper is a comparison of modulated-plasticity LSTMs with the sota from Melis et al., 2017.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 5, "text": "We agree that, ideally, a comparison with SOTA architectures would be desirable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 6, "text": "As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [13]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 7, "text": "We will keep trying to investigate such massive architectures in the future.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [13]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 8, "text": "Re: \"Why were zero-sequences necessary in Experiment 1? [...] Perhaps the authors could clarify on what a confounding \"time-locked scheduling strategy\" would look like in this task?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 9, "text": "The random zero-inputs make the timing of the cues unpredictable, forcing the network to be driven specifically by the stimuli - as opposed to learning a pre-programmed strategy at each given time step.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 10, "text": "This is merely a convenient choice to make the task more challenging.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 11, "text": "Re: \"Why does Experiment 1 present pairs of stimuli, rather than high-dimensional individual stimuli?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [20]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 12, "text": "Again, this simply makes the task more challenging.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 13, "text": "Non-target cues operate as distractors and having pairs of stimuli shown before each response increases the uncertainty in reward credit assignment (i.e. when receiving a reward, the network must still find out which of the two stimuli is the target).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 14, "text": "To better describe the task, we have added a schema of an episode to Figure 1.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [20]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 15, "text": "We hope this may facilitate understanding.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 16, "text": "Re: \"Why is non-plastic rnn left out of Figure 2b?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [21]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 17, "text": "As documented in Miconi et al 2018, non-plastic networks are terrible at this task.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 18, "text": "We are happy to run this experiment and include it if the reviewer finds it useful.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 19, "text": "Typos: \"However, in Nature,\" -- no caps", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22, 23, 24]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 20, "text": "in appendix: \"(see Figure A.4)\" -- the figure is labeled \"Figure 3\"\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22, 23, 24]]}, {"review_id": "ryeSkAjN37", "rebuttal_id": "S1gcKOwqR7", "sentence_index": 21, "text": "We thank the reviewer for noticing these typos and have fixed them in the text.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [22, 23, 24]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 0, "text": "We thank the reviewer for their time and welcome feedback, which we are incorporating into the revised version.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 1, "text": "R: - Theoretical contribution", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 2, "text": "A:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 3, "text": "-- Just a clarification regarding the first contribution: WE from the current task is not used to generate a saliency map for the next task; it is instead used to instruct the learner of the next task about which input areas are more important than others via the attention mechanism.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [4]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 4, "text": "This becomes a part of the future learning procedure, not just a post hoc visualisation method as in the original WE .", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [4]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 5, "text": "As such, the first contribution is not solely about generating new visualisations; it is more about using the learned saliency maps from the past to attend in future learners.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 6, "text": "We therefore believe that the potential of this first contribution as a conceptual framework via which a learner can understand, attend, and then enhance its attention for future tasks, is not small.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 7, "text": "-- Importantly, we are the first to combine interpretability with continual learning and show that interpretability can help continual learning. It is a significant step to bring these two communities together.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 8, "text": "-- It is worth noting that VCL has achieved very good results on most of these benchmarks, so it is very hard to outperform it with a large margin.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [17]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 9, "text": "Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [17]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 10, "text": "R: - Related work", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18, 19, 20]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 11, "text": "A:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18, 19, 20]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 12, "text": "-- Thank you.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [18, 19, 20]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 13, "text": "In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18, 19, 20]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 14, "text": "We have added more to the revised version.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18, 19, 20]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 15, "text": "R: - https://arxiv.org/pdf/1805.09733.pdf", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 16, "text": "A:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 17, "text": "-- Thank you. We have cited the paper in the revised version, and plan to take it into further consideration in future work.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [22]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 18, "text": "R: - Yellow color in plots", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [23]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 19, "text": "A:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [23]]}, {"review_id": "ryg4Fgr9hX", "rebuttal_id": "Bkggfq0VTm", "sentence_index": 20, "text": "-- We have changed the yellow colour in Figures 2, 3 and 4 to black. Yellow is now no longer used in any plots.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [23]]}, {"review_id": "rygRU5E5nm", "rebuttal_id": "BklGVJ92pX", "sentence_index": 0, "text": "Thank you for your thoughtful and helpful comments.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "rygRU5E5nm", "rebuttal_id": "BklGVJ92pX", "sentence_index": 1, "text": "Following the suggestions, we added additional results for the associative recall task for many network variants.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "rygRU5E5nm", "rebuttal_id": "BklGVJ92pX", "sentence_index": 2, "text": "We also report mean and variance of losses for different seeds.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "rygRU5E5nm", "rebuttal_id": "BklGVJ92pX", "sentence_index": 3, "text": "This shows that masking improves performance on this task especially when combined with improved de-allocation, while sharpness enhancements negatively affect performance in this case.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "rygRU5E5nm", "rebuttal_id": "BklGVJ92pX", "sentence_index": 4, "text": "From the variance plots it can be seen that some seeds of DNC-M and DNC-MD converge significantly faster than plain DNC.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "rygRU5E5nm", "rebuttal_id": "BklGVJ92pX", "sentence_index": 5, "text": "In our experimental section, we added requested references to methods performing better on bAbI, and point out that our goal is not to beat SOTA on bAbI, but to exhibit and overcome drawbacks of DNC.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13]]}, {"review_id": "rygRU5E5nm", "rebuttal_id": "BklGVJ92pX", "sentence_index": 6, "text": "Comparison to Sparse DNC is an interesting idea, and we are currently running experiments in this direction. We intend to make the results available in the near future.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [14, 14, 16]]}, {"review_id": "rygRU5E5nm", "rebuttal_id": "BklGVJ92pX", "sentence_index": 7, "text": "We are unable to provide a fair comparison for the lowest bAbi scores, having reported 8 seeds compared to the 20 seeds reported by Graves et al. Indeed, the high variance of DNC (Table 1) suggests that it may benefit a lot from exploring additional seeds.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [14, 14, 16]]}, {"review_id": "rygRU5E5nm", "rebuttal_id": "BklGVJ92pX", "sentence_index": 8, "text": "We incorporated all of the smaller notes, including a comparison to the original DNC equations in Appendix A.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 0, "text": "We thank you for the constructive feedback and are glad that you enjoyed the paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 1, "text": "Here we discuss some of your comments.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 2, "text": "R1: \"missing in the paper is the comparison to two other class of RL methods: count-based exploration... In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2, 3, 4]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 3, "text": "=> We chose to focus on dynamics-based approaches in this paper because we found them more straightforward to efficiently parallelize than the published pseudo-count methods.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3, 4]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 4, "text": "This allows us to be able to run more and larger experiments on many environments.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3, 4]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 5, "text": "Interestingly, increased parallelization also significantly helped the exploration strategies as shown in Figure 3(a).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2, 3, 4]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 6, "text": "Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [2, 3, 4]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 7, "text": "In particular, we were not able to find any official public implementation of the pseudo-count methods.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3, 4]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 8, "text": "We experimented with a third party implementation trying to see if it could play Breakout without extrinsic rewards, but did not achieve sufficient success and found it to be too slow for scaling it up to a large-scale study.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2, 3, 4]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 9, "text": "R1: \"the experiments around VAE... While the paper shows experimentally that they aren't as successful... there's no further discussion on the reasons for poor performance.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 10, "text": "=> We found that VAEs overall worked well and were sometimes better than other representation learning methods, but often were causing instability at training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 11, "text": "We don't claim such instability is an inherent property of the VAE feature learning method, but probably stems from the continually changing data distribution as agent makes progress.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 12, "text": "Indeed modeling the density of a non-stationary distribution, with modes appearing and disappearing, is a challenging and an active research problem.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 13, "text": "We will clarify this in the final version.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 14, "text": "R1: \"An interesting area for future work could be on early stopping techniques for embedding training\u2026 maybe somewhere in between could be the sweet spot with training\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 15, "text": "=> Thank you for the excellent suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 16, "text": "We agree that there may be some optimal tradeoff between features that are stable and features that adapt to the environment.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 17, "text": "Such tradeoffs would be interesting to investigate, and might be crucial to getting learned features to perform significantly better than fixed random features.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 18, "text": "We will add this in the discussion/future work section of paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 19, "text": "R1: \"What are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "ryg_N1TK2Q", "rebuttal_id": "rJxwGHYQRm", "sentence_index": 20, "text": "=> Thank you. We will add more details on the architectures to the appendix.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [7]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 0, "text": "Thanks for your valuable comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 1, "text": "It helps us to prepare the revision.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 2, "text": "We address all your concerns in the revision as below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 3, "text": "Q1: Was the auxiliary tower used during the training of the shared weights W?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 4, "text": "A1: Auxiliary tower is used only in the retraining stage.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 5, "text": "Q2: \u201cDid the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 6, "text": "A2:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 7, "text": "CIFAR: In the pretrain stage and search stage, the learning rate is fixed to 0.1 with batch size 128; In the retraining stage, we use cosine learning rate schedule.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 8, "text": "ImageNet: In the pretrain stage and search stage, the learning rate is fixed to 0.1 with batch 224; In the retraining stage, we use linear decay learning rate schedule.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 9, "text": "Q3: \u201cFigure 4 does not illustrate M=4 and N=4, e.g. which operation belongs to which layer?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [25]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 10, "text": "A3: In the revision, we replace the Figure 4 with a new version which has more details.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [25]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 11, "text": "As show in Figure 4, all the operators in level 4 are pruned.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [25]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 12, "text": "Q4: \u201cThe sparse regularization of \\lambda induces great difficulties in optimization\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [27, 28]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 13, "text": "A4: The non-smooth regularization introduced by l1 regularization makes traditional stochastic SGD failed to yield sparse results.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [27, 28]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 14, "text": "If we need exact zero, we have to use heuristic thresholding on the \\lambda learned, which has already been demonstrated in SSS [1] that is inferior.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [27, 28]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 15, "text": "Besides, traditional APG method is not friendly for deep learning as extra forward-backward computation is required, also as shown by SSS.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [27, 28]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 16, "text": "Q5: \u201cMissed citation: MnasNet also incorporates the cost of architectures in their search process. On ImageNet, your performance is similar to theirs. I think this will be a good comparison.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [29, 30, 31]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 17, "text": "A5: We have added the result of MnasNet [2] in Table 2.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [29, 30, 31]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 18, "text": "Indeed, MnasNet achieves similar results with us with less FLOPs.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [29, 30, 31]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 19, "text": "However, it is also need to note that MnasNet evaluates more than 8K models, which introduces much higher search cost than our method.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [29, 30, 31]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 20, "text": "Moreover, the design space of MnasNet is significant different from other existing NAS methods including ours.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [29, 30, 31]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 21, "text": "It is interesting to explore the combination of MnasNet with ours in the future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [29, 30, 31]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 22, "text": "Q6: \u201cThe paper has some grammatical errors.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [32]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 23, "text": "A6: We have fixed the typos and grammatical errors in the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [32]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 24, "text": "Q7: About \u201cfirst NAS algorithm to perform direct search on ImageNet\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 25, "text": "A7: We check this claim again and find methods like MnasNet [2] and one-shot architecture search [3] also have the ability to perform direct search on ImageNet, we have delete this claim in the paper.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 26, "text": "However, to the best of our knowledge, our method is the first method to perform directly search without block structure sharing.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 27, "text": "We also report preliminary results that directly search on task beyond classification (semantic segmentation).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 28, "text": "Please refer to Q1 of Reviewer3 for details.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 29, "text": "[1] Data-Driven Sparse Structure Selection for Deep Neural Networks. ECCV 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 30, "text": "[2] MnasNet: Platform-Aware Neural Architecture Search for Mobile. https://arxiv.org/pdf/1807.11626.pdf", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rylgsNqchQ", "rebuttal_id": "HyewJo1YAQ", "sentence_index": 31, "text": "[3] Understanding and simplifying one-shot architecture search. ICML 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 0, "text": "Thanks for your thoughtful review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 1, "text": "We have given serious considerations of your concerns and revise our manuscript to accommodate your suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 2, "text": "Please see the details below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 3, "text": "Q1: \u201cThere are a few grammatical/spelling errors that need ironing out.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 4, "text": "A1: We have fixed the typos and grammatical errors in the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 5, "text": "Q2: \u201cPioneering work is not necessarily equivalent to \"using all the GPUs\"\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 6, "text": "A2: This claim is indeed not accurate we have delete this claim in the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 7, "text": "Q3: \u201cThere are better words than \"decent\" to describe the performance of DARTS, as it's very similar to the results in this work!\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 8, "text": "A3: We have changed the word to \u201cimpressive\u201d in the revision. However, DSO-NAS indeed outperforms DARTS on ImageNet dataset as illustrated in Table2.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 9, "text": "Q4: \u201cFrom figure 2 it's not clear why all non-zero connections in (b) are then equally weighted in (c). Would keeping the non-zero weightings be at all helpful?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 10, "text": "A4: In the search stage, the scaling factors are only used to indicate which operators should be pruned.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 11, "text": "The value of scaling factors do not represent the importances of kept operators since they can be merged into the weights of convolution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 12, "text": "We also add experiments in CIFAR-10 to compare the performance between keeping the non-zero weightings and equal weightings.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 13, "text": "The result shows that both of them yield similar performances.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 14, "text": "---------------------------------------------------------------------------------------------------------------------------", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 15, "text": "Architecture         \t                    params(M)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 16, "text": "test error", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 17, "text": "---------------------------------------------------------------------------------------------------------------------------", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 18, "text": "DSO-NAS-share+c/o", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 19, "text": "3.0                        \t2.84", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 20, "text": "---------------------------------------------------------------------------------------------------------------------------", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 21, "text": "DSO-NAS-share+c/o+k/w", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 22, "text": "3.0                              2.88", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 23, "text": "---------------------------------------------------------------------------------------------------------------------------", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 24, "text": "DSO-NAS-full+c/o", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 25, "text": "3.0                        \t2.95", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 26, "text": "---------------------------------------------------------------------------------------------------------------------------", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 27, "text": "DSO-NAS-full+c/o+k/w", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 28, "text": "3.0                        \t2.96", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 29, "text": "---------------------------------------------------------------------------------------------------------------------------", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 30, "text": "where \u201cc/o\u201d represents that training the searched architectures with cutout and \u201ck/w\u201d represents keeping the non-zero weightings in the architectures.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 31, "text": "Q5: \u201cWhy have you chosen the 4 operations at the bottom of page 4?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 32, "text": "A5: These four operations were used by ENAS and commonly included in the search space of most NAS papers.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 33, "text": "Q6: \u201cHow do you specifically encode the number of surviving connections?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 34, "text": "A6: We don\u2019t directly encode the number of surviving connections.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 35, "text": "Instead, the number of surviving connections is determined by the weight for L1 regularization, which can be incorporated with certain budget.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 36, "text": "Q7: \u201cMeasuring in GPU days is only meaningful if you use the same GPU make for every experiment. Which did you use?\u201d", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 37, "text": "A7: All of our experiments were conducted by NVIDIA GTX 1080Ti GPU, which was also used by ENAS and DARTS.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "rylJA7G82Q", "rebuttal_id": "S1xBci1FAm", "sentence_index": 38, "text": "We have added it in the paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 0, "text": "We thank Reviewer 3 (R3) for their review and for clearly articulating their concerns regarding the paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 1, "text": "In our response below, we will clarify the design and results of our experiments as well as argue why we believe that these results should be of interest and are not, indeed, that predictable.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 2, "text": "R3 asked why training performance of many models is 100% when they do not generalize and suggested us to perform a large number of training runs to see if occasionally the right solution is found.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 3, "text": "First, we agree that from the point of view of training there are many equally good solutions, and in fact, this is the main and the only challenge of SQOOP.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 4, "text": "We designed the task with the goal of testing which models are more likely to converge to the right solution, with which they can handle all possible combinations of objects, despite being trained only on a small subset of objects.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 5, "text": "We argued extensively in the introduction that such an ability to find the systematic solution despite other alternatives being available is highly desirable for language understanding approaches.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 6, "text": "We fully agree with R3 that in investigations of whether or not a particular model converges to the right solution repeating every experiment several times is absolutely necessary, and we would like to emphasize that we did repeat each experiment 3, 5, or 10 times (see \u201cdetails\u201d in Table 1 and the paragraph \u201cParametrization Induction\u201d on page 8).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 7, "text": "In most cases we saw a consistent success or consistent failure, one exception being the parametrization induction results, where 4 out of 10 runs were successful (see Table 4, row 1 for the mean and the confidence interval).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 8, "text": "We hope that 3 takes this fact into account, and we will furthermore improve on the current level of rigor in the upcoming revision by repeating each experiment at least 5 times.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [13, 14, 15]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 9, "text": "We are not sure if we fully understand the question \u201cCould you somehow test for if a given trained model will show systematic generalization?\u201d that R3 asked.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 10, "text": "We test the systematic generalization of a model by evaluating it on all SQOOP questions that were not present in the training set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 11, "text": "We hope that this answers the question of R3 and we would be happy to engage in a further discussion regarding this and make edits to the paper if necessary.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 12, "text": "We thank R3 for the suggestion to investigate the influence of model size and regularization on systematic generalization.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [15]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 13, "text": "It is indeed a very appropriate question in the  context of our study, however, we note that there exists a wide variety of regularization methods and trying them all (and all their combinations) would be infeasible.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [15]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 14, "text": "In the upcoming update of the paper we will report results of an on-going ablation study for the MAC model, in which we vary the module size, the number of modules and experiment with weight decay.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [15]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 15, "text": "We would welcome any other specific experiment requests R3 may have.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 16, "text": "Finally, we would like to discuss the significance of our investigation and its results.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 17, "text": "While we agree that the results that we report may not shock the reader (although perhaps hindsight bias plays a role in what people find surprising or not after reading an article) we find them highly interesting and not at all easily predictable.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 18, "text": "Reading prior work on visual reasoning may lead a researcher to conclude, roughly speaking, that NMNs are a lost cause, since a variety of generic models perform comparably or better.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 19, "text": "In contrast, our rigorous investigation highlights their strong generalization capabilities and relates them to the specific design of NMNs.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 20, "text": "Notably, chain-structured NMNs were used in the literature prior to this work (e.g. in the model of Jonshon et al multiple filter_...[...] modules are often chained), so the fact that tree-structured NMNs show much stronger generalization was not obvious prior to this investigation and should be of a high interest to the research community.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 21, "text": "Last but not least, an important part of our investigation (which the review does not discuss) is the systematic generalization analysis of popular end-to-end NMN versions, that shows how making NMNs more end-to-end makes them more susceptible to finding spurious solutions.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 22, "text": "As we argued in our conclusion, these findings should be of a highest importance to researchers working on end-to-end NMNs, which is a very popular research direction nowadays.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17]]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 23, "text": "We conclude our response by announcing that an updated version of the paper, that among others incorporates valuable suggestions by R3, will soon be uploaded to OpenReview.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 24, "text": "We are currently performing a lot of additional experiments, the results of which will make our investigation even more rigorous and complete.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 25, "text": "We sincerely hope that R3 takes into account the arguments we have made here and the new results that we will publish soon and reevaluates our paper more positively.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 26, "text": "Dear Reviewer 3,", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 27, "text": "We thank you again for your informative review that you wrote before the revision period.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 28, "text": "In our response and the revised version of the paper we tried our best to address your concerns.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 29, "text": "We would highly appreciate to get some feedback from you regarding the changes that we have made and the arguments that we have presented.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 30, "text": "In particular, we report that NMN-Chains (with a lot of inductive bias built-in and also used in prior work such as Johnson et al. 2017) generalize poorly compared to even generic modules, and that layout/parameterization induction often fails to converge to the correct solution.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 31, "text": "We believe both these findings are quite surprising.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 32, "text": "We also report new experiments with the MAC model, including a hyperparameter search, a comparison against end-to-end NMNs, and a qualitative exploration of the failure modes of this model.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 33, "text": "All these experiments are repeated at least 5 times each, like you suggested in your review, although it\u2019s worth noting that results the original version of the paper also reported results after  multiple runs.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 34, "text": "We would highly appreciate a response on our newest revision and suggestions on how it could be improved. If you still think that paper is uninteresting or not well executed, could you then suggest what specifically it is lacking?", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_global", null]}, {"review_id": "rylJdHwn2Q", "rebuttal_id": "S1xJ92HIaX", "sentence_index": 35, "text": "We are sincerely hoping to hear from you.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 0, "text": "We would like to thank the reviewer for the time and useful feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 1, "text": "Our response is given below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 2, "text": "- Relationship to z-conditioning strategy in BigGAN.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 3, "text": "Thanks for pointing out the connection to this concurrent submission.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 4, "text": "We will discuss the connections in the related work section.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 5, "text": "The main differences are as follows:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 6, "text": "1. BigGAN performs conditional generation, whilst we primarily focus on unconditional generation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 7, "text": "BigGAN splits the latent vector z and concatenates it with the label embedding, whereas we transform z using a small MLP per layer, which is arguably more powerful.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 8, "text": "In the conditional case, we apply both additive and multiplicative interaction between the label and z, instead of concatenation as in BigGAN.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 9, "text": "2. Overall BigGAN focusses on scalability to demonstrate that one can train an impressive model for conditional generation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 10, "text": "Instead, we focus on a single idea, and show that it can be applied very broadly.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 11, "text": "We provide a thorough empirical evaluation across critical design decisions in GANs and demonstrate that it is a robust and practically useful contribution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 12, "text": "- Propagation of signal and ResNets.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 13, "text": "Indeed, ResNets provide a skip connection which helps signal propagation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 14, "text": "Arguably, self-modulation has a similar effect.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 15, "text": "However, there are critical differences in these mechanisms which may explain the benefits of self-modulation in a resnet architecture:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 16, "text": "1. Self-modulation applies a channel-wise additive and multiplicative operation to each layer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 17, "text": "In contrast, residual connections perform only an element-wise addition in the same spatial locality.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 18, "text": "As a result, channel-wise modulation allows trainable re-weighting of all feature maps, which is not the case for classic residual connections.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 19, "text": "2. The ResNet skip-connection is either an identity function or a learnable 1x1 convolution, both of which are linear.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 20, "text": "In self-modulation, the connection from z to each layer is a learnable non-linear function (MLP).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 21, "text": "- Reading Figure 2b, one could be tempted to draw a correlation between the complexity of the dataset and the gains achieved by self-modulation over the baseline (e.g., Bedroom shows less difference between the two approaches than ImageNet). Do the authors agree with that?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 22, "text": "Yes, we notice more improvements on the harder, more diverse datasets.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "rylkjAtu2m", "rebuttal_id": "rylaP-uP6m", "sentence_index": 23, "text": "These datasets also have more headroom for improvement.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 0, "text": "Thanks for your positive feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 1, "text": "(1).", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 2, "text": "Lemma 2.4, Point 1: The gradient in your example is indeed perpendicular to w which can be seen as follows.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2, 3, 4, 5, 6, 6, 8]]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 3, "text": "w\u2019 * \\nabla L(w) = w\u2019 * (2/w\u2019*w)(Aw - L(w)*w) =  (2/w\u2019*w)(w\u2019Aw - L(w)*(w\u2019*w)) =  (2/w\u2019*w)(w\u2019Aw - w\u2019Aw) = 0.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2, 3, 4, 5, 6, 6, 8]]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 4, "text": "In case of one variable vector, our proof is to take the derivative of c on both sides of F(w) = F(cw), which is the definition of scale-invariance.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [2, 3, 4, 5, 6, 6, 8]]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 5, "text": "Then the left-hand side becomes 0 and the right-hand side becomes w\u2019 * \\nabla F(cw)", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [2, 3, 4, 5, 6, 6, 8]]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 6, "text": "by chain rule.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [2, 3, 4, 5, 6, 6, 8]]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 7, "text": "Taking c = 1, we can conclude", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [2, 3, 4, 5, 6, 6, 8]]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 8, "text": "that w\u2019 * \\nabla F(w)  = 0.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [2, 3, 4, 5, 6, 6, 8]]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 9, "text": "(2)", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 10, "text": ".", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 11, "text": "Theorem 2.5: Sorry G_t should be G_t^{(i)}. We will correct this typo in the next revision of this paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 12, "text": "For t = 0, G_t^{(i)} are all initialized to some value.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "rylog8i62m", "rebuttal_id": "HkgOWxN0T7", "sentence_index": 13, "text": "The recursion formula for G_t^{(i)} is shown in equation (9).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "rylvbv_93Q", "rebuttal_id": "B1esP4YtAm", "sentence_index": 0, "text": "We thank for the reviewer for their positive comments on our work, and we share our responses below.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rylvbv_93Q", "rebuttal_id": "B1esP4YtAm", "sentence_index": 1, "text": "The purpose of our work is not to achieve state-of-the-art performance simply by incorporating the latest network architectures and optimisers.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 13]]}, {"review_id": "rylvbv_93Q", "rebuttal_id": "B1esP4YtAm", "sentence_index": 2, "text": "Instead, we provide a novel general framework for automating generalisation, and show that when used with standard classification networks across all baselines, our method performs the best.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13, 13]]}, {"review_id": "rylvbv_93Q", "rebuttal_id": "B1esP4YtAm", "sentence_index": 3, "text": "Furthermore, as we also explained in Reviewer #3, the hyper-parameters for defining a hierarchy is not critical, and we can choose an arbitrary hierarchy whilst still achieving better performance than baselines.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "rylvbv_93Q", "rebuttal_id": "B1esP4YtAm", "sentence_index": 4, "text": "In the future work, we would like to explore how to find the optimal hierarchy in an automatic manner, or provide an alternative solution on building a general type of auxiliary tasks (such as regression).", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [15]]}, {"review_id": "rylvbv_93Q", "rebuttal_id": "B1esP4YtAm", "sentence_index": 5, "text": "However, this is the first work to present a double-gradient method for auxiliary task generation, and we believe that it is important to present the success of this initial method now given how simple and general it is, and then fine-tune other aspects in future work.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "ryxs4ypg9r", "rebuttal_id": "B1eKyZOtoH", "sentence_index": 0, "text": "Thanks a lot for your review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "ryxs4ypg9r", "rebuttal_id": "B1eKyZOtoH", "sentence_index": 1, "text": "We address your remarks below:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "ryxs4ypg9r", "rebuttal_id": "B1eKyZOtoH", "sentence_index": 2, "text": "\"RNN with an accumulator / too minor a contribution \"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [1]]}, {"review_id": "ryxs4ypg9r", "rebuttal_id": "B1eKyZOtoH", "sentence_index": 3, "text": "We want to emphasis that the accumulator implemented in the newly proposed architectures has an inherently different nature than accumulators used so far: While accumulators such as LSTM cells accumulate knowledge about the state of a sequence, our architectures produce meaningful intermediate results, which can simply summed up to estimate the final set utility.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "ryxs4ypg9r", "rebuttal_id": "B1eKyZOtoH", "sentence_index": 4, "text": "Producing such intermediate results, which model the nature of the problem much better than previous approaches, is the key idea presented in this paper and a major benefit of the proposed architectures.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "ryxs4ypg9r", "rebuttal_id": "B1eKyZOtoH", "sentence_index": 5, "text": "This also follows the idea of the Choquet integral.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "ryxs4ypg9r", "rebuttal_id": "B1eKyZOtoH", "sentence_index": 6, "text": "\"Additionally, I believe the experimental tasks are new, and as a result all implementations of competing techniques are by the paper authors. This makes it difficult to have confidence in the higher reported performance of the proposed techniques.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "ryxs4ypg9r", "rebuttal_id": "B1eKyZOtoH", "sentence_index": 7, "text": "To improve reproducibility, we published the data and the code.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "ryxs4ypg9r", "rebuttal_id": "B1eKyZOtoH", "sentence_index": 8, "text": "We implemented in our work the most basic version of our idea as well as the most basic version of each reference model.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "ryxs4ypg9r", "rebuttal_id": "B1eKyZOtoH", "sentence_index": 9, "text": "Hence, the code of the implemented architectures only consists of few lines and can be checked easily.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "ryxs4ypg9r", "rebuttal_id": "B1eKyZOtoH", "sentence_index": 10, "text": "We think that it is not fair to simply mistrust our results since we made our work fully transparent.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 0, "text": "We thank the reviewer for his comments and observations.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 1, "text": "Following are the answers to each question you have raised.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 2, "text": "R1Q1(a): \u201cThe classification of base class into super classes seems questionable to me.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 3, "text": "In the meta-learning language, the author attempts to learn a good representation of graphs", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 4, "text": "based on different graph classification tasks generated by a task distribution", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 5, "text": ".", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 6, "text": "In terms of graph classification, the task distribution is supported on the joint distributions (G, Y).\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 7, "text": "R1A1(a): Our model has two major components, $C^{sup}$ (for super-class prediction) and $C^{GAT}$ (for graph label prediction).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 8, "text": "During the training phase of our classification, $C^{sup}$, which is a MLP layer, learns the super-class labels of the samples based on GIN\u2019s extracted feature vectors (which represent base class labeled graphs).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 9, "text": "While, $C^{GAT}$ takes as input the \u201cgraph of graphs\u201d (supergraph) which models the latent inter-class as well as intra-class information and is constructed in every training batch, along with base-class labels, to learn the associated class distribution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 10, "text": "Then, during the fine-tuning phase on graphs with novel class labels, the feature extractor\u2019s (GIN) parameters are fixed and $C^{sup}$ is used to infer the super-class label of the novel class labeled graphs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 11, "text": "Then, the parameters learned by $C^{GAT}$ get updated and further \u201cfine-tuned\u201d for better performance on the novel samples.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 12, "text": "In addition to our brief overview, you could also find a very neatly detailed summarization of our method in reviewer 2\u2019s comments (paragraphs 1-4).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 13, "text": "The meta-learning framework, where batches are sampled as \u201cepisodes\u201d with N-way K-shot setting, does not perform as well in our few-shot setting on graphs for the following reasons:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 14, "text": "1) We have very limited total number of training classes (in order of 10s), when compared to the image domain (order of 100s and 1000s).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 15, "text": "This limitation hampers learning across tasks and generalization to new unseen tasks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 16, "text": "2) In each of our batches, we randomly sample a fixed-size of training samples belonging to the set of N labels chosen.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 17, "text": "Therefore, when building our supergraph, we end up with k-NN graphs of \u201cvariable size\u201d per super-class, compared to fixed size (K nodes) k-NN graphs that we would have got using episodic learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 18, "text": "We suspect this further allows our GAT to learn and generalize better to unseen graphs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 19, "text": "Furthermore, in [1], the authors use a similar strategy in their \u201cbaseline++\u201d method and produce good results.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 20, "text": "Their findings are also in sync with our empirical finding.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 21, "text": "[1] Chen et~al. \u201cA closer look at Few-Shot Classification\u201d, ICLR 2019", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 22, "text": "R1Q1(b): Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 23, "text": "R1A1(b): In every batch of graphs during both training and fine-tuning phase, each graph is associated with its corresponding graph label.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [14]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 24, "text": "In case of training, its a base-class and in the case of fine-tuning its a novel class", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [14]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 25, "text": ".", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [14]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 26, "text": "In the case of $C^{GAT}$, the graph is accompanied by a regular class label and in case of $C^{sup}$, the graph is accompanied by a superclass label.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [14]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 27, "text": "R1Q2: \u201cThough seemingly very important to the architecture, the purpose of constructing the super-graph $g^{sup}$ in the training of $C^{CAT}$ seems to be unclear to me.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 28, "text": "R1A2: What makes few-shot learning particularly difficult compared to common machine learning settings is the dearth of training examples, which results in a bad empirical risk approximation for the expected risk and therefore gives rise to an empirical risk minimizer that is sub-optimal.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 29, "text": "Reducing the required sample complexity can result in a better empirical risk minimizer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 30, "text": "Therefore, given a very large space of hypotheses H, our goal is to further restrict and constrain H using some prior knowledge because a reduced H has reduced sample complexity and thus requires fewer training samples to be trained.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 31, "text": "We provide this \u201cprior knowledge\u201d in the form of a \u201cgraph of graphs\u201d, namely our super-graph $g^{sup}$, which captures both the latent inter-class and intra-class relationships between classes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 32, "text": "Observe that in $g^{sup}$, we build a k-NN graph PER super-class, restricting any flow of information between super-classes, thus further restricting H. We force our model to jointly learn both the superclass and graph class labels.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 33, "text": "This way similar classes (grouped under a superclass) together contribute to learning a general prior representing the superclasses and each superclass also provides \u201cguidance\u201d to better train with the few samples assigned to that superclass.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 34, "text": "The introduction of this prior knowledge in the form of a supergraph in $C^{GAT}$ during training also helps generalize better to the novel samples that are presented to our model in the fine-tuning stage.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "ryxUOGLHcH", "rebuttal_id": "S1x3ALTQjS", "sentence_index": 35, "text": "Additionally, we would also like to draw attention to the supergraph usage summary provided by reviewer 2 in paragraph 3 of their comments.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 0, "text": "Thank you to Reviewer 3 for your thoughtful critique and we are happy that you share our enthusiasm for the motivation behind our approach.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 1, "text": "We share your curiosity on the qualitative behavior of such systems, and as documented in this response we have augmented the paper to address that and other of your suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 2, "text": "Re: \"- no qualitative analysis on how modulation is actually use by the systems.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 20, 21]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 3, "text": "E.g., when is modulation strong and when is it not used \"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 20, 21]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 4, "text": "Following the reviewer\u2019s suggestion, we have added a figure that shows the dynamics of neuromodulation in the cue-response task (Figure 3, in the Appendix).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 20, 21]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 5, "text": "This figure shows that while neuromodulation clearly reacts to reward, this reaction is complex and varies both within each episode and between runs.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 20, 21]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 6, "text": "Re: \"- perplexity improvements of less than 1.3 points over plasticity alone (which is the actual baseline for this paper) can hardy be called \"significant\".", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [23]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 7, "text": "Even though they might be statistically significant (meaning nothing more than the two models being statistically different), minor architectural changes can lead to such improvements.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [23]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 8, "text": "Furthermore PTB is not a \"challenging\" LM benchmark.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 9, "text": "We agree that, while the differences are statistically significant, they are minor.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 10, "text": "We were using that word technically, but do not want to give the wrong impression.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 11, "text": "We have thus modified the text to make it clear that we mean \u201cstatistically significant\u201d only.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 12, "text": "We also removed the adjective \u201cchallenging\u201d as regards PTB.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 13, "text": "We agree that, ideally, a comparison with SOTA architectures would be desirable.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 14, "text": "As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 15, "text": "We will keep trying to investigate such massive architectures in the future.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 16, "text": "Importantly, our purpose in this task is to show that, **all other things being equal**, a neuromodulated plastic LSTM can outperform a standard LSTM in realistic settings.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "ryxWDI_Gsm", "rebuttal_id": "rJx0WYw9R7", "sentence_index": 17, "text": "We believe that outperforming standard LSTMs (again, all else being equal) on their \u201cworkhorse\u201d task domain (language processing) is worthy of notice, especially given the ease of implementation of our method which requires only adding a few lines of codes (<10) to a standard LSTM implementation and can then be used as a drop-in replacement to standard LSTM.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 0, "text": "We really appreciate your constructive comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 1, "text": "We respond to each comment as follows.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 2, "text": "1. Meta dropout does not regularize the variational framework because there is no variational inference framework.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 3, "text": "- Thank you for your comment.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 4, "text": "We agree with you that the current lower bound is not a variational form due to the assumption of q=p. In Section 3.2, we toned down the original expression \u201cLearning to regularize variational inference\u201c into \u201cConnection to variational inference\u201d, and corrected the corresponding sentences.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 5, "text": "Still, there exists a clear connection between standard variational inference and our learning framework.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 6, "text": "Thus we believe that discussion in Section 3.2 will be helpful to readers who want to understand the meaning of learning objective Eq.(2) in depth.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 7, "text": "2. Improving adversarial robustness experiment.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 8, "text": "- Thank you for the helpful suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 9, "text": "During the rebuttal period, we conducted additional experiments on adversarial robustness as you suggested:", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 10, "text": "a) We replaced the previous FGSM attack with stronger PGD attack (200 iter.), with $L_1$, $L_2$, and $L_\\infty$ norm constraints.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 11, "text": "b) We included more baselines (e.g. Mixup, VIB, and information dropout), and show that our meta-dropout largely and consistently outperforms all of them.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 12, "text": "c) We added more detailed descriptions of the adversarial meta-learning baseline and in-depth analysis on the results.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 13, "text": "d) We further show that the learned perturbation from our Meta-dropout also generalize across different types of adversarial attacks with $L_1$, $L_2$, and $L_\\infty$ attacks.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 14, "text": "The generalization to different types of attacks is an important problem in adversarial learning, and most existing models fail to achieve this goal.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 15, "text": "Please see the corresponding section in the revision.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "S1e1sc_GqB", "rebuttal_id": "ryg4oUWPsH", "sentence_index": 16, "text": "We believe that the adversarial robustness part of our paper has become much stronger than before, thanks to your suggestion.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 0, "text": "We appreciate your constructive feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 1, "text": "Specifically, your comments about our derivations greatly help us to improve the quality of our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 2, "text": "We hope you to also consider our notable experimental results as well.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 3, "text": "(Bounds of KL divergence) Thank you for this good comment.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [3, 4, 5, 6, 7]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 4, "text": "We claimed that the Equation 1 can be maximized indirectly by maximizing Equation 2 which is a lower bound of Equation 1.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5, 6, 7]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 5, "text": "If we understand your primary concern correctly, the concern comes from the bound of KL divergence in Equation 5.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5, 6, 7]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 6, "text": "To prove correctness of our formulation, we can rewrite the pointed term in Equation 5 by using simple bayes rule as follows:", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5, 6, 7]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 7, "text": "$$\\displaystyle\\sum_{\\mathrm{z}}\\hat{q}\\mathrm{(z|c)}\\ \\mathrm{log}\\ \\frac{\\hat{q}\\mathrm{(z|c)}}{\\hat{p}\\mathrm{(c|z)}} = \\displaystyle\\sum_{\\mathrm{z}}\\hat{q}\\mathrm{(z|c)}\\bigg(\\mathrm{log}\\ \\frac{\\hat{q}\\mathrm{(z|c)}}{\\hat{p}\\mathrm{(z|c)}} + \\mathrm{log}\\ \\frac{\\hat{p}\\mathrm{(z)}}{\\hat{p}\\mathrm{(c)}} \\bigg)$$", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5, 6, 7]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 8, "text": "Because the $\\hat{p}\\mathrm{(c)}$ is constant, and $\\hat{p}\\mathrm{(z)}$ is not included in our optimization, we just optimize $\\displaystyle\\sum_{\\mathrm{z}}\\hat{q}\\mathrm{(z|c)}\\mathrm{log}[\\hat{q}\\mathrm{(z|c)} / \\hat{p}\\mathrm{(z|c)}]$. Since the $\\hat{q}\\mathrm{(z|c)}$ and $\\hat{p}\\mathrm{(z|c)}$ are both normalized distributions, the $D_{KL}[\\hat{q}\\mathrm{(z|c)} || \\hat{p}\\mathrm{(z|c)}]$ is always positive.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5, 6, 7]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 9, "text": "Then, we can conclude that Equation 2 becomes the lower bound for Equation 1.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5, 6, 7]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 10, "text": "(lambda)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 11, "text": "Actually, Equation 7 consists of three terms.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 12, "text": "Since only the third term is proposed additional regularization, we applied weighting parameter lambda to the third term only.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 13, "text": "(Difference with CDVAE) To clarify the difference our DiVA with CDVAE, we write derivations for both models here.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 14, "text": "CDVAE: $\\mathbb{E}_{q_{\\theta}\\mathrm{(z|x)}}[\\mathrm{log}\\ p_{\\theta '}(\\mathrm{x|z)}] - D_{KL}[q_{\\theta}\\mathrm{(z|x)} || p\\mathrm{(z)]} + \\lambda \\mathbb{E}_{q_{\\theta}\\mathrm{(z|x)}}[\\mathrm{log}\\hat{p}_{\\phi '}\\mathrm{(c|z)}]$", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 15, "text": "DiVA: $\\mathbb{E}_{q_{\\theta}\\mathrm{(z|x)}}[\\mathrm{log}\\ p_{\\theta '}(\\mathrm{x|z)}] - D_{KL}[q_{\\theta}\\mathrm{(z|x)} || \\hat{q}_{\\phi}\\mathrm{(z|c)]} + \\lambda \\mathbb{E}_{q_{\\theta}\\mathrm{(z|x)}}[\\mathrm{log}\\ \\hat{p}_{\\phi '}\\mathrm{(c|z)}]$", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 16, "text": "As we discussed in section 4.1, below the table for Algorithm 1, the key difference is that we consider class-conditional Gaussian distributions as priors for variational posteriors.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 17, "text": "Since CDVAE assumes the prior as unit Gaussian for all classes and optimizes classification loss simultaneously with the KL divergence, the latent space does not follow the prior exactly.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 18, "text": "As a result, CDVAE sometimes generates ambiguous samples (Figure 2 (c)).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 19, "text": "Interestingly, RtF [1] also does not consider the class-conditional priors even though they consider a classifier integrated VAE similar to CDVAE.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 20, "text": "In contrast, we assume class-wise specific Gaussian for each class.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 21, "text": "As a result, we can stably generate more realistic samples than CDVAE.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 22, "text": "[Additional feedback]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 23, "text": "(dt in Algorithm 1) dt means the domain translation explained at section 5.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 24, "text": "(Figure 1)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 25, "text": "- We corrected the typo.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 26, "text": "- The 3d plot conceptually represents class-specific one mode Gaussians.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 27, "text": "- The classification loss has implicit dependency with input conditions by minimizing the KL divergence in Equation 2.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 28, "text": "(heavy classifier) A classifier such as resnet.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 29, "text": "We used this term to distinguish the additional classifier from our integrated encoder that has discriminative power.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 30, "text": "(Redundant weights)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 31, "text": "If we extend to a more complex dataset such as ImageNet, it will become highly redundant.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [18]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 32, "text": "Furthermore, if we consider fully-convolutional architecture (without fully-connected layers), redundancy becomes a serious problem.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [18]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 33, "text": "For example, a feature map that has shape of [W x H x dim] becomes [W x H x (dim + the number of classes)].", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [18]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 34, "text": "In contrast, using discriminative conditional distributions can keep the dimension of the feature map as [W x H x dim] regardless of the number of classes.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [18]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 35, "text": "(Notations) Thank you for commenting this.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 36, "text": "We corrected the notations of section 3 to match with later sections.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 37, "text": "(Complexity of encoder) We intended that the encoder network can have enough both discriminative and generative power with a powerful architecture such as a deep residual network.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21, 22, 22]]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 38, "text": "[References]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1eIP8MpYB", "rebuttal_id": "rJlpm6hSir", "sentence_index": 39, "text": "[1] van de Ven, Gido M., and Andreas S. Tolias. \"Generative replay with feedback connections as a general strategy for continual learning.\" arXiv preprint arXiv:1809.10635 (2018).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 0, "text": "We would like to thank Reviewer 2 for their review and constructive suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 1, "text": "Our responses inline:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 2, "text": ">Discussions sometimes lack depth or are absent.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 3, "text": "-We have added an additional section (Appendix G) expanding on our discussion and providing additional insight into the observed instabilities.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [15]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 4, "text": ">For example, it is unclear to me why some larger models are not amenable to truncation.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 5, "text": "Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 6, "text": "-Truncation introduces a train-test disparity in G\u2019s inputs--at sampling time, G is given a distribution it has effectively never seen in training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 7, "text": "The observation that imposing orthogonality constraints improves amenability to truncation is empirical.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 8, "text": "Our suspicion is that if G is not encouraged to be \u201csmooth\u201d in some sense, then it is likely that G will only properly generate images given points from the untruncated distribution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 9, "text": "We hypothesize that models which are not amenable end up learning mappings which, when given truncated noise, either attenuate or amplify certain activation pathways, leading to extreme output values (hence the observed saturation artifacts).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 10, "text": "We speculate that encouraging G\u2019s filters to have minimum pairwise cosine similarity means that, when exposed to distribution shift, the network\u2019s features are less correlated and less likely to align and amplify an activation path it would otherwise have learned to scale properly.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 11, "text": ">Were samples from those networks better without using truncation? Why would this be?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 12, "text": "-Samples from those networks without truncation do not have measurably different quality, and their training metrics (losses, singular values) show no differences.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 13, "text": "Aside from empirically testing each network individually for amenability to truncation, we found no other way to check for that amenability.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 14, "text": "> Authors report how wider networks perform best, and how deeper networks degrade performance.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [19]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 15, "text": "Again, discussions are lacking, and it doesn\u2019t seem the authors tried to understand why such behaviors were shown.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 16, "text": "Even though this is mostly an empirical investigation, I think some more efforts should be put in understanding and explaining why some of those behaviors are shown, as I think it can bootstrap future work more easily.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 17, "text": "-We are wary of explanations for which we do not have evidence.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 18, "text": "For each of the modifications introduced in Section 3, we offer a succinct conjecture as to why that change improves performance, but we are not aware of any existing reliable, informative metric which we could employ to understand or trace the source of each observed behavior, particularly with respect to GAN stability or performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 19, "text": "Regarding depth vs width: This paper is empirical, and we only briefly experimented with increasing depth analogously to increasing width.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 20, "text": "While increasing width provided an immediate measurable benefit, increasing depth did not.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 21, "text": "We felt that it was better to report the results of this brief investigation than to omit it for a lack of investigatory depth.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 22, "text": "> In Section 3.1 : \u201cAcross runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.\u201d For me, this is not particularly clear.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22, 22]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 23, "text": "Is this something the reader should understand from Table 1?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22, 22]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 24, "text": "-This means that of all the models we trained for the study presented in Table 1 which did not use Orthogonal Regularization, only 16% were amenable to truncation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 22]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 25, "text": "Of all the models which we trained for the study presented in Table 2 which did use Orthogonal Regularization, 60% were amenable to truncation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 22]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 26, "text": "This is not reflected in Table 1, which is merely a presentation of how the introduced modifications impact performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 22]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 27, "text": ">I question the choice of sections chosen to be in the main paper/appendices.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 28, "text": "I greatly appreciated the negative results reported in the main text as well as in the appendices and this has significant value.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 25]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 29, "text": "However, as this is to me mostly a detailed empirical investigation and presentation of high-performance GANs on large scales, I would be likely to share this with colleagues who want to tackle similar problems.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 25, 26]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 30, "text": "In this case, if future readers limit themselves to the main text, I think it can have more value to present some content form Appendix B and C than to have more than a full page on stability investigations and attempted tricks that turned out not to be used to reach maximal performance.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 25, 26, 27]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 31, "text": "However I do not want to discourage publishing of negative results, and I definitely wish to see this investigation in the paper, but I merely question the positioning of such information.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 25, 26, 27, 28]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 32, "text": "With regard to my first negative point above about the lack of discussions, it seems the analysis of Section 4 is disproportionate compared to other places.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 25, 26, 27, 28, 29]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 33, "text": "-We appreciate this suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [24, 25, 26, 27, 28, 29]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 34, "text": "While we recognize that this paper generally has a strong focus on implementation details, we felt that this instability was one of the most salient behaviors we observed, and that future work would be best served by presenting our investigations and attempts to understand its source, even if these methods did not improve performance.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [24, 25, 26, 27, 28, 29]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 35, "text": "The information in Appendix B and C is intended to be of interest to those who want to reproduce our experiments, so it largely comprises hyperparameters and architectural details that we felt were not necessary to understand the main results of the paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24, 25, 26, 27, 28, 29]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 36, "text": ">In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says \u201closses\u201d.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [36]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 37, "text": "-Thanks! This was indeed an error, which we\u2019ve corrected in the updated draft.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [36]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 38, "text": ">I would also be curious to see the proposed techniques applied on simpler datasets. Can this be useful for someone having less compute power and working on something similar to CelebA?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [42, 43]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 39, "text": "-The goal of this work is to explore GANs at large scale; the exploration of small or medium scale models would indeed be interesting for another study.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [42, 43]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 40, "text": "Having said that, we do evaluate BigGAN on conditional CIFAR-10 (mentioned briefly in Appendix C.2) and obtain an IS of 9.22 and an FID of 14.73 without truncation, which to our knowledge are better than any published results.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [42, 43]]}, {"review_id": "S1gQAUDB27", "rebuttal_id": "Bye3WZB907", "sentence_index": 0, "text": "Thank you for your thoughtful review. We will address your concerns in turn.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1gQAUDB27", "rebuttal_id": "Bye3WZB907", "sentence_index": 1, "text": "Q1: The degradation function F is challenging to obtain in real world scenarios.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [1, 2, 3, 4]]}, {"review_id": "S1gQAUDB27", "rebuttal_id": "Bye3WZB907", "sentence_index": 2, "text": "A1: Many state-of-the-art approach, including SRCNN and SRGAN, has their own implicitly defined degradation function.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [1, 2, 3, 4]]}, {"review_id": "S1gQAUDB27", "rebuttal_id": "Bye3WZB907", "sentence_index": 3, "text": "They use their function F to generate training samples during their training process, while we use our explicitly defined function F during the inference process.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [1, 2, 3, 4]]}, {"review_id": "S1gQAUDB27", "rebuttal_id": "Bye3WZB907", "sentence_index": 4, "text": "If the assumed degradation function F is not exactly the function in real scenarios, both these state-of-the-art approach and our method will suffer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [1, 2, 3, 4]]}, {"review_id": "S1gQAUDB27", "rebuttal_id": "Bye3WZB907", "sentence_index": 5, "text": "So it is unfair to criticize our motivation just because we explicitly write out the degradation function F.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1, 2, 3, 4]]}, {"review_id": "S1gQAUDB27", "rebuttal_id": "Bye3WZB907", "sentence_index": 6, "text": "Q2: TV can also be applied for different restoration tasks, and it is easier to be optimized.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "S1gQAUDB27", "rebuttal_id": "Bye3WZB907", "sentence_index": 7, "text": "A2: We think you underestimate the difficulty of those restoration problems.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9, 10]]}, {"review_id": "S1gQAUDB27", "rebuttal_id": "Bye3WZB907", "sentence_index": 8, "text": "Please check the degraded images in Table 3.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9, 10]]}, {"review_id": "S1gQAUDB27", "rebuttal_id": "Bye3WZB907", "sentence_index": 9, "text": "These images are damaged so badly that TV cannot recover any meaningful thing.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9, 10]]}, {"review_id": "S1gQAUDB27", "rebuttal_id": "Bye3WZB907", "sentence_index": 10, "text": "As a handcrafted prior, TV performs much worse than our data-driven baseline method in these tasks.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8, 9, 10]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 0, "text": "Thanks for your careful review! As mentioned in the intro, we are trying to give some principled insight into benefits of BN, which has proved tricky.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 1, "text": "Also, it is noted in the paper that BN probably has many desirable properties, of which auto-rate tuning is just one.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 2, "text": "(i) Speed of SGD vs GD:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 16, 16, 18]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 3, "text": "Note that \u201ctime\u201d here refers to number of iterations, not epochs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16, 16, 18, 19, 20]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 4, "text": "We are not aware of results establishing SGD is faster in this measure.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16, 16, 18, 19, 20]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 5, "text": "(As noted on p2,  we are working within the standard paradigm of convergence rates in optimization.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [15, 16, 16, 18, 19, 20]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 6, "text": "The only new part is the automatic rate tuning  behavior shown for most parameters when BN is used.)", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [15, 16, 16, 18, 19, 20]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 7, "text": "(ii) \u201cusually training is stopped much before convergence, in the hope of finding solutions close to minimum with high probability.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 16, 18, 19, 20]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 8, "text": "We\u2019re assuming training proceeds until gradient is small (stationary point).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 16, 18, 19, 20]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 9, "text": "We are not aware of any prior analysis of speed of convergence that deviates from this assumption.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 10, "text": "Perhaps the reviewer is thinking of early stopping in context of better generalization?", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [16, 16, 18, 19, 20, 21, 22, 23]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 11, "text": "(iii) \u201cclarify difference from Wu et al. (2018)\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [36, 37, 38, 39]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 12, "text": "Wu et al. 2018 introduces a *new* algorithm inspired by weight normalization (WN) and studies its convergence rate to stationary point.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [36, 37, 38, 39]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 13, "text": "This algorithm can be seen as an explicit way to tune the learning rate (thus it is conceptually analogous Adagrad).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [36, 37, 38, 39]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 14, "text": "They don't have any results about WN or BN itself.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [36, 37, 38, 39]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 15, "text": "Their analysis could be adapted to GD on one-neuron network with WN or BN without scale-variant parameters (gamma and beta).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [36, 37, 38, 39]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 16, "text": "Even this adaptation is not immediate because the goal of this work is to find a stationary point on the unit sphere rather than R^d.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [36, 37, 38, 39]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 17, "text": "Finally, they prove no results for SGD, whereas our paper does.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [36, 37, 38, 39]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 18, "text": "(iv) \u201csingle learning rate doesn\u2019t apply for all parameters\u201d", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [40, 41, 42, 43]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 19, "text": "Correct.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [40, 41, 42, 43]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 20, "text": "The algorithm can use a single learning rate for scale-invariant parameters but needs a tuned rate for the scale-variant ones.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [40, 41, 42, 43]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 21, "text": "In feedforward nets, the number of scale-variant parameters scales as the number of nodes and the number of scale-invariant parameters scales as the number of edges (up to weight sharing).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [40, 41, 42, 43]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 22, "text": "Thus the vast majority of parameters are scale-invariant.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [40, 41, 42, 43]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 23, "text": "(v) \u201cRelation between original loss and loss using BN.\u201d", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 45, 46, 47]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 24, "text": "Our results hold for the loss of batch-normalized network (\u201cBN-loss\u201d)  which is different from the loss of the original network (\u201cBN-less loss\u201d).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 45, 46, 47]]}, {"review_id": "S1gZOpsFnQ", "rebuttal_id": "B1eSf1V0aX", "sentence_index": 25, "text": "Probably the reshaping of loss function due to BN is very important but currently hard to analyse theoretically because we lack a good mathematical understanding of the loss landscape (even BN-less).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 45, 46, 47]]}, {"review_id": "S1llxt2cnQ", "rebuttal_id": "HJea7OJdpX", "sentence_index": 0, "text": "Many thanks for the review!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1llxt2cnQ", "rebuttal_id": "HJea7OJdpX", "sentence_index": 1, "text": "Good point regarding the negative results; we have added a subsection in the revised paper entitled ``A non-convolutional network'', where we compare to a convolutional decoder and conclude that ``Our simulations indicate that, indeed, linear combinations, yield more concise representations, albeit not by a huge factor.''.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "S1llxt2cnQ", "rebuttal_id": "HJea7OJdpX", "sentence_index": 2, "text": "Regarding the minor points, we have reworded the paragraph on regularizing, and changed `compression ratio' to `compression factor', and reworded such that `large compression factor' means large compression.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 0, "text": "We thank the referee for their review and the summary of our results", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 1, "text": "1. We have included some more attacks on the most robust model (a transfer attack and a Gaussian random noise attack).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 2, "text": "2.", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 3, "text": "(a) We have evaluated the adversarial resistance when training a Boltzmann machine with 256 fully connected latent variables directly on the 8x8 patches.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 4, "text": "The version with only 128 hidden units was not able to reduce the relative entropy to the values of the larger, stacked machine.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 5, "text": "We find that the model without stacking is not able to increase the adversarial resistance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 6, "text": "It is possible that we are unable to complete the training due to the approximations involved.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 7, "text": "For a small machine (16 units) of full hidden connectivity we can observe the noise rejection behaviour, as shown in appendix D.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 8, "text": "(b) We have trained a machine with the same connectivity as the stacked machine directly on the 8x8 patches.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 9, "text": "This training gives similar results to the training in stages.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 10, "text": "(c) From the result in (b) we conclude that the particular manner of the pre-training does not matter.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 11, "text": "Therefore also the choice of first training set (98% coverage or full coverage) does not influence adversarial resistance.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 12, "text": "(d) There are a total of 28800 parameters in the Boltzmann machine.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 13, "text": "We will gladly provide files with the trained weights and also fully trained neural networks on request.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 14, "text": "3. We currently do not have a full explanation for the large adversarial resistance, but noise resistance must play a part in it.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1lNOhec27", "rebuttal_id": "B1lA6qBKC7", "sentence_index": 15, "text": "The very strong rejection of Gaussian noise and the observations in Fig.\u00a04 point in this direction.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 0, "text": "Thank you very much for your strong recommendation!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 1, "text": "1) Intuition about the improvement", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 2, "text": "Though not explained in Section 4.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 3, "text": "The intuition for NADPEx is given in Section 3.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 4, "text": "Interpretation for as efficient or even faster exploration in dense environment (4.1) is that NADPEx could encourage more diverse exploration, while absorb experience from it in a relatively efficient way.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 5, "text": "For sparse environments (4.2), where temporally consistent exploration is crucial for learning signal acquisition, NADPEx outperforms vanilla PPO.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 6, "text": "It could also beat parameter noise if difficulty is increased, because intuitively low variance in gradients is a boon for faster learning.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 7, "text": "Improvement in 4.3 and 4.4 are basically from the theoretical grounding of NADPEx, which we believe is one of our contributions.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 8, "text": "Specifically, improvement in 4.3 is from high level stochasticity's adaptation to the low level; while that in 4.4 could be interpreted with the idea of trust region, that policy should be updated to somewhere near the sampling policy in the policy space, such that collected experience are usable (on-policy).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 9, "text": "In NADPEx, trust region also contains the meaning that dropout policies are close to each other for more efficient exploration.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 10, "text": "2) Limitation of NADPEx", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 11, "text": "One of the limitation we see from NADPEx is that dropout policies are not directly interpretable from their network structures, while interpretability and composibility are prerequisites for reusing them in more complicated tasks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 12, "text": "Luckily, modeled as latent random variables, an information term could be added to the objective as in [1, 2].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 13, "text": "This is also a direction for future research work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 14, "text": "[1] Florensa et al., \"Stochastic neural networks for hierarchical reinforcement learning\", ICLR 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1lzrrz637", "rebuttal_id": "rkxN4ktETm", "sentence_index": 15, "text": "[2] Hausman et al., \"Learning an Embedding Space for Transferable Robot Skills\", ICLR 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1x0HKfqh7", "rebuttal_id": "S1xAN6UJT7", "sentence_index": 0, "text": "The main topic of the paper is how to evaluate models that use confidence thresholding.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_global", null]}, {"review_id": "S1x0HKfqh7", "rebuttal_id": "S1xAN6UJT7", "sentence_index": 1, "text": "The primary purpose is to compare *defenses*. However, to justify the attack strategy that we propose to use, we also compare *attacks*. Specifically, we provide an experiment demonstrating that our attack actually is stronger than the baseline.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_unknown", null]}, {"review_id": "S1x0HKfqh7", "rebuttal_id": "S1xAN6UJT7", "sentence_index": 2, "text": "However, it is not really necessary to provide multiple experiments demonstrating that MaxConfidence is more powerful because the superiority of MaxConfidence is theoretically guaranteed.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 0, "text": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 1, "text": "Here we respond to your specific comments.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 2, "text": "\"Some technical details are missing.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 3, "text": "In Section 3.2.2, the authors only explain how they learn example-based \\sigma, but details on how to make graph construction end-to-end trainable are missing.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 4, "text": "Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 5, "text": ">>> Thanks for pointing out the details.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 6, "text": "We want to clarify the few-shot setting.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 7, "text": "We follow the widely-used episodic paradigm proposed by Matching Networks [1].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 8, "text": "In each episode (training batch), our algorithm solves a small classification problem which contains N classes each having K support and Q query examples (e.g., N=5, K=1, Q=15, totally 80 examples).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 9, "text": "The weight matrix is constructed on the support and query examples in each episode rather than the whole dataset.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 10, "text": "This is very fast and efficient.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 11, "text": "In deep neural networks, there is a common trick in computing the gradient of operations non-differentiable at some points, but differentiable elsewhere, such as Max-Pooling (top-1) and top-k.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 12, "text": "In forward computation pass, the index position of the max (or top-k) values are stored.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 13, "text": "While in the back propagation pass, the gradient is computed only with respect to these saved positions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 14, "text": "This trick is implemented in modern deep learning frameworks such as tensorflow and pytorch.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 15, "text": "In our paper, we use the tensorflow function tf.nn.top_k() to compute k-nearest neighbor operation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 16, "text": "\"Does episode training help label propagation? How about the results of label propagation without the episode training? \"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 17, "text": ">>> In our paper, the length scale parameter \\sigma is trained in an example-wise and episodic-wise way, as described in section 3.2.2 and Figure 4 of Appendix A. In order to investigate the benefit of episodic training, we combine the heuristic-based label propagation methods [2] with meta-learning to serve as a transductive baseline.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 18, "text": "Please refer to Table 1 and Table 2 line \"Label Propagation\".", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 19, "text": "It can be seen that TPN outperforms naive label propagation with a large margin, thus verifying the effectiveness of episode training.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [12]]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 20, "text": "[1] Vinyals, Oriol et al. \"Matching networks for one shot learning.\" NIPS. 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1x4ca-chQ", "rebuttal_id": "Hye0RIZ_RX", "sentence_index": 21, "text": "[2] Zhou, Denny et al. \"Learning with local and global consistency.\" NIPS. 2004.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1x6zNA92m", "rebuttal_id": "HJe5KS2-AX", "sentence_index": 0, "text": "We thank the reviewer for their positive evaluation of our work.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1x6zNA92m", "rebuttal_id": "HJe5KS2-AX", "sentence_index": 1, "text": "In response to their suggestion under Point 4, we have added the suggested references to sections 2.3 (Related Work) and 4 (Discussion and Conclusion)", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [31, 31, 31, 31, 31]]}, {"review_id": "S1x6zNA92m", "rebuttal_id": "HJe5KS2-AX", "sentence_index": 2, "text": "New text:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [31, 31, 31, 31, 31]]}, {"review_id": "S1x6zNA92m", "rebuttal_id": "HJe5KS2-AX", "sentence_index": 3, "text": "\u201cSimilarly, Wah et al. (2014) show a series of adaptive displays for an anchor c_i, where the subject must partition the queries c_j, c_l, \u2026 into a set of similar and a set of dissimilar queries.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [31, 31, 31, 31, 31]]}, {"review_id": "S1x6zNA92m", "rebuttal_id": "HJe5KS2-AX", "sentence_index": 4, "text": "In contrast to our work, the aforementioned studies did not use sparsity or positivity constraints, nor did they intend to evaluate the interpretability of the embedding.\u201d", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [31, 31, 31, 31, 31]]}, {"review_id": "S1x6zNA92m", "rebuttal_id": "HJe5KS2-AX", "sentence_index": 5, "text": "New text:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [31, 31, 31, 31, 31]]}, {"review_id": "S1x6zNA92m", "rebuttal_id": "HJe5KS2-AX", "sentence_index": 6, "text": "\u201cYet another possible extension is to consider different types of similarity judgments (Veit et al. 2017), e.g. resulting from asking subjects to group objects based on a specific attribute (size, color, etc.).\u201d", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [31, 31, 31, 31, 31]]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 0, "text": "Thank you for your supportive review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 1, "text": "We answer the specific queries below and have also added them to the revised version of the paper.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 2, "text": "1.         We found that the entropy regularizer produces an ME score that stays constant across training, at the cost of the model being less confident about predictions made for seen classes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 3, "text": "We added details regarding this condition to the manuscript.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12]]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 4, "text": "2.         The base rate is the probability of observing a new word in the target at that particular point in training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 5, "text": "We go through the remaining sentences in the corpus from the target compute the probability of sampling a sentence with at least one new word.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 6, "text": "Thus, the base rate at time t in training is defined as:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 7, "text": "$$P(\\text{new in target at t}) = \\frac{ \\text{# of unseen sentences in target with new words}} {\\text{# of unseen sentences}}$$", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 8, "text": "3.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 9, "text": "In Section 4.2, we use \u201cnew\u201d to refer to the set of all the unseen classes at a particular timepoint t.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 14]]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 10, "text": "For the classifier, P(N|t) is calculated by adding the probabilities the model assigns to all the \u201cnew\u201d classes when iterating through the remaining corpus (similar to Equation 1 in our paper).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 14]]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 11, "text": "For the dataset, we compute P(N|t) by sampling all unseen images in the corpus and compute the proportion from \u201cnew\u201d classes given their ground truth labels.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 14]]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 12, "text": "4.         We will release our code and data with the publication of the paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 13, "text": "Most of our experiments are easy to replicate as they use standard datasets, models, loss functions and optimizers.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 14, "text": "We sincerely hope that our challenge and these resources will stimulate progress in this area.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1xAWnjRFH", "rebuttal_id": "rklpjoQ9oH", "sentence_index": 15, "text": "Please also see above where we write a general response to all reviews.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 0, "text": "The following link is a figure that explains the difference between zero-confidence attack and fix-perturbation attack.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 1, "text": "https://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure1.pdf", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 2, "text": "As can be seen, the zero-confidence attack finds the closest point on the decision boundary; while fix perturbation-attack finds adversarial samples within a fix perturbation.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 3, "text": "Both attacks are equivalent if we only want to compute the attack success rate under a given perturbation level.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 4, "text": "However, we will be better off with zero-confidence attacks if we want to", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 5, "text": "1) Compute the margin of each individual example; and", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 6, "text": "2) Probe and study the decision boundary of a classifier", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 7, "text": "Of course, we can also measure the margin of each example using a fix-perturbation attack, for example PGD, by binary searching over the perturbation levels.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 8, "text": "However, the computation cost will significantly increase.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 9, "text": "Consider, for example, the CIFAR-10 dataset.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 10, "text": "Since for our model, most margins fall within 10, so let\u2019s assume the binary search range is 10 (for adversarially trained models this number will be much higher).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 11, "text": "If we want to achieve a accuracy of 0.1, then we need at least 7 binary search steps.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 12, "text": "In other words, the computation complexity increases by 7 times.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 13, "text": "In fact, CW applies a similar binary search idea to achieve zero-confidence attack, and that is why its computation cost is high.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 14, "text": "The above discussion is not saying that it is impossible to convert PGD to a zero-confidence attack efficiently, but it at least provides a perspective on why zero-confidence attack is challenging, and why the complexity reduction as well as accuracy improvement of MarginAttack is valuable.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 15, "text": "Although this is not the major focus of your comment, we would like to revisit the theorem assumptions.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 16, "text": "While there are nine assumptions, these assumptions are in fact more realistic than expected.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 17, "text": "Take the convexity assumption, which you mentioned in your review, as an example.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 18, "text": "This assumption does not say that the constraint has to be convex.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 19, "text": "It only says that the constraint should not be \u2018too concave\u2019.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 20, "text": "In particular, the curvature of the of the decision should not exceed that of the L2 or L-infinity ball.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 21, "text": "For better illustration, we plotted some decision boundaries that are allowed by the assumption, and some that are not.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 22, "text": "Please check the following link:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 23, "text": "https://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure2.pdf", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 24, "text": "As can be seen, the convexity assumption permits a wide variety of decision boundaries.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 25, "text": "Among the few cases that it does not permit is the case where the decision boundary bends more than the L2 ball does.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 26, "text": "In this case, the critical point becomes a local maximum rather than a local minimum.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 27, "text": "The other assumptions are also more realistic than their names sound.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 28, "text": "The differentiability assumption does not stipulate that the constraint has to be differentiable.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 29, "text": "It actually permits countably infinite jump discontinuities.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 30, "text": "The Lipchitz continuous assumption does not assume Lipchitz continuity everywhere, but only at x*. We are not saying that the assumptions are very loose, but they are realistic enough to shed some light on the actual convergence property of MarginAttack.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 31, "text": "Nevertheless, we are considering adding a 2D toy example as you suggested.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [9]]}, {"review_id": "S1xcRXhFnQ", "rebuttal_id": "r1eYIxfqT7", "sentence_index": 32, "text": "We will post further responses if there are further updates.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [9]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 0, "text": "Overall:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 1, "text": "We thank you for your time and appreciating the strengths of our work.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 2, "text": "Based on your guidance and suggestions, we have tried to do additional experiments to improve the quality of our work.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 3, "text": "Concern 1: Comparison to Meta-RevGrad", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 4, "text": "Meta-RevGrad tries to achieve feature invariance at the embedding level.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 5, "text": "Achieving such feature invariance for high-dimensional feature mapping can be a very weak constraint [1], causing limitation in performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 6, "text": "Recently, generative approaches, following image-to-image translation have been shown achieve better domain adaptation, as this constrains the feature embedding to generate the data in a new domain.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 7, "text": "Being a non-GAN based approach, concepts such idt (encouraging the styling network to behave as an identity when given a target domain instance as input) and revMap (constructing source instance back from generated target instance) are not applicable in this scenario, as no instances or images are being generated from a feature embedding.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 8, "text": "Concern 1, 2, 3: Experiments", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 9, "text": "Thank you for these suggestions.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [7, 8, 9, 10, 11, 12, 13]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 10, "text": "Taking your comments and comments from other reviewers into account, we have made improvements to the experimental section.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9, 10, 11, 12, 13]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 11, "text": "Specifically:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11, 12, 13]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 12, "text": "\u201cDomain Adaptation Baselines\u201d,", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 13, "text": "We have now added additional domain adaptation baselines, designed in the setting suggested by Reviewer 2.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 14, "text": "\u201cSimple baseline \u2013 combining a subset of a new domain as training set\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 15, "text": "We see the merit of this baseline, but there are several challenges in executing this.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [11]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 16, "text": "Designing it in a fair way is tricky.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [11]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 17, "text": "Using some labelled data in target domain maybe unfair, as we are not allowed to see meta-test data.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [11]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 18, "text": "Moreover, this is likely to not work, as the meta-train data would be too large, and would dominate, and we do not have a clear way to set the weights.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [11]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 19, "text": "\u201cDramatic Domain Shift, Omniglot to Fashion-MNIST\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 20, "text": "This could be an interesting setting, but we don\u2019t think this will work very well, as the tasks are themselves completely different. We would not expect a character recognition model to transfer to a object recognition task, as the visual features are very different.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 21, "text": "Minor:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 22, "text": "Thanks for this; we have updated the draft to make the presentation clearer.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 23, "text": "Unlabelled data refers to only the domain of the meta-test data, but the meta-test data is never used in meta-training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 24, "text": "L_da is essentially the sum of L_gan and L_cycle.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1xGt0Rq3m", "rebuttal_id": "BkesLOIt07", "sentence_index": 25, "text": "[1] Shu, R., Bui, H.H., Narui, H. and Ermon, S. A DIRT-T Approach to Unsupervised Domain Adaptation. ICLR 2018", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 0, "text": "Thank you for your comments and suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 1, "text": "We will address the issues you mentioned.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 2, "text": "1.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 3, "text": "ARTNet is not very much related to our V4D.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 4, "text": "Basically, ARTNet is an alternative for 3D CNNs by replacing 3D convolution layers with SMART blocks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 5, "text": "The SMART blocks are two branch units, with one branch for learning static appearance features and one branch for learning motion features.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 6, "text": "ARTNet is a clip-based method for learning short-term representations while our V4D is a video-level method for learning both short-term and long-term representations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 7, "text": "2.\tDuring training, we uniformly divide the whole video into U sections and randomly select one action unit from each section.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 8, "text": "So there are no overlaps for training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 9, "text": "For testing, there might be overlapping during sampling due to the limit length of video.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 10, "text": "However, our V4D inference algorithm in section 3.4 guarantees that only the non-overlapping action units will interact with each other during testing.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 11, "text": "3.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 12, "text": "Sure.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 13, "text": "We train all the models with 8 GPUs of GTX 1080 with memory capacity of 11178MB.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 14, "text": "For the inference speed, our V4D ResNet18 takes 0.67s per video and V4D ResNet50 takes 1.22s per video.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 15, "text": "In addition, we also reported the GFLOPs of V4D and compared it with other typical methods in Table 2(b).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 16, "text": "For training on Mini-Kinetics, V4D ResNet18 takes a bit more than 1 day while V4D ResNet50 takes a bit more than 3 days.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 17, "text": "4.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 18, "text": "We can only find the results of ARTNet ResNet18 in the published paper.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 19, "text": "After communication with the authors of ARTNet, we confirm that there are no results published for ARTNet ResNet50.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 20, "text": "So instead we implement ARTNet ResNet50 by ourselves and the top1 accuracy on Kinetics-400 is 74.3%.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 21, "text": "This is still lower than our V4D ResNet50 whose top1 on Kinetics-400 is 77.4%.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 22, "text": "Also, ARTNet ResNet18 reports an average metric of 81.4%, which is the average of top1 and top5 accuracy.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 23, "text": "While our V4D yields an average score of 85.3%.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 24, "text": "5.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 25, "text": "Thank you for providing this related work and we now cite this paper in the second version.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 26, "text": "This paper utilizes 4D CNN to process videos of point cloud so that their input is 4D data.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 27, "text": "Instead, our V4D processes videos of RGB frames so that our input is 3D data.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 28, "text": "This basically makes the methods and tasks quite different.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 29, "text": "Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7]]}, {"review_id": "S1xUCqYpYH", "rebuttal_id": "B1g4kymYjS", "sentence_index": 30, "text": "Hopefully our rebuttal could stress your concerns. If there are still any possible issues, please don\u2019t hesitate to tell us and we will response as soon as possible.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 0, "text": "Thanks so much for your valuable review comments!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 1, "text": "Following your suggestion, we evaluated the Byzantine settings Multi-Krum (Blanchard et al 2017) and Bulyan (El Mhamdi et al 2018 ICML).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 2, "text": "For both DBA and centralized attack we use the aggregation rule that can tolerate f Byzantine workers among n workers", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 3, "text": "(Blanchard et al 2017)", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 4, "text": ".", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 5, "text": "For centralized attack there is 1 attacker and n-1 non-Byzantine workers.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 6, "text": "For DBA there are f distributed attackers and n-f non-Byzantine workers.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 7, "text": "The total number of poisoned pixel amounts are kept the same.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 8, "text": "1. Multi-Krum", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 9, "text": "- To meet the assumption that 2f + 2 < n, we set  (n=10, f=3) for loan and (n=12, f=4) for image datasets.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 10, "text": "The Multi-Krum parameter m is set to m=n-f.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 11, "text": "For Tiny-imagenet we decrease the poison ratio to 5/64 for both attacks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 12, "text": "Other parameters are the same as described in the paper.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 13, "text": "- For CIFAR and Tiny-imagenet, we find that DBA is more effective.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 14, "text": "- For LOAN and MNIST, both attacks don\u2019t behave well.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 15, "text": "We believe the reason can be explained by the fact that Loan and MNIST are simpler tasks and benign clients quickly agree on the correct gradient direction, so malicious updates are more difficult to succeed.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 16, "text": "2. Bulyan", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 17, "text": "- We use Bulyan", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 18, "text": "based on the Byzantine\u2013resilient aggregation rule Krum", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 19, "text": ".", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 20, "text": "To meet the assumption that 4f + 3 <= n, we set  (n=15, f=3) for loan and (n=20, f=4) for image datasets.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 21, "text": "- For CIFAR, DBA is more effective.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 22, "text": "- For other datasets, both attacks fail.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 23, "text": "However, we note that our distributed and centralized backdoor attacks are not optimized for Byzantine setting.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 24, "text": "We believe it\u2019s worthwhile to explore the distributed version of other new attack algorithms, e.g. A Little Is Enough (Baruch et al 2019) that manipulates its update to mitigate Krum and Bulyan defenses.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 25, "text": "In summary, Multi-Krum and Bulyan have stricter assumptions on the proportion of attackers than RFA and FoolsGold.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 26, "text": "In addition, while RFA and FoolsGold still assign potential outliers with extreme low weights, Krum (Multi-Krum, Krum-based Bulyan) directly removes them, making it impossible to inject backdoors if the malicious updates are obviously far from the benign updates.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 27, "text": "The centralized attack for four datasets totally fails under Multi-Krum and Bulyan while DBA can still succeed in some cases.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJe4yM7lcr", "rebuttal_id": "rkgZSdB3oH", "sentence_index": 28, "text": "We have included these results in Appendix A.6 of the revised version.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 10, 10, 12]]}, {"review_id": "SJeDlWsjj7", "rebuttal_id": "S1gN_2Yl0m", "sentence_index": 0, "text": "Thank you for your suggestion of increasing the discussion of the results.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [8]]}, {"review_id": "SJeDlWsjj7", "rebuttal_id": "S1gN_2Yl0m", "sentence_index": 1, "text": "We\u2019ve expanded the discussion of the results as much as possible.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8]]}, {"review_id": "SJeDlWsjj7", "rebuttal_id": "S1gN_2Yl0m", "sentence_index": 2, "text": "For now we would prefer to keep the actual bar plot of individual module performance in the appendix in the interest of space, and keep the dataset description in the main part, as this was appreciated by the other two reviewers.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "SJeDlWsjj7", "rebuttal_id": "S1gN_2Yl0m", "sentence_index": 3, "text": "As you say, the ability to generalize is very important in mathematics.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "SJeDlWsjj7", "rebuttal_id": "S1gN_2Yl0m", "sentence_index": 4, "text": "The paper contains an extrapolation test set to do exactly this - these include generalization tests on larger numbers, longer sequences, more function compositions (which is similar to having more variables), etc (see Appendix B for more details).", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "SJeDlWsjj7", "rebuttal_id": "S1gN_2Yl0m", "sentence_index": 5, "text": "We haven\u2019t attempted to be exhaustive in types of generalization, but the extrapolation test set can be extended in the future to allow for this.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "SJeDlWsjj7", "rebuttal_id": "S1gN_2Yl0m", "sentence_index": 6, "text": "None of the modules currently include \u201cunsolvable\u201d as an answer, but this is something that would definitely fit within the framework.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SJeDlWsjj7", "rebuttal_id": "S1gN_2Yl0m", "sentence_index": 7, "text": "(As an aside: there would be no need to have a special character; we could simply select some consistent word like \u201cUnsolvable\u201d; neural models trained so far seem to have no problem outputting \u201cTrue\u201d or \u201cFalse\u201d.) More generally, there are many further types of problems, that could be included in the dataset - but we hope for now that the current range is comprehensive in types of reasoning required for school-level mathematics.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SJeDlWsjj7", "rebuttal_id": "S1gN_2Yl0m", "sentence_index": 8, "text": "We always welcome contributions to the dataset that extend the range of questions in a consistent manner.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 0, "text": "We thank the reviewer for the valuable feedback! We reply to the answers and comments in the order they were raised.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 1, "text": "Note, that regarding the methodology there were some misunderstandings (which we try to avoid for future readers in the revised version).", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [5]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 2, "text": "(1) The equations are indeed very related.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 3, "text": "Note however, that In a standard Bayesian neural network (BNN), one would assume that \\theta is a global random variable (i.e. does not depend on input x), whereas in the CDN, we assume that \\theta depends on x and is thus a local random variable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 4, "text": "Furthermore, in a Bayesian setting p(theta|...) would play the role of a approximate posterior, which would require variational inference (VI), and thus a different objective,  to estimate it.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 5, "text": "(2) In Equation (4) we followed with  p(D | \\psi) a standard notation for \\sum_n p(y_n | x_n; \\psi) (i.e. summation of Equation (3) wrt all data in D) which also can be found e.g. in the work of Graves (2011) [4] and Blundell et al. (2015) [5].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 10, 11, 12]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 6, "text": "The objective we introduce for CDNs differs from the ELBO-based objective in VI in the way the logarithm is placed in the first term of the objective: in the ELBO we have a logarithm inside the expectation, while the logarithm is outside the expectation in the CDN objective (note however, that the sample-based approximations get equivalent if only one sample is used).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 10, 11, 12]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 7, "text": "Furthermore, in the ELBO we have a fixed value of \\lambda = 1.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 10, 11, 12]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 8, "text": "We added a new Section 4 in the revised version of the paper discussing these differences.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8, 9, 10, 11, 12]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 9, "text": "Moreover, we investigated the impact of the different objectives empirically and found that the CDN-based objective led to significantly better results, as shown in the newly added Section 6.4 in the revised manuscript.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8, 9, 10, 11, 12]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 10, "text": "(3) Indeed we need the probabilistic version of hypernetworks to implement the model we described in Equation (3).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 11, "text": "We just wanted to point out that this is in contrast to the vanilla  hypernetworks proposed by Ha et al. (2016) [1] and Jia et al.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 12, "text": "(2016) [2] which would produce a point estimate for \\theta.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 13, "text": "(4) We used a matrix-variate normal (MVN) to reduce the parameters of the model.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15, 16, 17, 18]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 14, "text": "Using a diagonal MVN for X \\in R^{p x q} one needs pq+p+q parameters.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15, 16, 17, 18]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 15, "text": "In contrast a fully-factorized diagonal Gaussian needs pq+pq.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15, 16, 17, 18]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 16, "text": "But you are right, we could easily extend our approach to account for more flexible distributions by using a \"diagonal plus rank-one\" structure diag(a)+uu^T, with vectors a and u, as noted by Louizos and Welling ( 2016) [3] (the increase of parameters is negligible: adding additional vector u).", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [14, 15, 16, 17, 18]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 17, "text": "We will investigate the benefits of more flexible mixing distributions in future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [14, 15, 16, 17, 18]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 18, "text": "(5) Thanks for this valuable comment!", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [19, 20, 21, 22, 23, 24]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 19, "text": "We have revised the toy experiments to include a heteroscedastic regression task (see Section 6.1.).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [19, 20, 21, 22, 23, 24]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 20, "text": "It shows that CDNs are able to quantify the heteroscedastic aleatoric uncertainty.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20, 21, 22, 23, 24]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 21, "text": "However, we kept the OOD experiments on MNIST and notMNIST in the paper as well, since we consider it as very interesting that the CDN, while being designed for modelling aleatoric uncertainty, is very competitive on this task.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20, 21, 22, 23, 24]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 22, "text": "Moreover, we investigate the mixing distribution learned in Appendix G.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20, 21, 22, 23, 24]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 23, "text": "(6) Of course, you are right! Previously we showed the test accuracy in Appendix F.  To make it more directly accessible, we have now added the test accuracy achieved by the different models into the legends of the plots, showing that CDN achieves similar predictive power as the baselines.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [25, 26, 27, 28]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 24, "text": "We now present the validation accuracy instead in Appendix F.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [25, 26, 27, 28]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 25, "text": "(7) Sorry, for this unfortunate formulation!", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [29]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 26, "text": "We observed that generally as \\lambda increases, the uncertainty is increasing, while the accuracy is decreasing.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 27, "text": "Therefore a simple and effective heuristic for choosing \\lambda is to look at the validation set of MNIST and choose the highest \\lambda that still results in high accuracy (e.g. >. 0.97).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 28, "text": "We have made this procedure clear in the revised manuscript.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [29]]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 29, "text": "References:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 30, "text": "[1] Ha, David, Andrew Dai, and Quoc V. Le. \"Hypernetworks.\" arXiv preprint arXiv:1609.09106 (2016).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 31, "text": "[2] Jia, Xu, et al. \"Dynamic filter networks.\" Advances in Neural Information Processing Systems. 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 32, "text": "[3] Louizos, Christos, and Max Welling. \"Structured and efficient variational deep learning with matrix gaussian posteriors.\" International Conference on Machine Learning. 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 33, "text": "[4] Graves, A. (2011). Practical variational inference for neural networks. In Advances in neural information processing systems (pp. 2348-2356).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJekKHZ93Q", "rebuttal_id": "ByxbnbuB0Q", "sentence_index": 34, "text": "[5] Blundell, C., Cornebise, J., Kavukcuoglu, K. & Wierstra, D.. (2015). Weight Uncertainty in Neural Network. Proceedings of the 32nd International Conference on Machine Learning, in PMLR 37:1613-1622", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 0, "text": "Thank you for the review and accurate summary of our submission!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 1, "text": "> I am reluctant to give a higher score due to its incremental contribution.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 2, "text": "Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 3, "text": "SVG clearly differs from Dreamer in that it only considers 1-step model predictions in SVG(1) or multi-step predictions without value function in SVG(\u221e).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 4, "text": "SVG(0) does not use a dynamics model.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 5, "text": "In addition, Dreamer propagates gradients through transitions in a learned features, making it effective for high-dimensional control tasks.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 6, "text": "> Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 7, "text": "Besides the important technical difference described above, we highlight the empirical performance of Dreamer.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 8, "text": "A conclusion of the SVG paper was that the model did not yield substantial practical benefits beyond 1-step predictions.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 9, "text": "We found it important to revisit this topic in the light of recent substantial improvements to dynamics models (see below).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 10, "text": "> Effectiveness on very long horizon trajectories: Simulating long-horizon trajectories with a probabilistic model is known to be unsuitable for model-based RL due to accumulated errors.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 11, "text": "This is an open issue in model-based RL.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 12, "text": "While current dynamics models still cannot accurately predict full episodes, this is rarely needed in practice.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 13, "text": "Recent works successfully use learned dynamics for control from both proprioceptive inputs (Chua et al. 2018, Shyam et al. 2019, Wang & Ba 2019) and from images (Hafner et al. 2019, Zhang et al. 2019).", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 14, "text": "Dreamer shows that the relatively short model predictions (H=20) yield high-quality policy gradients, and that an additional value function in the latent space is effective for solving tasks that require longer-term credit assignment (e.g. with sparse rewards).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 15, "text": "Our experiments provide evidence that combination is effective in practice.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18, 19]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 16, "text": "> However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [21, 22, 23, 24]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 17, "text": "This horizon length is still short compared to the entire horizon length of many MDPs (e.g., 1000).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [21, 22, 23, 24]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 18, "text": "I think this point should be discussed in the paper.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [21, 22, 23, 24]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 19, "text": "That is, the issue still exists, and Dreamer is less effective with very long horizon.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [21, 22, 23, 24]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 20, "text": "We address the challenge of long horizons not using long-term model predictions but by learning a value function that estimates the infinite sum of discounted future rewards.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [21, 22, 23, 24]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 21, "text": "Figure 4 in our submission shows that this gives Dreamer robustness to the imagination horizon compared to two baselines.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [21, 22, 23, 24]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 22, "text": "> Inapplicability to discrete controls:  One restriction of re-parameterized gradients is that the technique is not applicable to discrete random variables.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27, 28, 29]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 23, "text": "This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27, 28, 29]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 24, "text": "Still, such approximations would make learning more challenging, especially with long-horizon backpropagation.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27, 28, 29]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 25, "text": "This restriction should be noted in the paper.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [26, 27, 28, 29]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 26, "text": "We applied Dreamer to environments with discrete actions using the DiCE estimator (Foerster et al. 2018) locally for the da/d\u03bc and da/d\u03c3 derivatives.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [26, 27, 28, 29]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 27, "text": "This was a drop-in replacement for the reparameterization estimator and slightly outperformed a Gumble-softmax actor.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 28, "text": "We find that with this 1 line change, Dreamer solves discrete action tasks of the Atari suite and a 3D DMLab environment.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28, 29]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 29, "text": "> There is no mention about variance of policy gradient estimates.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [30, 31]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 30, "text": "Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [30, 31]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 31, "text": "Dreamer uses reparamterization gradients that already have low variance (Kingma & Welling 2013, Rezende et al. 2014); although see Miller et al. (2017).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [30, 31]]}, {"review_id": "SJeRQb-oFH", "rebuttal_id": "SkxXFusisH", "sentence_index": 32, "text": "Learning baselines for variance reduction is common for Reinforce estimators as used in A3C and PPO (Mnih et al. 2016, Schulman et al. 2017) but not for reparameterization estimators as used in Dreamer, SVG, and SAC (Heess et al. 2015, Haarnoja et al. 2018).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [30, 31]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 0, "text": "First of all, thank you for taking your time to review our paper and providing feedback. We have judiciously taken the comments of the reviewers,  and apologize for the late response due to additional experiments and modifications of the paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 1, "text": "Remark 1. \"the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 2, "text": "A : The purpose of our experiments is to show that the SST algorithm is comparable to the conventional SSL algorithms.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 3, "text": "Therefore, we experimented with the popular setting.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 4, "text": "The following is the experimented dataset in other papers.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 5, "text": "- Temporal ensembling & \u03a0 model [1]: CIFAR-10 (4k), SVHN (500, 1k), CIFAR-100 (10k)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 6, "text": "- VAT [2] : CIFAR-10 (4k), SVHN (1k), CIFAR-100 (no experiment)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 7, "text": "- Mean Teacher [3]: CIFAR-10 (1k, 2k, 4k), SVHN (250, 500, 1k), CIFAR-100 (10k)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 8, "text": "We took the reviewer\u2019s comment judiciously and have added CIFAR-10 (1k, 2k) experiments in Section 6.5 of the supplementary material and their accuracies are comparable with those of the conventional SSL algorithms. (It took a long time to perform 5 runs of test for all additional experiments.)", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 9, "text": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 10, "text": "The result of CIFAR-10 (1k, 2k) with 5 runs", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 11, "text": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 12, "text": "(error     / standard deviation) |", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 13, "text": "1k        |        2k", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 14, "text": "supervised                       | ( 38.71 / 0.47 ) | ( 26.99 / 0.79 )", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 15, "text": "\u03a0 model [1]                      | ( 27.36 / 1.2 )  | ( 18.02 / 0.60 )", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 16, "text": "mean teacher [3]", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 17, "text": "| ( 21.55 / 1.48 ) | ( 15.73 / 0.31 )", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 18, "text": "SST                              | ( 23.15 / 0.61 ) | ( 15.72 / 0.50 )", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 19, "text": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 20, "text": "[1] Laine, Samuli, and Timo Aila. \"Temporal ensembling for semi-supervised learning.\" arXiv preprint arXiv:1610.02242 (2016).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 21, "text": "[2]Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 22, "text": "[3] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 23, "text": "Remark 2. \"Why does the SST algorithm use different epsilon policies for synthetic vs organic datasets?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 24, "text": "A : There are different network structures in the synthetic and organic dataset (table 4-5 in the supplementary materials in the new version).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 25, "text": "And, because there are only 12 initial points in the synthetic, it needs much higher confidence than organic datasets.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 26, "text": "(Epsilon is increased in log-scale and begins at a very small value (10^(\u22125)) where no data is added in the organic dataset.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 27, "text": "However, in synthetic data, unlabeled data is added when epsilon begins at (10^(\u22125)).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 28, "text": "Therefore, We have changed the epsilon value so that no data is added at the beginning of the iteration.)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 29, "text": "Remark 3.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 30, "text": "What is the problem in SVHN (balance problem or dataset or both)?", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 31, "text": "A : We have experimented with SVHN with data balancing.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 32, "text": "In SVHN, 1,000 images are used as the labeled data and 45,000 balanced unlabeled images are used.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 33, "text": "As a result, the SST is still worse than other algorithm [1].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 34, "text": "Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 35, "text": "Remark 4. \"You should also show the performance of regular SSL methods in the setup on Table 4.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 36, "text": "A : We have performed experiments of self-training without threshold and SST with softmax output.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 37, "text": "Although the experimental setting is a bit different from [1], the setting of 100% of non-animal unlabeled data is the same.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 38, "text": "They have shown that the performance degraded when the unlabeled dataset contained 100% of non-animal data in figure 2 in [4].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 39, "text": "The approximate score in Figure 2 of [4]", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 40, "text": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 41, "text": "100% out-of-class (error)", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 42, "text": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 43, "text": "supervised learning : about 23.5%", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 44, "text": "\u03a0 model : about 26.3 %", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 45, "text": "Mean Teacher : about 26.3 %", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 46, "text": "VAT : about 26%", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 47, "text": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 48, "text": "[4] Odena, Augustus, et al. \"Realistic Evaluation of Semi-Supervised Learning Algorithms.\" (2018). ( https://arxiv.org/abs/1804.09170 )", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 49, "text": "Remark 5. combining SST and other SSL", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "SJg9GkF9nQ", "rebuttal_id": "S1lmJUZFA7", "sentence_index": 50, "text": "A : Combining and the additional cost is expensive. Therefore, we have modified our expressions.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 0, "text": "Thank you very much for the positive comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 1, "text": "We added the more experimental data of runtime analysis to address the Reviewer's main concern.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 2, "text": "Q1. My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 3, "text": "Compressability is evaluated, but that was already present in the previous work.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 4, "text": "Therefore the novel contribution of this paper over [1] is not clearly outlined.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 5, "text": "We conducted additional simulations to evaluate the runtime benefit of the proposed method compared to that of the method in [1].", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 6, "text": "We generated random 512-by-512 matrices with pruning rate ranging from 70 % to 95 % and simulated the number of parameters fed to PEs in 10000 cycles.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 7, "text": "The assumptions used for the simulation and analysis data have been updated in Figure 6c of the revised manuscript.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 8, "text": "We could observe that proposed parallel weight decoding based on the second Viterbi decompressor allowed 10 % to 40 % more parameters to be fed to PEs than the previous design [1].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 9, "text": "The proposed method outperformed both baseline method and [1] in all simulation results.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 10, "text": "Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 11, "text": "While preparing for the rebuttal, we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 12, "text": "After further optimization, we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 13, "text": "Therefore, we updated Figure 7 in original manuscript to Figure 6c in the updated manuscript according to the new data.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 14, "text": "Q2. It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [19]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 15, "text": "In the revision, we added the more precise mathematical description of the input and output of each block in Figure 1 and showed the change of the exact weight representation at each process.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [19]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 16, "text": "We first prune weights in a neural network with the Viterbi-based pruning scheme [1], then we quantize the pruned weights with the alternating quantization method [2].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [19]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 17, "text": "Our main contribution is the third process, which includes encoding each weight with the Viterbi algorithm, and retraining for the recovery of accuracy.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [19]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 18, "text": "With our proposed method, the sparse and encoded weights are reconstructed to a dense matrix as described in Figure 2.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [19]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 19, "text": "Figure 2 illustrates the purpose of our proposed scheme, which is the parallelization of the whole sparse-to-dense conversion process with the VDs while maintaining the high compression rate.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [19]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 20, "text": "Q3. Minor grammatical mistakes (missing \"a\" or \"the\" in front of some terms, suggest proofread.)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [20]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 21, "text": "Thanks very much for the suggestions.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [20]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 22, "text": "We tried to fix grammatical mistakes as much as possible in the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [20]]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 23, "text": "Reference", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 24, "text": "[1] Dongsoo Lee, Daehyun Ahn, Taesu Kim, Pierce I. Chuang, and Jae-Joon Kim. Viterbi-based pruning for sparse matrix with fixed and high index compression ratio. International Conference on Learning Representations (ICLR), 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgAEEpDhQ", "rebuttal_id": "BJly3hutA7", "sentence_index": 25, "text": "[2] Chen Xu, Jianqiang Yao, Zouchen Lin, Wenwu Qu, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. International Conference on Learning Representations (ICLR), 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 0, "text": "We would like to thank the reviewer for the comments and for raising some subtle yet important questions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 1, "text": "We address and clarify specific comments below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 2, "text": "We have also made corresponding changes in the revised paper, and have added a proof map, in addition to the Table 3, for easier navigation of the results.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_none", null]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 3, "text": "We have also added comparisons with Mairal `09, and experimental evaluation of computational time.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_none", null]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 4, "text": "1. Noise Tolerance \u2014 NOODL also has similar tolerance to noise as Arora et. al. 2015 and can be used in noisy settings as well.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 5, "text": "We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 6, "text": "Nevertheless, the proposed algorithm can tolerate i.i.d. sub-Gaussian noise, including Gaussian noise and bounded noise, as long as the ``noise\u2019\u2019 is dominated by the ``signal\u2019\u2019.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 7, "text": "Under the noisy case, the recovered dictionary and coefficients will converge to a neighborhood of the true factors, where the neighborhood is defined by the properties of the additive noise.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 8, "text": "In other words, the noise terms will lead to additional terms which will need to be controlled for the convergence analysis.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 9, "text": "Specifically, the noise will add a term to the coefficient update in Lemma 2, and will effect the threshold, tau.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 10, "text": "For the dictionary, the noise will result in additional terms in Lemma 9 (which ensures that the updated dictionary maintains the closeness property).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 11, "text": "A precise characterization of the relationship between the level of noise the size of convergence neighborhood requires careful analysis, which we defer to future effort.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 12, "text": "2. On eps_t and A.4. \u2014  Indeed, we don\u2019t need to assume that eps_t is bounded.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 13, "text": "Specifically, using the result of Lemma 7, we have that eps_0 undergoes a contraction at every step, therefore, eps_t <= eps_0.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 14, "text": "For our analysis we fix eps_t = O^*(1/log(n)), which follows from the assumption on eps_0= O^*(1/log(n)) and Lemma 7.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 15, "text": "On reviewer\u2019s comments, we have updated A.4., and moved the note about eps_t = O^*(1/log(n)) to the Appendix A.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [15]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 16, "text": "3. Exact recovery of factors \u2014 Also, we would like to point that NOODL recovers both the dictionary and coefficients exactly at a geometric rate.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 17, "text": "This means that as t\u2014> infinity both the dictionary and coefficients estimates converge to the true factors without incurring any bias.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "SJgzc8XTnX", "rebuttal_id": "SJxXh0Q1A7", "sentence_index": 18, "text": "We have added a clarification corresponding to this in the revised paper in Section 1.1 and after the statement of Theorem 1 in Section 3.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 0, "text": "1. Comment:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 1, "text": "Missing comparison with parameter counting bounds [1, 2].", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 2, "text": "1. Response:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 3, "text": "[1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 4, "text": "Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 5, "text": "Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 6, "text": "[2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 7, "text": "2. Comment:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 8, "text": "Vacuous bounds in the regime \\beta >1.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6, 7, 8, 9]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 9, "text": "2. Response:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 10, "text": "We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 11, "text": "Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 12, "text": "The exponential term stems from the layer wise covering argument rather than the range of the output.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 13, "text": "The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 14, "text": "Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 15, "text": "This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5].", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 16, "text": "We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \\beta \\approx 1 helps balance the generalization and representation of RNNs.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 17, "text": "3. Comment:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 18, "text": "Technical contribution: marginal.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 19, "text": "3. Response:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 20, "text": "We provide new understandings of RNNs by connecting their generalization properties to their empirical success.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 21, "text": "We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 22, "text": "In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 23, "text": "This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 24, "text": "We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 25, "text": "The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 26, "text": "Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \\beta > 1.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 27, "text": "To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3):", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 28, "text": "1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 29, "text": "2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 30, "text": "Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 31, "text": "4. Comment:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 32, "text": "Missing experiments to validate nature of bounds.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 33, "text": "4. Response:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 34, "text": "Please refer to the revised version for numerical evaluations in Section 6.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 35, "text": "In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \\beta > 1.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 36, "text": "References", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 37, "text": "[1] Koiran, Pascal, and Eduardo D. Sontag. \"Vapnik-Chervonenkis Dimension of Recurrent Neural Networks.\" Discrete Applied Mathematics 86, no. 1 (1998): 63-79.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 38, "text": "[2] Dasgupta, Bhaskar, and Eduardo D. Sontag. \"Sample complexity for learning recurrent perceptron mappings.\" In Advances in Neural Information Processing Systems, pp. 204-210. 1996.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 39, "text": "[3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. \"On orthogonality and learning recurrent networks with long term dependencies.\" arXiv preprint arXiv:1702.00071 (2017).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 40, "text": "[4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. \"Unitary evolution recurrent neural networks.\" In International Conference on Machine Learning, pp. 1120-1128. 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 41, "text": "[5] Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-scale image recognition.\" arXiv preprint arXiv:1409.1556 (2014).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzW2Vqh7", "rebuttal_id": "H1gfl1CtAX", "sentence_index": 42, "text": "[6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. \"Spectrally-normalized margin bounds for neural networks.\" In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJgzxEO5hQ", "rebuttal_id": "S1xoNIkzAQ", "sentence_index": 0, "text": "Thanks for the review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJgzxEO5hQ", "rebuttal_id": "S1xoNIkzAQ", "sentence_index": 1, "text": "@Update overhead: We argue that per-iteration performance is a worthwhile objective in itself, which is less significant in some scenarios (e.g. costly function evaluation, like in RL, or expensive backprops, like in RNNs).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "SJgzxEO5hQ", "rebuttal_id": "S1xoNIkzAQ", "sentence_index": 2, "text": "That said, we were indeed not able to demonstrate end-to-end gains in vision.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "SJgzxEO5hQ", "rebuttal_id": "S1xoNIkzAQ", "sentence_index": 3, "text": "Please note that in the NLP benchmark our algorithm finds a better solution and wins in wall-clock time.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "SJgzxEO5hQ", "rebuttal_id": "S1xoNIkzAQ", "sentence_index": 4, "text": "@Switching: This is a good suggestion, and we indeed do cite one of the papers attempting to approach optimizer-switching in a principled way.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SJgzxEO5hQ", "rebuttal_id": "S1xoNIkzAQ", "sentence_index": 5, "text": "We found that we could squeeze out some wall-clock gains by applying the expensive update more sparingly, but the value of including this in the paper was unclear (effectively adding a host of hyperparameters orthogonal to the central idea).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 0, "text": "Thank you very much for your supportive remark! We are happy that the writing is clear to you. Below we provide additional comments regarding your questions.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_sentences", [3]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 1, "text": "1). Low-rank assumption:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 2, "text": "This work was primarily motivated by the observation that many systems exhibit strong relationship among states and actions, governed by potentially simple dynamics.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 3, "text": "This might eventually lead to structures within the optimal solution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 4, "text": "We hope that this empirical study would motivate further theoretical analysis on structures within the community.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 5, "text": "Below are some thoughts on the potential theoretical motivation for this assumption:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 6, "text": "a) It is possible that the states and actions in consideration have some latent variable representations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 7, "text": "If the optimal Q function is a piecewise analytic function on the latent variables, then there are works arguing the approximately low-rank property of the resulting matrix [1].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 8, "text": "b) There are theoretical works in RL and Markov process that assume that the transition kernel can be decomposed to a low-dimensional feature representation [2,3].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 9, "text": "These assumptions on the transition kernel may lead to low-rank optimal Q matrices.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 10, "text": "c) For continuous problems, theoretical analysis often needs to assume some sort of smoothness in the Q function [4,5].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 11, "text": "It is possible that such smoothness in the Q function will result in a low-rank Q matrix when evaluated at finite but fine enough discretized grid.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 12, "text": "[1] Udell, Madeleine, and Alex Townsend. \"Why Are Big Data Matrices Approximately Low Rank?.\" SIAM Journal on Mathematics of Data Science 1.1 (2019): 144-160.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 13, "text": "[2] Yang, Lin, and Mengdi Wang. \"Sample-Optimal Parametric Q-Learning Using Linearly Additive Features.\" International Conference on Machine Learning. 2019.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 14, "text": "[3] Sun, Yifan, et al. \"Learning low-dimensional state embeddings and metastable clusters from time series data.\" Neural Information Processing Systems 2019.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 15, "text": "[4] Yang, Zhuora, Yuchen Xie, and Zhaoran Wang. \"A theoretical analysis of deep Q-learning.\" arXiv preprint arXiv:1901.00137(2019).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 16, "text": "[5] Shah, Devavrat, and Qiaomin Xie. \"Q-learning with nearest neighbors.\" Advances in Neural Information Processing Systems. 2018.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 17, "text": "2). Dynamical manner for the number of incomplete observations and whether the strategy can be adapted to the nature of the problem:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 18, "text": "This is a great point and definitely an interesting future direction.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [7]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 19, "text": "In the current work, it is not immediately that one could easily detect the rank and adapt the algorithm in a principled manner.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [7]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 20, "text": "As one practical solution, it may be possible to dynamically adjust the regularization in a manner similar to cross validation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 21, "text": "At each step, for the submatrix, one could randomly sample a portion of the entries for ME, while keeping another fraction of the remaining entries as a validation set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 22, "text": "If the recovered matrix via ME has a low reconstruction error on the validation set, it is likely that a suitable low-rank approximation is sufficient and has been found by the ME oracle.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 23, "text": "In contrast, if the reconstruction error is large, the algorithm might have been too aggressive on finding a low-rank solution while a higher rank solution is indeed necessary.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 24, "text": "As such, one could then adjust the algorithm to increase the number of observations for ME or try to reduce the level of low-rank regularization.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 25, "text": "The above cross validation scheme might be an interesting complement to our current approach.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 26, "text": "Overall, we believe that principally solving those questions you posted are meaningful and important directions that worth further investigations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 27, "text": "3). Beyond the low-rank assumption and use a more elaborate type of prior:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 28, "text": "Thank you for your inspiring advice.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [8]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 29, "text": "Without any elaborate prior information, rank is a natural point to study the global property of a matrix.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 30, "text": "In principle, understanding structures in MDP could also be potentially explored, and we believe that it is possible to extend to other types of scenarios with prior information about the MDPs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 31, "text": "However, at the current stage, we do not have a particularly systematic approach to explore more elaborate type of structures in MDPs.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [8]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 32, "text": "While this paper is focusing on low-rank structures, as the reviewer noted, there can be other structures to be explored.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [8]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 33, "text": "We hope that our paper could serve as an example, and further motivates future studies for exploiting structures in MDP.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [8]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 34, "text": "4). Notation in Section 4.2:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 35, "text": "Thank you for the suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [9]]}, {"review_id": "SJl0rjD2tr", "rebuttal_id": "HyxOjDn_oH", "sentence_index": 36, "text": "We will expand the definition of the notation to make them clearer.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [9]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 0, "text": "We would like to thank Reviewer 3 for the review and constructive suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 1, "text": "Our responses inline:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 2, "text": ">it would be nice to also propose unconditioned experiments.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 3, "text": "-We agree; this was simply not within the scope of the work we conducted.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [13]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 4, "text": ">I understand that no data augmentation was used during training?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 5, "text": "-This is correct, and consistent with previous works (Spectral Normalization and WGAN-GP).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 6, "text": "We briefly experimented with data augmentation (random crops and horizontal flips) but did not notice any measurable performance difference.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 7, "text": ">clarification of the truncation trick: if the elements of z are re-sampled and are still above the threshold, are they re-sampled again and again until they are all below the given threshold?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 8, "text": "-Yes, this can effectively be seen as modifying the PDF of z to have no mass outside of the truncation threshold.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 9, "text": "TensorFlow offers a built-in implementation with tf.random.truncated_normal.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 10, "text": ">A sentence could be added to explain the truncation trick in the abstract directly since it is simple to understand and is key to the quality of the results.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 11, "text": "-We have revised the abstract to explain the truncation trick as controlling the tradeoff between fidelity and diversity by reducing the variance of the Generator\u2019s input.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 12, "text": ">A reference to Appendix C could be given at the beginning of the Experiments section to help the reader find these details more easily.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 13, "text": "-Thanks for the pointer! We have added this reference.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 14, "text": ">It would be nice to add a figure of random generations.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [20]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 15, "text": "-In the caption of Figure 5, we include a link to an anonymous drive folder with sample sheets at different resolutions and truncation values, with 12 random images per class.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [20]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 16, "text": ">make the bib uniform: remove unnecessary doi - url - cvpr page numbers", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [21]]}, {"review_id": "SJl68_Hx37", "rebuttal_id": "SkgkCbBm0Q", "sentence_index": 17, "text": "-Thanks, we have fixed this.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [21]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 0, "text": "Thank you for your helpful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 1, "text": "We have addressed your concern about the baseline models and learning rate schedules in our updated paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 2, "text": "Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 3, "text": "We have updated the baselines in our paper for CIFAR-10 and CIFAR-100, using a larger, modern network, ResNet34, in place of the VGG11 model used previously.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 4, "text": "We also compared APO to manual learning rate decay schedules.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 5, "text": "For CIFAR-10/100, we trained for 200 epochs, decaying the learning rate by a factor of 5 three times during training.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 6, "text": "The ResNet34 with a custom learning rate decay schedule achieves 93-94% test accuracy on CIFAR-10 and ~74% test accuracy on CIFAR-100.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 7, "text": "We believe that this is a strong baseline, and shows the applicability of APO in practical settings.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 8, "text": "The final test accuracies of the updated model using SGD/SGDm with and without APO are:", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 9, "text": "| CIFAR-10 | CIFAR-100 |", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 10, "text": "--------------------------+--------------+---------------+", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 11, "text": "SGD (fixed lr)", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 12, "text": "92.97            72.69", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 13, "text": "SGDm (fixed lr)", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 14, "text": "92.77            72.53", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 15, "text": "SGD (decayed lr)", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 16, "text": "93.29            73.45", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 17, "text": "SGDm (decayed lr)", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 18, "text": "93.53            73.80", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 19, "text": "SGD-APO", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 20, "text": "93.82            74.65", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 21, "text": "SGDm-APO", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 22, "text": "94.59            73.89", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 23, "text": "The test accuracies using RMSprop and K-FAC with APO are shown in our response to all reviewers at the top.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 24, "text": "The results in these tables show that APO is competitive with manual schedules in terms of test accuracy.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 25, "text": "The updated figures in our paper show that APO is competitive with manual schedules both in terms of test accuracy and training loss.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 26, "text": "Q: Does the hyperparameter lambda itself benefit from some scheduling?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 27, "text": "In our updated paper we show that APO with a fixed lambda achieves comparable performance to manual learning rate decay schedules.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 28, "text": "While using a schedule for lambda can potentially further improve performance, a simple grid search over fixed lambda values already leads to strong performance, and has the advantage that it is easy to use in practice.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [5, 6]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 29, "text": "Q: You mention that \"APO converges quickly from different starting points on the Rosenbrock surface\" but 10000 iterations is not quick at all for the 2-dimensional Rosenbrock, it is extremely slow compared to 100-200 function evaluations needed for Nelder-Mead to solve it.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 30, "text": "I guess you mean", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 31, "text": "w.r.t. the original RMSprop.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 32, "text": "Yes, we intended to say that on Rosenbrock, RMSprop-APO converges quickly compared to baseline RMSprop; we have updated the paper to clarify this.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 33, "text": "Thank you for your helpful feedback. We have incorporated your suggestions into the updated paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 34, "text": "Specifically, we have:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 35, "text": "* Updated the baseline model for CIFAR-10/100 from VGG11 to ResNet34.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 36, "text": "* Used manual learning rate decay schedules for the CIFAR-10/100 baselines.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 37, "text": "We obtained 93-94% test accuracy on CIFAR-10 (SGD/SGDm/RMSprop/K-FAC) and 73-74% test accuracy on CIFAR-100 (SGD/SGDm).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 38, "text": "All are compared to their APO variants, which performed as well or better.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 39, "text": "The final results are shown in the table in the response to all reviewers at the top.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 40, "text": "* Shown that APO is competitive with manual schedules both in terms of test accuracy and training loss with ResNet34.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 41, "text": "This demonstrates the practical applicability of APO for contemporary networks.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3]]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 42, "text": "* Updated Figure 2 on CIFAR-10 with SGD/SGDm/RMSprop, Figure 4 on CIFAR-100 with SGD/SGDm, and Figure 6 on CIFAR-10 with SGD.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_none", null]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 43, "text": "We also added Figure 3 on CIFAR-10 with K-FAC.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_none", null]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 44, "text": "Each figure compares the baseline optimizers with their APO variants.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJl8R-eTnQ", "rebuttal_id": "Bkld8r3FRQ", "sentence_index": 45, "text": "Thank you for having helped us improve the paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 0, "text": "Thank you very much for the highly constructive review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 1, "text": "> I think this is a very interesting direction, but the present paper is somewhat unclear. In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have \"training algorithms that are exactly equivalent.\" I think this example needs to be clarified.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 2, "text": "We realized that the naming was very confusing and consequently, we renamed \\tilde\\theta to \\tilde\\mu in the noise-injected model.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 3, "text": "Now,", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 4, "text": "- the original, noise-free model p has the structure \\theta -> D (no bottleneck) while", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 5, "text": "- the adapted, noise-injected model p\u2019 has the structure \\mu -> \\tilde\\mu -> D (containing a bottleneck).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 6, "text": "Hereby, \\tilde\\mu is a noise-corrupted version of the new parameters \\mu, and we obtain a limit on the mutual information between \\mu and D. We simplified Figure 2 and 8 to make this more clear.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 7, "text": "To better characterize Gaussian mean field inference on the original model, we aim to find an inference procedure on p\u2019 so that both algorithms result in exactly the same outcome, e. g. the same calculations are executed when running the corresponding program.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 8, "text": "We show that there is such an inference procedure on the noisy model, and it has the character of MAP.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 9, "text": "Note that only if generative and inference model are adapted simultaneously we end up with equivalence.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 10, "text": "Hereby, \\mu (the mean of the Gaussian q) and \\theta (the original parameter in p) correspond to \\mu (the MAP point-mass of q\u2019) and \\tilde\\mu (the noise-injected version of \\mu in p\u2019).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 11, "text": "> Many of the parameters here are also unclear and not properly defined/introduced.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 12, "text": "What is the relationship between \\theta and \\tilde\\theta exactly?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 13, "text": "In this example, \\theta and \\tilde\\theta never appear in the same model (they are part of p and p\u2019, respectively).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 14, "text": "We realized that this is confusing and have therefore renamed \\tilde\\theta to \\tilde\\mu.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 15, "text": "> In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 16, "text": "This is an excellent question.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 17, "text": "In fact, we believe that trying to construct noise-free deep models with a specific mutual information of data and parameters for the purpose of generalization would be an interesting research direction.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 18, "text": "Due to nonlinearities in typical deep models, it is at least not obvious how to calculate the mutual information between data and parameters.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 19, "text": "The main challenge here would certainly be to come up with an effective estimator.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 20, "text": "Relatedly, one would have to design priors and architecture to achieve a specific mutual information.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 21, "text": "> The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper [...]] and further exploration is desirable.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 22, "text": "This paper is giving an information-theoretic perspective on existing variational inference methods.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 23, "text": "Such a perspective is interesting, but needs to be further developed and explained.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 24, "text": "Specifically, how can mutual information in this context be formally linked to generalization/overfitting?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 25, "text": "We updated section 2.2 to relate to the references you mentioned.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 26, "text": "They explore the link of limiting mutual information and generalization error mostly in theory (and in particular for adaptive analysis).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 27, "text": "In contrast, we deploy this principle in a practical model structure that is easily applicable to many existing deep and variational learning approaches and provide empirical evidence of the validity of our framework.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 28, "text": ">", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 29, "text": "Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 30, "text": "As a result, constraining the model will alter the mutual information and I think the effect of this should be remarked on.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 31, "text": "We want to emphasize that we do use the standard definition of mutual information.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 32, "text": "Therefore, the bottleneck implied by Eq. 5 is purely a property of the generative model and not influenced by the approximate inference distribution q.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 33, "text": "Eq. 2 is only introduced to provide additional motivation for our approach as it allows to characterize overfitting in variational inference.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "SJllCbbahQ", "rebuttal_id": "SkEt8Cdp7", "sentence_index": 34, "text": "The guarantee derived in section 2.2 ties this quantity back to the mutual information from Eq. 5.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 0, "text": "We thank the reviewer for mentioning the benefits of the proposed model.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_global", null]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 1, "text": "We now clarify some of the reviewer\u2019s doubts:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 2, "text": "**QUESTION**", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 3, "text": "BERT-based models in the low-resource case is not very surprising", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 4, "text": "**ANSWER**", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 5, "text": "While this result may not look surprising, to the best of our knowledge it was not addressed before for the specific case of BERT.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 6, "text": "In [1], the authors claim that a large pre-trained model can be very helpful in transfer learning scenarios, and they also suggest how to best fine-tune BERT.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 7, "text": "We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [5]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 8, "text": "**QUESTION**", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 9, "text": "Looking at the experiments, the results on HATESPEECH show less differences between models than for SPOUSE or MOVIEREVIEW.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 10, "text": "**ANSWER**", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 11, "text": "The reviewer is correct.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 12, "text": "Apart from the reasons mentioned in the paper, it is possible to observe (by manual inspection) that the tweets of the HATESPEECH dataset are very noisy, short and often similar in meaning.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 13, "text": "This clearly helps models based on n-gram features, as we have argued in our work.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 14, "text": "SPOUSE and MOVIEREVIEW are different in this sense.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 15, "text": "SPOUSE, which is where PARCUS performs very well, contains sentences of very different nature and context, which makes it very important to focus on specific concepts (hence the use of prototypes seems appropriate).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 16, "text": "MOVIEREVIEW, on the other hand, contains very long reviews that need \u201cfiltering\u201d to highlight the important concepts.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "SJlsCX2CFr", "rebuttal_id": "S1eEulHdiH", "sentence_index": 17, "text": "This is another context in which PARCUS can be successfully applied, as training a complex model on few data points that contain \u201clengthy\u201d sentences can be a hard task to solve.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [6]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 0, "text": "Thank you for your review of our work.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 1, "text": "The following are your concerns of our work:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 2, "text": "a. Limited contribution to the qualitative understanding of the optimizers", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 3, "text": "a.i. Informativeness of the proposed w-tunability metric", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 4, "text": "b. Using wall-clock time instead of number of HPO oracle calls", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 5, "text": "We address these concerns one-by-one.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 6, "text": "a. Limited contribution to the qualitative understanding of the optimizers:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 7, "text": "We consider the three main contributions of our work to be 1) a systematic evaluation protocol of optimizers, with off-the-shelf HPO to account for the cost of tuning of hyperparameters.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 8, "text": "This is missing in existing papers, which consider best attained performance alone.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 9, "text": "The importance of a proper hyperparameter search protocol is emphasized by Choi et al., 2019 (published after our submission and under review at ICLR).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 10, "text": "2) a \u201cw-tunability\u201d measure of the cost of hyperparameter optimization, and 3) under the experiments considered we find that Adam (with default beta and epsilon values) is the most tunable.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [14]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 11, "text": "a.i. Informativeness of the tunability metric:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 12, "text": "We propose w-tunability as a metric to incorporate the HPO tuning too in reporting the performance of an optimizer, and compute it as a linear combination of the incumbents of the HPO algorithm, though one can use an arbitrarily complex function trading off interpretability.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 13, "text": "It is true that Figures 4-7 essentially contain all the information needed to judge about the optimizer\u2019s tunability.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 14, "text": "However, a metric that is easy to compute, interpret and compare optimizers across tasks is crucial, for which we propose w-tunability.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 15, "text": "This is analogous to computing specific quantities like accuracy, FPR, TPR from the confusion matrix, even though a confusion matrix contains all the information (and is quite cumbersome to compare).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 16, "text": "The summary metric in Figure 2 provides a different interpretation: It reports a normalized performance i.e., the  normalized incumbent performance at iteration $k$. This doesn\u2019t explicitly include information about the previous $k-1$ iterations (which is our central argument).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 17, "text": "Thus our proposed tunability metric provides more information than the summary statistics plot (figure 2).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 18, "text": "Due to this novelty, we argue that our setup does contribute to the qualitative understanding of the optimizers.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 19, "text": "In fact, it yields to a drastically different valuation of adaptive gradient methods than popular previous work (Wilson et al, Shah et al).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 20, "text": "You mention that our work is incremental to the work on benchmarking of optimizers.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 21, "text": "Can you please provide respective references?", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [12]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 22, "text": "We have modified parts of our paper to reflect these arguments better.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 23, "text": "b. Using wall-clock time instead of HPO oracle calls:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 24, "text": "Our reason for using a number of configuration trials instead of a time budget is that measuring number of hyperparameter configuration searches required is more relevant to understand the optimizers\u2019 dependence on the hyperparameters.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 25, "text": "However, we completely agree with you that computational budget is a relevant factor from the practitioner\u2019s point of view, and added a discussion of this in Appendix E of the paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 26, "text": "As you rightfully point out, the adaptive optimizers tend to converge in fewer number of epochs, amplifying the results that favor Adam over the variants of SGD.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [18]]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 27, "text": "References:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 28, "text": "Wilson, Ashia C., et al. \"The marginal value of adaptive gradient methods in machine learning.\" Advances in Neural Information Processing Systems. 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 29, "text": "Shah, Vatsal, Anastasios Kyrillidis, and Sujay Sanghavi. \"Minimum norm solutions do not always generalize well for over-parameterized problems.\" arXiv preprint arXiv:1811.07055 (2018).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJx0zByAYr", "rebuttal_id": "B1iEvPOjB", "sentence_index": 30, "text": "Choi, Dami, et al. \"On Empirical Comparisons of Optimizers for Deep Learning.\" arXiv preprint arXiv:1910.05446 (2019).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJxb8_g0FS", "rebuttal_id": "HkltWhSyjB", "sentence_index": 0, "text": "Thank you for the detailed review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJxb8_g0FS", "rebuttal_id": "HkltWhSyjB", "sentence_index": 1, "text": "We appreciate your comments on the contributions of our work and invaluable suggestions to improving our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJxb8_g0FS", "rebuttal_id": "HkltWhSyjB", "sentence_index": 2, "text": "Before delving into the details and providing a more detailed rebuttal, here are some of our initial thoughts.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SJxb8_g0FS", "rebuttal_id": "HkltWhSyjB", "sentence_index": 3, "text": "Regarding comparison with MINE on fMRI dataset, MINE could be applied to fMRI data for MI analysis.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJxb8_g0FS", "rebuttal_id": "HkltWhSyjB", "sentence_index": 4, "text": "After all, our DEMINE-vr is simply MINE under cross-validation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJxb8_g0FS", "rebuttal_id": "HkltWhSyjB", "sentence_index": 5, "text": "Our first guess is that MINE will not be able to identify significant dependency in Table.1 due to large confidence intervals, i.e, the rows and cols for MINE will be all 0s.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJxb8_g0FS", "rebuttal_id": "HkltWhSyjB", "sentence_index": 6, "text": "For segment classification, MINE-f-ES uses the same model architecture and training process as DEMINE-vr and uses hyper parameters from DEMINE-vr hyperparameter search, and will provide identical segment classification accuracy as DEMINE-vr.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12]]}, {"review_id": "SJxb8_g0FS", "rebuttal_id": "HkltWhSyjB", "sentence_index": 7, "text": "Thanks again for pointing out the typos, we'll post an updated version shortly.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [13]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 0, "text": "Thank you for the comments!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 1, "text": "To review\u2019s questions:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 2, "text": "- As the experimental results shows, with position information alone, the agent is able to learn to push or pick up the object, therefore we consider position information alone (without velocity information) is sufficient in our case.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 3, "text": "- For MISC, the method needs to know what are the states of interests and what are the context state.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 4, "text": "While, the states of interest can be any states that users are interested in, such as a part of the robot states or the object states.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 5, "text": "The context states are some other states, which are different from the states of interest.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 6, "text": "In robotic tasks, the states of the robot and the object states are normally available [Andrychowicz et al 2018, Plappert et al 2018].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 11]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 7, "text": "To automatically detect the state of interests and the context states, we can train the agent with random state splits and then chose the combination, which is suitable for the tasks at hand.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 8, "text": "References:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 9, "text": "[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048\u20135058, 2017.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_global", null]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 10, "text": "[2] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow- ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce- ment learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_global", null]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 11, "text": "- Yes, MISC can deal with the case, when there are multiple objects of interest.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 12, "text": "We added new experiments showing the agent can learn to manipulate two balls.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 13, "text": "We define the mutual information intrinsic reward as I(S^i_{1}, S^c)+I(S^i_{2}, S^c).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 14, "text": "The experimental results are shown in the new video at https://youtu.be/l5KaYJWWu70?t=148, where we show that a robot car can learn to manipulate two balls in the same episode.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_global", null]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 15, "text": "- Equation (4) is not the mutual information between two trajectories of states.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 16, "text": "It is an estimation of mutual information between two sets of states. And the states are sampled from the same trajectory.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 17, "text": "Therefore, we do not need to decompose Equation (4) to evaluate Equation (3).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 18, "text": "- We add the experimental details in the Appendix.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 11]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 19, "text": "- The discriminator is trained along with the policy.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 20, "text": "For example, in the case that we update the agent 200 times in each epoch, then we also update the MISC 200 times per epoch.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 21, "text": "For more detailed information, please refer to our code at https://github.com/misc-project/misc", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 22, "text": "- Compared to the dense reward, with the negative L2 distance between the robot and the object, the robot can only learn to reach the object but will not learn to push or pick up the object because when the robot reaches the object, the negative L2 distance is already zero.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 23, "text": "However, MISC has the advantage that it not only enables the agent to learn to reach but also learn to push and pick & place.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 24, "text": "- If we train the MISC and DIAYN at the same time, the DIAYN reward might be dominant. Subsequently, The agent might not learn to control the states of interests with MISC.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 25, "text": "- MISC-p works similarly to PER.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 26, "text": "The main difference is that MISC-p uses the estimated mutual information quantity as a priority, while PER uses the TD-error as a priority for replay.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 27, "text": "For more detail on PER, please refer to the original PER paper [Schaul et al 2016].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 28, "text": "Reference:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 29, "text": "Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In International Conference on Learning Representations, 2016.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 30, "text": "- We first scale the intrinsic and the extrinsic reward between 0 and 1 and then use equal weights for these two rewards.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 31, "text": "- For the opposite situation, we can use negative mutual information rewards to encourage the agent to learn to \u201cavoid\u201d some objects.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 32, "text": "- The discriminator uses the same amount of (s,a,s') experience as VIME consumes because the discriminator is fixed after pre-training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 33, "text": "VIME can only be trained along with the policy.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 34, "text": "VIME cannot be pre-trained, otherwise, it won\u2019t detect novel states.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "SJxy9Ndy5r", "rebuttal_id": "SJgUsHzqsr", "sentence_index": 35, "text": "- Transfer the learned discriminator from Push to the Pick&Place should still help the agent to learn the pick & place task because the transferred discriminator will help the agent to learn to reach the object at least. As long as the state inputs for the discriminator are the same, then MI discriminator can be transferred among different tasks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 0, "text": "We thank the reviewer for their time and detailed reading of the paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 1, "text": "In the following, we address each of the reviewers comments:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 2, "text": "> Table 4 should include the result of training a supervised network on top of random conv1/2 and Scattering network features [\u2026] Scattering is a perfectly fine approach for initial features.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 3, "text": "Our aim is to investigate the \u201cpower\u201d (or lack thereof) of current self-supervision techniques when applied to standard deep network models.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 4, "text": "This is of interest because self-supervision is a hot topic of research.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 5, "text": "Finding whether e.g. the Scattering Transform can replace the first few layers of a network is interesting, for example to know if handcrafted features can also do as well as (self) supervision for the first few layers, but not susbtitutive of our core investigation (furthermore, we also look ad deeper layers, where these features are unlikely to be competitive).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 6, "text": "Still, such an experiment can help put our findings in context.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 7, "text": "This is why we do include them in Table 2 of the paper, where we show that scattering is not quite as good as even single-image self-supervision.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 8, "text": "We do think that the suggestion of finetuning/retraining the rest of the model is also interesting after replacing the first few layers is also interesting.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 9, "text": "Still, we think that this can complement but not replace linear probing as the latter is a more direct way of finding what the probed layers can do.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 10, "text": "For instance, it is likely possible to learn a good network even by replacing the first layer with the identity function \u2014 it is just a slightly less deep model.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [11, 12, 13]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 11, "text": "For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 12, "text": "> Can the authors clarify how the neural style transfer experiment is performed?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 13, "text": "Indeed, the method by Gatys et al. uses deeper layers as well, which we also use \u2014 straight from the self-supervised method, without fine-tuning or anything else.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 14, "text": "We will update the paper with these details.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 15, "text": "> While I appreciate the computational burden of testing more images, it does feel that Image A and B are quite cherry-picked in being very visually diverse. [...]  It might be interesting to try to generate synthetic images (or modify real ones) that are good for this purpose and observe their properties.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [19, 20, 21]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 16, "text": "Thank you, finding the best single training image, or finding useful synthetic images, are both very interesting ideas. While we are happy to consider doing so as a next step, it is next to impossible to do so in time for the rebuttal (we do not have access to thousands of GPUs).", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [19, 20, 21]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 17, "text": "Nevertheless, we would argue that the paper stands on by making some interesting observation on the ability of self-supervision to extract useful information from more than one (or few) images, and by investigating the role of data augmentation in this process.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [19, 20, 21]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 18, "text": "We hope that the reviewer will agree that the community will be interested in hearing about these findings.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20, 21]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 19, "text": "> I disagree with the claim of practicality in the introduction (page 2, top). While training on one image does reduce the burden of number of images, the computational burden remains the same.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 20, "text": "Our intention wasn\u2019t to say that we can save compute time, but data collection effort (which is also a practical issue in some applications).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 21, "text": "Nevertheless, we agree that our findings have mostly a theoretical value, so we have adjusted the wording to reflect that.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [23, 24]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 22, "text": "> And as mentioned above, it doesn\u2019t seem likely that *any* image would work for this method.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 23, "text": "It is true that we did not quite prove that, so we have reworded the text to tone down this claim.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [24]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 24, "text": "To be a bit more specific, obviously a blank image would not work, and textureless images would probably not work well either.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [24]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 25, "text": "However, we did use in the paper the first two images we manually selected from Google Image Search (while we did select images with some texture, they have not been otherwise been optimized for good performance in our evaluation)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 26, "text": ".", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 27, "text": "Thus, we think that it is extremely likely that many other images would work just as well.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 28, "text": "> Finally, more images are needed to learn the deeper layers for the downstream task anyway.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [25]]}, {"review_id": "Ske6OaZ-qr", "rebuttal_id": "Syg3upgMjH", "sentence_index": 29, "text": "True, but even for deeper layers a single image achieves two thirds of the performance that self-supervision can squeeze out of a million images, which we think is interesting.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [25]]}, {"review_id": "SkedBT5ntB", "rebuttal_id": "HkeodF7qir", "sentence_index": 0, "text": "We thank you for your constructive feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SkedBT5ntB", "rebuttal_id": "HkeodF7qir", "sentence_index": 1, "text": "We reply to all reviews in a general response above.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkeOpvO227", "rebuttal_id": "ryxYrEt66X", "sentence_index": 0, "text": "We thank the reviewer for their encouraging words and will correct the errors of expression.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [4]]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 0, "text": "We thank Reviewer 1 for the constructive feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 1, "text": "Here is our point-to-point response to the comments and questions raised in the review:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 2, "text": "1. \u201cThe numbers reported in Figure 5 do not match with the performance of adversarial training in previous paper\u2026 I wonder why the numbers are so different.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 3, "text": "Table 1 of \"Obfuscated Gradients Give a False Sense of Security\" reports an accuracy of 47% under 0.031 norm-inf perturbation for the CIFAR10 dataset (55% is reported for the MNIST dataset), approximately the same as the 44% accuracy in our Figure 5.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 4, "text": "The difference in performance stems from how we preprocessed the CIFAR10 images: exactly in the manner described by (Zhang et al., 2017)\u2019s ICLR paper \u201cUnderstanding deep learning requires rethinking generalization\u201d (we whiten and crop each image).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 5, "text": "2. \u201cWhat's the training time of the proposed method compared with vanilla adversarial training?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 6, "text": "We have added Table 2 to the Appendix which reports the increase in runtime for each of the 42 experiments discussed in Table 1 after introducing spectral normalization.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11]]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 7, "text": "For 39 of the cases, our TensorFlow implementation of the proposed method results in longer training times (from 1.02 to 1.84 times longer).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 8, "text": "In the 3 cases of iterative adversarial attacks with the Inception architecture, the proposed method actually results in faster training time.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 9, "text": "This is likely due to how TensorFlow handles training in the backend.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 10, "text": "We provide the code for full transparency.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 11, "text": "3. \u201cThe idea of using SN to improve robustness has been introduced in the following paper: \"Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks\" (but this paper did not combine it with adv training).\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 12, "text": "Thank you for bringing this recent work to our attention.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "SkeXPX7hhm", "rebuttal_id": "ByeNL6dJAQ", "sentence_index": 13, "text": "We cite and discuss this NIPS paper in our updated draft.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [12, 13, 14]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 0, "text": "We thank you for the  detailed and thoughtful review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 1, "text": "We are glad that you found the paper well-contextualized and the presentation high-quality. Here we discuss some of your comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 2, "text": "R2: \"this 'finishes' [Pathak et al., ICML17] to its logical conclusion for game-based environments and should spur interesting conversations and further research. In terms of actual technical contributions, I believe much less significant.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [32, 33]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 3, "text": "=> In the light of the comments on originality and significance, we would like to highlight our finding that random features perform quite well and at times as well as learned features across many environments.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [32, 33]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 4, "text": "This is a novel contribution since prior works have relied on learned features as a crucial requirement for good performance [Pathak et. al. ICML17].", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [32, 33]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 5, "text": "We believe this investigation would allow random features to be seen as an easily reproducible and strong baseline for future investigations of feature learning in exploration.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [32, 33]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 6, "text": "Indeed, since the release of our paper, there has been some follow-ups on using random features for exploration in achieving state of the art results on hard exploration games when combined with extrinsic reward (in the interest of preserving anonymity, we don't include the references here).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [32, 33]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 7, "text": "R2: \"However, it isn't entirely clear if the primary contribution is showing that 'curiosity reward' is a potentially promising approach or if game environments aren't particularly good testbeds for practical RL algorithms\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 8, "text": "=> We believe that both are valuable insofar as generating discussion within the community and leading to follow-up experimentation.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 9, "text": "In particular, we hope our paper stimulates both, an interest in trying out more realistic/stochastic environments, *and* further research on curiosity as a potential useful reward.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 10, "text": "In addition to that, we have shown that curiosity could be a very strong baseline to compare against in future papers.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 11, "text": "All these, we argue, are valuable to the progress and health of the field.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 12, "text": "R2: \"Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method. However, as I said previously, this is probably a discussion worth having given the popularity and visibility of game-based testbeds\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [48, 49]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 13, "text": "=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [48, 49]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 14, "text": "Our vivid demonstration of this issue in the maze environment has already inspired some recent papers to look into, in particular, by incentivizing episodic reachability (in the interest of preserving anonymity, we don't include references to these, but we will include them in the final version of the paper).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [48, 49]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 15, "text": "R2: \"I think that more casual ML/RL researchers will find these results controversial and surprising while more experienced researchers will see curiosity-driven learning to be explainable primarily by the intuition...\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [46]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 16, "text": "R2: \"Even the 'focused experiments' can be explained with the intuitive narrative that in the state/action space\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 17, "text": "=> Indeed in our experience, although a few people were not surprised, most of them were very surprised at the agents being able to make progress without any any extrinsic rewards.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 46]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 18, "text": "This suggests that the game designers (similar to architects, urban planners, gardeners, etc.) are purposefully setting up curricula to guide agents through the task by curiosity alone [Lazzaro, 2004].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 46]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 19, "text": "R2: \"consider [Mirowski et al., ICLR17] with respect to auxiliary loss + RL extrinsic rewards to improve performance\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 20, "text": "R2: \"RL + auxiliary loss isn't evaluated in detail\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [43]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 21, "text": "=> We will add a discussion of recent works that deal with navigation tasks in maze environments [Mirowski et. al. ICLR 2017, Jaderberg et. al. ICLR 2017] in the related works section.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [16, 43]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 22, "text": "In contrast to these works, we don't assume privileged access to the maze environment in the form of depth estimation or loop closure supervision.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [16, 43]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 23, "text": "Auxiliary tasks are an important component of RL and exploration methods, however, in this work we chose to focus on the most generic setting with minimal assumptions about the environment: providing raw observations in response to actions.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [16, 43]]}, {"review_id": "Ske_-TWah7", "rebuttal_id": "HJx9J8YQA7", "sentence_index": 24, "text": "In environments with privileged access we expect auxiliary tasks to benefit both curiosity-driven and extrinsic-reward-driven RL methods.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [16, 43]]}, {"review_id": "Skg1tSgV5S", "rebuttal_id": "Hygpmj1EiH", "sentence_index": 0, "text": "Q: \"...I find these assumptions too strong for the task of learning disentangled representation.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "Skg1tSgV5S", "rebuttal_id": "Hygpmj1EiH", "sentence_index": 1, "text": "A: We wish to emphasize that:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "Skg1tSgV5S", "rebuttal_id": "Hygpmj1EiH", "sentence_index": 2, "text": "(1) we only require access to meta-labels on the source set", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "Skg1tSgV5S", "rebuttal_id": "Hygpmj1EiH", "sentence_index": 3, "text": "(2) our goal is not to find disentangled representations; Our goal is transferability so that we can learn on real data with minimal supervision.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1, 2]]}, {"review_id": "Skg1tSgV5S", "rebuttal_id": "Hygpmj1EiH", "sentence_index": 4, "text": "Q: \"you can simply train a network to predict the canonicalizations by simple supervised learning\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "Skg1tSgV5S", "rebuttal_id": "Hygpmj1EiH", "sentence_index": 5, "text": "A: Finding the canonicaliers is not our goal.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "Skg1tSgV5S", "rebuttal_id": "Hygpmj1EiH", "sentence_index": 6, "text": "These are used as auxiliary constraints which guide representation learning and allow for latent data augmentation on our limited target data.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "Skg1tSgV5S", "rebuttal_id": "Hygpmj1EiH", "sentence_index": 7, "text": "Specifically, predicting the factor values will not help us in manipulating the target samples (note the significant improvement we get by using the majority vote).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "Skg1tSgV5S", "rebuttal_id": "Hygpmj1EiH", "sentence_index": 8, "text": "Q: method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "Skg1tSgV5S", "rebuttal_id": "Hygpmj1EiH", "sentence_index": 9, "text": "A: The suggested methods [1-3] are self-supervised methods using less information than the baselines we have included (for example, the AE+classifier baseline is trained on the same synthetic data with the same access to digit labels).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "Skg1tSgV5S", "rebuttal_id": "Hygpmj1EiH", "sentence_index": 10, "text": "We therefore expect that these methods will perform worse than our baselines.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "Skg1tSgV5S", "rebuttal_id": "Hygpmj1EiH", "sentence_index": 11, "text": "We have included  a comparison with method [3] (see general comment to all reviewers).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 0, "text": "First of all, thank you for taking your time to review our paper and providing feedback. We have judiciously taken the comments of the reviewers, and apologize for the late response due to additional experiments and modifications of the paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 1, "text": "Remark 0. It needs other theoretical explanation (ex. co-training)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 2, "text": "A: We have modified our paper and added some theoretical explanation in the introduction on page 2.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 3, "text": "Remark 1. \" Considering the selection based on highest-confidence, the in or out of class unlabeled data in most cases does not matter. \"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 4, "text": "A: We do not agree with your opinion.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 5, "text": "The formulae of the softmax and sigmoid are as follows.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 6, "text": "The softmax function : exp(f_j(z)) / sigma(exp(f_k(z))", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 7, "text": "= 1 / ( 1 + exp(-f_j(z)) \u00d7 (exp(f_1(z)) + ... + exp(f_j-1(z)) + exp(f_j+1(z)) + ... exp(f_n(z)))", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 8, "text": "The sigmoid function : 1 / (1 + exp(-g(z)))", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 9, "text": "where z, f(z), and g(z) represent the final layer of the backbone network, classification network, and selection network respectively.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 10, "text": "As you said, if f_j(z) is very high and the other f(z)s are moderate, it can work like sigmoid.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 11, "text": "However, even if f_j(z) is not much high, the softmax output can be close to 1 with extremely smaller values for other f(z)s", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 12, "text": "because:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 13, "text": "The softmax output : 1 / ( 1 + exp(-f_j(z)) \u00d7 0) = 1", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 14, "text": "We experimented with a high softmax output threshold (epsilon = 10^(\u22124)).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 15, "text": "Although epsilon was 10^(\u22124) (threshold = 0.9999), an average of about 800 unlabeled data was added for the case of 100% of the non-animal data.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 16, "text": "Even at 0% of the non-animal data, performance is lower than the fixed mode of the sigmoid.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 17, "text": "This shows the limitation of thresholding with softmax.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 18, "text": "The result of new SSL problems on the CIFAR-10 dataset with 5 runs are as follows:", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 19, "text": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 20, "text": "activation function / softmax (error/added data) / sigmoid (error/added data)", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 21, "text": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 22, "text": "supervised", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 23, "text": "/", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 24, "text": "( 22.27 / 0 )", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 25, "text": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 26, "text": "0%", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 27, "text": "/", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 28, "text": "(18.27 / 4,306.8)", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 29, "text": "/", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 30, "text": "(17.84/2,338.8 )", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 31, "text": "25%", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 32, "text": "/", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 33, "text": "(18.35 / 3,350.4 )", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 34, "text": "/", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 35, "text": "(18.38 / 1470.0 )", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 36, "text": "50%", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 37, "text": "/", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 38, "text": "( 18.72 / 2580.0 )", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 39, "text": "/", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 40, "text": "(19.04 / 811.2 )", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 41, "text": "75%", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 42, "text": "/", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 43, "text": "(20.33 / 1,711.2 )", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 44, "text": "/", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 45, "text": "( 20.07 / 315.6 )", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 46, "text": "100%", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 47, "text": "/", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 48, "text": "(20.71 / 864.0 )", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 49, "text": "/", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 50, "text": "( 20.24 / 1.2 )", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 51, "text": "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 52, "text": "Remark 2. Expression, and details (ex. number of iterations, stopping criteria, typos and grammar errors)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 53, "text": "A : We apologize to the reviewer for the lack of clarity in the manuscript.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 54, "text": "We have modified our expression, typos and grammar errors.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 13, 14]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 55, "text": "Regarding the details on hyper-parameters:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 56, "text": "- We set parameters as follows.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 57, "text": "The number of training iteration and thresholding epsilon are very important parameters in our algorithm and have a considerable correlation with each other.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 58, "text": "In the first experiment, the iteration number remains fixed and the growth rate of epsilon is adjusted so that the validation accuracy saturates near the settled iteration number.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 59, "text": "While the validation accuracy is evaluated using the cross-validation, we set the number of training iteration to be 100 so that the model is trained enough until it saturates.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 60, "text": "Epsilon is increased in log-scale and begins at a very small value (10^(\u22125)) where no data is added.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 61, "text": "The growth rate of epsilon is determined according to when the validation accuracy saturates.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 62, "text": "The stopping criterion is that the accuracy of the current iteration reaches the average accuracy of the previous 20 steps.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 63, "text": "If the stopping iteration is much less than 100 times, the epsilon growth rate should be reduced so that the data is added more slowly.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 64, "text": "If the stopping iteration significantly exceeds 100 iterations, the epsilon growth rate should be increased so that the data is added more easily.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 65, "text": "We allow 5 iterations as a deviation from 100 iterations and the growth rate of epsilon is left unchanged in this interval.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 66, "text": "(In previous versions, the growth ratio of epsilon for CIFAR-10 was applied to SVHN and CIFAR-100.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 67, "text": "However, since the epsilon growth rate is different for each dataset, as the reviewer mentioned, we have performed the cross-validation for SVHN and CIFAR-100 and modified our results.) As a result, the epsilon is gradually increased in log-scale by 10 times every 33 iterations in CIFAR-10 and SVHN.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 68, "text": "In the case of CIFAR-100, the epsilon is increased by 10 times in log-scale every 27 iterations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 69, "text": "In the second experiment, we leave the epsilon fixed and simply train the model until the stopping criterion is satisfied.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 70, "text": "Other details are the same as those of the first experiment.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 71, "text": "(In previous versions, the training iterations of fixed mode had been fixed.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 72, "text": "Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 73, "text": "Remark 3. \"SST itself is only comparable with or even worse than the state-of-art methods.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 74, "text": "A : As mentioned in our paper, SST has comparable performance to other conventional SSL algorithms.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [15]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 75, "text": "In Table 2 of our paper, SST achieves 34.89% on CIFAR-100, which is higher than TempEns[1](38.65%), 11.82% on CIFAR-10, which is slightly worse than VAT+EntMin[2](10.55%), and perform worse 6.88% on SVHN.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [15, 16, 16, 19, 20, 21, 22]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 76, "text": "However, SST can solve the real problem of the existence of out-of-class unlabeled data.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [15, 16, 16, 19, 20, 21, 22]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 77, "text": "[1] Laine, Samuli, and Timo Aila. \"Temporal ensembling for semi-supervised learning.\" arXiv preprint arXiv:1610.02242 (2016).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [15, 16, 16, 19, 20, 21, 22]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 78, "text": "[2] Miyato, Takeru, et al. \"Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning.\" arXiv preprint arXiv:1704.03976 (2017).", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [15, 16, 16, 19, 20, 21, 22]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 79, "text": "Remark 4. \"Combining SST with other existing techniques can help.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [15, 16, 16, 19, 20, 21, 22]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 80, "text": "However, the additional cost is expensive.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [15, 16, 16, 19, 20, 21, 22]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 81, "text": "Further demonstrations are necessary for the proposed SST method.\"", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [15, 16, 16, 19, 20, 21, 22]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 82, "text": "A : It is true that combining and the additional cost is expensive.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "Skg9OOEf6X", "rebuttal_id": "SkeObu-t07", "sentence_index": 83, "text": "Therefore, we have modified our expressions in the paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [21, 22]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 0, "text": "Thank you for your comments. Please find below our response to your questions and concerns.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 1, "text": "1) Pseudocode", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 3]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 2, "text": "We apologise that the optimization procedure was unclear.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [3, 3]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 3, "text": "We have added pseudocode of the general optimization procedure in Appendix A.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3, 3]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 4, "text": "2) Hyperparameter selection", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 5, "text": "The reviewer is completely right that we are removing one hyperparameter by introducing another.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 6, "text": "However, there are two reasons why this might still be beneficial: one is that the penalty coefficient is now effectively dynamic and can change during training, ensuring higher chances of finding a good solution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 7, "text": "Second, by elevating the hyperparameter one level up, we hope that the learning is indeed less sensitive to its specific setting.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 8, "text": "Indeed, we found in practice that we get similar results for \\beta within some orders of magnitude, which requires significantly less tuning compared to a fixed \\alpha.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 9, "text": "3) Relation to safe reinforcement learning", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 8, 8]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 10, "text": "It is indeed the case that constrained MDPs are often considered in safe RL.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 8]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 11, "text": "In those cases there is generally an upper bound on a penalty function that should never be exceeded, including during training itself.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 8]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 12, "text": "These algorithms generally restrict policy updates to remain within the constraint-satisfying regime.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 8]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 13, "text": "While our approach can similarly be applied to upper bounds on penalties, there\u2019s unfortunately no guarantee that the constraints will be satisfied at every moment during training, but only at convergence.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 8]]}, {"review_id": "SkggB79t2X", "rebuttal_id": "HkgX7K2FA7", "sentence_index": 14, "text": "As such it is not clear how these methods would apply to our specific experimental setups.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 8]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 0, "text": "We appreciate your constructive feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 1, "text": "Specifically, your comments about the motivation and problem definitions greatly help us to improve the quality of our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 2, "text": "(Importance and motivation)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 3, "text": "To step forward to artificial general intelligence, we should further consider making an agent that can learn and remember many tasks incrementally [1].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 4, "text": "However, this is particularly challenging in real-world settings: the agent may observe different tasks sequentially, and an individual task may not recur for a long time.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 5, "text": "In this settings, a learned model might overfit to the most recently seen data, forgetting the rest, a phenomenon referred to as catastrophic forgetting, which is a core issue CL systems aim to address [2].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 6, "text": "Recently, GR-based methods, inspired by the generative nature of the hippocampus as a short-term memory system in the primate brain [3], have been widely studied to address the catastrophic forgetting problem.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 7, "text": "In terms of GR, we are trying to address the two open questions mentioned above.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 8, "text": "(Use of labels and novelty) In GR-based approaches, the quality of generated samples is crucial to keep the performance of previous tasks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 9, "text": "If we use labels, we can construct a conditional generative model.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 10, "text": "Generally, conditioning on a generative model yields higher quality samples than unconditional one and makes it possible to generate class-balanced samples [4]; the importance of conditional generation is also described in section 6.1 in our paper.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 11, "text": "In this paper, we showed that discriminative regularization could make VAE possible to conduct both class conditional generation and classification with one integrated model.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 12, "text": "Thus, we do not need to train an additional classifier, e.g., deep CNN, which is necessary for other works, including Narayanaswamy et al. There is also classifier integrated VAE such as [6].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 13, "text": "The difference with [6] is the use of class-conditional priors; more details are explained at the response (Difference with CDVAE) for reviewer 3 and section 4.1 in our paper.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 14, "text": "(Domain translation) Even though the conditional generation improves the quality of the generated samples, there is still a big difference between real and generated images.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 15, "text": "Because a deep neural network is vulnerable to even single-pixel perturbation [5], the difference can seriously affect the classification performance of GR-based algorithms.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 16, "text": "Thus, we suggested applying the domain translation to address this issue.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 17, "text": "By narrowing distribution discrepancy between real and generated images using the domain translation technique, we were able to alleviate the catastrophic forgetting problem successfully (Table 2).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 18, "text": "(Modeling) Good point.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 19, "text": "Since we consider finite discrete conditions, we can directly optimize $\\mathrm{\\mu_c}$ and $\\mathrm{\\sigma_c}$ for each c as parameters without the prior network.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 20, "text": "However, introducing a prior network makes our model become a more general framework that can address continuous-valued conditions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 21, "text": "Also, in our paper, we set the prior network as a single fully-connected layer for easy handling of conditions and simple implementation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 22, "text": "Otherwise, we should keep an additional mapping table between class conditions and its $\\mathrm{\\mu_c}$ and $\\mathrm{\\sigma_c}$.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 23, "text": "(Experimental settings)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 24, "text": "Generally, CL systems assume that each task comes sequentially, and an agent can not directly access previous experience [2].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 25, "text": "We exactly follow the assumption.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 26, "text": "Also, we train DiVA sequentially for each task with one same model.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 27, "text": "To clarify our training process, we provide a brief summarization.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 28, "text": "Firstly, we train DiVA with task 1 that consists of real images and labels.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 29, "text": "Then, when new task 2 is coming, DiVA generates images and its labels of task 1 and learns both task 2 and the generated task 1 simultaneously.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 30, "text": "We added an additional figure in Figure 6 in Appendix E, for helping conceptual understanding.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 31, "text": "(Domains of CIFAR dataset) Since current generative models are not perfect for generating complex natural images, there is always a discrepancy between generated images and real images.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [15]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 32, "text": "Thus, we can define two domains: real image domain (realistic) and generated image domain (blurry).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [15]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 33, "text": "We used the domain translation for narrowing the gap.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [15]]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 34, "text": "[1] Legg, Shane, and Marcus Hutter. \"Universal intelligence: A definition of machine intelligence.\" Minds and machines 17.4 (2007): 391-444.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 35, "text": "[2] https://sites.google.com/view/continual2018", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 36, "text": "[3] Shin, Hanul, et al. \"Continual learning with deep generative replay.\" Advances in Neural Information Processing Systems. 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 37, "text": "[4] Lesort, Timoth\u00e9e, et al. \"Marginal Replay vs Conditional Replay for Continual Learning.\" International Conference on Artificial Neural Networks. Springer, Cham, 2019.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 38, "text": "[5] Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. \"One pixel attack for fooling deep neural networks.\" IEEE Transactions on Evolutionary Computation (2019).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkgI3OVTYH", "rebuttal_id": "ryl1Y2nHoH", "sentence_index": 39, "text": "[6] Mundt, Martin, et al. \"Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition.\" arXiv preprint arXiv:1905.12019 (2019).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 0, "text": "1- We briefly mentioned the way problem embedding with similarity metric is used in the recommendation system in this work, but here is more explanation on that.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 1, "text": "The most similar problem is not necessarily recommended to a student.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 2, "text": "On a high level, if a student performs well on problems, we assume he/she performs well on similar problems as well, so we recommend a dissimilar problem and vice versa.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 3, "text": "More specifically, we project the performance of students on problems they solved onto the problems that they have not solved.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 4, "text": "This way, we have an evaluation of the performance of students on unseen problems.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 5, "text": "A problem is recommended that is within the capacity of students close to their boundary to help them learn, and at the same time recommendation is done so that all the concepts necessary for students are practiced by them.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 6, "text": "An evaluation on real students is presented in part 2 of the comment titled \u201cResponse to questions about Prob2Vec\u201d on this page, and we observed that similar problems are more likely to be solved correctly at the same time or wrong at the same time.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 7, "text": "The math expressions are not ignored in our proposed Prob2Vec method.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 8, "text": "In the example given in the last paragraph on page 3 for example, math expressions are used to extract the concept n-choose-k.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 9, "text": "We both use math expressions and text to label problems with appropriate concepts.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 10, "text": "2- Prob2Vec only uses expert knowledge for rule-based concept extractor, but does not use selected informative words.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 11, "text": "The effort put for rule-based concept extractor is negligible compared to effort needed for annotation of all problems with their corresponding concepts.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 12, "text": "We both annotated all problems manually and used rule-based concept extractor for annotation.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 13, "text": "In the former method, we observed 100% accuracy in the similarity detection test and observed 96.88% accuracy in the latter method.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 14, "text": "However, the rule-based concept extractor needs much less manual effort than manual problem annotation and is capable to provide us with relatively high level of accuracy we need in our application.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 15, "text": "Note that our method is scalable as long as problems are in the same domain as the rule-based concept extractor is automated for a single domain, but for the case that problems span many different domains, it is the natural complexity of the data set that requires a more sophisticated rule-based concept extractor.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 16, "text": "Furthermore, in most realistic cases for education purposes, problems span a single domain not multiple ones.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 17, "text": "We also like to grab your attention to the negative pre-training method proposed for training on imbalanced data sets.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "Skgof-pjiX", "rebuttal_id": "r1eU8lgW67", "sentence_index": 18, "text": "You may want to refer to part 2 of comment titled \u201cResponse to Question on Negative Pre-Training\u201d and part 1 of our response to reviewer2.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 0, "text": "Thanks for pointing out the pros and cons of our method.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 1, "text": "We address your concerns as follows:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 2, "text": "Q1. \u201cThe search space of the proposed method, such as the number of operations in the convolution block, is limited.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 3, "text": "A1: First, the size of search space is not determined by the number of operations but the number of connections.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 4, "text": "The search space of our method is different from exiting NAS methods in that the number of input of certain operation is not limited.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 5, "text": "Second, the search space without block share is even much larger than existing NAS methods.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 6, "text": "Third, we can trivially extend our DSO-NAS to accommodate more operations such as dilated conv like our ongoing experiments on PASCAL VOC semantic segmentation task, we extend our search space to accommodate 3x3 and 5x5 separable convolution with dilated = 2.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 7, "text": "The following table shows the performance of our model on the PASCAL VOC 2012 semantic segmentation task, where DSO-NAS-cls represents the architecture searched on ImageNet with block structure sharing and DSO-NAS-seg represents the architecture searched on PASCAL VOC segmentation task.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 8, "text": "---------------------------------------------------------------------------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 9, "text": "Architecture", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 10, "text": "mIOU                     Params(M)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 11, "text": "FLOPS(B)", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 12, "text": "---------------------------------------------------------------------------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 13, "text": "DSO-NAS-cls", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 14, "text": "72.1", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 15, "text": "6.5                               13.0", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 16, "text": "---------------------------------------------------------------------------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 17, "text": "DSO-NAS-seg(more operations)", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 18, "text": "72.7                            6.7                               13.2", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 19, "text": "---------------------------------------------------------------------------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 20, "text": "We combine DSO-NAS with Deeplab v3 and search for the architecture of feature extractor with block sharing.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 21, "text": "All above models have been pre-trained on ImageNet classification task first.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 22, "text": "It\u2019s notable that the architecture searched on semantic segmentation task with additional operations achieve better performance in our preliminary experiment, indicating that our DSO-NAS is capable to incorporate additional operations.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 23, "text": "We will present the full experiments of semantic segmentation in the future revision.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [11]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 24, "text": "Q2: \u201cThe technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 25, "text": "A2: Please refer to Q1.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 26, "text": "Moreover, we never claim the main contribution of our work lies in augmenting the search space.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 27, "text": "And in fact, most existing NAS papers share the same architecture search space, the main differences between them is the search strategy.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "SkgrQMiFhQ", "rebuttal_id": "H1lLBiyK0Q", "sentence_index": 28, "text": "We believe that judging the novelty of a NAS paper solely by its architecture space is unfair.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 0, "text": "We thank the reviewer for their insightful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 1, "text": "Q1 \"all the experiments except the last row of Table 2 concern adaptation between two domains. Given the paper title, the reviewer would have expected more experiments in a multiple domain context.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 2, "text": "A1 A main difference between domain adaptation and MDL is the fact that the former aims to minimize the target error, while the latter aims to minimize the average error.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 3, "text": "In this sense, our goal (and the validation experiments on Cell) are focused on MDL.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 4, "text": "Q2", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 5, "text": "\"Although the paper introduces the generalization bound for MDL, it does not give new formulation or algorithm to handle MDL\" [...] \"There is no connection between the theoretical results on MDL generalization bound and the proposed method MULANN.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 6, "text": "A2 This issue is related to the above: the new generalization bound extends that of Ben David et al. in the sense that it considers all pairs of domains involved, thus bounding the *average* risk; and this bound is the one underlying the proposed algorithm and its MDL experiments.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [17, 18]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 7, "text": "We have clarified this in the manuscript.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17, 18]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 8, "text": "Q3 \"the reviewer was interested to see how the proposed MDL performs on jointly adapting SVHN, MNIST, MNIST-M, and USPS or jointly adapting DSLR, Amazon, and Webcam for OFFICE dataset. \"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 9, "text": "A3: We added 3 domain experiments for Office, which are now displayed in Appendix E.1 table 6.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 10, "text": "As discussed in [3], we also find that the addition of a second source is not necessarily beneficial to target accuracy.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [12]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 11, "text": "Q4: Comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 12, "text": "A4: ADDA, an unsupervised DA method, proceeds by training sequentially a classifier on Source, then learning the Target feature space by making it indistinguishable from the Source one.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 13, "text": "However this is not applicable to the semi-supervised setting: either target labels would not be used in the first training step, or they would be used but without any domain loss to account for the fact that two domains are being used at the same time.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 14, "text": "Thus, the classifier would actually learn two sub-classifiers: one for each domain, which would turn counter-productive in the second step where this strong distinction between source and target would have to be un-learned.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 15, "text": "We are re-programming DSN and experimental results will be added.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 16, "text": "We thank the reviewer for the suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [13]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 17, "text": "Q5: \"The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [21]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 18, "text": "A5 Zhao et al. [3] consider the multiple source context; they define a weighted scheme where the weight of a source depends on its H-divergence with the target, plus its own classification error.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 19, "text": "The feature extractor is trained either from the best source only (in the sense of this weight), or from a weighted sum of the sources.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 20, "text": "When interested in multi-domain learning (thus aiming to minimize the average risk), it seems that there are two possibilities: a single feature extractor; or a feature extractor per domain.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 21, "text": "In the former case, the feature extractor might be overly conservative; in both cases, scalability w.r.t. the number of domains might be an issue.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 22, "text": "Hoffman et al. [4] also consider the multiple source context, assuming that the target is a unknown mixture of the sources (or not too far thereof in terms of Renyi divergence).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 23, "text": "Their experiments follow this assumption (using as target a mixture of sources Amazon, Webcam and DSLR).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 24, "text": "In our case this assumption does not hold, e.g. the joint distribution of England(x,y) is *not* a mixture of Texas(x,y) and California(x,y) (as can be seen by eye, and confirmed by experiments).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 25, "text": "The adversarial change of representation only enforces the merge of the marginals.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [21]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 26, "text": "Q6 \"The authors propose to rank the unlabeled samples of each domain according to the entropy of their classification of the current classifier. Obviously there must be some false ranking (specially at the initial stages of updating the classifier) for the unlabeled samples\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 27, "text": "A6", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 28, "text": "As the reviewer suggests, there are indeed misclassifications of samples using their entropy ranking in early training stages.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 29, "text": "We mention this section 4.3.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 30, "text": "This misclassification is the reason why it is better that hyper-parameter p slightly underestimates p* than is equal to it, as can be seen in Fig. 1, right (except when p*=1 as one could expect).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 31, "text": "Q7", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 32, "text": "\"Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [20]]}, {"review_id": "SkgS2HI5hQ", "rebuttal_id": "SygyoFtapX", "sentence_index": 33, "text": "A7 L is the cardinal of the union of classes with labeled examples in at least one domain.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 0, "text": "We thank the referee for finding our paper clearly written and in an interesting domain.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 1, "text": "1. We are not training Restricted Boltzmann Machines (RBMs), but Boltzmann machines where the hidden units can be fully connected.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 2, "text": "We have shown in our experiments (see Fig.\u00a03) that using a single RBM as a pre-processor will not result in increased adversarial resistance.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 3, "text": "A comparison of mean field training vs. constrastive divergence for RBMs has been made by [III,IV].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 4, "text": "Nevertheless, we have added a comparison between our method and the exact solution for a small Boltzmann machine of 16 units in the appendix.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 5, "text": "While we don't claim that this explains the performance of the method under all circumstances, it gives an indication of how well it works.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 6, "text": "We agree that a more thorough study of the training method would be desirable but in this article we are concentrating on reporting the results on increased adversarial resistance.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 7, "text": "We believe that these results should be interesting to the community even if some doubts about the training procedure remain.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 8, "text": "Lastly, we want to obtain a differentiable building block that can be used in standard neural nets.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 9, "text": "The unrolling of the mean field iterations (see Fig.\u00a01) provides a straightforward to achieve this.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 10, "text": "Propagating a gradient through a sampling based building block, while possible, would be considerably harder.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [4, 5]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 11, "text": "2. In [III,IV] the authors have succesfully used a mean field approximation beyond the trivial first order to train RBMs.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 12, "text": "In [II] a first order approximation is used.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 13, "text": "In our approach we use the approximation up to fourth order in the coupling J.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 14, "text": "3. The approach of the authors in [III,IV] and also our approach is based on free energy.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 15, "text": "The systematic expansion of the free energy in the coupling constant forms the basis of our approach.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [10]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 16, "text": "4. D is calculated as the relative entropy over a batch of 10000 examples.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 17, "text": "The reference probabilities are taken to be uniform and the model probabilities are calculated according to the procedure outlined in section 2.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 18, "text": "Due to the approximations involved it is possible that the sum of the model probabilities exceeds one.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 19, "text": "In this case we rescale the unclamped free energy to limit the total probability to one.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 20, "text": "We have added a note to that effect to the article.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 21, "text": "5. We have included a comparison to the, to our knowledge, currently most adversarially resistant model on MNIST.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 22, "text": "Since our results are derived for MNIST we can only compare to methods in the literature that are evaluated on MNIST.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 23, "text": "We acknowledge that other methods perform very well on more sophisticated tasks and have added a reference.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 24, "text": "6. While we have not explicitely labelled white box and black box attacks we are using a strong white box attack (gradient based) and the, to our knowledge, strongest black box attack (boundary method).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 18]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 25, "text": "We have added the labels in the text and Table 1 to make this more clear.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17, 18]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 26, "text": "7. We have added some more attacks on the most robust model.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [19, 19]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 27, "text": "We completely agree that more experiments on other datasets are needed to show the universality of the method.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [19, 19]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 28, "text": "Due to the computational complexity this will need to remain a topic for a follow up article.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [19, 19]]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 29, "text": "[I] Seyed-Mohsen Moosavi-Dezfooli, Ashish Shrivastava, Oncel Tuzel: Divide, Denoise, and Defend against Adversarial Attacks. arXiv 1802.06806 (2018).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 30, "text": "[II] Ruslan Salakhutdinov, Geoffrey Hinton: Deep Boltzmann Machines. Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, 448-455, (2009)", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 31, "text": "[III] Pawe\u0142 Budzianowski. Training Restricted Boltzmann Machines Using High-Temperature Expansions. Master\u2019s thesis, University of Cambridge, 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skl1-09T2m", "rebuttal_id": "BygEO5SY0m", "sentence_index": 32, "text": "[IV] Marylou Gabri\u00e9, Eric W. Tramel, and Florent Krzakala. Training restricted boltzmann machines via the Thouless-Anderson-Palmer free energy. CoRR, abs/1506.02914, 2015.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 0, "text": "Thank you for your review and comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 1, "text": "We\u2019ve made a number of additions and improvements to address them in the updated version of the paper, which we will submit before the end of the discussion period.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 2, "text": "First, we have performed a new set of experiments on the larger dataset in [1].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 3, "text": "HOF shows greater average reconstruction accuracy than the methods compared in [1].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 4, "text": "Second, we also perform ablation experiments to demonstrate that HOF performs competitively even when we vary the encoder architecture, decoder depth, decoder activation function, or input sampling for the decoder network.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 5, "text": "For example, using Resnet18 as the encoder architecture or using a decoder network with twice as many hidden layers showed nearly identical performance in terms of average Chamfer distance on the test set.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 6, "text": "The complete quantitative results will be included in an updated PDF before the end of the discussion period.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 7, "text": "\"The purpose of generative models is not to interpolate per se; the interpolation is really a sanity check that the model is capturing the underlying distribution rather than just memorizing training examples.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 16, 17, 18, 19, 19, 19, 19]]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 8, "text": "The function composition doesn't capture that.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 16, 17, 18, 19, 19, 19, 19]]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 9, "text": "I think the authors should just acknowledge that you can't soundly *sample* from their generative model the way e.g. VAE or GAN allows (their function composition is not a sampling method).", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 16, 17, 18, 19, 19, 19, 19]]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 10, "text": "But I think there are lots of useful things you can do without that capability, e.g. do 3D point cloud completion, go image -> structure, etc. I think this function composition angle should be deemphasized in the title/abstract, but I think the paper stands  reasonably on its own without that.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 16, 17, 18, 19, 19, 19, 19]]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 11, "text": "We agree that the current formulation of composition is not equivalent to a generative model.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [15, 16, 17, 18, 19, 19, 19, 19]]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 12, "text": "In our work, function composition primarily serves the purpose of demonstrating that the model learns a meaningful subspace of objects (rather than memorizing the training set, as you mentioned).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [15, 16, 17, 18, 19, 19, 19, 19]]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 13, "text": "We have revised the abstract to clarify this point.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [15, 16, 17, 18, 19, 19, 19, 19]]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 14, "text": "\"In Figure 2 it's pretty hard to see the differences between the methods. What exactly is being visualized here? DeepSDF shold be visualizing surface normals vs. HOF which is point clouds, right?\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24]]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 15, "text": "We have clarified in the manuscript that our comparisons are between architectures, rather than training objectives/output representations.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [24]]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 16, "text": "Thus our DeepSDF, FoldingNet, and HOF architectures all output point clouds, which can be compared directly.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24]]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 17, "text": "\"For predicting a deformation R^3 -> R^3 function composition sort of makes sense, but how generalizable is this approach e.g. to directly predicting a function R^3 -> R (a la DeepSDF)? I think there are ways this function composition approach could generalize, e.g. using skip connections and layer dropout (which encourages layers to be composable).\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [25, 25, 27]]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 18, "text": "Additional techniques for promoting learning of composable representations such as skip connections and layer dropout are an exciting direction for future research.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [25, 25, 27]]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 19, "text": "One way function composition might allow for R^3 -> R mappings by composing a mapping from R^3 -> R^3 and taking the only first dimension of each element in the final output.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [25, 25, 27]]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 20, "text": "Thank you again for your feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SklBkB-f9r", "rebuttal_id": "rkl6-fY9jr", "sentence_index": 21, "text": "[1] M. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, and T. Brox, \u201cWhat do single-view 3d reconstruction networks learn?,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3405\u2013 3414, 2019.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 0, "text": "Thank you for your helpful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 1, "text": "We have improved the writing to incorporate your feedback.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 2, "text": "We have also performed more experiments to compare APO to manual learning rate schedules.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 3, "text": "Q: Please explain more how gradients w.r.t hyper-parameters are computed.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 4, "text": "We implemented custom versions of the optimizers we consider (SGD, RMSprop, and K-FAC) that treat the optimization hyperparameters as variables in the computation graph for an optimization step.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 5, "text": "We then use automatic differentiation to compute the gradient of the meta-objective with respect to the hyperparameters (e.g., the learning rate).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 6, "text": "Q: Could authors explain the time complexity of inner loop in Algorithm 1? Does it take more time than that of updating model parameters?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 7, "text": "Each meta-optimization step requires approximately the same amount of computation as a parameter update for the model.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 8, "text": "By using the default meta learning rate suggested in our updated paper, we can amortize the meta-optimization by performing 1 meta-update for every K steps of the base optimization.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 9, "text": "We found that K=10 works well across our settings, while reducing the computational requirements of APO to just a small fraction more than the original training procedure.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 10, "text": "We have added a discussion of our meta-optimization setup and the efficiency of APO in Section 5 of the updated paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [15]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 11, "text": "Q: No need to write so much decorated bounds in section 3.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 12, "text": "The convergence analysis is on Z, not on parameters x and hyper-parameters theta.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 13, "text": "So, bounds here cannot be used to explain empirical observations in Section 5.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 14, "text": "The convergence of the network output Z directly indicates the rate of decrease of the loss function, which is exactly what we observe in practice.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 15, "text": "Although the assumption of a global optimization oracle is not realistic, we believe our theoretical justification provides insight into why the method works.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 16, "text": "One important takeaway from the theoretical analysis is that running gradient descent on output space can potentially accelerate the optimization (since the convergence bounds have better constants).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 17, "text": "This directly motivates the regularization term in our meta objective to be defined as the discrepancy of network outputs instead of the network parameters, which is essential to our technique.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 18, "text": "Q: Could the authors compare with changing step-size?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 19, "text": "Thank you for the suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [17]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 20, "text": "We have added comparisons with custom learning rate schedules for CIFAR-10 and CIFAR-100.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 21, "text": "We updated our results for CIFAR-10/100 using a larger network, ResNet34, instead of the VGG11 model used in the previous version, and we used a manual learning rate decay schedule where we trained for 200 epochs, decaying the learning rate by a factor of 5 three times during training.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 22, "text": "We found that APO is competitive with the custom schedule, achieving similar training loss and test accuracy.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 23, "text": "We provide results in our response to all reviewers at the top.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 24, "text": "Q: How to tune lambda?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 25, "text": "Tuning a good lambda v.s. tuning a good step-size, which one costs more?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [27]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 26, "text": "We tune lambda by performing a grid search over the range {1e-1, 1e-2, 1e-3, 1e-4, 1e-5}. Because each lambda value gives rise to a learning rate schedule, tuning lambda yields significantly more value than tuning a fixed learning rate.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22]]}, {"review_id": "SklCKPP1h7", "rebuttal_id": "Bkg51P6KCX", "sentence_index": 27, "text": "Instead of trying to come up with a custom learning rate schedule, which would require deciding how frequently to decay the learning rate, and by what factor it should be decayed, all one needs to do is perform a grid search over a fixed set of lambdas to find an automated schedule that is competitive with hand-designed schedules (which are the result of years of accumulated experience in the field).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 0, "text": "We\u2019re glad that you found the paper interesting and well-written. To address your comments and questions:", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 1, "text": "1. the NPN model seems a good alternative, will be good to have a discussion about why your model is better than NPN.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 2, "text": "Also, NPN can probably be modified to output spans of a sentence.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 3, "text": "I will be curious to know how it performs.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 4, "text": "The NPN model requires a pre-defined lexicon of action types (i.e., verbs), such as cut, bake, boil, etc.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 5, "text": "For the recipes dataset, the action types and their causal effects were manually collected and defined.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 6, "text": "Since the ProPara dataset does not have these annotations, we would have to manually identify action types to apply NPN to it.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 7, "text": "Also, NPN treats the state change as a classification problem (of about 260 classes that are also manually defined).", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 8, "text": "In contrast, KG-MRC finds the state-describing span in the text directly, which we believe is a more generic approach.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 9, "text": "2. A more detailed illustration of the system / network is needed. Would have made it much easier to understand the paper.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 10, "text": "We agree that more detail would help readers to understand the model better.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 11, "text": "We\u2019ve made some hopefully significant updates to Section 4 (model description and notation) to improve clarity, and we hope you\u2019ll take the time to read the new manuscript.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 12, "text": "Two important additions are a high-level summary of the model, which we give at the beginning of Section 4, and a table (Table 2) that lists what each symbol represents along with its dimensions.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 13, "text": "3. What are the results when using the whole training set of Recipes ?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 14, "text": "We\u2019ve completed an experiment on the full Recipes dataset and updated the paper to describe the result (this experiment did not finish in time for the initial submission).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9]]}, {"review_id": "Sklcn-_c3m", "rebuttal_id": "S1xtKI4mAX", "sentence_index": 15, "text": "The model\u2019s F1 score improves from 51.64 on the partial data to 54.27 on the full data, surpassing the previous state of the art by a more significant margin.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 0, "text": "Dear reviewer:", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 1, "text": "We appreciate your comments but it appears that there is some misunderstanding regarding our contribution in this work.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 2, "text": "Our work is for softmax inference speedup", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 3, "text": "while Sparse-Gated MoE (MoE) was not designed to do so.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 4, "text": "It was designed to increase the model expressiveness.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 5, "text": "It cannot achieve speedup because each expert still contains full softmax space as we mentioned in the background section (page 2 line 21st) and method section (page 2 last 4th line).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 6, "text": "And since it is slower than the standard softmax by definition, we chose not to compare with it in the paper.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 7, "text": "Our algorithm addresses speed up in softmax inference.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 8, "text": "This is fundamentally different from Sparse-gated MoE. We divide the output space into multiple overlapped subsets.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 9, "text": "To find top-k predictions, we only search a few subsets.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 10, "text": "While in full softmax or MoE, the complexity is linear with output dimension.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 11, "text": "Therefore, we did not include a comparison with Sparsely-Gated MoE in our article and only compare with full softmax.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 12, "text": "Just for additional reference, we tested Sparsely-Gated MoE with different experts in PTB dataset; we compared the results to DS-Softmax.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 13, "text": "As expected, the Sparsely-Gated MoE does not achieve speedup in terms of softmax inference.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 14, "text": "_____________________________________________", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 15, "text": "_", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 16, "text": "Method | Top 1 | Top 5 |Top 10| FLOPs|", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 17, "text": "DS-8       | 0.257 | 0.448 | 0.530 | 2.84x |", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5]]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 18, "text": "MoE-8    | 0.258 | 0.448 | 0.530 |  1x", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 19, "text": "|", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 20, "text": "DS-16     | 0.258 | 0.450 | 0.529 | 5.13x |", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 21, "text": "MoE-16  | 0.258 | 0.449 | 0.530 | 1x", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 22, "text": "|", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 23, "text": "DS-32     | 0.259 | 0.449 | 0.529 | 9.43x |", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 24, "text": "MoE-32  | 0.259 | 0.450 | 0.531 | 1x", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 25, "text": "|", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 26, "text": "DS-64     | 0.258 | 0.450 | 0.529 |15.99x|", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 27, "text": "MoE-64  | 0.260 | 0.451 | 0.531 | 1x", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 28, "text": "|", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 29, "text": "_____________________________________________", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 30, "text": "_", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklHkeMohX", "rebuttal_id": "HJgvmMlBTQ", "sentence_index": 31, "text": "* FLOPs means FLOPs reduction (i.e. baseline's FLOPs / target method's FLOPs).", "coarse": "other", "fine": "rebuttal_none", "alignment": ["context_error", null]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 0, "text": "1. Comment:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 1, "text": "However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 2, "text": "1. Response:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 3, "text": "We study seq2seq classification tasks since they have been widely used in real world applications for RNNs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 4, "text": "To name a few, in speech recognition, [1] hybridizes hidden Markov model with RNNs to label unsegmented sequence data; In computer vision, [2, 3] demonstrate scene labeling with LSTM and RNNs, achieving higher accuracy than baseline methods; In healthcare, [4] proposes a model, Doctor AI, to perform multiple label prediction (one for each disease or medication category).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 5, "text": "In addition, [5, 6] both apply RNNs to real-world healthcare datasets (MIMIC-III, PhysioNet, and ICU data) for mortality prediction and other multiple classifications tasks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 6, "text": "We establish bounds for classification because it is typical in learning theory and is easy to compare among existing literature.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 7, "text": "On the other hand, our analysis applies in other tasks as long as a suitable Lipschitz loss function is chosen.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 8, "text": "Specifically, Lemma 4 establishes an upper bound for empirical Rademacher complexity of general Lipschitz loss functions (the last line in Appendix A.4).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 9, "text": "By replacing the loss function in Lemma 1, we can derive generalization bounds for various tasks other than classification.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9]]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 10, "text": "References", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 11, "text": "[1] Graves, Alex, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber. \"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.\" In Proceedings of the 23rd international conference on Machine learning, pp. 369-376. ACM, 2006.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 12, "text": "[2] Byeon, Wonmin, Thomas M. Breuel, Federico Raue, and Marcus Liwicki. \"Scene labeling with lstm recurrent neural networks.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3547-3555. 2015.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 13, "text": "[3] Socher, Richard, Cliff C. Lin, Chris Manning, and Andrew Y. Ng. \"Parsing natural scenes and natural language with recursive neural networks.\" In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 129-136. 2011.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 14, "text": "[4] Choi, Edward, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. \"Doctor ai: Predicting clinical events via recurrent neural networks.\" In Machine Learning for Healthcare Conference, pp. 301-318. 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 15, "text": "[5] Che, Zhengping, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. \"Recurrent neural networks for multivariate time series with missing values.\" Scientific reports 8, no. 1 (2018): 6085.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SklswyoFnX", "rebuttal_id": "Hyx17yRtAQ", "sentence_index": 16, "text": "[6] Xu, Yanbo, Siddharth Biswal, Shriprasad R. Deshpande, Kevin O. Maher, and Jimeng Sun. \"RAIM: Recurrent Attentive and Intensive Model of Multimodal Patient Monitoring Data.\" In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2565-2573. ACM, 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 0, "text": "Thanks for your comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 1, "text": "To our knowledge, close to 2% improvement of accuracy is not small in CIFAR100. Because we only change pooling layers while keeping others exactly the same.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 2, "text": "Now, we respond to your questions one by one:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 3, "text": "1.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 4, "text": "The results of AA-pooling and F-pooling are not the same.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 5, "text": "In Fig. 1, we show the results of average-pooling and F-pooling.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 6, "text": "If you carefully look at the corner of curves, you can find the differences.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 7, "text": "Without convolution, AA-pooling is similar to average-pooling (both of them are low-pass filters but with sightly different kernels.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 8, "text": "So AA-pooling gives different results for sine waves.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 9, "text": "2. We believe F-pooling plays a more important rule in applications where shift-equivalent is serious, such as object detection and object tracking.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 10, "text": "Because we need to predict the location or shifts of an image object.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 11, "text": "Moreover, F-pooling may be better for complex-valued CNNs, such as [1].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6, 7, 8]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 12, "text": "3. The limitation of imaginary part is easy to overcome: set the resolution of F-pooling\u2019s output to an odd number or padding it to an odd number when the resolution is an even number.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 13, "text": "In this way, the imaginary part is zero.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 14, "text": "Moreover, the word shift in this paper means circular shift.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 15, "text": "So it is better to use circular padding in convolutional layers.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 16, "text": "However, we find circular padding slower the training speed in PyTorch.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 17, "text": "If we use zero paddings as in most situations, the beneficial of F-pooling is reduced.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 18, "text": "Our current experiments use zero paddings.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 19, "text": "See our general response for what happens when we replace zero paddings with circular padding.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 20, "text": "4. In all experiments of our current paper, the imaginary part is already ignored.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 21, "text": "We can\u2019t directly measure how the imaginary part affects the performance unless we use complex-valued CNNs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 22, "text": "Ignoring this part will destroy the reconstruction optimality, but the effect is small.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 23, "text": "Suppose the output size of F-pooling is 2N+1.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 24, "text": "We first transform a signal into frequency domain and keep 2N+1 components with the lowest frequencies: f(-N), \u2026 , f(0), \u2026 ,f(N).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 25, "text": "Then we transform it back into time domain.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 26, "text": "In this case, the imaginary part in time domain is zero because of symmetry.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 27, "text": "Now, suppose the output size is 2N+2: f(-N), \u2026 , f(0),", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 28, "text": "\u2026 f(N), f(N+1)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 29, "text": ".", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 30, "text": "In this case, the imaginary part is not zero.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 31, "text": "However, if we set f(N+1) to 0, it imaginary part becomes zero again.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 32, "text": "Thus, the error of ignoring imaginary part is not larger than ||f(N+1)||.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 33, "text": "Fig.4 shows an example of odd and even output size of F-pooling.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "Skx-jEBr5B", "rebuttal_id": "Hyl3QZqMjS", "sentence_index": 34, "text": "[1] Deep complex networks, ICLR2018", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 0, "text": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 1, "text": "Here we respond to your specific comments.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 2, "text": "\"(1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 3, "text": ">>> As mentioned in the main response, the proposed TPN is not a mere combination of CNN representation learning and label propagation.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 4, "text": "The original label propagation constructs a fixed graph (Eq (1)) to explore the correlation between examples.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 5, "text": "While in our work, we adaptively construct the graph structure for each episode (training task) with a learnable graph construction module (Figure 4, Appendix A).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 6, "text": "This leads to better generalization ability for test tasks.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 7, "text": "In Table 1 and Table 2, the proposed TPN achieved much higher accuracy than the mere combination model (referred to as \"Label Propagation\").", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [5, 6, 7]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 8, "text": "\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 9, "text": "(2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 10, "text": "Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 11, "text": "For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 12, "text": "This is a major concern.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 13, "text": ">>> At first, we want to clarify the few-shot network architecture setting.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 14, "text": "Currently, there are two common network architectures: 4-layer ConvNets (e.g., [1][2][3]) and 12-layer ResNet (e.g., [4][5][6][7]).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 15, "text": "Our method belongs to the first one, which contains much fewer layers than the ResNet setting.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 16, "text": "Thus, it is more reasonable to compare TADAM with ResNet version of our method.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 17, "text": "To better relieve the reviewer's concern, we implemented our algorithm with ResNet architecture on miniImagenet dataset and show the results as follow:", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 18, "text": "Method", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 19, "text": "1-shot    5-shot", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 20, "text": "SNAIL [4]", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 21, "text": "55.71     68.88", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 22, "text": "adaResNet [5]                        56.88     71.94", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 23, "text": "Discriminative k-shot [6]", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 24, "text": "56.30     73.90", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 25, "text": "TADAM [7]", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 26, "text": "58.50     76.70", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 27, "text": "--------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 28, "text": "Ours", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 29, "text": "59.46     75.65", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 30, "text": "--------------------------------------------------------", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 31, "text": "It can be seen that we beat TADAM for 1-shot setting.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 32, "text": "For 5-shot, we outperform all other recent high-performance methods except for TADAM.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 33, "text": ">>> We want to clarify that \"Label Propagation\" in Table 1 and Table 2 is a strong baseline.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 34, "text": "It combines label propagation method [8] with episodic meta-learning.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 35, "text": "The usage of transductive inference makes this baseline outperform most published state-of-the-art methods.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 36, "text": "Moreover, the performance of TPN over label propagation is not very small.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 37, "text": "For example, in miniImagenet, TPN outperforms label propagation with 1.44% and 1.25% for 1-shot and 5-shot respectively, but this advantage grows to 3.20% and 1.68% with \"Higher Shot\" training.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 38, "text": "The improvements are even larger for tieredImagenet with 4.68% and 2.87%.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 39, "text": "We believe in few-shot learning, this is a large improvement.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 40, "text": "[1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" ICML. 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 41, "text": "[2] Snell, Jake, Kevin Swersky, and Richard Zemel. \"Prototypical networks for few-shot learning.\" NIPS. 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 42, "text": "[3] Yang, Flood Sung Yongxin et al. \"Learning to compare: Relation network for few-shot learning.\" CVPR. 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 43, "text": "[4] Mishra, Nikhil et al. \"A simple neural attentive meta-learner.\" ICLR. 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 44, "text": "[5] Munkhdalai, Tsendsuren et al. \"Rapid adaptation with conditionally shifted neurons.\" ICML. 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 45, "text": "[6] Bauer, Matthias et al. \"Discriminative k-shot learning using probabilistic models.\" arXiv. 2017.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 46, "text": "[7] Oreshkin, B.N., Lacoste, A. and Rodriguez, P., 2018. \"TADAM: Task dependent adaptive metric for improved few-shot learning.\" NIPS. 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Skx7vDii3X", "rebuttal_id": "r1xNOL-uCX", "sentence_index": 47, "text": "[8] Zhou, Denny, et al. \"Learning with local and global consistency.\" NIPS. 2004.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkxAD0ECtB", "rebuttal_id": "BJgEvjdmsH", "sentence_index": 0, "text": "We thank the reviewer for acknowledging the technical aspects of the paper and for noting that our\u200b \u200bresults\u200b \u200bare\u200b \u200bsolid\u200b \u200band\u200b \u200bour\u200b \u200banalysis\u200b \u200bis\u200b \u200bthorough.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SkxAD0ECtB", "rebuttal_id": "BJgEvjdmsH", "sentence_index": 1, "text": "RE: Source code", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SkxAD0ECtB", "rebuttal_id": "BJgEvjdmsH", "sentence_index": 2, "text": "The reviewer makes an important point about the availability of the source code.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "SkxAD0ECtB", "rebuttal_id": "BJgEvjdmsH", "sentence_index": 3, "text": "To address this point, in the link privately shared with the reviewers, we have provided all of our code, datasets together with their train/test splits, as well as our pre-trained models, to help with the reproducibility of our results.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8]]}, {"review_id": "SkxAD0ECtB", "rebuttal_id": "BJgEvjdmsH", "sentence_index": 4, "text": "We note that we will share PyTorch implementations of all pre-training methods and datasets with the community upon publication.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [8]]}, {"review_id": "SkxAD0ECtB", "rebuttal_id": "BJgEvjdmsH", "sentence_index": 5, "text": "Please feel free to ask any further questions regarding our code and implementation.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [8]]}, {"review_id": "SkxAD0ECtB", "rebuttal_id": "BJgEvjdmsH", "sentence_index": 6, "text": "RE: Linear time complexity in Appendix F", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkxAD0ECtB", "rebuttal_id": "BJgEvjdmsH", "sentence_index": 7, "text": "We acknowledge that the time complexity of our pre-training methods was not well explained in Appendix F. In Figure 2 (a) we show that we only sample one node per graph.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [9]]}, {"review_id": "SkxAD0ECtB", "rebuttal_id": "BJgEvjdmsH", "sentence_index": 8, "text": "We then use breadth-first search to extract a K-hop neighborhood of the node, which takes at most linear time with respect to the number of edges in the graph.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "SkxAD0ECtB", "rebuttal_id": "BJgEvjdmsH", "sentence_index": 9, "text": "As a result, pre-training via context prediction has linear time complexity.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9]]}, {"review_id": "SkxAD0ECtB", "rebuttal_id": "BJgEvjdmsH", "sentence_index": 10, "text": "We will edit Appendix F to include more detailed information and cover this important point.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [9]]}, {"review_id": "SkxAD0ECtB", "rebuttal_id": "BJgEvjdmsH", "sentence_index": 11, "text": "Please let us know if you have any further questions or comments!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 0, "text": "Thank you for the comments!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 1, "text": "To review\u2019s feedback:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 2, "text": "- We pay attention to the term \u201cskill discovery\u201d and made it more clear about the connection between prior works and the current work in the revised version.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_error", null]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 3, "text": "Our method can also be combined with DIAYN to learn the skill-conditioned policy as mentioned in the paper.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_error", null]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 4, "text": "- We added both a theoretical connection and new experimental results to compare MISC and the empowerment method in the revised version.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5, 6, 7, 9]]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 5, "text": "In the navigation tasks, we show that our method outperforms the empowerment method.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5, 6, 7, 9]]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 6, "text": "- An intuition for why I(s_c, s_i) could be superior to I(a, s_i) is that in robotic tasks, the mutual information between the robotic sates, s_c, and the object states, s_i, could be easier to be estimated than the mutual information between the action, a, and the object states, s_i, as shown in Figure 4 in the paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 7, "text": "Therefore, the agent receives a higher MI reward more easily and learns to control s_i more efficiently.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 8, "text": "The context states can be seen as the summary information of the agent\u2019s action and the transition model of the environment, which could be more relevant in terms of estimating the object states in comparison to the agent\u2019s actions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8]]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 9, "text": "- VIME and PER are used as described in their original papers.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 10, "text": "- We have added an appendix to provide more information about experiment details.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11]]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 11, "text": "- We also newly evaluated our method on gazebo-based robotic simulations, including the cases when there is no object, a single object of interest, and multiple objects of interests.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_unknown", null]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 12, "text": "A video showing new experimental results is available at https://youtu.be/l5KaYJWWu70?t=104", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_unknown", null]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 13, "text": "In this experiment, we also compare MISC with two additional baselines, including ICM and empowerment (with state of interest), see Figure 4 in the paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_unknown", null]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 14, "text": "- We now mention that the states of interests vs context are given in the revised paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_unknown", null]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 15, "text": "However, when they are not given. They can also be automatically learned/selected by iterating over all possible combinations", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_unknown", null]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 16, "text": ".", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_unknown", null]}, {"review_id": "SkxED7_nKS", "rebuttal_id": "rkgrTHMcjB", "sentence_index": 17, "text": "Afterwards, an optimal combination can be chosen by the user via testing in the task at hand.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_unknown", null]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 0, "text": "We thank the reviewer for the evaluation.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 1, "text": "Please see our detailed response to several recurring issues at https://openreview.net/forum?id=Hkx-ii05FQ&noteId=HygFbNmL6X.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 2, "text": "In that response we address the following issues:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 3, "text": "(1) We emphasize fundamental differences between Cakewalk and CE.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 4, "text": "(2) How the sampling distribution should not be considered as a part of Cakewalk, and that it is mostly provided as an example, and a basis for the reported experiments.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 5, "text": "(3) The experiments include results two tasks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 6, "text": "Nonetheless, it appears the paper doesn\u2019t convey this clearly, and we suggest two possible ways how to update the paper in this regard.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 7, "text": "Next, we\u2019ll try to provide some intuition as to why a sampling distribution that assumes independence between the different dimensions can be useful in some cases.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 3, 5]]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 8, "text": "The simple explanation is that in some problems the conditional expectation of the objective given that some x_i=j is much better than for other values x_i=k. In such cases, for each dimension the algorithm will tend to sample values which are useful to many possible solutions.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 3, 5]]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 9, "text": "In the clique problem for example, if some node i is part of a large clique, then sampling x_i=1 is likely to result in a good objective as there are many nodes that are connected to i, and the chance of not sampling any of them decreases with the clique size.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 3, 5]]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 10, "text": "In this way, over time the probability for sampling such nodes becomes higher, and the chance of sampling all of them together increases.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 3, 5]]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 11, "text": "Lastly, we note that these kind of factorized distributions have a long history of being useful  in machine learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 3, 5]]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 12, "text": "In a similar context to the one studied in the paper, such distributions have been studied by Rubinstein in his paper which discusses CE as an algorithm for combinatorial optimization, and in the classical bandit papers Exp3 is applied independently to several dimensions to study game theoretic problems.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 3, 5]]}, {"review_id": "SkxZmGP027", "rebuttal_id": "HkxGgdxjp7", "sentence_index": 13, "text": "In different contexts, such distributions have also been used as naive mean field approximations in variational inference.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 3, 5]]}, {"review_id": "Sye1Z22R5B", "rebuttal_id": "Hkl2wG0joB", "sentence_index": 0, "text": "We thank the reviewer for the comments on our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Sye1Z22R5B", "rebuttal_id": "Hkl2wG0joB", "sentence_index": 1, "text": "A prior work of Hsu et al. (ICLR'18) showed that heavy hitter oracles exist and that they can be constructed using machine learning techniques.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "Sye1Z22R5B", "rebuttal_id": "Hkl2wG0joB", "sentence_index": 2, "text": "We are using the same type of oracles in our current submission.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "Sye1Z22R5B", "rebuttal_id": "Hkl2wG0joB", "sentence_index": 3, "text": "Similar oracles have been studied in previous works too, e.g., membership oracles for Bloom filters in Kraska et al. Both the previous works and the experiments in the current submission demonstrate that it is reasonable to make such an oracle assumption.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "Sye1Z22R5B", "rebuttal_id": "Hkl2wG0joB", "sentence_index": 4, "text": "We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "Sye1Z22R5B", "rebuttal_id": "Hkl2wG0joB", "sentence_index": 5, "text": "The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "Sye1Z22R5B", "rebuttal_id": "Hkl2wG0joB", "sentence_index": 6, "text": "The prior work by Hsu et al. showed that the oracle trained by deep learning has high accuracy (see Section 5.3 in their paper): for Internet traffic data, the AUC score is 0.9, and for search query data, the AUC score is 0.8.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "Sye1Z22R5B", "rebuttal_id": "Hkl2wG0joB", "sentence_index": 7, "text": "The performance of a simple online algorithm would likely depend on the type of classifier used and input feature representation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "Sye1Z22R5B", "rebuttal_id": "Hkl2wG0joB", "sentence_index": 8, "text": "Linear classifiers with IP addresses represented as individual bits are unlikely to work well because their expressive power is limited.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "Sye1Z22R5B", "rebuttal_id": "Hkl2wG0joB", "sentence_index": 9, "text": "For instance, at the very least, we would like our classifier to express a DNF hypothesis of the form:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "Sye1Z22R5B", "rebuttal_id": "Hkl2wG0joB", "sentence_index": 10, "text": "(IP address = a1) or (IP address  = a2) or ...", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "Sye1Z22R5B", "rebuttal_id": "Hkl2wG0joB", "sentence_index": 11, "text": "We have updated the introduction to rephrase and clarify the lower bound claims.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "Sye1Z22R5B", "rebuttal_id": "Hkl2wG0joB", "sentence_index": 12, "text": "The added/modified text are highlighted in the blue color.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8, 9, 9]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 0, "text": "Thank you for your comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 1, "text": "We have performed additional experiments in order to address them, particularly training and evaluating HOF as in [1].", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 2, "text": "Q1: Evaluation as in [1]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 16, 17]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 3, "text": "In accordance with these recommendations in [1], we have trained HOF on the dataset provided by the authors of [1] and evaluated it according to the F1 metric that is proposed.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [15, 16, 17]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 4, "text": "We find that HOF provides competitive performance to existing methods, giving the highest average F1 score of all methods in [1].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16, 17]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 5, "text": "We will report quantitative results on this benchmark in a revised PDF submitted before the end of the discussion period.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [15, 16, 17]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 6, "text": "Q2: Input shape", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 7, "text": "We have performed new comparison experiments to address this question.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 8, "text": "Varying the input shape does affect performance; sampling the surface of the 3d sphere gives worse performance than sampling the interior of the sphere (1.369 average chamfer distance for surface of sphere versus 1.247 for interior).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 9, "text": "In addition, we find that sampling the interior of the 4D sphere, rather than the 3D sphere gives a fairly significant improvement in performance (1.195 average chamfer distance for 4D sphere vs 1.247 for 3D).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 10, "text": "Since we are learning an arbitrary mapping (rather than, for example, a projection to a manifold), the mapping domain does not explicitly induce any topological constraints such as genus.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 11, "text": "If our mapping didn't have to be continuous, it wouldn't matter what shape we sampled.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 12, "text": "However, because we use neural networks with continuous activation functions (relus) to represent the mapping, it must be continuous.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 13, "text": "And because the experiments above indicate that the mapping domain affects the quality of the reconstructions, it is possible that the sampling domain imposes topological constraints on the set of objects.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 14, "text": "It's difficult to say what the \"best\" shape to sample from is; however, in future work, we would like to investigate strategies for learning the best input shape to sample from.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 15, "text": "Q3: Sampling the input shape", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 16, "text": "Our input 3D points are sampled from the interior of the unit sphere, rather than the surface (we have clarified this in the revised manuscript).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 17, "text": "Samples are drawn uniformly at random from within the sphere in order to avoid overfitting to a particular gridding structure.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 18, "text": "Q4: Value of $c$ for composition networks", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 19, "text": "In order to apply composition, the reviewer is correct that c must equal 3 (this is the case in our composition experiments).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 20, "text": "However, if we are not composing the reconstruction function, c could be anything greater than zero.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 21, "text": "For example, c=4 in our ablation experiment in which we sample input points from the 4D sphere rather than the 3D sphere.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 22, "text": "Q5: Clarifying values of $k$", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [18, 19, 20, 21]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 23, "text": "$k=1$", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20, 21]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 24, "text": "for both HOF models in Table 1", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20, 21]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 25, "text": ".", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20, 21]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 26, "text": "We have edited the manuscript to clarify this point.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18, 19, 20, 21]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 27, "text": "The small difference in results between HOF-1 in Table 1 and HOF-1 ($k=1$) in Table 4 as well as HOF-3 in Table 1 and HOF-3 ($k=1$) in Table 4 is a matter of different initialization on a later training run.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20, 21]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 28, "text": "We have updated these tables so that the numbers are computed from the same model, rather than separate training runs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20, 21]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 29, "text": "If we keep the number of compositions fixed while training and test with a larger value of k, we observe that the performance degrades significantly.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20, 21]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 30, "text": "On the other hand, when we use a varying number of compositions (1,2,...,k) at training time, we find that the results do generalize to higher values of k. After several additional compositions (such as k+3, k+4) however, the results start worsening similar to the trend in the fixed k evaluation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20, 21]]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 31, "text": "Finally, we have updated our pdf to reflect the additional related work and stylistic suggestion that you have brought to our attention.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_in-rebuttal", null]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 32, "text": "Thank you again for your feedback, and please let us know if you have any additional questions or concerns.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Syee2fmjYH", "rebuttal_id": "BkxjmrtcjH", "sentence_index": 33, "text": "[1] M. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, and T. Brox, \u201cWhat do single-view 3d reconstruction networks learn?,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3405\u2013 3414, 2019.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 0, "text": "Thank you for the insightful review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 1, "text": "We updated the paper with better results and more tasks.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 2, "text": "We show that our method outperforms the human baseline in terms of training speed and either matches or outperforms the human in terms of final accuracy on all tasks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 3, "text": "While it is true that the human baseline does not require any additional computational resources for training, it does require domain expertise acquired through years of learning, which is arguably even more costly.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 4, "text": "Notably, in all 4 problems where we compare to the human baseline, we believe that human researchers used a similar or higher number of runs as our tuner to design the baseline schedules that we compare against.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 5, "text": "We also updated the paper with more details regarding Transformer and Proximal Policy Optimization.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7]]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 6, "text": "Thank you for mentioning the existing learning curve modeling methods.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 7, "text": "We added an explanation of differences of our method with those works.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8, 9, 11]]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 8, "text": "[1] learn a probabilistic model of one training curve using a handcrafted basis of nonlinear functions of shapes similar to the training curves being modelled.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 11]]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 9, "text": "Our method does not make any assumptions about the shape of the modelled curves and is able to jointly model many training curves - in our experiments, training and validation loss and accuracy.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 11]]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 10, "text": "[2] learn a deterministic model of a learning curve, while our method also models stochasticity, hence providing diverse experience for training a reinforcement learning agent.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 11]]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 11, "text": "Also in contrast to [1] and [2], our method allows the hyperparameters to change over the course of training and models the influence of those changes on the training metrics.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [8, 9, 11]]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 12, "text": "[1] Baker, Bowen, et al. \"Accelerating neural architecture search using performance prediction.\" arXiv preprint arXiv:1705.10823 (2017).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SyeR04XiuS", "rebuttal_id": "rkgeDR_ojB", "sentence_index": 13, "text": "[2] Domhan, Tobias, Jost Tobias Springenberg, and Frank Hutter. \"Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves.\" Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 0, "text": "1. We apologize for typos and if any term is not defined at the appropriate places.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 1, "text": "We  fixed all the typos and define the abbreviation for IPM at the first occurrence.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 2, "text": "Please check the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_global", null]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 3, "text": "P and G, the elements of the divergence D(P||G) that appears in the first paragraph of section 3, is defined in the subsequent two sentences in the same paragraph.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 4, "text": "By the notation G_theta(u) \\sim p(theta), we mean that we want to train the generator G_theta such that when fed a random variable u \\sim p(u), the distribution of G_theta(u) matches that of p(theta).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 5, "text": "Sorry if it is confusing, but G_theta is not parameterized by theta, it just indicates that its the generator for theta. (Like G_x indicates that it is the generator for x).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 6, "text": "2. The training of G_theta is described in the subsection titled \u201cHierarchical Sampling\u201d.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 7, "text": "As correctly pointed out by the reviewer, that G_theta does not appear in the objective function (4).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 8, "text": "Using (4), we train G_x and Q networks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 9, "text": "After training G_x and Q, we use trained Q to collect inferred Q(X), for each point cloud X.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 10, "text": "Then we train the generator G_theta using ordinary WGAN formulation to produce samples from same distribution as that of the samples Q(X) for each point cloud X. In addition to such two step training, a joint training also works, but is slower computationally, thus we report only the two step training in the paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 11, "text": "3. Quantitative evaluation of generative modeling performance is unfortunately very hard for real world problems like point clouds, which is the probable cause for it being missing from much of GAN literature.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [41]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 12, "text": "Thus, to provide some quantitative results for generation, we resorted to the toy problem.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [41]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 13, "text": "In the toy problem, we can accurately gauge the generation capabilities as can be seen from Figure 5.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [41]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 14, "text": "(We did not explicitly provide numbers like KL divergence, as it is evident from the Figure that PC-GAN would be significantly better than AAEs if we evaluate the numbers.) The same protocol can be extended for measuring the quality of the final hierarchical sampling.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [41]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 15, "text": "4. To showcase the effect of varying s, we chose the reasonable sized ModelNet10 dataset and ran for s=0, s=1, and three values s_1<s_2<s_3 in between.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 16, "text": "The results are as follows:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 17, "text": "D2F (Distance to Face)       Coverage", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 18, "text": "s=0                        6.03E+00                     3.36E-01", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 19, "text": "s1", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 20, "text": "6.06E+00                     3.41E-01", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 21, "text": "s2", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 22, "text": "5.77E+00                     3.47E-01", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 23, "text": "s3", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 24, "text": "6.85E+00                     3.56E-01", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 25, "text": "s=1", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 26, "text": "9.19E+00                     3.67E-01", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 44]]}, {"review_id": "SyeS7CQ83m", "rebuttal_id": "S1x_7qgLAX", "sentence_index": 27, "text": "4. Yes the model nicely captures simple topological features of the object, like presence of holes versus being one solid object. Even in the latent space, objects with hole group together.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [46]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 0, "text": "We are afraid that there seems to be some confusion regarding our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 1, "text": "We apologize if this is caused by the lack of clarity in the use of abbreviation \u201cES\u201d (see general response).", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_global", null]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 2, "text": "In the latest revision, \u201cEvolutionary structure search\u201d is abbreviated as \u201cESS\u201d for clarity.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 3, "text": "We emphasize that in the paper, NO \u201cevolutionary strategy\u201d but rather PPO is used to train the policy (see Section 2.1 and 3.2).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 4, "text": "We hope the reviewer can take time to revisit the paper in the light of this inconsistency.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_none", null]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 5, "text": "Also, we now have 5 baselines from previous research and modern variants, which we believe further showcases our contributions.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 6, "text": "Q1: The experiments do not include any strong baseline", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 7, "text": "We added more baselines to further strengthen the significance of our work with respect to the previous approaches.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 8, "text": "The baselines now include (a)\u201cESS-Sims\u201d (Sims, 1994), (Cheney, 2014), (Taylor, 2017), (b) ESS-Sims-AF, (c) ESS-GM-UC, (d) ESS-BodyShare and (5) Random graph search.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 9, "text": "We refer to the details of each baseline in the general response.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 10, "text": "|      NGE         | ESS-Sims  | ESS-Sims-AF  | ESS-GM-UC | ESS-BodyShare |  RGS", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 11, "text": "fish         | **70.21**    |", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 12, "text": "38.32     |", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 13, "text": "51.24         |", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 14, "text": "54.40", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 15, "text": "|", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 16, "text": "54.97         |", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 17, "text": "20.96", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 18, "text": "Walker   |", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 19, "text": "**4157.9** |", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 20, "text": "1804.4   |", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 21, "text": "2486.9        |", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 22, "text": "2458.19   |", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 23, "text": "2185.1        |", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 24, "text": "1777.3", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 25, "text": "The results show that NGE is significantly better than previous approaches and baselines.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 26, "text": "We did an ablation study by sequentially adding each sub-module of NGE separately.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 27, "text": "The table shows that submodules are effective and increase the performance of graph search.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [2]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 28, "text": "Q2: a) Optimizing both the controller and the hardware has been previously studied in the literature.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 29, "text": "Is it worth using a neural graph?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 30, "text": "b) All algorithms should optimize both G and theta for a fair comparison.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 31, "text": "By \u201coptimizing both G and theta\u201d, we meant to indicate that the learned controllers can be transferred to the next generation even if the topologies are changed (instead of throwing away old controllers).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 32, "text": "We note that only NGE among all the baselines has the ability to do that.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 33, "text": "Graph neural network formulation is KEY here, enabling it to perform this efficient policy transfer.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 34, "text": "To the best of our knowledge, the traditional methods require re-optimizing theta from scratch for each different topology, which is computationally demanding and breaks the joint-optimization.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 35, "text": "NGE approximately doubles the performance of previous approach (Sims, 1994) as shown in Q1.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 36, "text": "Please refer to Section 3.1 and Section 3.4 for more details.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3, 4, 5, 6]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 37, "text": "Q3: You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 38, "text": "Again, we apologize for the confusing use of \u201cES\u201d abbreviation.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 39, "text": "Evolutionary strategy is not used in the paper.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [7]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 40, "text": "We invite the reviewer to re-read our paper, since it seems to have led to a major misunderstanding.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [7]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 41, "text": "CMA-ES updates and utilize the covariance matrix of sampling distribution, which is not directly applicable to discrete structure optimization.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [7]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 42, "text": "We believe it will be a valuable future research direction.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [7]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 43, "text": "Q4: Providing the same computational budget seem rather arbitrary and depends on implementation.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [31]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 44, "text": "We are unsure what the reviewer is indicating, and would appreciate the additional clarification.", "coarse": "nonarg", "fine": "rebuttal_followup", "alignment": ["context_sentences", [31]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 45, "text": "In terms of the computational budget for each experiment, we compared different algorithms under different computational budget metrics, more specifically,  \u201cwall-clock time\u201d, \u201cnumber of updates\u201d, and the \u201cfinal converged performance\u201d.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [31]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 46, "text": "NGE performs best among all algorithms.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [31]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 47, "text": "We emphasize the fact that wall-clock time is a more common and realistic metric for comparing the structure search in practice.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [31]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 48, "text": "We agree that computational budget depends on implementation, and the curves in the paper are plotted based on the number of iterations/parameter update, which is independent of the implementation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [31]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 49, "text": "Q5: The writing of the paper", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15, 22]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 50, "text": "We sincerely thank the reviewer for the suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [15, 22]]}, {"review_id": "SygmVm7XpQ", "rebuttal_id": "rkeEvt6xA7", "sentence_index": 51, "text": "We updated the changes in the latest version accordingly.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [15, 22]]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 0, "text": "We thank the referee for their review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 1, "text": "1. We are not training Restricted Boltzmann Machines (RBMs), but Boltzmann machines where the hidden units can be fully connected.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 2, "text": "2. The complete connectivity graph for our Boltzmann machine, as presented in Fig 2, can be interpreted as having two hidden layers.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 3, "text": "The graph has bipartite connectivity between the visible units and the first 128 hidden units and bipartite connectivity between the first 128 hidden units and the second 128 hidden units.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 4, "text": "We thank the referee for bringing the article [V] to our attention and we now have acknowledged the prior work properly in our introduction.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 5, "text": "We agree that it would be very instructive to evaluate the model in [V] for adversarial resistance, but we would argue that this evaluation is beyond the scope of this article.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [4, 5, 6]]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 6, "text": "3. Due to the complexity of the network compared to e.g. LeNet and the higher adversarial resistance the optimisation procedure to find adversarial images takes a long time, making it hard to evaluate 10000 images for all training stages and different attacks.", "coarse": "dispute", "fine": "rebuttal_reject-request", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 7, "text": "We have now evaluated the adversarial resistance throughout the article for 1000 images randomly selected from the 10000 MNIST test images.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 8, "text": "This should avoid placing too much emphasis on the cleaner images in the beginning of the MNIST test set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 9, "text": "Fig.\u00a03 and other evaluations have been updated for the new test set.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9]]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 10, "text": "4. To our knowledge the boundary method is the strongest black box attack.", "coarse": "dispute", "fine": "rebuttal_contradict-assertion", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 11, "text": "The succesful transfer attack on CapsNet is based on transfer of adversarial images from a different model (LeNet).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 12, "text": "We have implemented this attack and added it to our evaluation.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "Syl0HVschm", "rebuttal_id": "BygvqqHKCQ", "sentence_index": 13, "text": "[V] Norouzi, Mohammad Ranjbar, Mani Mori, Greg: Stacks of convolutional Restricted Boltzmann Machines for shift-invariant feature learning. 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2735-2742 (2009).", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 0, "text": "Thank you for your helpful comments!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 1, "text": "1. You are right that Frank-Wolfe would be advantageous over PGD when the constraints are more complicated and adversarial attack may not be such a case.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [2]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 2, "text": "Yet it is also well-known that Frank-Wolfe has quite different optimization behavior compared with PGD even though they have the same order of convergence rate.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 3, "text": "Therefore, it is interesting and important to examine the performance of Frank-Wolfe algorithm for adversarial attack, given the fact that PGD has been shown to be a very effective for adversarial attack.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 4, "text": "In fact, from our work, we found that Frank-Wolfe based methods are generally more efficient than PGD method.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 5, "text": "From another perspective, Frank-Wolfe solves the problem by calling Linear Minimization Oracle (LMO) over the constraint set at each iteration.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 6, "text": "This LMO shares the same intuition as FGSM, which also tries to linearize the neural network loss function to find the adversarial examples.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 7, "text": "In this sense, it is a quite natural attempt to revisit FGSM under the Frank-Wolfe framework.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [2]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 8, "text": "2. We are sorry maybe we didn\u2019t explain it very well in the paper, but this is a misunderstanding.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 9, "text": "We indeed compared our method with generalized I-FGSM/BIM, which is exactly the same as PGD (In [Madry et al.] they also mentioned this in Section 2.1 and they refer it as FGSM^k).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 10, "text": "We decide to just call it PGD in the revision to avoid confusion.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 11, "text": "We hope this remove your concern.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [3, 4]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 12, "text": "3", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 13, "text": ".", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 14, "text": "Indeed, theoretically we can only prove for $\\lambda$ = 1 case.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 15, "text": "Yet we found that larger \\lambda brings us more speedup.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 16, "text": "We have added further empirical evidence (performance comparison of our method with different \\lambda in Figure 1 in the revised paper) to justify it.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 17, "text": "Intuitively speaking, using lambda>1 is essentially a \u201crelax and tighten\u201d step by first relax the constraint to make the problem easier, and then tighten it back to the real constraint.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 18, "text": "The \u201crelax and tighten\u201d idea has been widely used in constrained optimization, and in this paper we adapted this idea into Frank-Wolfe algorithm to make it even faster.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 19, "text": "4. [Lacoste-Julien 2016] considered the general first-order Frank-Wolfe algorithm for nonconvex smooth optimization.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 20, "text": "The result of Theorem 4.3 in our paper is almost the same as the result in (Lacoste-Julien 2016), except that the choices the learning rate in these two papers are different though.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 21, "text": "We have made it clear in the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 22, "text": "5. We have added detailed hyperparameter settings for CW and EAD in the revision in the supplemental materials.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [8, 9, 10]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 23, "text": "6. While Theorem 4.7 is new and may be of independent interest in the optimization community,  it is not the main contribution in this paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 24, "text": "We would like to emphasize that our major contribution in this paper is a Frank-Wolfe based algorithm for adversarial attack, which is more efficient than PGD based adversarial attack algorithm and other baselines.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 25, "text": "7. Sorry about the confusion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 26, "text": "$y$ should be replace by $y_{tar}$. It is a simplified notation we mentioned in the proof in the appendix.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 27, "text": "Thank you for your suggestion and we have revised the notation $f(x,y_{tar})$ to $f(x)$.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [13, 14, 15, 16]]}, {"review_id": "SyllU9Iq37", "rebuttal_id": "ByxEFM-c0Q", "sentence_index": 28, "text": "8. Thank you for pointing out several typos. We have fixed all of them in the revision.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17, 19, 20]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 0, "text": "Thanks for your valuable comments and feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 1, "text": "We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 3, 4, 12]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 2, "text": "New experiments have also been added.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 3, "text": "Please find bellow our answers to your specific remarks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_global", null]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 4, "text": "R: \"In the modeling section, authors use p(I|D) as q^D(.) in the Eq. 12, ...  Beyond this simplification, I am not clear if that is actually intended by the authors.\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 5, "text": "A: You are totally right, eq.10 simplifies to $\\log p(D) \\geq", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 6, "text": "\\mathbb{E}_{I \\sim", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 7, "text": "q^D}", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 8, "text": "\\left[ \\sum\\limits_{i=1}^{|D|-1} \\log p(D_i |D_{<i},I_{<i})  + \\sum\\limits_{v \\not\\in U^D} \\log p(v \\not\\in U^D| D_{\\leq |D|-1}, I)\\right]$. We were aware of this but for simplicity and the conciseness of presentation we chose to not give this final derivation.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 9, "text": "Also, this was because $P(D_i,I_i|D_{<i},I_{<i})$ is much easier to compute (and more efficient) than $P(D_i |D_{<i},I_{<i})$, given that the computation of $P(I_i| D_{\\leq i},I_{<i})$ is in both case required for sampling ($P(D_i,I_i|D_{<i},I_{<i})$ involves a simple product while $P(D_i|D_{<i},I_{<i})$ involves a sum of products).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 10, "text": "But we agree this was not a good choice, since this simplification appears obvious to the reader.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 11, "text": "We hesitated a lot on this, our decision was taken to present things as closely as possible to our implementation (by the way there was a mistake in the algorithm 2 resulting from this hesitation - the implementation for our experiments was correct however).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 12, "text": "We agree that this may appear confusing.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 13, "text": "Particularly since it gives the feeling that parents of infections are not involved anymore in the computation.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 14, "text": "But there are actually, since there remains $P(I_i|D_{<i},I_{<i})$ terms in the expectation.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 15, "text": "The process learns parameters that tend to give high probabilities to the most likely parent vectors regarding infections from the episode.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 16, "text": "In the revision of the paper, we gave the final derivation you suggested, since it is better for comprehension (It also allowed us to discuss more precisely on what is optimized by considering the given gradient update), and we discuss about its implementation in the appendix as an explanation for the algorithm 2 (that only slightly changed to correct the aforementioned mistake).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [5, 6, 7, 8]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 17, "text": "Thanks for this remark that helped us to much improve the paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 18, "text": "R: \"The authors explain how they trained their own model but there is no mention on how they trained benchmark models\"", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 19, "text": "A: You are totally right, it is missing.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 20, "text": "Baseline models are trained on the same training set as our model following the methods proposed in their original paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 21, "text": "Our model and the baselines were tuned by a grid search process on a validation set for each dataset (whose size is given in the description of the datasets),", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 22, "text": "although the best hyper-parameters obtained for Arti1 remained near optimal for the other ones.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 23, "text": "For every model with an embedding space (i.e., all except CTIC), we set its dimension to $d=50$ (larger dimensions induce a more difficult convergence of the learning without significant gain in accuracy).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "SylvLhvph7", "rebuttal_id": "Skx3-KvOC7", "sentence_index": 24, "text": "We added this explanation in the new version of the paper.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [10, 11]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 0, "text": "We very much appreciate your valuable comments, efforts and times on our paper.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 1, "text": "Our responses for all your questions are provided below. Our major revisions in the new draft are colored by red.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 2, "text": "Q1. Comparison with [1, 2, 3, 4].", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6, 7, 8, 9, 10]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 3, "text": "The main difference between our method and [1, 2] is that we do not directly train the Gaussian mixture model, i.e., generative classifier but we post-process it on hidden feature spaces of pre-trained deep models.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 4, "text": "In addition, we study a robust inference method to handle noisy labels in training samples, while they did not.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 5, "text": "Next, [3,4] also assume clean training labels, and aim for detecting abnormal test samples after \u2019clean\u2019 training.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 6, "text": "Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [6, 7, 8, 9, 10]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 7, "text": "We clarified this in Section 2.1 of the revised draft.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [6, 7, 8, 9, 10]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 8, "text": "Q2. Computational cost.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 9, "text": "As you expect, estimating the parameters of LDA is very cheap compared to training original deep models like ResNet and DenseNet, since it requires only one forward pass to extract the hidden features.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 10, "text": "Q3. Version of backward/forward losses.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 11, "text": "As mentioned in Appendix B of the previous draft, we use the estimated noise transition matrices for backward/forward losses.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 12, "text": "We clarified more details of experimental setups in Appendix B of the revised draft.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 13, "text": "Q4. Updated abstract and performance evaluation.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 18, 19, 20, 21, 22, 23, 24, 25, 26]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 14, "text": "As AnonReviewer 3 mentioned, our main contribution is developing a new inference method which can be used under any pre-trained deep model.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 18, 19, 20, 21, 22, 23, 24, 25, 26]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 15, "text": "In other words, our goal is not outperforming the performance of prior training methods and complementary to them, i.e., our inference method can improve the performance of any prior training methods (see our common response to all reviewers).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [17, 18, 19, 20, 21, 22, 23, 24, 25, 26]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 16, "text": "Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [17, 18, 19, 20, 21, 22, 23, 24, 25, 26]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 17, "text": "In the abstract of the revised draft, we report our improvement over Co-teaching [5] which is the most recent and state-of-the-art training method.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [17, 18, 19, 20, 21, 22, 23, 24, 25, 26]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 18, "text": "Q5. Evaluation on adversarial attacks.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [27, 28, 29, 30]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 19, "text": "In the revised draft, we also consider optimization-based adaptive attacks against our method under the black-box setup (see Table 5) and the white-box setup (see Table 10).", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [27, 28, 29, 30]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 20, "text": "In both setups, our inference method is shown to be more robust compared to the softmax inference.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [27, 28, 29, 30]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 21, "text": "We further show that our method further improves the robustness of deep models optimized by adversarial training (see Table 6 and 11).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [27, 28, 29, 30]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 22, "text": "Such experimental results support our claim that the proposed generative classifier can improve the robustness against adversarial attacks as it utilizes multiple hidden features (i.e., harder to attack all of them).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [27, 28, 29, 30]]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 23, "text": "We very much appreciate your valuable comments again.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 24, "text": "[1] Wen, Y., Zhang, K., Li, Z. and Qiao, Y., A discriminative feature learning approach for deep face recognition. In ECCV, 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 25, "text": "[2] Wan, W., Zhong, Y., Li, T. and Chen, J., Rethinking feature distribution for loss functions in image classification. In CVPR, 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 26, "text": "[3] Lee, K., Lee, K., Lee, H. and Shin, J., A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks. In NIPS, 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 27, "text": "[4] Ma, X., Li, B., Wang, Y., Erfani, S.M., Wijewickrema, S., Houle, M.E., Schoenebeck, G., Song, D. and Bailey, J. Characterizing adversarial subspaces using local intrinsic dimensionality. In ICLR, 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 28, "text": "[5] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: robust training deep neural networks with extremely noisy labels. In NIPS, 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_in-rebuttal", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 29, "text": "Thanks a lot,", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 30, "text": "Authors", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 31, "text": "Dear AnonReviewer2,", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 32, "text": "We hope that you found our rebuttal/revision for you and other reviewers in common.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 33, "text": "If you have any remaining questions/concerns, please do not hesitate to let us know and we would be happy to answer.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 34, "text": "Thank you very much,", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyxhCjde3Q", "rebuttal_id": "Skx-bI5HCX", "sentence_index": 35, "text": "Authors", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 0, "text": "We thank the reviewer for the thoughtful review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 1, "text": "The reviewer points out that CNE works well for link prediction and visualization.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 2, "text": "We wish to point out that our experiments indicate CNE consistently outperforms the state-of-the-art not only for these tasks but also for multi-label classification.", "coarse": "concur", "fine": "rebuttal_accept-praise", "alignment": ["context_sentences", [4]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 3, "text": "Our responses:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 4, "text": "a) Variational inference is useful in particular when the partition function is hard to compute, which is not the case here.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 5, "text": "So we believe it would be overkill in this case.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 6, "text": "In any case, it is not needed to achieve the performances CNE achieves at a very modest computational complexity and fast practical runtimes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 7, "text": "b) No hand-engineering is needed to use CNE even when using more informative priors modeling node degrees and block structure.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 8, "text": "The degree of each node can simply be computed on the training set, so no hand-engineering is needed (just the choice to include it or not -- and it always better to include it).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 9, "text": "The block structure, on the other hand, will often be part of the data specification or meta-data of the nodes.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 10, "text": "For example, the network may be a multi-partite network representing a relational database, or it may be a company social network where the nodes are employees, and generic job titles are known for each of the employees (as attributes of the nodes).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 11, "text": "The entity types in the first example, and the job titles in the second example, would then define the blocks, and the density of the parts of the adjacency matrix between any two such blocks can again easily be computed on the network.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 12, "text": "Our method imposes no constraints on such blocks (e.g. they may even be partially overlapping).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 13, "text": "Again, all that is needed is choosing whether to use a block prior for any specified attribute (in the two examples: entity type, and node attribute).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 14, "text": "Again, empirically, including it always appears to be better, so one could even avoid having to make the choice.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 15, "text": "Inferring structural properties of the graph to be used in the prior (e.g. using GraphRNN), as we understand the reviewer suggests, certainly sounds potentially interesting.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 16, "text": "However, while it may improve accuracy, we do not believe that it adds value in reducing the amount of hand-engineering needed, as the amount of hand-engineering needed is very minimal already.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 17, "text": "The fact that CNE could be combined with such inferred structural properties increases its potential impact though, and this remark of the reviewer further underscores the need for methods such as CNE that can take such structural information into account.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "SyxO5eTc3X", "rebuttal_id": "SkxlQv-m0X", "sentence_index": 18, "text": "The boost in accuracy achieved by CNE, using a model that is arguably also a lot simpler than the state-of-the-art network embedding approaches, is thus achieved without any increased need for hand-engineering.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 0, "text": "Thank you very much for the constructive comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 1, "text": "We tried to strengthen our claims by adding more experimental data which the Reviewer requested.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 2, "text": "1. The proposed \"Multi-bit-quantization + Viterbi-based binary code encoding\" requires slightly larger memory footprint than \"Multi-bit quantization only ([4])\" because some of the Viterbi encoded bits have different indices from their corresponding quantization bits.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 3, "text": "Hence, the \"Multi-bit quantization only\" requires 10 % to 20 % smaller memory footprint than \"Multi-bit-quantization + Viterbi-based binary code encoding\" case.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 4, "text": "However, the main reason why we apply the Viterbi weight encoding is that parallel sparse-to-dense matrix conversion can be done by applying same Viterbi encoding process to the non-zero values and indices of the non-zero values in parallel.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 5, "text": "This parallel sparse-to-dense conversion makes the speed of feeding parameters to PEs 10 % to 40 % faster compared to [1] (Figure 6c).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 6, "text": "2. Per Reviewer\u2019s suggestion, the experimental results for the effectiveness of \"Don\u2019t Care\" term have been moved to Section 4.1.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [11]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 7, "text": "3. Per Reviewer's suggestion, we measured accuracy differences before and after Viterbi encoding for several quantization methods such as linear quantization ([2]), logarithmic quantization ([3]), and alternating quantization ([4]) methods with the same quantization bits (3-bit).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 13, 15]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 8, "text": "The result shows that combination with alternating quantization and Viterbi weight encoding had only 2 % validation accuracy degradation after the Viterbi encoding was applied first right after the quantization and the accuracy was easily recovered with retraining.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 13, 15]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 9, "text": "On the other hand, the combination with the other quantization methods and Viterbi weight encoding showed accuracy degradation as much as 71 %, which was too large to recover the accuracy with retraining.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 13, 15]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 10, "text": "The accuracy difference mainly results from the uneven weight distribution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 13, 15]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 11, "text": "Because weights of neural networks usually are normally distributed, the composition ratio of '0' and '1' is not equal when the linear or logarithmic quantization is applied to the weights of neural networks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 13, 15]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 12, "text": "As we stated in the manuscript, Viterbi encoder tends to produce similar number of '0' and '1'.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 13, 15]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 13, "text": "Therefore, we can conclude that under the same bit condition, alternating quantization method shows best accuracy and compatibility with our bit-by-bit Viterbi encoding scheme regardless of the type of neural networks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13, 13, 15]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 14, "text": "4. We conducted additional simulations to compare sparse matrix reconstruction speed of [1] and the proposed method.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [16]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 15, "text": "We used a random 512-by-512 size matrix with various pruning rate ranging from 75 % to 95 %.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 16, "text": "We conducted the simulations under the assumptions described in Figure 6c.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 17, "text": "The simulation results are shown in Figure 6c in updated manuscript.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 18, "text": "We could observe that the proposed method could feed 10 % to 40 % more nonzero weights and input activations to PEs in same 10000 cycles compared to [1].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 19, "text": "Proposed method could also feed parameters to PEs 20 % to 106 % faster compared to baseline method, which reads dense weight and activation matrices directly from DRAM.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 20, "text": "The improvement in the proposed scheme mainly comes from the parallelized process of assigning non-zero values to their corresponding indices in the weight matrix.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 21, "text": "While preparing addition data for the rebuttal, we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 22, "text": "After further optimization, we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 23, "text": "Therefore, we updated Figure 7 in original manuscript to Figure 6c in updated manuscript according to the new data.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 24, "text": "5. We added the change of the exact weight representation at each process in Figure 1 to clarify the flowchart.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [18, 19]]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 25, "text": "Reference", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_none", null]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 26, "text": "[1] Dongsoo Lee, Daehyun Ahn, Taesu Kim, Pierce I. Chuang, and Jae-Joon Kim. Viterbi-based pruning for sparse matrix with fixed and high index compression ratio. International Conference on Learning Representations (ICLR), 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_none", null]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 27, "text": "[2] Darryl D. Lin, Sachin S. Talathi, and V. Sreekanth Annapureddy. Fixed point quantization of deep convolutional networks.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_none", null]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 28, "text": "In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML\u201916, pp. 2849\u20132858. 2016.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_none", null]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 29, "text": "[3] Daisuke Miyashita, Edward H. Lee, and Boris Murmann. Convolutional Neural Networks using Logarithmic Data Representation. CoRR, abs/1603.01025, 2016. URL https://arxiv.org/abs/1603.01025.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_none", null]}, {"review_id": "SyxPPFpDhm", "rebuttal_id": "SJxecAdFRX", "sentence_index": 30, "text": "[4] Chen Xu, Jianqiang Yao, Zouchen Lin, Wenwu Qu, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. International Conference on Learning Representations (ICLR), 2018.", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_none", null]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 0, "text": "1- The idea of using concepts to represent a problem is simple, but using it along with neural network based embedding gives us the opportunity to gain concept continuity as discussed on the last paragraph on page 7 and table 2, which is an active field of research in education.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 1, "text": "The focus of this work is on problem embedding and its application in a recommendation system that uses problem embedding to project students\u2019 performance for the problems they solved onto the problems that they have not solved yet.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 2, "text": "Using the evaluation on unseen problems, a problem is recommended that is within the capacity of students close to their boundary to help them learn, and at the same time we cover all the concepts necessary for them to learn.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 3, "text": "In the meanwhile, we got the interesting idea of negative pre-training on training with imbalanced training data and tested our hypothesis and included in the paper.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 4, "text": "Due to space limit, we did not include the literature review and comparison of other methods in terms of memory use and training complexity, but you can find them in the response of a previous comment below titled \u201cResponse to Question on Negative Pre-Training\u201d on this page to see the comparison.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 5, "text": "We can include the literature review for training on imbalanced data sets as well as comparison of other methods with negative pre-training in terms of memory use and training complexity in the final version.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 6, "text": "In summary, a) oversampling extremely suffers from over-fitting, b) SMOTE method that generates synthetic data sample is not feasible in word space, so the generated synthetic data (that are mathematical problems) are not of use for our training purpose, c) borderline-SMOTE both suffers from the same issue as SMOTE and its high complexity for finding the pairwise distance between all data samples, which is a burden in high dimensional data, and d) hybrid methods need m >> 1 weak learners in contrast to negative pre-training that uses a single learner.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 7, "text": "Memory use and training time is an issue for hybrid method when the weak learners are deep neural networks with too many parameters.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 8, "text": "We are currently running a broader experiment for negative pre-training on other data sets to gain more insight on it, but for the purpose of the task proposed in this work, it outperforms one-shot learning, which cannot be said that is the state-of-the art, but is a common practice. There is no notion of state-of-the-art in training on imbalanced data sets since due to our best knowledge, there is no method that outperforms all the other ones, and the performance of different methods depends more on the nature of the data set.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [9, 10, 11, 12, 13]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 9, "text": "2- The data set being small", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 10, "text": "is the nature of the application since creating mathematical problems is a creative process, so it is hard to have a very big data set.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 11, "text": "The Prob2Vec method is performing well on this not relatively big data set, which is our goal, but if we have a bigger data set (as we have right now with more than 2400 problems), Prob2Vec may even have a better performance since with more data we can have a more precise concept and problem embedding.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14, 15]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 12, "text": "3- Thanks for your suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [16, 17, 18]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 13, "text": "4- It is difficult for humans to determine a similarity score consistent across a large enough training set, so it is not feasible to simply apply supervised methods to learn a similarity score for problems.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19]]}, {"review_id": "Syxz3gT_2Q", "rebuttal_id": "B1l3_3TepQ", "sentence_index": 14, "text": "Even if problem-problem similarity annotation is feasible, a lot of effort should go into the annotation, which is not scalable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19]]}]}